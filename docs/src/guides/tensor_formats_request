Okay, here we go. I'm going to describe some docstrings and the properties of several of my level formats for the finch compiler, and I'd like you to summarize the main properties in a Markdown table. Please normalize the way the information is presented. Ask me for clarification if needed.

"""
    DenseLevel{[Ti=Int]}(lvl, [dim])

A subfiber of a dense level is an array which stores every slice `A[:, ..., :,
i]` as a distinct subfiber in `lvl`. Optionally, `dim` is the size of the last
dimension. `Ti` is the type of the indices used to index the level.

```jldoctest
julia> ndims(Tensor(Dense(Element(0.0))))
1

julia> ndims(Tensor(Dense(Dense(Element(0.0)))))
2

julia> Tensor(Dense(Dense(Element(0.0))), [1 2; 3 4])
Dense [:,1:2]
├─[:,1]: Dense [1:2]
│ ├─[1]: 1.0
│ ├─[2]: 3.0
├─[:,2]: Dense [1:2]
│ ├─[1]: 2.0
│ ├─[2]: 4.0
```
"""

"""
    ElementLevel{D, [Tv=typeof(D), Tp=Int, Val]}()

A subfiber of an element level is a scalar of type `Tv`, initialized to `D`. `D`
may optionally be given as the first argument.

The data is stored in a vector
of type `Val` with `eltype(Val) = Tv`. The type `Ti` is the index type used to
access Val.

```jldoctest
julia> Tensor(Dense(Element(0.0)), [1, 2, 3])
Dense [1:3]
├─[1]: 1.0
├─[2]: 2.0
├─[3]: 3.0
```
"""

"""
    PatternLevel{[Tp]}()

A subfiber of a pattern level is the Boolean value true, but it's `default` is
false. PatternLevels are used to create tensors that represent which values
are stored by other fibers. See [`pattern`](@ref) for usage examples.

```jldoctest
julia> Tensor(Dense(Pattern()), 3)
Dense [1:3]
├─[1]: true
├─[2]: true
├─[3]: true
```
"""

"""
    RepeatRLELevel{[D], [Ti=Int], [Tp=Int], [Tv=typeof(D)]}([dim])

A subfiber of a repeat level is a vector that only stores contiguous repeated
values once. The RepeatRLELevel records locations of repeats using a sorted
list. Optionally, `dim` is the size of the vectors.

The fibers have type `Tv`, initialized to `D`. `D` may optionally be given as
the first argument.  `Ti` is the type of the last tensor index, and `Tp` is the
type used for positions in the level.

```jldoctest
julia> Tensor(RepeatRLE(0.0), [11, 11, 22, 22, 00, 00, 00, 33, 33])
RepeatRLE (0.0) [1:9]
├─[1:2]: 11.0
├─[3:4]: 22.0
├─[5:7]: 0.0
├─[8:9]: 33.0
├─[10:9]: 0.0

```
"""

"""
    SparseByteMapLevel{[Ti=Tuple{Int...}], [Tp=Int], [Ptr] [Tbl]}(lvl, [dims])

Like the [`SparseListLevel`](@ref), but a dense bitmap is used to encode
which slices are stored. This allows the ByteMap level to support random access.

`Ti` is the type of the last tensor index, and `Tp` is the type used for
positions in the level. 

```jldoctest
julia> Tensor(Dense(SparseByteMap(Element(0.0))), [10 0 20; 30 0 0; 0 0 40])
Dense [:,1:3]
├─[:,1]: SparseByteMap (0.0) [1:3]
│ ├─[1]: 10.0
│ ├─[2]: 30.0
├─[:,2]: SparseByteMap (0.0) [1:3]
├─[:,3]: SparseByteMap (0.0) [1:3]
│ ├─[1]: 20.0
│ ├─[3]: 40.0

julia> Tensor(SparseByteMap(SparseByteMap(Element(0.0))), [10 0 20; 30 0 0; 0 0 40])
SparseByteMap (0.0) [:,1:3]
├─[:,1]: SparseByteMap (0.0) [1:3]
│ ├─[1]: 10.0
│ ├─[2]: 30.0
├─[:,3]: SparseByteMap (0.0) [1:3]
│ ├─[1]: 20.0
│ ├─[3]: 40.0
```
"""

"""
    SparseCOOLevel{[N], [TI=Tuple{Int...}], [Ptr], [Tbl]}(lvl, [dims])

A subfiber of a sparse level does not need to represent slices which are
entirely [`default`](@ref). Instead, only potentially non-default slices are
stored as subfibers in `lvl`. The sparse coo level corresponds to `N` indices in
the subfiber, so fibers in the sublevel are the slices `A[:, ..., :, i_1, ...,
i_n]`.  A set of `N` lists (one for each index) are used to record which slices
are stored. The coordinates (sets of `N` indices) are sorted in column major
order.  Optionally, `dims` are the sizes of the last dimensions.

`TI` is the type of the last `N` tensor indices, and `Tp` is the type used for
positions in the level.

The type `Tbl` is an NTuple type where each entry k is a subtype `AbstractVector{TI[k]}`.

The type `Ptr` is the type for the pointer array.

```jldoctest
julia> Tensor(Dense(SparseCOO{1}(Element(0.0))), [10 0 20; 30 0 0; 0 0 40])
Dense [:,1:3]
├─[:,1]: SparseCOO (0.0) [1:3]
│ ├─[1]: 10.0
│ ├─[2]: 30.0
├─[:,2]: SparseCOO (0.0) [1:3]
├─[:,3]: SparseCOO (0.0) [1:3]
│ ├─[1]: 20.0
│ ├─[3]: 40.0

julia> Tensor(SparseCOO{2}(Element(0.0)), [10 0 20; 30 0 0; 0 0 40])
SparseCOO (0.0) [1:3,1:3]
├─├─[1, 1]: 10.0
├─├─[2, 1]: 30.0
├─├─[1, 3]: 20.0
├─├─[3, 3]: 40.0
```
"""

"""
    SparseHashLevel{[N], [TI=Tuple{Int...}], [Ptr], [Tbl], [Srt]}(lvl, [dims])

A subfiber of a sparse level does not need to represent slices which are
entirely [`default`](@ref). Instead, only potentially non-default slices are
stored as subfibers in `lvl`. The sparse hash level corresponds to `N` indices
in the subfiber, so fibers in the sublevel are the slices `A[:, ..., :, i_1,
..., i_n]`.  A hash table is used to record which slices are stored. Optionally,
`dims` are the sizes of the last dimensions.

`TI` is the type of the last `N` tensor indices, and `Tp` is the type used for
positions in the level. `Tbl` is the type of the dictionary used to do hashing,
`Ptr` stores the positions of subfibers, and `Srt` stores the sorted key/value
pairs in the hash table.

```jldoctest
julia> Tensor(Dense(SparseHash{1}(Element(0.0))), [10 0 20; 30 0 0; 0 0 40])
Dense [:,1:3]
├─[:,1]: SparseHash (0.0) [1:3]
│ ├─[1]: 10.0
│ ├─[2]: 30.0
├─[:,2]: SparseHash (0.0) [1:3]
├─[:,3]: SparseHash (0.0) [1:3]
│ ├─[1]: 20.0
│ ├─[3]: 40.0

julia> Tensor(SparseHash{2}(Element(0.0)), [10 0 20; 30 0 0; 0 0 40])
SparseHash (0.0) [1:3,1:3]
├─├─[1, 1]: 10.0
├─├─[2, 1]: 30.0
├─├─[1, 3]: 20.0
├─├─[3, 3]: 40.0
```
"""

"""
    SparseListLevel{[Ti=Int], [Tp=Int], [Ptr=Vector{Tp}], [Idx=Vector{Ti}]}(lvl, [dim])

A subfiber of a sparse level does not need to represent slices `A[:, ..., :, i]`
which are entirely [`default`](@ref). Instead, only potentially non-default
slices are stored as subfibers in `lvl`.  A sorted list is used to record which
slices are stored. Optionally, `dim` is the size of the last dimension.

`Ti` is the type of the last tensor index, and `Tp` is the type used for
positions in the level. The types `Ptr` and `Idx` are the types of the
arrays used to store positions and indicies. 

```jldoctest
julia> Tensor(Dense(SparseList(Element(0.0))), [10 0 20; 30 0 0; 0 0 40])
Dense [:,1:3]
├─[:,1]: SparseList (0.0) [1:3]
│ ├─[1]: 10.0
│ ├─[2]: 30.0
├─[:,2]: SparseList (0.0) [1:3]
├─[:,3]: SparseList (0.0) [1:3]
│ ├─[1]: 20.0
│ ├─[3]: 40.0

julia> Tensor(SparseList(SparseList(Element(0.0))), [10 0 20; 30 0 0; 0 0 40])
SparseList (0.0) [:,1:3]
├─[:,1]: SparseList (0.0) [1:3]
│ ├─[1]: 10.0
│ ├─[2]: 30.0
├─[:,3]: SparseList (0.0) [1:3]
│ ├─[1]: 20.0
│ ├─[3]: 40.0

```
"""

"""
SparseVBLLevel{[Ti=Int], [Tp=Int], [Ptr=Vector{Tp}], [Idx=Vector{Ti}], [Ofs=Vector{Ofs}]}(lvl, [dim])

Like the [`SparseListLevel`](@ref), but contiguous subfibers are stored together in blocks.

```jldoctest
julia> Tensor(Dense(SparseVBL(Element(0.0))), [10 0 20; 30 0 0; 0 0 40])
Dense [:,1:3]
├─[:,1]: SparseList (0.0) [1:3]
│ ├─[1]: 10.0
│ ├─[2]: 30.0
├─[:,2]: SparseList (0.0) [1:3]
├─[:,3]: SparseList (0.0) [1:3]
│ ├─[1]: 20.0
│ ├─[3]: 40.0

julia> Tensor(SparseVBL(SparseVBL(Element(0.0))), [10 0 20; 30 0 0; 0 0 40])
SparseList (0.0) [:,1:3]
├─[:,1]: SparseList (0.0) [1:3]
│ ├─[1]: 10.0
│ ├─[2]: 30.0
├─[:,3]: SparseList (0.0) [1:3]
│ ├─[1]: 20.0
│ ├─[3]: 40.0
"""

| Level   | Data            | Column-Major | Random     | Column-Major | Random      | Random   | Usage       |
| Format  | Characteristic  | Reads        | Reads      | Bulk Update  | Bulk Update | Updates  | Description |


I'm going to describe some properties of several of my level formats for the
finch compiler, and I'd like you to summarize them in a Markdown table. The
columns should be as follows:
| Level Format               | Data Characteristic          | Column-Major Reads | Random Reads | Column-Major Bulk Update  | Random Bulk Update | Random Updates | Usage Description |
Data characteristic is Sparse, Dense, Sparse Run-Length, Run-Length, or Sparse Blocks.
Usage Description is a helpful description of when to use each.

Heres a table and some descriptions. Please provide descriptions of the columns of the table

Finch supports a variety of storage formats for each level of the tensor tree,
each with advantages and disadvantages. Some storage formats support in-order access, while others support random access. Some storage formats must be written to in column-major order, while others support out-of-order writes.

| Level Format         | Group    | Data Characteristic   | Column-Major Reads | Random Reads | Column-Major Bulk Update | Random Bulk Update | Random Updates | Status | Usage Description |
|----------------------|----------|-----------------------|:------------------:|:------------:|:------------------------:|:------------------:|:--------------:|:-----:|-------------------|
| Dense                | Core     | Dense                 | ✅                | ✅          | ✅                      | ✅                | ✅            | ✅    | Stores every subtensor. |
| SparseTree           | Core     | Sparse                | ✅                | ✅          | ✅                      | ❌                | ✅            | ⚙️    | Suitable for levels with few nonzeros. |
| SparseRunTree        | Core     | Sparse Run-Length     | ✅                | ✅          | ✅                      | ❌                | ✅            | ⚙️    | Suitable for levels with runs of repeated values. |
| Element              | Core     | Dense                 | ✅                | ✅          | ✅                      | ✅                | ✅            | ✅    | Leaf level for storing tensor elements. |
| Pattern              | Core     | Dense                 | ✅                | ✅          | ✅                      | ✅                | ✅            | ✅    | Leaf level true if stored, false otherwise. |
| SparseList           | Advanced | Sparse                | ✅                | ❌          | ✅                      | ❌                | ❌            | ✅    | Efficient for sparse data. |
| SparseBytemap        | Advanced | Sparse                | ✅                | ✅          | ✅                      | ❌                | ❌            | ✅    | Efficient for sparse temporary data in a loop. Stores as much as dense does. |
| SparseRunList        | Advanced | Sparse Run-Length     | ✅                | ❌          | ✅                      | ❌                | ❌            | ✅    | Efficient for runs with zero annihilation.|
| RepeatedList         | Advanced | Run-Length            | ✅                | ❌          | ✅                      | ❌                | ❌            | ✅    | Efficient for runs, but no zero annihilation. |
| SparseRunTree        | Advanced | Sparse Run-Length     | ✅                | ✅          | ✅                      | ❌                | ✅            | ⚙️    | Suitable for levels with runs of repeated values. |
| Element              | Advanced | Dense                 | ✅                | ✅          | ✅                      | ✅                | ✅            | ✅    | Leaf level for storing tensor elements. |
| SparseVBL            | Advanced | Sparse Blocks         | ✅                | ❌          | ✅                      | ❌                | ❌            | ✅    | Efficient for sparse data with blocks of nonzeros. |
| SingleSparsePinpoint | Advanced | Sparse                | ✅                | ✅          | ✅                      | ❌                | ❌            | ✅    | Stores a single nonzero; useful with a parent level to represent IDs. |
| SingleSparseRun      | Advanced | Sparse Run-Length     | ✅                | ✅          | ✅                      | ❌                | ❌            | ✅    | Stores a single run of a repeated nonzero value; useful with a parent level to represent IDs. |
| SingleBlock          | Advanced | Dense                 | ✅                | ✅          | ✅                      | ❌                | ❌            | ✅    | Stores a run of contiguous nonzeros; Suitable for representing ragged, banded, or triangular patterns. |
| SparseCOO            | Legacy   | Sparse                | ✅                | ✅          | ✅                      | ❌                | ✅            | ✅️    | Legacy format; not recommended except for COO format interfacing. |
| SparseHash           | Legacy   | Sparse                | ✅                | ✅          | ✅                      | ✅                | ✅            | 🕸️    | Legacy format; not recommended except for Hash format interfacing. |

the Dense level supports efficient in-order and random access. It may be written to in any order, and supports updates. It is a good choice for a level where we expect each subtensor to be distinct and nonzero. Updating a dense level is free.

THe SparseList level supports efficient in-order access, but random access is inefficient. SparseListLevels must be declared to be zero before use. It must be written to in column-major order. no updates It is a good choice for a level that is mostly zero, but has a few nonzeros. It is the main ingredient in a CSC sparse matrix. 

THe RepeatedList level supports efficient in-order access, but random access is inefficient. RepeatedListLevels must be declared to be zero before use. They Store all of their values and do not have an implicit zero, so you won't get zero annihilation out of the repeatedLIstLevel. It's more like a run-length-compressed dense level. It must be written to in column major order

THe SparseRunList level supports efficient in-order access, and is capable of representing runs of identical nonzeros with the same subtensor. Column major writes. does have a zero value. good choice for a representing intervals of nonzero values.

THe SparseVBL level supports efficient in-order access, and is capable of representing blocks of different nonzeros with the same index, improving access time. Column major writes. no updates. does have a zero value. good choice for a representing intervals of nonzero values.

THe SingleSparseRun level supports efficient in-order and random access, and is capable of representing a single run of identical nonzeros with a single subtensor. Column major writes. does have a zero value. good choice for a representing a single nonzero values. ALso useful if you have an "ID" level above it, and you want to store a single interval for each ID.

THe SingleSparsePinpoint level supports efficient in-order and random access, and is capable of representing a single nonzero with a single subtensor. Column major writes. does have a zero value. good choice for a representing a single nonzero values. ALso useful if you have an "ID" level above it, and you want to store a single valuefor each ID.

THe SingleBlock Level supports efficient in-order and random access, and is capable of representing a single nonzero with a single subtensor. Column major writes. does have a zero value. good choice for a representing the rows of a banded or triangular matrix.

The SparseCOO Level is a legacy format that represents a sparse matrix in coordinate format. It is not recommended for use except for interfacing with COO format. This format represents more than one dimension at once. efficient in-order and random access,Column major writes. does have a zero value.Any updates are O(nnz) 

THe SparseHash Level is a legacy format the represents a sparse tensor in dictionary-of-keys format.  It is not recommended for use except for interfacing with the Hash format. This format represents more than one dimension at once. efficient in-order and random access, random writes are allowed. does have a zero value. Any updates trigger an O(nnz) sort step.

THe SparseTree Level supports efficient in-order and random access. THey may be written out of order. Updates are O(log(n)). It is a good choice for a level that is mostly zero, but has a few nonzeros. Use this level as a temporary to construct sparse or block-sparse levels.

THe SparseRunTree Level supports efficient in-order and random access. THey may be written out of order. Updates are O(log(n)). It is a good choice for a level that is mostly zero, but has a few nonzero runs. Use this level as a temporary to construct run-length-compressed levels.

The Element Level is a leaf level that stores elements of a tensor. May be written or read in any order.
The Pattern Level is a leaf level that stores boolean elements of a tensor, true if stored and false otherwise. May be written or read in any order.


