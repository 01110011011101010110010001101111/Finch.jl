julia> @finch begin
        CR .= 0
        for i = _
            for j = _
                for k = _
                    CR[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(CR = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.2504314291953376, 0.0, 0.0, 0.9648065476989627, 0.2835978542736629, 0.13211849643271117, 0.2796391765298554, 0.22540993882752317, 0.5512218518807817, 0.5478145335437775, 0.0, 0.2291959647754706, 0.23520231615614767, 0.03296826745685447, 0.18421763750874248, 0.6035878434468992, 0.7441004608135994, 0.2535097758200323, 0.20236453274735783, 0.09228241774990334, 0.13384560784694935, 0.5269755678736615, 0.03880627317810495, 0.17704264007357723, 0.0, 0.2632287465084173, 0.0, 0.02720231717357045, 0.5246531850072871, 0.0, 0.0, 0.3360269038793386, 0.0, 0.22538882742471622, 0.22621665568148489, 0.0, 0.09095677592426996, 0.0, 0.2343687318872728, 0.0, 0.00044179348092010485, 0.07281055209847118, 0.6821285312867097, 0.0, 0.0, 0.0, 0.8944403740767686, 0.11973372388784431, 0.2613237068921269, 0.5923534538840982, 0.0, 0.0, 0.0, 0.8426453838331012, 0.0, 0.0, 1.0769929937540788, 0.43300907588983417, 0.0, 0.6905133736710757, 0.0, 0.0, 0.0, 0.07271653050953786, 0.8664857754450395, 0.08570530865158534, 0.0, 0.7169860381548754, 0.20498067747201715, 0.0, 0.23050103487101342, 0.0, 0.0, 0.9152746488430813, 0.14299878494120105, 0.44815460009097313, 0.0, 0.0, 0.0, 0.23162241551000032, 1.0326337985335274, 1.0530846396126474, 0.22109306040172927, 0.0, 0.0, 0.0, 1.0289491599002607, 0.008076930682810486, 0.037079469071792215, 0.0, 0.03308196940629956, 0.07165705922647192, 0.18447863887953236, 0.0, 0.24885144181470292, 0.0, 0.0, 0.0, 0.7193963651558962, 0.0, 0.6007574484421223, 0.0, 0.11361690641306497, 0.0, 0.0, 0.0, 0.0, 0.5189539173895105, 0.027360695267410504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.769945776489901, 0.00013219515459577855, 0.0, 0.0, 0.0, 0.0, 0.21518747264967641, 0.0, 0.0, 0.0, 0.10980938651098104, 0.07347552139071808, 0.0, 0.3613462418752601, 0.04465135919773286, 0.027102240057490925, 0.0, 0.0, 0.2594566815123038, 0.417973871558788, 0.02337304044012675, 0.5498222432776159, 0.011558313103301096, 0.13519961718643905, 0.0830809338177889, 0.0, 0.0, 0.0, 0.17989871031119878, 0.05233157238393502, 0.0, 0.051508809526025434, 0.225133841185459, 0.18950167625902217, 0.0, 0.0, 0.0, 0.20173577654712102, 0.7662693488070871, 0.0, 0.33259178463685746, 0.13865467767460202, 0.0, 0.8944580603877651, 0.0, 0.060682785060590494, 0.06051599339484606, 0.0, 0.0, 0.0, 0.12305720211410776, 0.10081888448105404, 0.0, 0.0, 0.2522377522294325, 0.0, 0.12103967793114603, 0.06077105223878225, 0.0, 0.0, 0.5586258592154407, 0.0, 0.0, 0.0, 0.2984848816909581, 0.029564782247173068, 0.6276543733906952, 0.0, 0.07353625635562432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08001934416956356, 0.13808411155288577, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0013608908425346389, 0.15477792583676012, 0.8924302032481646, 0.0, 0.13515079773758784, 0.0, 0.059793517595463114, 0.0, 0.04933055693313389, 0.0, 0.0, 0.3342781850907459, 0.02578025634698341, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1229501707927792, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287208489034296, 0.0, 0.0, 0.28149278769379354, 0.0, 0.0, 0.0, 0.031923890851782225, 0.23740383631799186, 0.0, 0.0, 0.0, 0.0, 0.06362127081034188, 0.2629034525588847, 0.0, 0.9151807425831917, 0.0, 0.0, 0.0, 0.6693739198309284, 0.0, 0.0, 0.2061930933620024, 0.27528937724536684, 0.0, 0.0, 0.0, 0.0, 0.2675780483794807, 0.02063619759744684, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5460074055587408, 0.0, 0.6541555536266587, 0.0, 0.0, 0.07150154126300666, 0.0, 0.15048122306270975, 0.034115472912426725, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.050926612139623545, 0.21044506008920905, 0.0, 0.0, 0.4253961501964202, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5242476712268254, 0.0, 0.0, 0.6721198397890595, 0.02913210340779019, 0.0, 0.20059561631676462, 0.0, 0.29433701033990906, 0.0, 0.0, 0.0, 0.12559144077146528, 0.0, 0.0, 0.32229898127496104, 0.0, 0.0, 0.05500938293182685, 0.1114468235961369, 0.09601244908001759, 0.255231140185047, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05242264559033129, 0.07189305704474794, 0.29708512061039594, 0.0, 0.0, 0.16167981650220184, 0.0, 0.0, 0.42894283906704944, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24120550186102305, 0.002462183433889286, 0.1185465354728046, 0.0, 0.0, 0.20924909285728116, 0.0, 0.2383879501918044, 0.0, 0.08928505124693381, 0.0, 0.0, 0.2291277246538886, 0.0, 0.0, 0.039107088380868994, 0.03339412468553525, 0.0, 0.17867986571670036, 0.0, 0.15885581639487184, 0.0, 0.4770109811329409, 0.46521958209705216, 0.08213268605220507, 0.8570033319080816, 0.5762497735248863, 0.025109002675938377, 0.0, 0.0, 0.0, 0.006062678222071191, 0.6895253688974624, 0.0, 0.0, 0.14425623656373723, 0.0, 0.26058653129843573, 0.2499129405223175, 0.5556369963578781, 0.0, 0.0, 0.14846473797228377, 1.311439497677356, 0.0, 0.0, 0.0, 0.14844299347189643, 0.0, 0.06204384333300274, 0.5085214657536261, 0.06333953518463412, 0.0, 0.4087271917522072, 0.56471815720299, 0.0, 0.7613475113276124, 0.31092441757595435, 0.25657570652463557, 0.0, 0.6284352010004853, 0.0, 0.44245087677575873, 0.0, 0.5840306487685422, 0.886297089335757, 0.0, 0.2141960416332438, 0.0, 0.0, 0.7455493113099605, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9841442926021067, 0.155496454991287, 0.0, 0.0, 0.07678252545804619, 0.0, 0.0, 0.3721354350358947, 0.020164712785984047, 0.0, 0.0, 0.0, 0.0, 0.1088564167894987, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9595213220321046, 0.0, 0.026588936071193094, 0.0, 0.0, 0.18249673153915044, 0.041373682209080435, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04976306812868327, 0.2056369238513726, 0.0, 0.0, 0.341205976763526, 0.0, 0.0, 0.0, 0.0, 0.5785815925839551, 0.0, 0.0, 0.0, 0.5989984098523007, 0.041848978991102986, 0.0, 0.7058966144833084, 0.0, 0.1357909427918936, 0.0, 0.0, 0.0, 0.6943234317201388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03329077451447904, 0.0, 0.16959332194846893, 0.03825181104342904, 0.0, 0.030570483496490792, 0.0, 0.181963825346749, 0.0, 0.0, 0.0861751567882612, 0.0, 0.23433890396321935, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06468139820825311, 0.4901043774165395, 0.0, 0.1652405132589817, 0.0, 0.0, 0.07483443864111192, 0.0, 0.0, 0.0, 0.2810295005684995, 0.0, 0.3898372849078984, 0.0, 0.27070466717435465, 0.5873234092956581, 0.0, 0.0, 0.8033877947986148, 0.0, 0.0, 0.0, 0.5030084273245645, 0.1643009637310288, 0.0, 0.0, 0.0, 0.052313365367879956, 0.02924539070601191, 0.0, 0.7815339827038686, 0.0, 0.631919493277437, 0.08243985181699306, 0.0, 0.0, 0.0, 0.0, 0.12868680907403285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.1259698971001804, 0.7198499769348474, 0.0, 0.41807281750787384, 0.0, 0.0, 0.6290848565988848, 0.7773412611170788, 0.0, 0.3270721417747804, 0.0, 0.0, 0.6187710034515789, 0.07069501936159496, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15113911645003905, 0.0, 0.15654877177402662, 1.1322176354521876, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2636197382380641, 0.0, 0.0, 0.34206988627899454, 0.0, 0.0, 0.2168559436204628, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12189174979618164, 0.009400555275465744, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02319896532052816, 0.09586554938115534, 0.0, 0.0, 0.47758214062526055, 0.0, 0.0, 0.0, 0.0, 0.766521270904984, 0.0, 0.0, 0.0, 0.05687724891036996, 0.0, 0.0, 0.0, 0.019988327516632248, 0.0, 0.0, 0.24020596976351885, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5319793134128888, 0.0, 0.0, 0.12310759546793347, 0.28424710350697324, 0.0, 0.9704064926727941, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.595062055789435, 0.0, 0.022922464313533127, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7479520138852049, 0.0, 0.0, 0.0, 0.0, 0.11201726895804533, 0.0, 0.0, 0.5882991104057551, 0.0, 0.0, 0.0, 0.5372591710353254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4185803721513073, 0.0, 0.4160894350708626, 0.35712090084273435, 0.0, 0.0, 0.0, 0.0, 0.3329375067097101, 0.0, 0.0, 0.0, 0.9791513934288851, 0.040171778764664906, 0.0, 0.0, 0.18774405618493867, 0.47311893679528827, 0.2590438343789052, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12077476728428499, 0.0, 0.06327325623124767, 0.0, 0.0, 0.0, 0.0, 0.34849519635188847, 0.0, 0.4382603746311635, 0.0, 0.0, 0.0, 0.09913709085437171, 0.4096661875861404, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4230910500526733, 0.0, 0.0, 0.0, 0.17776900851570368, 0.0, 0.5438917161748563, 0.0, 0.43287084617637955, 0.0, 0.0, 0.0, 0.5349746055023918, 0.0, 0.7894262258085124, 0.0, 0.0, 0.0, 0.416696264214944, 0.1414148713022835, 0.0, 0.1306712640828635, 0.5944339267365731, 0.0, 0.0, 0.0, 0.3098486230376566, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27475814956764166, 0.0, 0.0, 0.0, 0.572874722755641, 0.7993131511508778, 0.0, 0.2979949962228512, 0.14400098705349634, 0.0, 0.0, 0.02684711041673838, 0.0, 0.0575193250192605, 0.13038147684896279, 0.07982177723780243, 0.0, 0.0, 0.22370954598082965, 0.0, 0.0, 0.0, 0.16207849359014098, 0.8780589957563658, 0.0, 0.0, 0.0, 0.03773454641900064, 0.0, 0.190720074864707, 0.03823162707299666, 0.0, 0.027092618673790602, 0.07782668325822914, 0.0, 0.0, 0.025612768291380317, 0.0, 0.1339183762706947, 0.03147511447091019, 0.0, 0.0, 0.22256660522037766, 0.0, 0.050981846070955524, 0.33080849696175413, 0.22670468547648376, 0.0, 0.0, 0.2394131641393997, 0.0, 0.6763225254232694, 0.0, 0.32609931596811353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02551395955593748, 0.0, 0.19870976845837512, 0.0, 0.8838430389981123, 0.0, 0.8900590234719281, 0.0, 0.1395885727389325, 0.39274638865688727, 0.13258182339863903, 1.0427544725112363, 0.007925765580094073, 0.0, 0.0, 0.9248219821941955, 0.0, 0.38782207933590473, 0.20331266090801786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4620448777897825, 0.5673723076761421, 0.13444042454417618, 0.0, 0.0012927651989113214, 0.0, 0.0, 0.0, 0.3049721916420315, 0.017266202832261764, 0.0, 0.0, 0.07071993264415252, 0.04567425634491729, 0.3943633692304047, 0.0, 0.15861807151929683, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05848946000294637, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0002825960061227963, 0.18362488935764, 0.0, 0.0, 0.0, 0.16115194339742445, 0.5974334511167834, 0.0, 0.04024167944046561, 0.33382381844541353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6087942383002851, 0.0, 0.0, 0.7807479082612564, 0.3974967659255351, 1.5487027213337248, 0.03765559375444021, 0.17692333788215128, 0.23426356503546714, 0.0, 0.22388851869475412, 0.04007956243736881, 1.0262040210122396, 0.0, 0.7972348331243138, 1.0049259227957363, 0.051433760809377164, 0.0, 0.8283458296766737, 0.0, 0.7561556098507006, 0.0, 0.0, 0.0, 0.24907496608552146, 0.0, 0.5011916402324644, 0.1747189814102487, 0.9899595982235516, 0.6164188940137083, 0.11559029428984438, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.593849645786338, 0.0, 0.0, 0.0, 0.0, 0.13660851844466146, 0.030970403587585463, 0.23157107362581741, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13749939723080143, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7110898100418397, 0.0, 0.0, 0.0, 0.46081314114808425, 0.7109856621285974, 0.2884350146527927, 0.0, 0.0, 0.30337236072169704, 0.06739814120009983, 0.0, 0.7785291912169707, 0.0, 0.0, 0.13287789565404662, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3593823347180517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4624623396236477, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024288127378650458, 0.0, 0.793430084070044, 0.0, 0.0, 0.07003161792104487, 0.0, 0.390525043836891, 0.7636039620295462, 0.06950112764284275, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019321154098251664, 0.0, 0.18651557279691772, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05792018405822482, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18265223354453325, 0.12454723256219671, 0.0, 0.0002798455087109209, 0.0, 0.0, 0.11611318457978691, 0.05398921986713746, 0.0, 0.8752290763913787, 0.0, 0.0, 0.0, 0.0, 0.13107895831547542, 0.0, 0.0, 0.45521248179167606, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11685661958627322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44522459151527227, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26435993065610003, 0.021758541234477975, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2146225974372531, 0.0, 0.13433790512658406, 0.030902835047588124, 0.0, 0.0, 0.03139052000724026, 0.0, 0.0, 0.7200671942026683, 0.0, 0.0, 0.05946084260814123, 0.0, 0.14663715828117352, 0.03324398818324252, 0.008023890708802274, 0.05967012120515395, 0.0, 0.0, 0.0, 0.10840406404650259, 0.0, 0.0, 0.0, 0.23002554078955822, 0.0, 0.21539070760796813, 0.0, 0.15809846801096508, 0.0, 0.0, 0.023379160669788793, 0.06919243918501702, 0.0, 0.4184368340983768, 0.0, 0.0, 0.0, 0.025499570825031452, 0.0, 0.054632252014063354, 0.12383722685885369, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08100665304577326, 0.0, 0.0, 0.0, 1.137863121002405, 0.0, 0.0, 0.18114724382910816, 1.0175246627827756, 0.0, 0.8319051265218698, 0.7408406030496757, 0.0, 0.0, 0.4793159619581115, 0.26457362772519605, 0.7690370272987045, 0.02989528102721059, 0.0, 0.0, 0.0, 0.34236772701032764, 0.22777638147070367, 0.20013535639204974, 0.7445351180710278, 0.0, 0.0, 0.32085924041220354, 0.0, 0.0, 0.0, 0.5922078604864867, 0.0, 0.0, 0.18324289767504168, 0.0, 0.0, 0.05247138335835671, 0.48819810625100485, 0.0, 0.4058243472838195, 0.0, 0.0, 0.6944817185701155, 0.21684000174364465, 0.0, 0.12484778900119534, 0.0, 0.5314768166536554, 0.0, 0.0, 0.0, 0.0, 0.11888353831534566, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4539474830982486, 0.0, 0.12931153719488578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23351697026332235, 0.0, 0.0, 0.0, 0.0, 0.2291429172294868, 0.4258647172142874, 0.0, 0.0, 0.0, 0.22479141295616378, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05853977920447593, 0.0, 0.16715127001769428, 0.0, 0.3712702672746247, 0.13508244363790445, 0.06392604260421969, 0.5912989058272206, 0.4437811402099471, 0.0, 0.0, 0.0, 0.0, 0.005827558703832637, 0.6627845677844773, 0.0, 0.0, 0.0, 0.30904361146056275, 0.6816789334794826, 0.1436649772664439, 0.16334297841077242, 0.0, 0.0, 0.7882286990025497, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7753056808075847, 0.035768380854023674, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44917737188494944, 0.0, 0.18937379890511313, 0.06882274914369534, 0.0, 0.0, 0.0, 0.009287224889287345, 0.06906497794085972, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26624227636214304, 0.0, 0.0, 0.0, 0.1829905316954157, 0.0, 0.06582989974405196, 0.2036305981722081, 0.08008655235603437, 0.0, 0.0, 0.0, 0.0, 0.0, 0.41729810318329397, 0.0, 0.0, 0.0, 0.0, 0.0, 0.037951024806923965, 0.0, 0.0, 0.0, 0.0, 0.41651629143780994, 0.0, 0.2053954771505675, 0.29245802337269494, 0.0368786813523771, 0.0, 0.38440171693921094, 0.0, 1.0326219326945734, 0.0, 0.02604768015780454, 0.9153304998316836, 0.0, 0.0, 0.0, 0.021211537114504155, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3418184639399641, 0.9530187941808128, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5415447555144779, 0.0, 0.16269589476095273, 0.0, 0.2704207942532035, 0.6129742049927741, 0.0, 0.0, 0.0, 0.0, 0.07078757436524862, 0.0, 0.7077105145475581, 0.34262729838304967, 0.5909987644476324, 0.0, 0.11177131716736882, 0.4359653240929594, 0.04723041930008277, 0.45146742793812006, 0.896649501844373, 0.0, 0.0, 0.6373803605539574, 0.5201327056992473, 0.10980024784153514, 0.07242729038979111, 0.0, 0.25075448235999825, 0.0, 0.14797679652158516, 0.0, 0.0, 0.0, 0.0, 0.6939917265420261, 0.9906375819864492, 0.9257686181712707, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37693205342755914, 0.5232740229070877, 0.0, 0.060081052233409524, 0.0, 0.3350366056610552, 0.0, 0.20067269510180097, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5308540215160052, 0.0, 0.0, 0.04969049276705586, 0.4729163126308979, 0.17206519556589087, 0.08142765800071788, 0.0, 0.6342266715653555, 0.28491323267898133, 0.0, 0.0, 0.0, 0.0, 0.3249315382934875, 0.27888197557292793, 0.0, 0.00024008316707892853, 0.0, 0.0, 0.4429941865632019, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08351200806876206, 0.0, 0.0, 0.3644144714136701, 0.0, 0.25850208657414453, 0.3044172676246769, 0.0, 0.0, 0.0, 0.42064905943656483, 0.5410542431685182, 0.0, 0.0, 0.0235250266831074, 0.17227146855360218, 0.0, 0.0, 0.11190852746246406, 0.0, 0.0, 0.0, 0.0, 0.14411942384547258, 0.0, 0.0, 0.4237744107411662, 0.0, 0.2972186074559964, 0.0, 0.5397314242092841, 0.4632402464002124, 0.0, 0.5786606886346829, 0.0, 0.0, 0.47789419623388885, 0.0, 0.0, 0.0, 0.10580588439167306, 0.0, 0.0, 0.7377357792707717, 0.0, 0.10579038780894534, 0.0, 0.0, 0.0, 0.045139981578782816, 0.0, 0.0, 0.11584045845995011, 0.0, 0.0, 0.019771431213384455, 0.36490249309768175, 0.35310751173091487, 1.1149184594112733, 0.0, 0.0, 0.0, 0.5857527772485364, 0.44959707967332574, 0.1008560617277558, 0.018841708015473753, 0.7001531183110316, 0.0, 0.0, 0.0, 0.5946141177445001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22666010948008636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014782177343261123, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02695824533437821, 0.0, 0.0, 0.0, 0.016838719448302838, 0.0, 0.0, 0.0, 0.08231997245985787, 0.0, 0.0, 0.0, 0.01058107131899943, 0.0, 0.05814319369896461, 0.0, 0.0, 0.0, 0.0, 0.047348036509572325, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019563192629043182, 0.027295875565384514, 0.0, 0.0, 0.0, 0.0, 0.33629932740438834, 0.0, 0.0, 0.0, 0.32394391802925426, 0.5704070896351352, 0.0, 0.14173474826201982, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6013127745008292, 0.0, 0.7561985490243033, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2959306772073682, 0.2539911401819114, 0.0, 0.0, 0.7300245622804938, 0.0, 0.2367914528076339, 0.0, 0.0, 0.0, 0.33628005236128533, 0.025934644678944505, 0.0, 0.06026134263178236, 0.0, 0.0, 0.03662105153743461, 0.06883834406728133, 0.0, 0.0, 0.21279478779844782, 0.0, 0.0, 0.07129569822964256, 0.0, 0.09738688258412649, 0.06246220065656896, 0.0, 0.059045957391141574, 0.0, 0.006923917652553149, 0.051490108793371445, 0.0, 0.16644468958034975, 0.19015418061029377, 0.10882376279915186, 0.06400227485256132, 0.2644778831991808, 0.0, 0.19849197355887785, 0.0, 0.0010725148648749981, 0.12198011848273568, 0.1548472256356659, 0.0, 0.009305339154304988, 0.0, 0.10589543929012146, 0.0, 0.0, 0.17178211974814458, 0.0, 0.23085925511580369, 0.0, 0.0, 0.0, 0.31374773386794685, 0.0, 0.4234572393895347, 0.0, 0.0, 0.0, 0.045888478821935044, 0.0, 0.0, 0.13665231391596233, 0.0, 0.05546447903889757, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15847133453228077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3148706017686264, 0.2655048691833022, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28333620483908656, 0.0, 0.0, 0.0, 0.3208604446677971, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25931056270601605, 0.0, 0.0, 0.20842242702467234, 0.0, 0.0, 0.28681902268117093, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2978149918318377, 0.0, 0.0, 0.09574337597144772, 0.0, 0.0, 0.38017827065442816, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03128644702667798, 0.0, 0.0, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = ((ex.bodies[1]).bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    result = nothing
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = (Finch).moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = (Finch).moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            phase_start_2 = max(1, 1 + fld(A_lvl.shape[1] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                for i_7 = phase_start_2:phase_stop_2
                    B_lvl_q = B_lvl_ptr[1]
                    B_lvl_q_stop = B_lvl_ptr[1 + 1]
                    if B_lvl_q < B_lvl_q_stop
                        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                    else
                        B_lvl_i_stop = 0
                    end
                    phase_stop_3 = min(B_lvl.shape[2], B_lvl_i_stop)
                    if phase_stop_3 >= 1
                        if B_lvl_tbl2[B_lvl_q] < 1
                            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        while true
                            B_lvl_i = B_lvl_tbl2[B_lvl_q]
                            B_lvl_q_step = B_lvl_q
                            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                            end
                            if B_lvl_i < phase_stop_3
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_5 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_5 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_5
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_6 = min(B_lvl_i_2, phase_stop_5, A_lvl_i)
                                        if A_lvl_i == phase_stop_6 && B_lvl_i_2 == phase_stop_6
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_7 = min(i_7, A_lvl_i_stop_2)
                                            if phase_stop_7 >= i_7
                                                if A_lvl_tbl1[A_lvl_q] < i_7
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_7
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_9 = min(A_lvl_i_2, phase_stop_7)
                                                        if A_lvl_i_2 == phase_stop_9
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_6
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_6
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_6 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            else
                                phase_stop_14 = min(B_lvl_i, phase_stop_3)
                                if B_lvl_i == phase_stop_14
                                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_14
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_15 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_15 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_15
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_16 = min(B_lvl_i_2, A_lvl_i, phase_stop_15)
                                            if A_lvl_i == phase_stop_16 && B_lvl_i_2 == phase_stop_16
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_17 = min(i_7, A_lvl_i_stop_4)
                                                if phase_stop_17 >= i_7
                                                    if A_lvl_tbl1[A_lvl_q] < i_7
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_17
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_19 = min(A_lvl_i_4, phase_stop_17)
                                                            if A_lvl_i_4 == phase_stop_19
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_16
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_16
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_16 + 1
                                        end
                                    end
                                    B_lvl_q = B_lvl_q_step
                                end
                                break
                            end
                        end
                    end
                end
            end
        end
    resize!(val, A_lvl.shape[1] * B_lvl.shape[2])
    result = (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
    result
end
julia> @finch begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.2504314291953376, 0.0, 0.0, 0.9648065476989627, 0.2835978542736629, 0.13211849643271117, 0.2796391765298554, 0.22540993882752317, 0.5512218518807817, 0.5478145335437775, 0.0, 0.2291959647754706, 0.23520231615614767, 0.03296826745685447, 0.18421763750874248, 0.6035878434468992, 0.7441004608135994, 0.2535097758200323, 0.20236453274735783, 0.09228241774990334, 0.13384560784694935, 0.5269755678736615, 0.03880627317810495, 0.17704264007357723, 0.0, 0.2632287465084173, 0.0, 0.02720231717357045, 0.5246531850072871, 0.0, 0.0, 0.3360269038793386, 0.0, 0.22538882742471622, 0.22621665568148489, 0.0, 0.09095677592426996, 0.0, 0.2343687318872728, 0.0, 0.00044179348092010485, 0.07281055209847118, 0.6821285312867097, 0.0, 0.0, 0.0, 0.8944403740767686, 0.11973372388784431, 0.2613237068921269, 0.5923534538840982, 0.0, 0.0, 0.0, 0.8426453838331012, 0.0, 0.0, 1.0769929937540788, 0.43300907588983417, 0.0, 0.6905133736710757, 0.0, 0.0, 0.0, 0.07271653050953786, 0.8664857754450395, 0.08570530865158534, 0.0, 0.7169860381548754, 0.20498067747201715, 0.0, 0.23050103487101342, 0.0, 0.0, 0.9152746488430813, 0.14299878494120105, 0.44815460009097313, 0.0, 0.0, 0.0, 0.23162241551000032, 1.0326337985335274, 1.0530846396126474, 0.22109306040172927, 0.0, 0.0, 0.0, 1.0289491599002607, 0.008076930682810486, 0.037079469071792215, 0.0, 0.03308196940629956, 0.07165705922647192, 0.18447863887953236, 0.0, 0.24885144181470292, 0.0, 0.0, 0.0, 0.7193963651558962, 0.0, 0.6007574484421223, 0.0, 0.11361690641306497, 0.0, 0.0, 0.0, 0.0, 0.5189539173895105, 0.027360695267410504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.769945776489901, 0.00013219515459577855, 0.0, 0.0, 0.0, 0.0, 0.21518747264967641, 0.0, 0.0, 0.0, 0.10980938651098104, 0.07347552139071808, 0.0, 0.3613462418752601, 0.04465135919773286, 0.027102240057490925, 0.0, 0.0, 0.2594566815123038, 0.417973871558788, 0.02337304044012675, 0.5498222432776159, 0.011558313103301096, 0.13519961718643905, 0.0830809338177889, 0.0, 0.0, 0.0, 0.17989871031119878, 0.05233157238393502, 0.0, 0.051508809526025434, 0.225133841185459, 0.18950167625902217, 0.0, 0.0, 0.0, 0.20173577654712102, 0.7662693488070871, 0.0, 0.33259178463685746, 0.13865467767460202, 0.0, 0.8944580603877651, 0.0, 0.060682785060590494, 0.06051599339484606, 0.0, 0.0, 0.0, 0.12305720211410776, 0.10081888448105404, 0.0, 0.0, 0.2522377522294325, 0.0, 0.12103967793114603, 0.06077105223878225, 0.0, 0.0, 0.5586258592154407, 0.0, 0.0, 0.0, 0.2984848816909581, 0.029564782247173068, 0.6276543733906952, 0.0, 0.07353625635562432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08001934416956356, 0.13808411155288577, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0013608908425346389, 0.15477792583676012, 0.8924302032481646, 0.0, 0.13515079773758784, 0.0, 0.059793517595463114, 0.0, 0.04933055693313389, 0.0, 0.0, 0.3342781850907459, 0.02578025634698341, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1229501707927792, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287208489034296, 0.0, 0.0, 0.28149278769379354, 0.0, 0.0, 0.0, 0.031923890851782225, 0.23740383631799186, 0.0, 0.0, 0.0, 0.0, 0.06362127081034188, 0.2629034525588847, 0.0, 0.9151807425831917, 0.0, 0.0, 0.0, 0.6693739198309284, 0.0, 0.0, 0.2061930933620024, 0.27528937724536684, 0.0, 0.0, 0.0, 0.0, 0.2675780483794807, 0.02063619759744684, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5460074055587408, 0.0, 0.6541555536266587, 0.0, 0.0, 0.07150154126300666, 0.0, 0.15048122306270975, 0.034115472912426725, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.050926612139623545, 0.21044506008920905, 0.0, 0.0, 0.4253961501964202, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5242476712268254, 0.0, 0.0, 0.6721198397890595, 0.02913210340779019, 0.0, 0.20059561631676462, 0.0, 0.29433701033990906, 0.0, 0.0, 0.0, 0.12559144077146528, 0.0, 0.0, 0.32229898127496104, 0.0, 0.0, 0.05500938293182685, 0.1114468235961369, 0.09601244908001759, 0.255231140185047, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05242264559033129, 0.07189305704474794, 0.29708512061039594, 0.0, 0.0, 0.16167981650220184, 0.0, 0.0, 0.42894283906704944, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24120550186102305, 0.002462183433889286, 0.1185465354728046, 0.0, 0.0, 0.20924909285728116, 0.0, 0.2383879501918044, 0.0, 0.08928505124693381, 0.0, 0.0, 0.2291277246538886, 0.0, 0.0, 0.039107088380868994, 0.03339412468553525, 0.0, 0.17867986571670036, 0.0, 0.15885581639487184, 0.0, 0.4770109811329409, 0.46521958209705216, 0.08213268605220507, 0.8570033319080816, 0.5762497735248863, 0.025109002675938377, 0.0, 0.0, 0.0, 0.006062678222071191, 0.6895253688974624, 0.0, 0.0, 0.14425623656373723, 0.0, 0.26058653129843573, 0.2499129405223175, 0.5556369963578781, 0.0, 0.0, 0.14846473797228377, 1.311439497677356, 0.0, 0.0, 0.0, 0.14844299347189643, 0.0, 0.06204384333300274, 0.5085214657536261, 0.06333953518463412, 0.0, 0.4087271917522072, 0.56471815720299, 0.0, 0.7613475113276124, 0.31092441757595435, 0.25657570652463557, 0.0, 0.6284352010004853, 0.0, 0.44245087677575873, 0.0, 0.5840306487685422, 0.886297089335757, 0.0, 0.2141960416332438, 0.0, 0.0, 0.7455493113099605, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9841442926021067, 0.155496454991287, 0.0, 0.0, 0.07678252545804619, 0.0, 0.0, 0.3721354350358947, 0.020164712785984047, 0.0, 0.0, 0.0, 0.0, 0.1088564167894987, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9595213220321046, 0.0, 0.026588936071193094, 0.0, 0.0, 0.18249673153915044, 0.041373682209080435, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04976306812868327, 0.2056369238513726, 0.0, 0.0, 0.341205976763526, 0.0, 0.0, 0.0, 0.0, 0.5785815925839551, 0.0, 0.0, 0.0, 0.5989984098523007, 0.041848978991102986, 0.0, 0.7058966144833084, 0.0, 0.1357909427918936, 0.0, 0.0, 0.0, 0.6943234317201388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03329077451447904, 0.0, 0.16959332194846893, 0.03825181104342904, 0.0, 0.030570483496490792, 0.0, 0.181963825346749, 0.0, 0.0, 0.0861751567882612, 0.0, 0.23433890396321935, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06468139820825311, 0.4901043774165395, 0.0, 0.1652405132589817, 0.0, 0.0, 0.07483443864111192, 0.0, 0.0, 0.0, 0.2810295005684995, 0.0, 0.3898372849078984, 0.0, 0.27070466717435465, 0.5873234092956581, 0.0, 0.0, 0.8033877947986148, 0.0, 0.0, 0.0, 0.5030084273245645, 0.1643009637310288, 0.0, 0.0, 0.0, 0.052313365367879956, 0.02924539070601191, 0.0, 0.7815339827038686, 0.0, 0.631919493277437, 0.08243985181699306, 0.0, 0.0, 0.0, 0.0, 0.12868680907403285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.1259698971001804, 0.7198499769348474, 0.0, 0.41807281750787384, 0.0, 0.0, 0.6290848565988848, 0.7773412611170788, 0.0, 0.3270721417747804, 0.0, 0.0, 0.6187710034515789, 0.07069501936159496, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15113911645003905, 0.0, 0.15654877177402662, 1.1322176354521876, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2636197382380641, 0.0, 0.0, 0.34206988627899454, 0.0, 0.0, 0.2168559436204628, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12189174979618164, 0.009400555275465744, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02319896532052816, 0.09586554938115534, 0.0, 0.0, 0.47758214062526055, 0.0, 0.0, 0.0, 0.0, 0.766521270904984, 0.0, 0.0, 0.0, 0.05687724891036996, 0.0, 0.0, 0.0, 0.019988327516632248, 0.0, 0.0, 0.24020596976351885, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5319793134128888, 0.0, 0.0, 0.12310759546793347, 0.28424710350697324, 0.0, 0.9704064926727941, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.595062055789435, 0.0, 0.022922464313533127, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7479520138852049, 0.0, 0.0, 0.0, 0.0, 0.11201726895804533, 0.0, 0.0, 0.5882991104057551, 0.0, 0.0, 0.0, 0.5372591710353254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4185803721513073, 0.0, 0.4160894350708626, 0.35712090084273435, 0.0, 0.0, 0.0, 0.0, 0.3329375067097101, 0.0, 0.0, 0.0, 0.9791513934288851, 0.040171778764664906, 0.0, 0.0, 0.18774405618493867, 0.47311893679528827, 0.2590438343789052, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12077476728428499, 0.0, 0.06327325623124767, 0.0, 0.0, 0.0, 0.0, 0.34849519635188847, 0.0, 0.4382603746311635, 0.0, 0.0, 0.0, 0.09913709085437171, 0.4096661875861404, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4230910500526733, 0.0, 0.0, 0.0, 0.17776900851570368, 0.0, 0.5438917161748563, 0.0, 0.43287084617637955, 0.0, 0.0, 0.0, 0.5349746055023918, 0.0, 0.7894262258085124, 0.0, 0.0, 0.0, 0.416696264214944, 0.1414148713022835, 0.0, 0.1306712640828635, 0.5944339267365731, 0.0, 0.0, 0.0, 0.3098486230376566, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27475814956764166, 0.0, 0.0, 0.0, 0.572874722755641, 0.7993131511508778, 0.0, 0.2979949962228512, 0.14400098705349634, 0.0, 0.0, 0.02684711041673838, 0.0, 0.0575193250192605, 0.13038147684896279, 0.07982177723780243, 0.0, 0.0, 0.22370954598082965, 0.0, 0.0, 0.0, 0.16207849359014098, 0.8780589957563658, 0.0, 0.0, 0.0, 0.03773454641900064, 0.0, 0.190720074864707, 0.03823162707299666, 0.0, 0.027092618673790602, 0.07782668325822914, 0.0, 0.0, 0.025612768291380317, 0.0, 0.1339183762706947, 0.03147511447091019, 0.0, 0.0, 0.22256660522037766, 0.0, 0.050981846070955524, 0.33080849696175413, 0.22670468547648376, 0.0, 0.0, 0.2394131641393997, 0.0, 0.6763225254232694, 0.0, 0.32609931596811353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02551395955593748, 0.0, 0.19870976845837512, 0.0, 0.8838430389981123, 0.0, 0.8900590234719281, 0.0, 0.1395885727389325, 0.39274638865688727, 0.13258182339863903, 1.0427544725112363, 0.007925765580094073, 0.0, 0.0, 0.9248219821941955, 0.0, 0.38782207933590473, 0.20331266090801786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4620448777897825, 0.5673723076761421, 0.13444042454417618, 0.0, 0.0012927651989113214, 0.0, 0.0, 0.0, 0.3049721916420315, 0.017266202832261764, 0.0, 0.0, 0.07071993264415252, 0.04567425634491729, 0.3943633692304047, 0.0, 0.15861807151929683, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05848946000294637, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0002825960061227963, 0.18362488935764, 0.0, 0.0, 0.0, 0.16115194339742445, 0.5974334511167834, 0.0, 0.04024167944046561, 0.33382381844541353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6087942383002851, 0.0, 0.0, 0.7807479082612564, 0.3974967659255351, 1.5487027213337248, 0.03765559375444021, 0.17692333788215128, 0.23426356503546714, 0.0, 0.22388851869475412, 0.04007956243736881, 1.0262040210122396, 0.0, 0.7972348331243138, 1.0049259227957363, 0.051433760809377164, 0.0, 0.8283458296766737, 0.0, 0.7561556098507006, 0.0, 0.0, 0.0, 0.24907496608552146, 0.0, 0.5011916402324644, 0.1747189814102487, 0.9899595982235516, 0.6164188940137083, 0.11559029428984438, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.593849645786338, 0.0, 0.0, 0.0, 0.0, 0.13660851844466146, 0.030970403587585463, 0.23157107362581741, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13749939723080143, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7110898100418397, 0.0, 0.0, 0.0, 0.46081314114808425, 0.7109856621285974, 0.2884350146527927, 0.0, 0.0, 0.30337236072169704, 0.06739814120009983, 0.0, 0.7785291912169707, 0.0, 0.0, 0.13287789565404662, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3593823347180517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4624623396236477, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024288127378650458, 0.0, 0.793430084070044, 0.0, 0.0, 0.07003161792104487, 0.0, 0.390525043836891, 0.7636039620295462, 0.06950112764284275, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019321154098251664, 0.0, 0.18651557279691772, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05792018405822482, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18265223354453325, 0.12454723256219671, 0.0, 0.0002798455087109209, 0.0, 0.0, 0.11611318457978691, 0.05398921986713746, 0.0, 0.8752290763913787, 0.0, 0.0, 0.0, 0.0, 0.13107895831547542, 0.0, 0.0, 0.45521248179167606, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11685661958627322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44522459151527227, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26435993065610003, 0.021758541234477975, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2146225974372531, 0.0, 0.13433790512658406, 0.030902835047588124, 0.0, 0.0, 0.03139052000724026, 0.0, 0.0, 0.7200671942026683, 0.0, 0.0, 0.05946084260814123, 0.0, 0.14663715828117352, 0.03324398818324252, 0.008023890708802274, 0.05967012120515395, 0.0, 0.0, 0.0, 0.10840406404650259, 0.0, 0.0, 0.0, 0.23002554078955822, 0.0, 0.21539070760796813, 0.0, 0.15809846801096508, 0.0, 0.0, 0.023379160669788793, 0.06919243918501702, 0.0, 0.4184368340983768, 0.0, 0.0, 0.0, 0.025499570825031452, 0.0, 0.054632252014063354, 0.12383722685885369, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08100665304577326, 0.0, 0.0, 0.0, 1.137863121002405, 0.0, 0.0, 0.18114724382910816, 1.0175246627827756, 0.0, 0.8319051265218698, 0.7408406030496757, 0.0, 0.0, 0.4793159619581115, 0.26457362772519605, 0.7690370272987045, 0.02989528102721059, 0.0, 0.0, 0.0, 0.34236772701032764, 0.22777638147070367, 0.20013535639204974, 0.7445351180710278, 0.0, 0.0, 0.32085924041220354, 0.0, 0.0, 0.0, 0.5922078604864867, 0.0, 0.0, 0.18324289767504168, 0.0, 0.0, 0.05247138335835671, 0.48819810625100485, 0.0, 0.4058243472838195, 0.0, 0.0, 0.6944817185701155, 0.21684000174364465, 0.0, 0.12484778900119534, 0.0, 0.5314768166536554, 0.0, 0.0, 0.0, 0.0, 0.11888353831534566, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4539474830982486, 0.0, 0.12931153719488578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23351697026332235, 0.0, 0.0, 0.0, 0.0, 0.2291429172294868, 0.4258647172142874, 0.0, 0.0, 0.0, 0.22479141295616378, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05853977920447593, 0.0, 0.16715127001769428, 0.0, 0.3712702672746247, 0.13508244363790445, 0.06392604260421969, 0.5912989058272206, 0.4437811402099471, 0.0, 0.0, 0.0, 0.0, 0.005827558703832637, 0.6627845677844773, 0.0, 0.0, 0.0, 0.30904361146056275, 0.6816789334794826, 0.1436649772664439, 0.16334297841077242, 0.0, 0.0, 0.7882286990025497, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7753056808075847, 0.035768380854023674, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44917737188494944, 0.0, 0.18937379890511313, 0.06882274914369534, 0.0, 0.0, 0.0, 0.009287224889287345, 0.06906497794085972, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26624227636214304, 0.0, 0.0, 0.0, 0.1829905316954157, 0.0, 0.06582989974405196, 0.2036305981722081, 0.08008655235603437, 0.0, 0.0, 0.0, 0.0, 0.0, 0.41729810318329397, 0.0, 0.0, 0.0, 0.0, 0.0, 0.037951024806923965, 0.0, 0.0, 0.0, 0.0, 0.41651629143780994, 0.0, 0.2053954771505675, 0.29245802337269494, 0.0368786813523771, 0.0, 0.38440171693921094, 0.0, 1.0326219326945734, 0.0, 0.02604768015780454, 0.9153304998316836, 0.0, 0.0, 0.0, 0.021211537114504155, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3418184639399641, 0.9530187941808128, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5415447555144779, 0.0, 0.16269589476095273, 0.0, 0.2704207942532035, 0.6129742049927741, 0.0, 0.0, 0.0, 0.0, 0.07078757436524862, 0.0, 0.7077105145475581, 0.34262729838304967, 0.5909987644476324, 0.0, 0.11177131716736882, 0.4359653240929594, 0.04723041930008277, 0.45146742793812006, 0.896649501844373, 0.0, 0.0, 0.6373803605539574, 0.5201327056992473, 0.10980024784153514, 0.07242729038979111, 0.0, 0.25075448235999825, 0.0, 0.14797679652158516, 0.0, 0.0, 0.0, 0.0, 0.6939917265420261, 0.9906375819864492, 0.9257686181712707, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37693205342755914, 0.5232740229070877, 0.0, 0.060081052233409524, 0.0, 0.3350366056610552, 0.0, 0.20067269510180097, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5308540215160052, 0.0, 0.0, 0.04969049276705586, 0.4729163126308979, 0.17206519556589087, 0.08142765800071788, 0.0, 0.6342266715653555, 0.28491323267898133, 0.0, 0.0, 0.0, 0.0, 0.3249315382934875, 0.27888197557292793, 0.0, 0.00024008316707892853, 0.0, 0.0, 0.4429941865632019, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08351200806876206, 0.0, 0.0, 0.3644144714136701, 0.0, 0.25850208657414453, 0.3044172676246769, 0.0, 0.0, 0.0, 0.42064905943656483, 0.5410542431685182, 0.0, 0.0, 0.0235250266831074, 0.17227146855360218, 0.0, 0.0, 0.11190852746246406, 0.0, 0.0, 0.0, 0.0, 0.14411942384547258, 0.0, 0.0, 0.4237744107411662, 0.0, 0.2972186074559964, 0.0, 0.5397314242092841, 0.4632402464002124, 0.0, 0.5786606886346829, 0.0, 0.0, 0.47789419623388885, 0.0, 0.0, 0.0, 0.10580588439167306, 0.0, 0.0, 0.7377357792707717, 0.0, 0.10579038780894534, 0.0, 0.0, 0.0, 0.045139981578782816, 0.0, 0.0, 0.11584045845995011, 0.0, 0.0, 0.019771431213384455, 0.36490249309768175, 0.35310751173091487, 1.1149184594112733, 0.0, 0.0, 0.0, 0.5857527772485364, 0.44959707967332574, 0.1008560617277558, 0.018841708015473753, 0.7001531183110316, 0.0, 0.0, 0.0, 0.5946141177445001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22666010948008636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014782177343261123, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02695824533437821, 0.0, 0.0, 0.0, 0.016838719448302838, 0.0, 0.0, 0.0, 0.08231997245985787, 0.0, 0.0, 0.0, 0.01058107131899943, 0.0, 0.05814319369896461, 0.0, 0.0, 0.0, 0.0, 0.047348036509572325, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019563192629043182, 0.027295875565384514, 0.0, 0.0, 0.0, 0.0, 0.33629932740438834, 0.0, 0.0, 0.0, 0.32394391802925426, 0.5704070896351352, 0.0, 0.14173474826201982, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6013127745008292, 0.0, 0.7561985490243033, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2959306772073682, 0.2539911401819114, 0.0, 0.0, 0.7300245622804938, 0.0, 0.2367914528076339, 0.0, 0.0, 0.0, 0.33628005236128533, 0.025934644678944505, 0.0, 0.06026134263178236, 0.0, 0.0, 0.03662105153743461, 0.06883834406728133, 0.0, 0.0, 0.21279478779844782, 0.0, 0.0, 0.07129569822964256, 0.0, 0.09738688258412649, 0.06246220065656896, 0.0, 0.059045957391141574, 0.0, 0.006923917652553149, 0.051490108793371445, 0.0, 0.16644468958034975, 0.19015418061029377, 0.10882376279915186, 0.06400227485256132, 0.2644778831991808, 0.0, 0.19849197355887785, 0.0, 0.0010725148648749981, 0.12198011848273568, 0.1548472256356659, 0.0, 0.009305339154304988, 0.0, 0.10589543929012146, 0.0, 0.0, 0.17178211974814458, 0.0, 0.23085925511580369, 0.0, 0.0, 0.0, 0.31374773386794685, 0.0, 0.4234572393895347, 0.0, 0.0, 0.0, 0.045888478821935044, 0.0, 0.0, 0.13665231391596233, 0.0, 0.05546447903889757, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15847133453228077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3148706017686264, 0.2655048691833022, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28333620483908656, 0.0, 0.0, 0.0, 0.3208604446677971, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25931056270601605, 0.0, 0.0, 0.20842242702467234, 0.0, 0.0, 0.28681902268117093, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2978149918318377, 0.0, 0.0, 0.09574337597144772, 0.0, 0.0, 0.38017827065442816, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03128644702667798, 0.0, 0.0, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = ((ex.bodies[1]).bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    result = nothing
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    for i_4 = 1:A_lvl.shape[1]
        val = Ct_lvl_2_val
        Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
        B_lvl_ptr_2 = B_lvl_ptr
        B_lvl_ptr = (Finch).moveto(B_lvl_ptr, CPU(Threads.nthreads()))
        B_lvl_tbl1_2 = B_lvl_tbl1
        B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
        B_lvl_tbl2_2 = B_lvl_tbl2
        B_lvl_tbl2 = (Finch).moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
        val_2 = B_lvl_val
        B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
        A_lvl_ptr_2 = A_lvl_ptr
        A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
        A_lvl_tbl1_2 = A_lvl_tbl1
        A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
        A_lvl_tbl2_2 = A_lvl_tbl2
        A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
        val_3 = A_lvl_val
        A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
        Threads.@threads for i_5 = 1:Threads.nthreads()
                B_lvl_q = B_lvl_ptr[1]
                B_lvl_q_stop = B_lvl_ptr[1 + 1]
                if B_lvl_q < B_lvl_q_stop
                    B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                else
                    B_lvl_i_stop = 0
                end
                phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_5 + -1), Threads.nthreads()))
                phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_5, Threads.nthreads()))
                if phase_stop_2 >= phase_start_2
                    if B_lvl_tbl2[B_lvl_q] < phase_start_2
                        B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    while true
                        B_lvl_i = B_lvl_tbl2[B_lvl_q]
                        B_lvl_q_step = B_lvl_q
                        if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                            B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        if B_lvl_i < phase_stop_2
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_4, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_4
                                            if A_lvl_tbl1[A_lvl_q] < i_4
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        else
                            phase_stop_13 = min(B_lvl_i, phase_stop_2)
                            if B_lvl_i == phase_stop_13
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_4, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_4
                                                if A_lvl_tbl1[A_lvl_q] < i_4
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            end
                            break
                        end
                    end
                end
            end
        Ct_lvl_2_val = val
        B_lvl_ptr = B_lvl_ptr_2
        B_lvl_tbl1 = B_lvl_tbl1_2
        B_lvl_tbl2 = B_lvl_tbl2_2
        B_lvl_val = val_2
        A_lvl_ptr = A_lvl_ptr_2
        A_lvl_tbl1 = A_lvl_tbl1_2
        A_lvl_tbl2 = A_lvl_tbl2_2
        A_lvl_val = val_3
    end
    resize!(Ct_lvl_2_val, A_lvl.shape[1] * B_lvl.shape[2])
    result = (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
    result
end
julia> @finch begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.2504314291953376, 0.0, 0.0, 0.9648065476989627, 0.2835978542736629, 0.13211849643271117, 0.2796391765298554, 0.22540993882752317, 0.5512218518807817, 0.5478145335437775, 0.0, 0.2291959647754706, 0.23520231615614767, 0.03296826745685447, 0.18421763750874248, 0.6035878434468992, 0.7441004608135994, 0.2535097758200323, 0.20236453274735783, 0.09228241774990334, 0.13384560784694935, 0.5269755678736615, 0.03880627317810495, 0.17704264007357723, 0.0, 0.2632287465084173, 0.0, 0.02720231717357045, 0.5246531850072871, 0.0, 0.0, 0.3360269038793386, 0.0, 0.22538882742471622, 0.22621665568148489, 0.0, 0.09095677592426996, 0.0, 0.2343687318872728, 0.0, 0.00044179348092010485, 0.07281055209847118, 0.6821285312867097, 0.0, 0.0, 0.0, 0.8944403740767686, 0.11973372388784431, 0.2613237068921269, 0.5923534538840982, 0.0, 0.0, 0.0, 0.8426453838331012, 0.0, 0.0, 1.0769929937540788, 0.43300907588983417, 0.0, 0.6905133736710757, 0.0, 0.0, 0.0, 0.07271653050953786, 0.8664857754450395, 0.08570530865158534, 0.0, 0.7169860381548754, 0.20498067747201715, 0.0, 0.23050103487101342, 0.0, 0.0, 0.9152746488430813, 0.14299878494120105, 0.44815460009097313, 0.0, 0.0, 0.0, 0.23162241551000032, 1.0326337985335274, 1.0530846396126474, 0.22109306040172927, 0.0, 0.0, 0.0, 1.0289491599002607, 0.008076930682810486, 0.037079469071792215, 0.0, 0.03308196940629956, 0.07165705922647192, 0.18447863887953236, 0.0, 0.24885144181470292, 0.0, 0.0, 0.0, 0.7193963651558962, 0.0, 0.6007574484421223, 0.0, 0.11361690641306497, 0.0, 0.0, 0.0, 0.0, 0.5189539173895105, 0.027360695267410504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.769945776489901, 0.00013219515459577855, 0.0, 0.0, 0.0, 0.0, 0.21518747264967641, 0.0, 0.0, 0.0, 0.10980938651098104, 0.07347552139071808, 0.0, 0.3613462418752601, 0.04465135919773286, 0.027102240057490925, 0.0, 0.0, 0.2594566815123038, 0.417973871558788, 0.02337304044012675, 0.5498222432776159, 0.011558313103301096, 0.13519961718643905, 0.0830809338177889, 0.0, 0.0, 0.0, 0.17989871031119878, 0.05233157238393502, 0.0, 0.051508809526025434, 0.225133841185459, 0.18950167625902217, 0.0, 0.0, 0.0, 0.20173577654712102, 0.7662693488070871, 0.0, 0.33259178463685746, 0.13865467767460202, 0.0, 0.8944580603877651, 0.0, 0.060682785060590494, 0.06051599339484606, 0.0, 0.0, 0.0, 0.12305720211410776, 0.10081888448105404, 0.0, 0.0, 0.2522377522294325, 0.0, 0.12103967793114603, 0.06077105223878225, 0.0, 0.0, 0.5586258592154407, 0.0, 0.0, 0.0, 0.2984848816909581, 0.029564782247173068, 0.6276543733906952, 0.0, 0.07353625635562432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08001934416956356, 0.13808411155288577, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0013608908425346389, 0.15477792583676012, 0.8924302032481646, 0.0, 0.13515079773758784, 0.0, 0.059793517595463114, 0.0, 0.04933055693313389, 0.0, 0.0, 0.3342781850907459, 0.02578025634698341, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1229501707927792, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287208489034296, 0.0, 0.0, 0.28149278769379354, 0.0, 0.0, 0.0, 0.031923890851782225, 0.23740383631799186, 0.0, 0.0, 0.0, 0.0, 0.06362127081034188, 0.2629034525588847, 0.0, 0.9151807425831917, 0.0, 0.0, 0.0, 0.6693739198309284, 0.0, 0.0, 0.2061930933620024, 0.27528937724536684, 0.0, 0.0, 0.0, 0.0, 0.2675780483794807, 0.02063619759744684, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5460074055587408, 0.0, 0.6541555536266587, 0.0, 0.0, 0.07150154126300666, 0.0, 0.15048122306270975, 0.034115472912426725, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.050926612139623545, 0.21044506008920905, 0.0, 0.0, 0.4253961501964202, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5242476712268254, 0.0, 0.0, 0.6721198397890595, 0.02913210340779019, 0.0, 0.20059561631676462, 0.0, 0.29433701033990906, 0.0, 0.0, 0.0, 0.12559144077146528, 0.0, 0.0, 0.32229898127496104, 0.0, 0.0, 0.05500938293182685, 0.1114468235961369, 0.09601244908001759, 0.255231140185047, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05242264559033129, 0.07189305704474794, 0.29708512061039594, 0.0, 0.0, 0.16167981650220184, 0.0, 0.0, 0.42894283906704944, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24120550186102305, 0.002462183433889286, 0.1185465354728046, 0.0, 0.0, 0.20924909285728116, 0.0, 0.2383879501918044, 0.0, 0.08928505124693381, 0.0, 0.0, 0.2291277246538886, 0.0, 0.0, 0.039107088380868994, 0.03339412468553525, 0.0, 0.17867986571670036, 0.0, 0.15885581639487184, 0.0, 0.4770109811329409, 0.46521958209705216, 0.08213268605220507, 0.8570033319080816, 0.5762497735248863, 0.025109002675938377, 0.0, 0.0, 0.0, 0.006062678222071191, 0.6895253688974624, 0.0, 0.0, 0.14425623656373723, 0.0, 0.26058653129843573, 0.2499129405223175, 0.5556369963578781, 0.0, 0.0, 0.14846473797228377, 1.311439497677356, 0.0, 0.0, 0.0, 0.14844299347189643, 0.0, 0.06204384333300274, 0.5085214657536261, 0.06333953518463412, 0.0, 0.4087271917522072, 0.56471815720299, 0.0, 0.7613475113276124, 0.31092441757595435, 0.25657570652463557, 0.0, 0.6284352010004853, 0.0, 0.44245087677575873, 0.0, 0.5840306487685422, 0.886297089335757, 0.0, 0.2141960416332438, 0.0, 0.0, 0.7455493113099605, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9841442926021067, 0.155496454991287, 0.0, 0.0, 0.07678252545804619, 0.0, 0.0, 0.3721354350358947, 0.020164712785984047, 0.0, 0.0, 0.0, 0.0, 0.1088564167894987, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9595213220321046, 0.0, 0.026588936071193094, 0.0, 0.0, 0.18249673153915044, 0.041373682209080435, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04976306812868327, 0.2056369238513726, 0.0, 0.0, 0.341205976763526, 0.0, 0.0, 0.0, 0.0, 0.5785815925839551, 0.0, 0.0, 0.0, 0.5989984098523007, 0.041848978991102986, 0.0, 0.7058966144833084, 0.0, 0.1357909427918936, 0.0, 0.0, 0.0, 0.6943234317201388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03329077451447904, 0.0, 0.16959332194846893, 0.03825181104342904, 0.0, 0.030570483496490792, 0.0, 0.181963825346749, 0.0, 0.0, 0.0861751567882612, 0.0, 0.23433890396321935, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06468139820825311, 0.4901043774165395, 0.0, 0.1652405132589817, 0.0, 0.0, 0.07483443864111192, 0.0, 0.0, 0.0, 0.2810295005684995, 0.0, 0.3898372849078984, 0.0, 0.27070466717435465, 0.5873234092956581, 0.0, 0.0, 0.8033877947986148, 0.0, 0.0, 0.0, 0.5030084273245645, 0.1643009637310288, 0.0, 0.0, 0.0, 0.052313365367879956, 0.02924539070601191, 0.0, 0.7815339827038686, 0.0, 0.631919493277437, 0.08243985181699306, 0.0, 0.0, 0.0, 0.0, 0.12868680907403285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.1259698971001804, 0.7198499769348474, 0.0, 0.41807281750787384, 0.0, 0.0, 0.6290848565988848, 0.7773412611170788, 0.0, 0.3270721417747804, 0.0, 0.0, 0.6187710034515789, 0.07069501936159496, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15113911645003905, 0.0, 0.15654877177402662, 1.1322176354521876, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2636197382380641, 0.0, 0.0, 0.34206988627899454, 0.0, 0.0, 0.2168559436204628, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12189174979618164, 0.009400555275465744, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02319896532052816, 0.09586554938115534, 0.0, 0.0, 0.47758214062526055, 0.0, 0.0, 0.0, 0.0, 0.766521270904984, 0.0, 0.0, 0.0, 0.05687724891036996, 0.0, 0.0, 0.0, 0.019988327516632248, 0.0, 0.0, 0.24020596976351885, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5319793134128888, 0.0, 0.0, 0.12310759546793347, 0.28424710350697324, 0.0, 0.9704064926727941, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.595062055789435, 0.0, 0.022922464313533127, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7479520138852049, 0.0, 0.0, 0.0, 0.0, 0.11201726895804533, 0.0, 0.0, 0.5882991104057551, 0.0, 0.0, 0.0, 0.5372591710353254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4185803721513073, 0.0, 0.4160894350708626, 0.35712090084273435, 0.0, 0.0, 0.0, 0.0, 0.3329375067097101, 0.0, 0.0, 0.0, 0.9791513934288851, 0.040171778764664906, 0.0, 0.0, 0.18774405618493867, 0.47311893679528827, 0.2590438343789052, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12077476728428499, 0.0, 0.06327325623124767, 0.0, 0.0, 0.0, 0.0, 0.34849519635188847, 0.0, 0.4382603746311635, 0.0, 0.0, 0.0, 0.09913709085437171, 0.4096661875861404, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4230910500526733, 0.0, 0.0, 0.0, 0.17776900851570368, 0.0, 0.5438917161748563, 0.0, 0.43287084617637955, 0.0, 0.0, 0.0, 0.5349746055023918, 0.0, 0.7894262258085124, 0.0, 0.0, 0.0, 0.416696264214944, 0.1414148713022835, 0.0, 0.1306712640828635, 0.5944339267365731, 0.0, 0.0, 0.0, 0.3098486230376566, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27475814956764166, 0.0, 0.0, 0.0, 0.572874722755641, 0.7993131511508778, 0.0, 0.2979949962228512, 0.14400098705349634, 0.0, 0.0, 0.02684711041673838, 0.0, 0.0575193250192605, 0.13038147684896279, 0.07982177723780243, 0.0, 0.0, 0.22370954598082965, 0.0, 0.0, 0.0, 0.16207849359014098, 0.8780589957563658, 0.0, 0.0, 0.0, 0.03773454641900064, 0.0, 0.190720074864707, 0.03823162707299666, 0.0, 0.027092618673790602, 0.07782668325822914, 0.0, 0.0, 0.025612768291380317, 0.0, 0.1339183762706947, 0.03147511447091019, 0.0, 0.0, 0.22256660522037766, 0.0, 0.050981846070955524, 0.33080849696175413, 0.22670468547648376, 0.0, 0.0, 0.2394131641393997, 0.0, 0.6763225254232694, 0.0, 0.32609931596811353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02551395955593748, 0.0, 0.19870976845837512, 0.0, 0.8838430389981123, 0.0, 0.8900590234719281, 0.0, 0.1395885727389325, 0.39274638865688727, 0.13258182339863903, 1.0427544725112363, 0.007925765580094073, 0.0, 0.0, 0.9248219821941955, 0.0, 0.38782207933590473, 0.20331266090801786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4620448777897825, 0.5673723076761421, 0.13444042454417618, 0.0, 0.0012927651989113214, 0.0, 0.0, 0.0, 0.3049721916420315, 0.017266202832261764, 0.0, 0.0, 0.07071993264415252, 0.04567425634491729, 0.3943633692304047, 0.0, 0.15861807151929683, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05848946000294637, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0002825960061227963, 0.18362488935764, 0.0, 0.0, 0.0, 0.16115194339742445, 0.5974334511167834, 0.0, 0.04024167944046561, 0.33382381844541353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6087942383002851, 0.0, 0.0, 0.7807479082612564, 0.3974967659255351, 1.5487027213337248, 0.03765559375444021, 0.17692333788215128, 0.23426356503546714, 0.0, 0.22388851869475412, 0.04007956243736881, 1.0262040210122396, 0.0, 0.7972348331243138, 1.0049259227957363, 0.051433760809377164, 0.0, 0.8283458296766737, 0.0, 0.7561556098507006, 0.0, 0.0, 0.0, 0.24907496608552146, 0.0, 0.5011916402324644, 0.1747189814102487, 0.9899595982235516, 0.6164188940137083, 0.11559029428984438, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.593849645786338, 0.0, 0.0, 0.0, 0.0, 0.13660851844466146, 0.030970403587585463, 0.23157107362581741, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13749939723080143, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7110898100418397, 0.0, 0.0, 0.0, 0.46081314114808425, 0.7109856621285974, 0.2884350146527927, 0.0, 0.0, 0.30337236072169704, 0.06739814120009983, 0.0, 0.7785291912169707, 0.0, 0.0, 0.13287789565404662, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3593823347180517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4624623396236477, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024288127378650458, 0.0, 0.793430084070044, 0.0, 0.0, 0.07003161792104487, 0.0, 0.390525043836891, 0.7636039620295462, 0.06950112764284275, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019321154098251664, 0.0, 0.18651557279691772, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05792018405822482, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18265223354453325, 0.12454723256219671, 0.0, 0.0002798455087109209, 0.0, 0.0, 0.11611318457978691, 0.05398921986713746, 0.0, 0.8752290763913787, 0.0, 0.0, 0.0, 0.0, 0.13107895831547542, 0.0, 0.0, 0.45521248179167606, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11685661958627322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44522459151527227, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26435993065610003, 0.021758541234477975, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2146225974372531, 0.0, 0.13433790512658406, 0.030902835047588124, 0.0, 0.0, 0.03139052000724026, 0.0, 0.0, 0.7200671942026683, 0.0, 0.0, 0.05946084260814123, 0.0, 0.14663715828117352, 0.03324398818324252, 0.008023890708802274, 0.05967012120515395, 0.0, 0.0, 0.0, 0.10840406404650259, 0.0, 0.0, 0.0, 0.23002554078955822, 0.0, 0.21539070760796813, 0.0, 0.15809846801096508, 0.0, 0.0, 0.023379160669788793, 0.06919243918501702, 0.0, 0.4184368340983768, 0.0, 0.0, 0.0, 0.025499570825031452, 0.0, 0.054632252014063354, 0.12383722685885369, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08100665304577326, 0.0, 0.0, 0.0, 1.137863121002405, 0.0, 0.0, 0.18114724382910816, 1.0175246627827756, 0.0, 0.8319051265218698, 0.7408406030496757, 0.0, 0.0, 0.4793159619581115, 0.26457362772519605, 0.7690370272987045, 0.02989528102721059, 0.0, 0.0, 0.0, 0.34236772701032764, 0.22777638147070367, 0.20013535639204974, 0.7445351180710278, 0.0, 0.0, 0.32085924041220354, 0.0, 0.0, 0.0, 0.5922078604864867, 0.0, 0.0, 0.18324289767504168, 0.0, 0.0, 0.05247138335835671, 0.48819810625100485, 0.0, 0.4058243472838195, 0.0, 0.0, 0.6944817185701155, 0.21684000174364465, 0.0, 0.12484778900119534, 0.0, 0.5314768166536554, 0.0, 0.0, 0.0, 0.0, 0.11888353831534566, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4539474830982486, 0.0, 0.12931153719488578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23351697026332235, 0.0, 0.0, 0.0, 0.0, 0.2291429172294868, 0.4258647172142874, 0.0, 0.0, 0.0, 0.22479141295616378, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05853977920447593, 0.0, 0.16715127001769428, 0.0, 0.3712702672746247, 0.13508244363790445, 0.06392604260421969, 0.5912989058272206, 0.4437811402099471, 0.0, 0.0, 0.0, 0.0, 0.005827558703832637, 0.6627845677844773, 0.0, 0.0, 0.0, 0.30904361146056275, 0.6816789334794826, 0.1436649772664439, 0.16334297841077242, 0.0, 0.0, 0.7882286990025497, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7753056808075847, 0.035768380854023674, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44917737188494944, 0.0, 0.18937379890511313, 0.06882274914369534, 0.0, 0.0, 0.0, 0.009287224889287345, 0.06906497794085972, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26624227636214304, 0.0, 0.0, 0.0, 0.1829905316954157, 0.0, 0.06582989974405196, 0.2036305981722081, 0.08008655235603437, 0.0, 0.0, 0.0, 0.0, 0.0, 0.41729810318329397, 0.0, 0.0, 0.0, 0.0, 0.0, 0.037951024806923965, 0.0, 0.0, 0.0, 0.0, 0.41651629143780994, 0.0, 0.2053954771505675, 0.29245802337269494, 0.0368786813523771, 0.0, 0.38440171693921094, 0.0, 1.0326219326945734, 0.0, 0.02604768015780454, 0.9153304998316836, 0.0, 0.0, 0.0, 0.021211537114504155, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3418184639399641, 0.9530187941808128, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5415447555144779, 0.0, 0.16269589476095273, 0.0, 0.2704207942532035, 0.6129742049927741, 0.0, 0.0, 0.0, 0.0, 0.07078757436524862, 0.0, 0.7077105145475581, 0.34262729838304967, 0.5909987644476324, 0.0, 0.11177131716736882, 0.4359653240929594, 0.04723041930008277, 0.45146742793812006, 0.896649501844373, 0.0, 0.0, 0.6373803605539574, 0.5201327056992473, 0.10980024784153514, 0.07242729038979111, 0.0, 0.25075448235999825, 0.0, 0.14797679652158516, 0.0, 0.0, 0.0, 0.0, 0.6939917265420261, 0.9906375819864492, 0.9257686181712707, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37693205342755914, 0.5232740229070877, 0.0, 0.060081052233409524, 0.0, 0.3350366056610552, 0.0, 0.20067269510180097, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5308540215160052, 0.0, 0.0, 0.04969049276705586, 0.4729163126308979, 0.17206519556589087, 0.08142765800071788, 0.0, 0.6342266715653555, 0.28491323267898133, 0.0, 0.0, 0.0, 0.0, 0.3249315382934875, 0.27888197557292793, 0.0, 0.00024008316707892853, 0.0, 0.0, 0.4429941865632019, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08351200806876206, 0.0, 0.0, 0.3644144714136701, 0.0, 0.25850208657414453, 0.3044172676246769, 0.0, 0.0, 0.0, 0.42064905943656483, 0.5410542431685182, 0.0, 0.0, 0.0235250266831074, 0.17227146855360218, 0.0, 0.0, 0.11190852746246406, 0.0, 0.0, 0.0, 0.0, 0.14411942384547258, 0.0, 0.0, 0.4237744107411662, 0.0, 0.2972186074559964, 0.0, 0.5397314242092841, 0.4632402464002124, 0.0, 0.5786606886346829, 0.0, 0.0, 0.47789419623388885, 0.0, 0.0, 0.0, 0.10580588439167306, 0.0, 0.0, 0.7377357792707717, 0.0, 0.10579038780894534, 0.0, 0.0, 0.0, 0.045139981578782816, 0.0, 0.0, 0.11584045845995011, 0.0, 0.0, 0.019771431213384455, 0.36490249309768175, 0.35310751173091487, 1.1149184594112733, 0.0, 0.0, 0.0, 0.5857527772485364, 0.44959707967332574, 0.1008560617277558, 0.018841708015473753, 0.7001531183110316, 0.0, 0.0, 0.0, 0.5946141177445001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22666010948008636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014782177343261123, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02695824533437821, 0.0, 0.0, 0.0, 0.016838719448302838, 0.0, 0.0, 0.0, 0.08231997245985787, 0.0, 0.0, 0.0, 0.01058107131899943, 0.0, 0.05814319369896461, 0.0, 0.0, 0.0, 0.0, 0.047348036509572325, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019563192629043182, 0.027295875565384514, 0.0, 0.0, 0.0, 0.0, 0.33629932740438834, 0.0, 0.0, 0.0, 0.32394391802925426, 0.5704070896351352, 0.0, 0.14173474826201982, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6013127745008292, 0.0, 0.7561985490243033, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2959306772073682, 0.2539911401819114, 0.0, 0.0, 0.7300245622804938, 0.0, 0.2367914528076339, 0.0, 0.0, 0.0, 0.33628005236128533, 0.025934644678944505, 0.0, 0.06026134263178236, 0.0, 0.0, 0.03662105153743461, 0.06883834406728133, 0.0, 0.0, 0.21279478779844782, 0.0, 0.0, 0.07129569822964256, 0.0, 0.09738688258412649, 0.06246220065656896, 0.0, 0.059045957391141574, 0.0, 0.006923917652553149, 0.051490108793371445, 0.0, 0.16644468958034975, 0.19015418061029377, 0.10882376279915186, 0.06400227485256132, 0.2644778831991808, 0.0, 0.19849197355887785, 0.0, 0.0010725148648749981, 0.12198011848273568, 0.1548472256356659, 0.0, 0.009305339154304988, 0.0, 0.10589543929012146, 0.0, 0.0, 0.17178211974814458, 0.0, 0.23085925511580369, 0.0, 0.0, 0.0, 0.31374773386794685, 0.0, 0.4234572393895347, 0.0, 0.0, 0.0, 0.045888478821935044, 0.0, 0.0, 0.13665231391596233, 0.0, 0.05546447903889757, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15847133453228077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3148706017686264, 0.2655048691833022, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28333620483908656, 0.0, 0.0, 0.0, 0.3208604446677971, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25931056270601605, 0.0, 0.0, 0.20842242702467234, 0.0, 0.0, 0.28681902268117093, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2978149918318377, 0.0, 0.0, 0.09574337597144772, 0.0, 0.0, 0.38017827065442816, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03128644702667798, 0.0, 0.0, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = ((ex.bodies[1]).bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    result = nothing
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = (Finch).moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = (Finch).moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            B_lvl_q = B_lvl_ptr[1]
            B_lvl_q_stop = B_lvl_ptr[1 + 1]
            if B_lvl_q < B_lvl_q_stop
                B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
            else
                B_lvl_i_stop = 0
            end
            phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                if B_lvl_tbl2[B_lvl_q] < phase_start_2
                    B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                end
                while true
                    B_lvl_i = B_lvl_tbl2[B_lvl_q]
                    B_lvl_q_step = B_lvl_q
                    if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                        B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    if B_lvl_i < phase_stop_2
                        Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                        for i_6 = 1:A_lvl.shape[1]
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_6
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_6, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_6
                                            if A_lvl_tbl1[A_lvl_q] < i_6
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_6, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                        end
                        B_lvl_q = B_lvl_q_step
                    else
                        phase_stop_13 = min(B_lvl_i, phase_stop_2)
                        if B_lvl_i == phase_stop_13
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                            for i_8 = 1:A_lvl.shape[1]
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_8
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_8, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_8
                                                if A_lvl_tbl1[A_lvl_q] < i_8
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_8, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        end
                        break
                    end
                end
            end
        end
    resize!(val, A_lvl.shape[1] * B_lvl.shape[2])
    result = (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
    result
end
julia> @finch begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.2504314291953376, 0.0, 0.0, 0.9648065476989627, 0.2835978542736629, 0.13211849643271117, 0.2796391765298554, 0.22540993882752317, 0.5512218518807817, 0.5478145335437775, 0.0, 0.2291959647754706, 0.23520231615614767, 0.03296826745685447, 0.18421763750874248, 0.6035878434468992, 0.7441004608135994, 0.2535097758200323, 0.20236453274735783, 0.09228241774990334, 0.13384560784694935, 0.5269755678736615, 0.03880627317810495, 0.17704264007357723, 0.0, 0.2632287465084173, 0.0, 0.02720231717357045, 0.5246531850072871, 0.0, 0.0, 0.3360269038793386, 0.0, 0.22538882742471622, 0.22621665568148489, 0.0, 0.09095677592426996, 0.0, 0.2343687318872728, 0.0, 0.00044179348092010485, 0.07281055209847118, 0.6821285312867097, 0.0, 0.0, 0.0, 0.8944403740767686, 0.11973372388784431, 0.2613237068921269, 0.5923534538840982, 0.0, 0.0, 0.0, 0.8426453838331012, 0.0, 0.0, 1.0769929937540788, 0.43300907588983417, 0.0, 0.6905133736710757, 0.0, 0.0, 0.0, 0.07271653050953786, 0.8664857754450395, 0.08570530865158534, 0.0, 0.7169860381548754, 0.20498067747201715, 0.0, 0.23050103487101342, 0.0, 0.0, 0.9152746488430813, 0.14299878494120105, 0.44815460009097313, 0.0, 0.0, 0.0, 0.23162241551000032, 1.0326337985335274, 1.0530846396126474, 0.22109306040172927, 0.0, 0.0, 0.0, 1.0289491599002607, 0.008076930682810486, 0.037079469071792215, 0.0, 0.03308196940629956, 0.07165705922647192, 0.18447863887953236, 0.0, 0.24885144181470292, 0.0, 0.0, 0.0, 0.7193963651558962, 0.0, 0.6007574484421223, 0.0, 0.11361690641306497, 0.0, 0.0, 0.0, 0.0, 0.5189539173895105, 0.027360695267410504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.769945776489901, 0.00013219515459577855, 0.0, 0.0, 0.0, 0.0, 0.21518747264967641, 0.0, 0.0, 0.0, 0.10980938651098104, 0.07347552139071808, 0.0, 0.3613462418752601, 0.04465135919773286, 0.027102240057490925, 0.0, 0.0, 0.2594566815123038, 0.417973871558788, 0.02337304044012675, 0.5498222432776159, 0.011558313103301096, 0.13519961718643905, 0.0830809338177889, 0.0, 0.0, 0.0, 0.17989871031119878, 0.05233157238393502, 0.0, 0.051508809526025434, 0.225133841185459, 0.18950167625902217, 0.0, 0.0, 0.0, 0.20173577654712102, 0.7662693488070871, 0.0, 0.33259178463685746, 0.13865467767460202, 0.0, 0.8944580603877651, 0.0, 0.060682785060590494, 0.06051599339484606, 0.0, 0.0, 0.0, 0.12305720211410776, 0.10081888448105404, 0.0, 0.0, 0.2522377522294325, 0.0, 0.12103967793114603, 0.06077105223878225, 0.0, 0.0, 0.5586258592154407, 0.0, 0.0, 0.0, 0.2984848816909581, 0.029564782247173068, 0.6276543733906952, 0.0, 0.07353625635562432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08001934416956356, 0.13808411155288577, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0013608908425346389, 0.15477792583676012, 0.8924302032481646, 0.0, 0.13515079773758784, 0.0, 0.059793517595463114, 0.0, 0.04933055693313389, 0.0, 0.0, 0.3342781850907459, 0.02578025634698341, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1229501707927792, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287208489034296, 0.0, 0.0, 0.28149278769379354, 0.0, 0.0, 0.0, 0.031923890851782225, 0.23740383631799186, 0.0, 0.0, 0.0, 0.0, 0.06362127081034188, 0.2629034525588847, 0.0, 0.9151807425831917, 0.0, 0.0, 0.0, 0.6693739198309284, 0.0, 0.0, 0.2061930933620024, 0.27528937724536684, 0.0, 0.0, 0.0, 0.0, 0.2675780483794807, 0.02063619759744684, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5460074055587408, 0.0, 0.6541555536266587, 0.0, 0.0, 0.07150154126300666, 0.0, 0.15048122306270975, 0.034115472912426725, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.050926612139623545, 0.21044506008920905, 0.0, 0.0, 0.4253961501964202, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5242476712268254, 0.0, 0.0, 0.6721198397890595, 0.02913210340779019, 0.0, 0.20059561631676462, 0.0, 0.29433701033990906, 0.0, 0.0, 0.0, 0.12559144077146528, 0.0, 0.0, 0.32229898127496104, 0.0, 0.0, 0.05500938293182685, 0.1114468235961369, 0.09601244908001759, 0.255231140185047, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05242264559033129, 0.07189305704474794, 0.29708512061039594, 0.0, 0.0, 0.16167981650220184, 0.0, 0.0, 0.42894283906704944, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24120550186102305, 0.002462183433889286, 0.1185465354728046, 0.0, 0.0, 0.20924909285728116, 0.0, 0.2383879501918044, 0.0, 0.08928505124693381, 0.0, 0.0, 0.2291277246538886, 0.0, 0.0, 0.039107088380868994, 0.03339412468553525, 0.0, 0.17867986571670036, 0.0, 0.15885581639487184, 0.0, 0.4770109811329409, 0.46521958209705216, 0.08213268605220507, 0.8570033319080816, 0.5762497735248863, 0.025109002675938377, 0.0, 0.0, 0.0, 0.006062678222071191, 0.6895253688974624, 0.0, 0.0, 0.14425623656373723, 0.0, 0.26058653129843573, 0.2499129405223175, 0.5556369963578781, 0.0, 0.0, 0.14846473797228377, 1.311439497677356, 0.0, 0.0, 0.0, 0.14844299347189643, 0.0, 0.06204384333300274, 0.5085214657536261, 0.06333953518463412, 0.0, 0.4087271917522072, 0.56471815720299, 0.0, 0.7613475113276124, 0.31092441757595435, 0.25657570652463557, 0.0, 0.6284352010004853, 0.0, 0.44245087677575873, 0.0, 0.5840306487685422, 0.886297089335757, 0.0, 0.2141960416332438, 0.0, 0.0, 0.7455493113099605, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9841442926021067, 0.155496454991287, 0.0, 0.0, 0.07678252545804619, 0.0, 0.0, 0.3721354350358947, 0.020164712785984047, 0.0, 0.0, 0.0, 0.0, 0.1088564167894987, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9595213220321046, 0.0, 0.026588936071193094, 0.0, 0.0, 0.18249673153915044, 0.041373682209080435, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04976306812868327, 0.2056369238513726, 0.0, 0.0, 0.341205976763526, 0.0, 0.0, 0.0, 0.0, 0.5785815925839551, 0.0, 0.0, 0.0, 0.5989984098523007, 0.041848978991102986, 0.0, 0.7058966144833084, 0.0, 0.1357909427918936, 0.0, 0.0, 0.0, 0.6943234317201388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03329077451447904, 0.0, 0.16959332194846893, 0.03825181104342904, 0.0, 0.030570483496490792, 0.0, 0.181963825346749, 0.0, 0.0, 0.0861751567882612, 0.0, 0.23433890396321935, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06468139820825311, 0.4901043774165395, 0.0, 0.1652405132589817, 0.0, 0.0, 0.07483443864111192, 0.0, 0.0, 0.0, 0.2810295005684995, 0.0, 0.3898372849078984, 0.0, 0.27070466717435465, 0.5873234092956581, 0.0, 0.0, 0.8033877947986148, 0.0, 0.0, 0.0, 0.5030084273245645, 0.1643009637310288, 0.0, 0.0, 0.0, 0.052313365367879956, 0.02924539070601191, 0.0, 0.7815339827038686, 0.0, 0.631919493277437, 0.08243985181699306, 0.0, 0.0, 0.0, 0.0, 0.12868680907403285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.1259698971001804, 0.7198499769348474, 0.0, 0.41807281750787384, 0.0, 0.0, 0.6290848565988848, 0.7773412611170788, 0.0, 0.3270721417747804, 0.0, 0.0, 0.6187710034515789, 0.07069501936159496, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15113911645003905, 0.0, 0.15654877177402662, 1.1322176354521876, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2636197382380641, 0.0, 0.0, 0.34206988627899454, 0.0, 0.0, 0.2168559436204628, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12189174979618164, 0.009400555275465744, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02319896532052816, 0.09586554938115534, 0.0, 0.0, 0.47758214062526055, 0.0, 0.0, 0.0, 0.0, 0.766521270904984, 0.0, 0.0, 0.0, 0.05687724891036996, 0.0, 0.0, 0.0, 0.019988327516632248, 0.0, 0.0, 0.24020596976351885, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5319793134128888, 0.0, 0.0, 0.12310759546793347, 0.28424710350697324, 0.0, 0.9704064926727941, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.595062055789435, 0.0, 0.022922464313533127, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7479520138852049, 0.0, 0.0, 0.0, 0.0, 0.11201726895804533, 0.0, 0.0, 0.5882991104057551, 0.0, 0.0, 0.0, 0.5372591710353254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4185803721513073, 0.0, 0.4160894350708626, 0.35712090084273435, 0.0, 0.0, 0.0, 0.0, 0.3329375067097101, 0.0, 0.0, 0.0, 0.9791513934288851, 0.040171778764664906, 0.0, 0.0, 0.18774405618493867, 0.47311893679528827, 0.2590438343789052, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12077476728428499, 0.0, 0.06327325623124767, 0.0, 0.0, 0.0, 0.0, 0.34849519635188847, 0.0, 0.4382603746311635, 0.0, 0.0, 0.0, 0.09913709085437171, 0.4096661875861404, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4230910500526733, 0.0, 0.0, 0.0, 0.17776900851570368, 0.0, 0.5438917161748563, 0.0, 0.43287084617637955, 0.0, 0.0, 0.0, 0.5349746055023918, 0.0, 0.7894262258085124, 0.0, 0.0, 0.0, 0.416696264214944, 0.1414148713022835, 0.0, 0.1306712640828635, 0.5944339267365731, 0.0, 0.0, 0.0, 0.3098486230376566, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27475814956764166, 0.0, 0.0, 0.0, 0.572874722755641, 0.7993131511508778, 0.0, 0.2979949962228512, 0.14400098705349634, 0.0, 0.0, 0.02684711041673838, 0.0, 0.0575193250192605, 0.13038147684896279, 0.07982177723780243, 0.0, 0.0, 0.22370954598082965, 0.0, 0.0, 0.0, 0.16207849359014098, 0.8780589957563658, 0.0, 0.0, 0.0, 0.03773454641900064, 0.0, 0.190720074864707, 0.03823162707299666, 0.0, 0.027092618673790602, 0.07782668325822914, 0.0, 0.0, 0.025612768291380317, 0.0, 0.1339183762706947, 0.03147511447091019, 0.0, 0.0, 0.22256660522037766, 0.0, 0.050981846070955524, 0.33080849696175413, 0.22670468547648376, 0.0, 0.0, 0.2394131641393997, 0.0, 0.6763225254232694, 0.0, 0.32609931596811353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02551395955593748, 0.0, 0.19870976845837512, 0.0, 0.8838430389981123, 0.0, 0.8900590234719281, 0.0, 0.1395885727389325, 0.39274638865688727, 0.13258182339863903, 1.0427544725112363, 0.007925765580094073, 0.0, 0.0, 0.9248219821941955, 0.0, 0.38782207933590473, 0.20331266090801786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4620448777897825, 0.5673723076761421, 0.13444042454417618, 0.0, 0.0012927651989113214, 0.0, 0.0, 0.0, 0.3049721916420315, 0.017266202832261764, 0.0, 0.0, 0.07071993264415252, 0.04567425634491729, 0.3943633692304047, 0.0, 0.15861807151929683, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05848946000294637, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0002825960061227963, 0.18362488935764, 0.0, 0.0, 0.0, 0.16115194339742445, 0.5974334511167834, 0.0, 0.04024167944046561, 0.33382381844541353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6087942383002851, 0.0, 0.0, 0.7807479082612564, 0.3974967659255351, 1.5487027213337248, 0.03765559375444021, 0.17692333788215128, 0.23426356503546714, 0.0, 0.22388851869475412, 0.04007956243736881, 1.0262040210122396, 0.0, 0.7972348331243138, 1.0049259227957363, 0.051433760809377164, 0.0, 0.8283458296766737, 0.0, 0.7561556098507006, 0.0, 0.0, 0.0, 0.24907496608552146, 0.0, 0.5011916402324644, 0.1747189814102487, 0.9899595982235516, 0.6164188940137083, 0.11559029428984438, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.593849645786338, 0.0, 0.0, 0.0, 0.0, 0.13660851844466146, 0.030970403587585463, 0.23157107362581741, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13749939723080143, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7110898100418397, 0.0, 0.0, 0.0, 0.46081314114808425, 0.7109856621285974, 0.2884350146527927, 0.0, 0.0, 0.30337236072169704, 0.06739814120009983, 0.0, 0.7785291912169707, 0.0, 0.0, 0.13287789565404662, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3593823347180517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4624623396236477, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024288127378650458, 0.0, 0.793430084070044, 0.0, 0.0, 0.07003161792104487, 0.0, 0.390525043836891, 0.7636039620295462, 0.06950112764284275, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019321154098251664, 0.0, 0.18651557279691772, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05792018405822482, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18265223354453325, 0.12454723256219671, 0.0, 0.0002798455087109209, 0.0, 0.0, 0.11611318457978691, 0.05398921986713746, 0.0, 0.8752290763913787, 0.0, 0.0, 0.0, 0.0, 0.13107895831547542, 0.0, 0.0, 0.45521248179167606, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11685661958627322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44522459151527227, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26435993065610003, 0.021758541234477975, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2146225974372531, 0.0, 0.13433790512658406, 0.030902835047588124, 0.0, 0.0, 0.03139052000724026, 0.0, 0.0, 0.7200671942026683, 0.0, 0.0, 0.05946084260814123, 0.0, 0.14663715828117352, 0.03324398818324252, 0.008023890708802274, 0.05967012120515395, 0.0, 0.0, 0.0, 0.10840406404650259, 0.0, 0.0, 0.0, 0.23002554078955822, 0.0, 0.21539070760796813, 0.0, 0.15809846801096508, 0.0, 0.0, 0.023379160669788793, 0.06919243918501702, 0.0, 0.4184368340983768, 0.0, 0.0, 0.0, 0.025499570825031452, 0.0, 0.054632252014063354, 0.12383722685885369, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08100665304577326, 0.0, 0.0, 0.0, 1.137863121002405, 0.0, 0.0, 0.18114724382910816, 1.0175246627827756, 0.0, 0.8319051265218698, 0.7408406030496757, 0.0, 0.0, 0.4793159619581115, 0.26457362772519605, 0.7690370272987045, 0.02989528102721059, 0.0, 0.0, 0.0, 0.34236772701032764, 0.22777638147070367, 0.20013535639204974, 0.7445351180710278, 0.0, 0.0, 0.32085924041220354, 0.0, 0.0, 0.0, 0.5922078604864867, 0.0, 0.0, 0.18324289767504168, 0.0, 0.0, 0.05247138335835671, 0.48819810625100485, 0.0, 0.4058243472838195, 0.0, 0.0, 0.6944817185701155, 0.21684000174364465, 0.0, 0.12484778900119534, 0.0, 0.5314768166536554, 0.0, 0.0, 0.0, 0.0, 0.11888353831534566, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4539474830982486, 0.0, 0.12931153719488578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23351697026332235, 0.0, 0.0, 0.0, 0.0, 0.2291429172294868, 0.4258647172142874, 0.0, 0.0, 0.0, 0.22479141295616378, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05853977920447593, 0.0, 0.16715127001769428, 0.0, 0.3712702672746247, 0.13508244363790445, 0.06392604260421969, 0.5912989058272206, 0.4437811402099471, 0.0, 0.0, 0.0, 0.0, 0.005827558703832637, 0.6627845677844773, 0.0, 0.0, 0.0, 0.30904361146056275, 0.6816789334794826, 0.1436649772664439, 0.16334297841077242, 0.0, 0.0, 0.7882286990025497, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7753056808075847, 0.035768380854023674, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44917737188494944, 0.0, 0.18937379890511313, 0.06882274914369534, 0.0, 0.0, 0.0, 0.009287224889287345, 0.06906497794085972, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26624227636214304, 0.0, 0.0, 0.0, 0.1829905316954157, 0.0, 0.06582989974405196, 0.2036305981722081, 0.08008655235603437, 0.0, 0.0, 0.0, 0.0, 0.0, 0.41729810318329397, 0.0, 0.0, 0.0, 0.0, 0.0, 0.037951024806923965, 0.0, 0.0, 0.0, 0.0, 0.41651629143780994, 0.0, 0.2053954771505675, 0.29245802337269494, 0.0368786813523771, 0.0, 0.38440171693921094, 0.0, 1.0326219326945734, 0.0, 0.02604768015780454, 0.9153304998316836, 0.0, 0.0, 0.0, 0.021211537114504155, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3418184639399641, 0.9530187941808128, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5415447555144779, 0.0, 0.16269589476095273, 0.0, 0.2704207942532035, 0.6129742049927741, 0.0, 0.0, 0.0, 0.0, 0.07078757436524862, 0.0, 0.7077105145475581, 0.34262729838304967, 0.5909987644476324, 0.0, 0.11177131716736882, 0.4359653240929594, 0.04723041930008277, 0.45146742793812006, 0.896649501844373, 0.0, 0.0, 0.6373803605539574, 0.5201327056992473, 0.10980024784153514, 0.07242729038979111, 0.0, 0.25075448235999825, 0.0, 0.14797679652158516, 0.0, 0.0, 0.0, 0.0, 0.6939917265420261, 0.9906375819864492, 0.9257686181712707, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37693205342755914, 0.5232740229070877, 0.0, 0.060081052233409524, 0.0, 0.3350366056610552, 0.0, 0.20067269510180097, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5308540215160052, 0.0, 0.0, 0.04969049276705586, 0.4729163126308979, 0.17206519556589087, 0.08142765800071788, 0.0, 0.6342266715653555, 0.28491323267898133, 0.0, 0.0, 0.0, 0.0, 0.3249315382934875, 0.27888197557292793, 0.0, 0.00024008316707892853, 0.0, 0.0, 0.4429941865632019, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08351200806876206, 0.0, 0.0, 0.3644144714136701, 0.0, 0.25850208657414453, 0.3044172676246769, 0.0, 0.0, 0.0, 0.42064905943656483, 0.5410542431685182, 0.0, 0.0, 0.0235250266831074, 0.17227146855360218, 0.0, 0.0, 0.11190852746246406, 0.0, 0.0, 0.0, 0.0, 0.14411942384547258, 0.0, 0.0, 0.4237744107411662, 0.0, 0.2972186074559964, 0.0, 0.5397314242092841, 0.4632402464002124, 0.0, 0.5786606886346829, 0.0, 0.0, 0.47789419623388885, 0.0, 0.0, 0.0, 0.10580588439167306, 0.0, 0.0, 0.7377357792707717, 0.0, 0.10579038780894534, 0.0, 0.0, 0.0, 0.045139981578782816, 0.0, 0.0, 0.11584045845995011, 0.0, 0.0, 0.019771431213384455, 0.36490249309768175, 0.35310751173091487, 1.1149184594112733, 0.0, 0.0, 0.0, 0.5857527772485364, 0.44959707967332574, 0.1008560617277558, 0.018841708015473753, 0.7001531183110316, 0.0, 0.0, 0.0, 0.5946141177445001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22666010948008636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014782177343261123, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02695824533437821, 0.0, 0.0, 0.0, 0.016838719448302838, 0.0, 0.0, 0.0, 0.08231997245985787, 0.0, 0.0, 0.0, 0.01058107131899943, 0.0, 0.05814319369896461, 0.0, 0.0, 0.0, 0.0, 0.047348036509572325, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019563192629043182, 0.027295875565384514, 0.0, 0.0, 0.0, 0.0, 0.33629932740438834, 0.0, 0.0, 0.0, 0.32394391802925426, 0.5704070896351352, 0.0, 0.14173474826201982, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6013127745008292, 0.0, 0.7561985490243033, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2959306772073682, 0.2539911401819114, 0.0, 0.0, 0.7300245622804938, 0.0, 0.2367914528076339, 0.0, 0.0, 0.0, 0.33628005236128533, 0.025934644678944505, 0.0, 0.06026134263178236, 0.0, 0.0, 0.03662105153743461, 0.06883834406728133, 0.0, 0.0, 0.21279478779844782, 0.0, 0.0, 0.07129569822964256, 0.0, 0.09738688258412649, 0.06246220065656896, 0.0, 0.059045957391141574, 0.0, 0.006923917652553149, 0.051490108793371445, 0.0, 0.16644468958034975, 0.19015418061029377, 0.10882376279915186, 0.06400227485256132, 0.2644778831991808, 0.0, 0.19849197355887785, 0.0, 0.0010725148648749981, 0.12198011848273568, 0.1548472256356659, 0.0, 0.009305339154304988, 0.0, 0.10589543929012146, 0.0, 0.0, 0.17178211974814458, 0.0, 0.23085925511580369, 0.0, 0.0, 0.0, 0.31374773386794685, 0.0, 0.4234572393895347, 0.0, 0.0, 0.0, 0.045888478821935044, 0.0, 0.0, 0.13665231391596233, 0.0, 0.05546447903889757, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15847133453228077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3148706017686264, 0.2655048691833022, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28333620483908656, 0.0, 0.0, 0.0, 0.3208604446677971, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25931056270601605, 0.0, 0.0, 0.20842242702467234, 0.0, 0.0, 0.28681902268117093, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2978149918318377, 0.0, 0.0, 0.09574337597144772, 0.0, 0.0, 0.38017827065442816, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03128644702667798, 0.0, 0.0, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = ((ex.bodies[1]).bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    result = nothing
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    B_lvl_q = B_lvl_ptr[1]
    B_lvl_q_stop = B_lvl_ptr[1 + 1]
    if B_lvl_q < B_lvl_q_stop
        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
    else
        B_lvl_i_stop = 0
    end
    phase_stop = min(B_lvl.shape[2], B_lvl_i_stop)
    if phase_stop >= 1
        if B_lvl_tbl2[B_lvl_q] < 1
            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
        end
        while true
            B_lvl_i = B_lvl_tbl2[B_lvl_q]
            B_lvl_q_step = B_lvl_q
            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
            end
            if B_lvl_i < phase_stop
                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                val = Ct_lvl_2_val
                Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                B_lvl_tbl1_2 = B_lvl_tbl1
                B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                B_lvl_tbl2_2 = B_lvl_tbl2
                val_2 = B_lvl_val
                B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
                A_lvl_ptr_2 = A_lvl_ptr
                A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                A_lvl_tbl1_2 = A_lvl_tbl1
                A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                A_lvl_tbl2_2 = A_lvl_tbl2
                A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                val_3 = A_lvl_val
                A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
                Threads.@threads for i_9 = 1:Threads.nthreads()
                        phase_start_6 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_9), Threads.nthreads()))
                        phase_stop_7 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_9, Threads.nthreads()))
                        if phase_stop_7 >= phase_start_6
                            for i_12 = phase_start_6:phase_stop_7
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_12
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_8 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_8 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_8
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_9 = min(B_lvl_i_2, phase_stop_8, A_lvl_i)
                                        if A_lvl_i == phase_stop_9 && B_lvl_i_2 == phase_stop_9
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_10 = min(i_12, A_lvl_i_stop_2)
                                            if phase_stop_10 >= i_12
                                                if A_lvl_tbl1[A_lvl_q] < i_12
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_12, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_10
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_12 = min(A_lvl_i_2, phase_stop_10)
                                                        if A_lvl_i_2 == phase_stop_12
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_9
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_9
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_9 + 1
                                    end
                                end
                            end
                        end
                    end
                Ct_lvl_2_val = val
                B_lvl_tbl1 = B_lvl_tbl1_2
                B_lvl_tbl2 = B_lvl_tbl2_2
                B_lvl_val = val_2
                A_lvl_ptr = A_lvl_ptr_2
                A_lvl_tbl1 = A_lvl_tbl1_2
                A_lvl_tbl2 = A_lvl_tbl2_2
                A_lvl_val = val_3
                B_lvl_q = B_lvl_q_step
            else
                phase_stop_18 = min(B_lvl_i, phase_stop)
                if B_lvl_i == phase_stop_18
                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_18
                    val_4 = Ct_lvl_2_val
                    Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                    B_lvl_tbl1_3 = B_lvl_tbl1
                    B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                    B_lvl_tbl2_3 = B_lvl_tbl2
                    val_5 = B_lvl_val
                    B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
                    A_lvl_ptr_3 = A_lvl_ptr
                    A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                    A_lvl_tbl1_3 = A_lvl_tbl1
                    A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                    A_lvl_tbl2_3 = A_lvl_tbl2
                    A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                    val_6 = A_lvl_val
                    A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
                    Threads.@threads for i_19 = 1:Threads.nthreads()
                            phase_start_21 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_19), Threads.nthreads()))
                            phase_stop_23 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_19, Threads.nthreads()))
                            if phase_stop_23 >= phase_start_21
                                for i_22 = phase_start_21:phase_stop_23
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_22
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_24 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_24 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_24
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_25 = min(B_lvl_i_2, A_lvl_i, phase_stop_24)
                                            if A_lvl_i == phase_stop_25 && B_lvl_i_2 == phase_stop_25
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_26 = min(i_22, A_lvl_i_stop_4)
                                                if phase_stop_26 >= i_22
                                                    if A_lvl_tbl1[A_lvl_q] < i_22
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_22, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_26
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_28 = min(A_lvl_i_4, phase_stop_26)
                                                            if A_lvl_i_4 == phase_stop_28
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_25
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_25
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_25 + 1
                                        end
                                    end
                                end
                            end
                        end
                    Ct_lvl_2_val = val_4
                    B_lvl_tbl1 = B_lvl_tbl1_3
                    B_lvl_tbl2 = B_lvl_tbl2_3
                    B_lvl_val = val_5
                    A_lvl_ptr = A_lvl_ptr_3
                    A_lvl_tbl1 = A_lvl_tbl1_3
                    A_lvl_tbl2 = A_lvl_tbl2_3
                    A_lvl_val = val_6
                    B_lvl_q = B_lvl_q_step
                end
                break
            end
        end
    end
    resize!(Ct_lvl_2_val, A_lvl.shape[1] * B_lvl.shape[2])
    result = (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
    result
end
julia> @finch begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.2504314291953376, 0.0, 0.0, 0.9648065476989627, 0.2835978542736629, 0.13211849643271117, 0.2796391765298554, 0.22540993882752317, 0.5512218518807817, 0.5478145335437775, 0.0, 0.2291959647754706, 0.23520231615614767, 0.03296826745685447, 0.18421763750874248, 0.6035878434468992, 0.7441004608135994, 0.2535097758200323, 0.20236453274735783, 0.09228241774990334, 0.13384560784694935, 0.5269755678736615, 0.03880627317810495, 0.17704264007357723, 0.0, 0.2632287465084173, 0.0, 0.02720231717357045, 0.5246531850072871, 0.0, 0.0, 0.3360269038793386, 0.0, 0.22538882742471622, 0.22621665568148489, 0.0, 0.09095677592426996, 0.0, 0.2343687318872728, 0.0, 0.00044179348092010485, 0.07281055209847118, 0.6821285312867097, 0.0, 0.0, 0.0, 0.8944403740767686, 0.11973372388784431, 0.2613237068921269, 0.5923534538840982, 0.0, 0.0, 0.0, 0.8426453838331012, 0.0, 0.0, 1.0769929937540788, 0.43300907588983417, 0.0, 0.6905133736710757, 0.0, 0.0, 0.0, 0.07271653050953786, 0.8664857754450395, 0.08570530865158534, 0.0, 0.7169860381548754, 0.20498067747201715, 0.0, 0.23050103487101342, 0.0, 0.0, 0.9152746488430813, 0.14299878494120105, 0.44815460009097313, 0.0, 0.0, 0.0, 0.23162241551000032, 1.0326337985335274, 1.0530846396126474, 0.22109306040172927, 0.0, 0.0, 0.0, 1.0289491599002607, 0.008076930682810486, 0.037079469071792215, 0.0, 0.03308196940629956, 0.07165705922647192, 0.18447863887953236, 0.0, 0.24885144181470292, 0.0, 0.0, 0.0, 0.7193963651558962, 0.0, 0.6007574484421223, 0.0, 0.11361690641306497, 0.0, 0.0, 0.0, 0.0, 0.5189539173895105, 0.027360695267410504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.769945776489901, 0.00013219515459577855, 0.0, 0.0, 0.0, 0.0, 0.21518747264967641, 0.0, 0.0, 0.0, 0.10980938651098104, 0.07347552139071808, 0.0, 0.3613462418752601, 0.04465135919773286, 0.027102240057490925, 0.0, 0.0, 0.2594566815123038, 0.417973871558788, 0.02337304044012675, 0.5498222432776159, 0.011558313103301096, 0.13519961718643905, 0.0830809338177889, 0.0, 0.0, 0.0, 0.17989871031119878, 0.05233157238393502, 0.0, 0.051508809526025434, 0.225133841185459, 0.18950167625902217, 0.0, 0.0, 0.0, 0.20173577654712102, 0.7662693488070871, 0.0, 0.33259178463685746, 0.13865467767460202, 0.0, 0.8944580603877651, 0.0, 0.060682785060590494, 0.06051599339484606, 0.0, 0.0, 0.0, 0.12305720211410776, 0.10081888448105404, 0.0, 0.0, 0.2522377522294325, 0.0, 0.12103967793114603, 0.06077105223878225, 0.0, 0.0, 0.5586258592154407, 0.0, 0.0, 0.0, 0.2984848816909581, 0.029564782247173068, 0.6276543733906952, 0.0, 0.07353625635562432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08001934416956356, 0.13808411155288577, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0013608908425346389, 0.15477792583676012, 0.8924302032481646, 0.0, 0.13515079773758784, 0.0, 0.059793517595463114, 0.0, 0.04933055693313389, 0.0, 0.0, 0.3342781850907459, 0.02578025634698341, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1229501707927792, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287208489034296, 0.0, 0.0, 0.28149278769379354, 0.0, 0.0, 0.0, 0.031923890851782225, 0.23740383631799186, 0.0, 0.0, 0.0, 0.0, 0.06362127081034188, 0.2629034525588847, 0.0, 0.9151807425831917, 0.0, 0.0, 0.0, 0.6693739198309284, 0.0, 0.0, 0.2061930933620024, 0.27528937724536684, 0.0, 0.0, 0.0, 0.0, 0.2675780483794807, 0.02063619759744684, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5460074055587408, 0.0, 0.6541555536266587, 0.0, 0.0, 0.07150154126300666, 0.0, 0.15048122306270975, 0.034115472912426725, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.050926612139623545, 0.21044506008920905, 0.0, 0.0, 0.4253961501964202, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5242476712268254, 0.0, 0.0, 0.6721198397890595, 0.02913210340779019, 0.0, 0.20059561631676462, 0.0, 0.29433701033990906, 0.0, 0.0, 0.0, 0.12559144077146528, 0.0, 0.0, 0.32229898127496104, 0.0, 0.0, 0.05500938293182685, 0.1114468235961369, 0.09601244908001759, 0.255231140185047, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05242264559033129, 0.07189305704474794, 0.29708512061039594, 0.0, 0.0, 0.16167981650220184, 0.0, 0.0, 0.42894283906704944, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24120550186102305, 0.002462183433889286, 0.1185465354728046, 0.0, 0.0, 0.20924909285728116, 0.0, 0.2383879501918044, 0.0, 0.08928505124693381, 0.0, 0.0, 0.2291277246538886, 0.0, 0.0, 0.039107088380868994, 0.03339412468553525, 0.0, 0.17867986571670036, 0.0, 0.15885581639487184, 0.0, 0.4770109811329409, 0.46521958209705216, 0.08213268605220507, 0.8570033319080816, 0.5762497735248863, 0.025109002675938377, 0.0, 0.0, 0.0, 0.006062678222071191, 0.6895253688974624, 0.0, 0.0, 0.14425623656373723, 0.0, 0.26058653129843573, 0.2499129405223175, 0.5556369963578781, 0.0, 0.0, 0.14846473797228377, 1.311439497677356, 0.0, 0.0, 0.0, 0.14844299347189643, 0.0, 0.06204384333300274, 0.5085214657536261, 0.06333953518463412, 0.0, 0.4087271917522072, 0.56471815720299, 0.0, 0.7613475113276124, 0.31092441757595435, 0.25657570652463557, 0.0, 0.6284352010004853, 0.0, 0.44245087677575873, 0.0, 0.5840306487685422, 0.886297089335757, 0.0, 0.2141960416332438, 0.0, 0.0, 0.7455493113099605, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9841442926021067, 0.155496454991287, 0.0, 0.0, 0.07678252545804619, 0.0, 0.0, 0.3721354350358947, 0.020164712785984047, 0.0, 0.0, 0.0, 0.0, 0.1088564167894987, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9595213220321046, 0.0, 0.026588936071193094, 0.0, 0.0, 0.18249673153915044, 0.041373682209080435, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04976306812868327, 0.2056369238513726, 0.0, 0.0, 0.341205976763526, 0.0, 0.0, 0.0, 0.0, 0.5785815925839551, 0.0, 0.0, 0.0, 0.5989984098523007, 0.041848978991102986, 0.0, 0.7058966144833084, 0.0, 0.1357909427918936, 0.0, 0.0, 0.0, 0.6943234317201388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03329077451447904, 0.0, 0.16959332194846893, 0.03825181104342904, 0.0, 0.030570483496490792, 0.0, 0.181963825346749, 0.0, 0.0, 0.0861751567882612, 0.0, 0.23433890396321935, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06468139820825311, 0.4901043774165395, 0.0, 0.1652405132589817, 0.0, 0.0, 0.07483443864111192, 0.0, 0.0, 0.0, 0.2810295005684995, 0.0, 0.3898372849078984, 0.0, 0.27070466717435465, 0.5873234092956581, 0.0, 0.0, 0.8033877947986148, 0.0, 0.0, 0.0, 0.5030084273245645, 0.1643009637310288, 0.0, 0.0, 0.0, 0.052313365367879956, 0.02924539070601191, 0.0, 0.7815339827038686, 0.0, 0.631919493277437, 0.08243985181699306, 0.0, 0.0, 0.0, 0.0, 0.12868680907403285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.1259698971001804, 0.7198499769348474, 0.0, 0.41807281750787384, 0.0, 0.0, 0.6290848565988848, 0.7773412611170788, 0.0, 0.3270721417747804, 0.0, 0.0, 0.6187710034515789, 0.07069501936159496, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15113911645003905, 0.0, 0.15654877177402662, 1.1322176354521876, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2636197382380641, 0.0, 0.0, 0.34206988627899454, 0.0, 0.0, 0.2168559436204628, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12189174979618164, 0.009400555275465744, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02319896532052816, 0.09586554938115534, 0.0, 0.0, 0.47758214062526055, 0.0, 0.0, 0.0, 0.0, 0.766521270904984, 0.0, 0.0, 0.0, 0.05687724891036996, 0.0, 0.0, 0.0, 0.019988327516632248, 0.0, 0.0, 0.24020596976351885, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5319793134128888, 0.0, 0.0, 0.12310759546793347, 0.28424710350697324, 0.0, 0.9704064926727941, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.595062055789435, 0.0, 0.022922464313533127, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7479520138852049, 0.0, 0.0, 0.0, 0.0, 0.11201726895804533, 0.0, 0.0, 0.5882991104057551, 0.0, 0.0, 0.0, 0.5372591710353254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4185803721513073, 0.0, 0.4160894350708626, 0.35712090084273435, 0.0, 0.0, 0.0, 0.0, 0.3329375067097101, 0.0, 0.0, 0.0, 0.9791513934288851, 0.040171778764664906, 0.0, 0.0, 0.18774405618493867, 0.47311893679528827, 0.2590438343789052, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12077476728428499, 0.0, 0.06327325623124767, 0.0, 0.0, 0.0, 0.0, 0.34849519635188847, 0.0, 0.4382603746311635, 0.0, 0.0, 0.0, 0.09913709085437171, 0.4096661875861404, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4230910500526733, 0.0, 0.0, 0.0, 0.17776900851570368, 0.0, 0.5438917161748563, 0.0, 0.43287084617637955, 0.0, 0.0, 0.0, 0.5349746055023918, 0.0, 0.7894262258085124, 0.0, 0.0, 0.0, 0.416696264214944, 0.1414148713022835, 0.0, 0.1306712640828635, 0.5944339267365731, 0.0, 0.0, 0.0, 0.3098486230376566, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27475814956764166, 0.0, 0.0, 0.0, 0.572874722755641, 0.7993131511508778, 0.0, 0.2979949962228512, 0.14400098705349634, 0.0, 0.0, 0.02684711041673838, 0.0, 0.0575193250192605, 0.13038147684896279, 0.07982177723780243, 0.0, 0.0, 0.22370954598082965, 0.0, 0.0, 0.0, 0.16207849359014098, 0.8780589957563658, 0.0, 0.0, 0.0, 0.03773454641900064, 0.0, 0.190720074864707, 0.03823162707299666, 0.0, 0.027092618673790602, 0.07782668325822914, 0.0, 0.0, 0.025612768291380317, 0.0, 0.1339183762706947, 0.03147511447091019, 0.0, 0.0, 0.22256660522037766, 0.0, 0.050981846070955524, 0.33080849696175413, 0.22670468547648376, 0.0, 0.0, 0.2394131641393997, 0.0, 0.6763225254232694, 0.0, 0.32609931596811353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02551395955593748, 0.0, 0.19870976845837512, 0.0, 0.8838430389981123, 0.0, 0.8900590234719281, 0.0, 0.1395885727389325, 0.39274638865688727, 0.13258182339863903, 1.0427544725112363, 0.007925765580094073, 0.0, 0.0, 0.9248219821941955, 0.0, 0.38782207933590473, 0.20331266090801786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4620448777897825, 0.5673723076761421, 0.13444042454417618, 0.0, 0.0012927651989113214, 0.0, 0.0, 0.0, 0.3049721916420315, 0.017266202832261764, 0.0, 0.0, 0.07071993264415252, 0.04567425634491729, 0.3943633692304047, 0.0, 0.15861807151929683, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05848946000294637, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0002825960061227963, 0.18362488935764, 0.0, 0.0, 0.0, 0.16115194339742445, 0.5974334511167834, 0.0, 0.04024167944046561, 0.33382381844541353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6087942383002851, 0.0, 0.0, 0.7807479082612564, 0.3974967659255351, 1.5487027213337248, 0.03765559375444021, 0.17692333788215128, 0.23426356503546714, 0.0, 0.22388851869475412, 0.04007956243736881, 1.0262040210122396, 0.0, 0.7972348331243138, 1.0049259227957363, 0.051433760809377164, 0.0, 0.8283458296766737, 0.0, 0.7561556098507006, 0.0, 0.0, 0.0, 0.24907496608552146, 0.0, 0.5011916402324644, 0.1747189814102487, 0.9899595982235516, 0.6164188940137083, 0.11559029428984438, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.593849645786338, 0.0, 0.0, 0.0, 0.0, 0.13660851844466146, 0.030970403587585463, 0.23157107362581741, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13749939723080143, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7110898100418397, 0.0, 0.0, 0.0, 0.46081314114808425, 0.7109856621285974, 0.2884350146527927, 0.0, 0.0, 0.30337236072169704, 0.06739814120009983, 0.0, 0.7785291912169707, 0.0, 0.0, 0.13287789565404662, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3593823347180517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4624623396236477, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024288127378650458, 0.0, 0.793430084070044, 0.0, 0.0, 0.07003161792104487, 0.0, 0.390525043836891, 0.7636039620295462, 0.06950112764284275, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019321154098251664, 0.0, 0.18651557279691772, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05792018405822482, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18265223354453325, 0.12454723256219671, 0.0, 0.0002798455087109209, 0.0, 0.0, 0.11611318457978691, 0.05398921986713746, 0.0, 0.8752290763913787, 0.0, 0.0, 0.0, 0.0, 0.13107895831547542, 0.0, 0.0, 0.45521248179167606, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11685661958627322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44522459151527227, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26435993065610003, 0.021758541234477975, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2146225974372531, 0.0, 0.13433790512658406, 0.030902835047588124, 0.0, 0.0, 0.03139052000724026, 0.0, 0.0, 0.7200671942026683, 0.0, 0.0, 0.05946084260814123, 0.0, 0.14663715828117352, 0.03324398818324252, 0.008023890708802274, 0.05967012120515395, 0.0, 0.0, 0.0, 0.10840406404650259, 0.0, 0.0, 0.0, 0.23002554078955822, 0.0, 0.21539070760796813, 0.0, 0.15809846801096508, 0.0, 0.0, 0.023379160669788793, 0.06919243918501702, 0.0, 0.4184368340983768, 0.0, 0.0, 0.0, 0.025499570825031452, 0.0, 0.054632252014063354, 0.12383722685885369, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08100665304577326, 0.0, 0.0, 0.0, 1.137863121002405, 0.0, 0.0, 0.18114724382910816, 1.0175246627827756, 0.0, 0.8319051265218698, 0.7408406030496757, 0.0, 0.0, 0.4793159619581115, 0.26457362772519605, 0.7690370272987045, 0.02989528102721059, 0.0, 0.0, 0.0, 0.34236772701032764, 0.22777638147070367, 0.20013535639204974, 0.7445351180710278, 0.0, 0.0, 0.32085924041220354, 0.0, 0.0, 0.0, 0.5922078604864867, 0.0, 0.0, 0.18324289767504168, 0.0, 0.0, 0.05247138335835671, 0.48819810625100485, 0.0, 0.4058243472838195, 0.0, 0.0, 0.6944817185701155, 0.21684000174364465, 0.0, 0.12484778900119534, 0.0, 0.5314768166536554, 0.0, 0.0, 0.0, 0.0, 0.11888353831534566, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4539474830982486, 0.0, 0.12931153719488578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23351697026332235, 0.0, 0.0, 0.0, 0.0, 0.2291429172294868, 0.4258647172142874, 0.0, 0.0, 0.0, 0.22479141295616378, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05853977920447593, 0.0, 0.16715127001769428, 0.0, 0.3712702672746247, 0.13508244363790445, 0.06392604260421969, 0.5912989058272206, 0.4437811402099471, 0.0, 0.0, 0.0, 0.0, 0.005827558703832637, 0.6627845677844773, 0.0, 0.0, 0.0, 0.30904361146056275, 0.6816789334794826, 0.1436649772664439, 0.16334297841077242, 0.0, 0.0, 0.7882286990025497, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7753056808075847, 0.035768380854023674, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44917737188494944, 0.0, 0.18937379890511313, 0.06882274914369534, 0.0, 0.0, 0.0, 0.009287224889287345, 0.06906497794085972, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26624227636214304, 0.0, 0.0, 0.0, 0.1829905316954157, 0.0, 0.06582989974405196, 0.2036305981722081, 0.08008655235603437, 0.0, 0.0, 0.0, 0.0, 0.0, 0.41729810318329397, 0.0, 0.0, 0.0, 0.0, 0.0, 0.037951024806923965, 0.0, 0.0, 0.0, 0.0, 0.41651629143780994, 0.0, 0.2053954771505675, 0.29245802337269494, 0.0368786813523771, 0.0, 0.38440171693921094, 0.0, 1.0326219326945734, 0.0, 0.02604768015780454, 0.9153304998316836, 0.0, 0.0, 0.0, 0.021211537114504155, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3418184639399641, 0.9530187941808128, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5415447555144779, 0.0, 0.16269589476095273, 0.0, 0.2704207942532035, 0.6129742049927741, 0.0, 0.0, 0.0, 0.0, 0.07078757436524862, 0.0, 0.7077105145475581, 0.34262729838304967, 0.5909987644476324, 0.0, 0.11177131716736882, 0.4359653240929594, 0.04723041930008277, 0.45146742793812006, 0.896649501844373, 0.0, 0.0, 0.6373803605539574, 0.5201327056992473, 0.10980024784153514, 0.07242729038979111, 0.0, 0.25075448235999825, 0.0, 0.14797679652158516, 0.0, 0.0, 0.0, 0.0, 0.6939917265420261, 0.9906375819864492, 0.9257686181712707, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37693205342755914, 0.5232740229070877, 0.0, 0.060081052233409524, 0.0, 0.3350366056610552, 0.0, 0.20067269510180097, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5308540215160052, 0.0, 0.0, 0.04969049276705586, 0.4729163126308979, 0.17206519556589087, 0.08142765800071788, 0.0, 0.6342266715653555, 0.28491323267898133, 0.0, 0.0, 0.0, 0.0, 0.3249315382934875, 0.27888197557292793, 0.0, 0.00024008316707892853, 0.0, 0.0, 0.4429941865632019, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08351200806876206, 0.0, 0.0, 0.3644144714136701, 0.0, 0.25850208657414453, 0.3044172676246769, 0.0, 0.0, 0.0, 0.42064905943656483, 0.5410542431685182, 0.0, 0.0, 0.0235250266831074, 0.17227146855360218, 0.0, 0.0, 0.11190852746246406, 0.0, 0.0, 0.0, 0.0, 0.14411942384547258, 0.0, 0.0, 0.4237744107411662, 0.0, 0.2972186074559964, 0.0, 0.5397314242092841, 0.4632402464002124, 0.0, 0.5786606886346829, 0.0, 0.0, 0.47789419623388885, 0.0, 0.0, 0.0, 0.10580588439167306, 0.0, 0.0, 0.7377357792707717, 0.0, 0.10579038780894534, 0.0, 0.0, 0.0, 0.045139981578782816, 0.0, 0.0, 0.11584045845995011, 0.0, 0.0, 0.019771431213384455, 0.36490249309768175, 0.35310751173091487, 1.1149184594112733, 0.0, 0.0, 0.0, 0.5857527772485364, 0.44959707967332574, 0.1008560617277558, 0.018841708015473753, 0.7001531183110316, 0.0, 0.0, 0.0, 0.5946141177445001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22666010948008636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014782177343261123, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02695824533437821, 0.0, 0.0, 0.0, 0.016838719448302838, 0.0, 0.0, 0.0, 0.08231997245985787, 0.0, 0.0, 0.0, 0.01058107131899943, 0.0, 0.05814319369896461, 0.0, 0.0, 0.0, 0.0, 0.047348036509572325, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019563192629043182, 0.027295875565384514, 0.0, 0.0, 0.0, 0.0, 0.33629932740438834, 0.0, 0.0, 0.0, 0.32394391802925426, 0.5704070896351352, 0.0, 0.14173474826201982, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6013127745008292, 0.0, 0.7561985490243033, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2959306772073682, 0.2539911401819114, 0.0, 0.0, 0.7300245622804938, 0.0, 0.2367914528076339, 0.0, 0.0, 0.0, 0.33628005236128533, 0.025934644678944505, 0.0, 0.06026134263178236, 0.0, 0.0, 0.03662105153743461, 0.06883834406728133, 0.0, 0.0, 0.21279478779844782, 0.0, 0.0, 0.07129569822964256, 0.0, 0.09738688258412649, 0.06246220065656896, 0.0, 0.059045957391141574, 0.0, 0.006923917652553149, 0.051490108793371445, 0.0, 0.16644468958034975, 0.19015418061029377, 0.10882376279915186, 0.06400227485256132, 0.2644778831991808, 0.0, 0.19849197355887785, 0.0, 0.0010725148648749981, 0.12198011848273568, 0.1548472256356659, 0.0, 0.009305339154304988, 0.0, 0.10589543929012146, 0.0, 0.0, 0.17178211974814458, 0.0, 0.23085925511580369, 0.0, 0.0, 0.0, 0.31374773386794685, 0.0, 0.4234572393895347, 0.0, 0.0, 0.0, 0.045888478821935044, 0.0, 0.0, 0.13665231391596233, 0.0, 0.05546447903889757, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15847133453228077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3148706017686264, 0.2655048691833022, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28333620483908656, 0.0, 0.0, 0.0, 0.3208604446677971, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25931056270601605, 0.0, 0.0, 0.20842242702467234, 0.0, 0.0, 0.28681902268117093, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2978149918318377, 0.0, 0.0, 0.09574337597144772, 0.0, 0.0, 0.38017827065442816, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03128644702667798, 0.0, 0.0, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = parallel(_)
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = ((ex.bodies[1]).bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    result = nothing
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = (Finch).moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = (Finch).moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            B_lvl_q = B_lvl_ptr[1]
            B_lvl_q_stop = B_lvl_ptr[1 + 1]
            if B_lvl_q < B_lvl_q_stop
                B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
            else
                B_lvl_i_stop = 0
            end
            phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                if B_lvl_tbl2[B_lvl_q] < phase_start_2
                    B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                end
                while true
                    B_lvl_i = B_lvl_tbl2[B_lvl_q]
                    B_lvl_q_step = B_lvl_q
                    if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                        B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    if B_lvl_i < phase_stop_2
                        Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                        val_4 = Ct_lvl_2_val
                        Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                        A_lvl_ptr_3 = A_lvl_ptr
                        A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                        A_lvl_tbl1_3 = A_lvl_tbl1
                        A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                        A_lvl_tbl2_3 = A_lvl_tbl2
                        A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                        val_5 = A_lvl_val
                        A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
                        B_lvl_ptr_3 = B_lvl_ptr
                        B_lvl_tbl1_3 = B_lvl_tbl1
                        B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                        B_lvl_tbl2_3 = B_lvl_tbl2
                        val_6 = B_lvl_val
                        B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
                        Threads.@threads for i_10 = 1:Threads.nthreads()
                                phase_start_7 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_10), Threads.nthreads()))
                                phase_stop_8 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_10, Threads.nthreads()))
                                if phase_stop_8 >= phase_start_7
                                    for i_13 = phase_start_7:phase_stop_8
                                        Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_13
                                        A_lvl_q = A_lvl_ptr[1]
                                        A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                        if A_lvl_q < A_lvl_q_stop
                                            A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                        else
                                            A_lvl_i_stop = 0
                                        end
                                        B_lvl_q_3 = B_lvl_q
                                        if B_lvl_q < B_lvl_q_step
                                            B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                        else
                                            B_lvl_i_stop_3 = 0
                                        end
                                        phase_stop_9 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                        if phase_stop_9 >= 1
                                            k = 1
                                            if A_lvl_tbl2[A_lvl_q] < 1
                                                A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            if B_lvl_tbl1[B_lvl_q] < 1
                                                B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                            end
                                            while k <= phase_stop_9
                                                A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                                A_lvl_q_step = A_lvl_q
                                                if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                    A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                                end
                                                B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                                phase_stop_10 = min(B_lvl_i_3, phase_stop_9, A_lvl_i)
                                                if A_lvl_i == phase_stop_10 && B_lvl_i_3 == phase_stop_10
                                                    B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                                    A_lvl_q_2 = A_lvl_q
                                                    if A_lvl_q < A_lvl_q_step
                                                        A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                    else
                                                        A_lvl_i_stop_2 = 0
                                                    end
                                                    phase_stop_11 = min(i_13, A_lvl_i_stop_2)
                                                    if phase_stop_11 >= i_13
                                                        if A_lvl_tbl1[A_lvl_q] < i_13
                                                            A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_13, A_lvl_q, A_lvl_q_step - 1)
                                                        end
                                                        while true
                                                            A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                            if A_lvl_i_2 < phase_stop_11
                                                                A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                                Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                                A_lvl_q_2 += 1
                                                            else
                                                                phase_stop_13 = min(A_lvl_i_2, phase_stop_11)
                                                                if A_lvl_i_2 == phase_stop_13
                                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                                    Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                                    A_lvl_q_2 += 1
                                                                end
                                                                break
                                                            end
                                                        end
                                                    end
                                                    A_lvl_q = A_lvl_q_step
                                                    B_lvl_q_3 += 1
                                                elseif B_lvl_i_3 == phase_stop_10
                                                    B_lvl_q_3 += 1
                                                elseif A_lvl_i == phase_stop_10
                                                    A_lvl_q = A_lvl_q_step
                                                end
                                                k = phase_stop_10 + 1
                                            end
                                        end
                                    end
                                end
                            end
                        Ct_lvl_2_val = val_4
                        A_lvl_ptr = A_lvl_ptr_3
                        A_lvl_tbl1 = A_lvl_tbl1_3
                        A_lvl_tbl2 = A_lvl_tbl2_3
                        A_lvl_val = val_5
                        B_lvl_ptr = B_lvl_ptr_3
                        B_lvl_tbl1 = B_lvl_tbl1_3
                        B_lvl_tbl2 = B_lvl_tbl2_3
                        B_lvl_val = val_6
                        B_lvl_q = B_lvl_q_step
                    else
                        phase_stop_19 = min(B_lvl_i, phase_stop_2)
                        if B_lvl_i == phase_stop_19
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_19
                            val_7 = Ct_lvl_2_val
                            Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                            A_lvl_ptr_4 = A_lvl_ptr
                            A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                            A_lvl_tbl1_4 = A_lvl_tbl1
                            A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                            A_lvl_tbl2_4 = A_lvl_tbl2
                            A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                            val_8 = A_lvl_val
                            A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
                            B_lvl_ptr_4 = B_lvl_ptr
                            B_lvl_tbl1_4 = B_lvl_tbl1
                            B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                            B_lvl_tbl2_4 = B_lvl_tbl2
                            val_9 = B_lvl_val
                            B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
                            Threads.@threads for i_20 = 1:Threads.nthreads()
                                    phase_start_22 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_20), Threads.nthreads()))
                                    phase_stop_24 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_20, Threads.nthreads()))
                                    if phase_stop_24 >= phase_start_22
                                        for i_23 = phase_start_22:phase_stop_24
                                            Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_23
                                            A_lvl_q = A_lvl_ptr[1]
                                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                            if A_lvl_q < A_lvl_q_stop
                                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                            else
                                                A_lvl_i_stop = 0
                                            end
                                            B_lvl_q_3 = B_lvl_q
                                            if B_lvl_q < B_lvl_q_step
                                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                            else
                                                B_lvl_i_stop_3 = 0
                                            end
                                            phase_stop_25 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                            if phase_stop_25 >= 1
                                                k = 1
                                                if A_lvl_tbl2[A_lvl_q] < 1
                                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                                end
                                                if B_lvl_tbl1[B_lvl_q] < 1
                                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                                end
                                                while k <= phase_stop_25
                                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                                    A_lvl_q_step = A_lvl_q
                                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                                    end
                                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                                    phase_stop_26 = min(B_lvl_i_3, A_lvl_i, phase_stop_25)
                                                    if A_lvl_i == phase_stop_26 && B_lvl_i_3 == phase_stop_26
                                                        B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                                        A_lvl_q_4 = A_lvl_q
                                                        if A_lvl_q < A_lvl_q_step
                                                            A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                        else
                                                            A_lvl_i_stop_4 = 0
                                                        end
                                                        phase_stop_27 = min(i_23, A_lvl_i_stop_4)
                                                        if phase_stop_27 >= i_23
                                                            if A_lvl_tbl1[A_lvl_q] < i_23
                                                                A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_23, A_lvl_q, A_lvl_q_step - 1)
                                                            end
                                                            while true
                                                                A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                                if A_lvl_i_4 < phase_stop_27
                                                                    A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                    Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                                    A_lvl_q_4 += 1
                                                                else
                                                                    phase_stop_29 = min(A_lvl_i_4, phase_stop_27)
                                                                    if A_lvl_i_4 == phase_stop_29
                                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                                        A_lvl_q_4 += 1
                                                                    end
                                                                    break
                                                                end
                                                            end
                                                        end
                                                        A_lvl_q = A_lvl_q_step
                                                        B_lvl_q_3 += 1
                                                    elseif B_lvl_i_3 == phase_stop_26
                                                        B_lvl_q_3 += 1
                                                    elseif A_lvl_i == phase_stop_26
                                                        A_lvl_q = A_lvl_q_step
                                                    end
                                                    k = phase_stop_26 + 1
                                                end
                                            end
                                        end
                                    end
                                end
                            Ct_lvl_2_val = val_7
                            A_lvl_ptr = A_lvl_ptr_4
                            A_lvl_tbl1 = A_lvl_tbl1_4
                            A_lvl_tbl2 = A_lvl_tbl2_4
                            A_lvl_val = val_8
                            B_lvl_ptr = B_lvl_ptr_4
                            B_lvl_tbl1 = B_lvl_tbl1_4
                            B_lvl_tbl2 = B_lvl_tbl2_4
                            B_lvl_val = val_9
                            B_lvl_q = B_lvl_q_step
                        end
                        break
                    end
                end
            end
        end
    resize!(val, A_lvl.shape[1] * B_lvl.shape[2])
    result = (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
    result
end
julia> @finch begin
        Ct .= 0
        for j = parallel(_)
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.2504314291953376, 0.0, 0.0, 0.9648065476989627, 0.2835978542736629, 0.13211849643271117, 0.2796391765298554, 0.22540993882752317, 0.5512218518807817, 0.5478145335437775, 0.0, 0.2291959647754706, 0.23520231615614767, 0.03296826745685447, 0.18421763750874248, 0.6035878434468992, 0.7441004608135994, 0.2535097758200323, 0.20236453274735783, 0.09228241774990334, 0.13384560784694935, 0.5269755678736615, 0.03880627317810495, 0.17704264007357723, 0.0, 0.2632287465084173, 0.0, 0.02720231717357045, 0.5246531850072871, 0.0, 0.0, 0.3360269038793386, 0.0, 0.22538882742471622, 0.22621665568148489, 0.0, 0.09095677592426996, 0.0, 0.2343687318872728, 0.0, 0.00044179348092010485, 0.07281055209847118, 0.6821285312867097, 0.0, 0.0, 0.0, 0.8944403740767686, 0.11973372388784431, 0.2613237068921269, 0.5923534538840982, 0.0, 0.0, 0.0, 0.8426453838331012, 0.0, 0.0, 1.0769929937540788, 0.43300907588983417, 0.0, 0.6905133736710757, 0.0, 0.0, 0.0, 0.07271653050953786, 0.8664857754450395, 0.08570530865158534, 0.0, 0.7169860381548754, 0.20498067747201715, 0.0, 0.23050103487101342, 0.0, 0.0, 0.9152746488430813, 0.14299878494120105, 0.44815460009097313, 0.0, 0.0, 0.0, 0.23162241551000032, 1.0326337985335274, 1.0530846396126474, 0.22109306040172927, 0.0, 0.0, 0.0, 1.0289491599002607, 0.008076930682810486, 0.037079469071792215, 0.0, 0.03308196940629956, 0.07165705922647192, 0.18447863887953236, 0.0, 0.24885144181470292, 0.0, 0.0, 0.0, 0.7193963651558962, 0.0, 0.6007574484421223, 0.0, 0.11361690641306497, 0.0, 0.0, 0.0, 0.0, 0.5189539173895105, 0.027360695267410504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.769945776489901, 0.00013219515459577855, 0.0, 0.0, 0.0, 0.0, 0.21518747264967641, 0.0, 0.0, 0.0, 0.10980938651098104, 0.07347552139071808, 0.0, 0.3613462418752601, 0.04465135919773286, 0.027102240057490925, 0.0, 0.0, 0.2594566815123038, 0.417973871558788, 0.02337304044012675, 0.5498222432776159, 0.011558313103301096, 0.13519961718643905, 0.0830809338177889, 0.0, 0.0, 0.0, 0.17989871031119878, 0.05233157238393502, 0.0, 0.051508809526025434, 0.225133841185459, 0.18950167625902217, 0.0, 0.0, 0.0, 0.20173577654712102, 0.7662693488070871, 0.0, 0.33259178463685746, 0.13865467767460202, 0.0, 0.8944580603877651, 0.0, 0.060682785060590494, 0.06051599339484606, 0.0, 0.0, 0.0, 0.12305720211410776, 0.10081888448105404, 0.0, 0.0, 0.2522377522294325, 0.0, 0.12103967793114603, 0.06077105223878225, 0.0, 0.0, 0.5586258592154407, 0.0, 0.0, 0.0, 0.2984848816909581, 0.029564782247173068, 0.6276543733906952, 0.0, 0.07353625635562432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08001934416956356, 0.13808411155288577, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0013608908425346389, 0.15477792583676012, 0.8924302032481646, 0.0, 0.13515079773758784, 0.0, 0.059793517595463114, 0.0, 0.04933055693313389, 0.0, 0.0, 0.3342781850907459, 0.02578025634698341, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1229501707927792, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287208489034296, 0.0, 0.0, 0.28149278769379354, 0.0, 0.0, 0.0, 0.031923890851782225, 0.23740383631799186, 0.0, 0.0, 0.0, 0.0, 0.06362127081034188, 0.2629034525588847, 0.0, 0.9151807425831917, 0.0, 0.0, 0.0, 0.6693739198309284, 0.0, 0.0, 0.2061930933620024, 0.27528937724536684, 0.0, 0.0, 0.0, 0.0, 0.2675780483794807, 0.02063619759744684, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5460074055587408, 0.0, 0.6541555536266587, 0.0, 0.0, 0.07150154126300666, 0.0, 0.15048122306270975, 0.034115472912426725, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.050926612139623545, 0.21044506008920905, 0.0, 0.0, 0.4253961501964202, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5242476712268254, 0.0, 0.0, 0.6721198397890595, 0.02913210340779019, 0.0, 0.20059561631676462, 0.0, 0.29433701033990906, 0.0, 0.0, 0.0, 0.12559144077146528, 0.0, 0.0, 0.32229898127496104, 0.0, 0.0, 0.05500938293182685, 0.1114468235961369, 0.09601244908001759, 0.255231140185047, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05242264559033129, 0.07189305704474794, 0.29708512061039594, 0.0, 0.0, 0.16167981650220184, 0.0, 0.0, 0.42894283906704944, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24120550186102305, 0.002462183433889286, 0.1185465354728046, 0.0, 0.0, 0.20924909285728116, 0.0, 0.2383879501918044, 0.0, 0.08928505124693381, 0.0, 0.0, 0.2291277246538886, 0.0, 0.0, 0.039107088380868994, 0.03339412468553525, 0.0, 0.17867986571670036, 0.0, 0.15885581639487184, 0.0, 0.4770109811329409, 0.46521958209705216, 0.08213268605220507, 0.8570033319080816, 0.5762497735248863, 0.025109002675938377, 0.0, 0.0, 0.0, 0.006062678222071191, 0.6895253688974624, 0.0, 0.0, 0.14425623656373723, 0.0, 0.26058653129843573, 0.2499129405223175, 0.5556369963578781, 0.0, 0.0, 0.14846473797228377, 1.311439497677356, 0.0, 0.0, 0.0, 0.14844299347189643, 0.0, 0.06204384333300274, 0.5085214657536261, 0.06333953518463412, 0.0, 0.4087271917522072, 0.56471815720299, 0.0, 0.7613475113276124, 0.31092441757595435, 0.25657570652463557, 0.0, 0.6284352010004853, 0.0, 0.44245087677575873, 0.0, 0.5840306487685422, 0.886297089335757, 0.0, 0.2141960416332438, 0.0, 0.0, 0.7455493113099605, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9841442926021067, 0.155496454991287, 0.0, 0.0, 0.07678252545804619, 0.0, 0.0, 0.3721354350358947, 0.020164712785984047, 0.0, 0.0, 0.0, 0.0, 0.1088564167894987, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9595213220321046, 0.0, 0.026588936071193094, 0.0, 0.0, 0.18249673153915044, 0.041373682209080435, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04976306812868327, 0.2056369238513726, 0.0, 0.0, 0.341205976763526, 0.0, 0.0, 0.0, 0.0, 0.5785815925839551, 0.0, 0.0, 0.0, 0.5989984098523007, 0.041848978991102986, 0.0, 0.7058966144833084, 0.0, 0.1357909427918936, 0.0, 0.0, 0.0, 0.6943234317201388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03329077451447904, 0.0, 0.16959332194846893, 0.03825181104342904, 0.0, 0.030570483496490792, 0.0, 0.181963825346749, 0.0, 0.0, 0.0861751567882612, 0.0, 0.23433890396321935, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06468139820825311, 0.4901043774165395, 0.0, 0.1652405132589817, 0.0, 0.0, 0.07483443864111192, 0.0, 0.0, 0.0, 0.2810295005684995, 0.0, 0.3898372849078984, 0.0, 0.27070466717435465, 0.5873234092956581, 0.0, 0.0, 0.8033877947986148, 0.0, 0.0, 0.0, 0.5030084273245645, 0.1643009637310288, 0.0, 0.0, 0.0, 0.052313365367879956, 0.02924539070601191, 0.0, 0.7815339827038686, 0.0, 0.631919493277437, 0.08243985181699306, 0.0, 0.0, 0.0, 0.0, 0.12868680907403285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.1259698971001804, 0.7198499769348474, 0.0, 0.41807281750787384, 0.0, 0.0, 0.6290848565988848, 0.7773412611170788, 0.0, 0.3270721417747804, 0.0, 0.0, 0.6187710034515789, 0.07069501936159496, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15113911645003905, 0.0, 0.15654877177402662, 1.1322176354521876, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2636197382380641, 0.0, 0.0, 0.34206988627899454, 0.0, 0.0, 0.2168559436204628, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12189174979618164, 0.009400555275465744, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02319896532052816, 0.09586554938115534, 0.0, 0.0, 0.47758214062526055, 0.0, 0.0, 0.0, 0.0, 0.766521270904984, 0.0, 0.0, 0.0, 0.05687724891036996, 0.0, 0.0, 0.0, 0.019988327516632248, 0.0, 0.0, 0.24020596976351885, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5319793134128888, 0.0, 0.0, 0.12310759546793347, 0.28424710350697324, 0.0, 0.9704064926727941, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.595062055789435, 0.0, 0.022922464313533127, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7479520138852049, 0.0, 0.0, 0.0, 0.0, 0.11201726895804533, 0.0, 0.0, 0.5882991104057551, 0.0, 0.0, 0.0, 0.5372591710353254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4185803721513073, 0.0, 0.4160894350708626, 0.35712090084273435, 0.0, 0.0, 0.0, 0.0, 0.3329375067097101, 0.0, 0.0, 0.0, 0.9791513934288851, 0.040171778764664906, 0.0, 0.0, 0.18774405618493867, 0.47311893679528827, 0.2590438343789052, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12077476728428499, 0.0, 0.06327325623124767, 0.0, 0.0, 0.0, 0.0, 0.34849519635188847, 0.0, 0.4382603746311635, 0.0, 0.0, 0.0, 0.09913709085437171, 0.4096661875861404, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4230910500526733, 0.0, 0.0, 0.0, 0.17776900851570368, 0.0, 0.5438917161748563, 0.0, 0.43287084617637955, 0.0, 0.0, 0.0, 0.5349746055023918, 0.0, 0.7894262258085124, 0.0, 0.0, 0.0, 0.416696264214944, 0.1414148713022835, 0.0, 0.1306712640828635, 0.5944339267365731, 0.0, 0.0, 0.0, 0.3098486230376566, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27475814956764166, 0.0, 0.0, 0.0, 0.572874722755641, 0.7993131511508778, 0.0, 0.2979949962228512, 0.14400098705349634, 0.0, 0.0, 0.02684711041673838, 0.0, 0.0575193250192605, 0.13038147684896279, 0.07982177723780243, 0.0, 0.0, 0.22370954598082965, 0.0, 0.0, 0.0, 0.16207849359014098, 0.8780589957563658, 0.0, 0.0, 0.0, 0.03773454641900064, 0.0, 0.190720074864707, 0.03823162707299666, 0.0, 0.027092618673790602, 0.07782668325822914, 0.0, 0.0, 0.025612768291380317, 0.0, 0.1339183762706947, 0.03147511447091019, 0.0, 0.0, 0.22256660522037766, 0.0, 0.050981846070955524, 0.33080849696175413, 0.22670468547648376, 0.0, 0.0, 0.2394131641393997, 0.0, 0.6763225254232694, 0.0, 0.32609931596811353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02551395955593748, 0.0, 0.19870976845837512, 0.0, 0.8838430389981123, 0.0, 0.8900590234719281, 0.0, 0.1395885727389325, 0.39274638865688727, 0.13258182339863903, 1.0427544725112363, 0.007925765580094073, 0.0, 0.0, 0.9248219821941955, 0.0, 0.38782207933590473, 0.20331266090801786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4620448777897825, 0.5673723076761421, 0.13444042454417618, 0.0, 0.0012927651989113214, 0.0, 0.0, 0.0, 0.3049721916420315, 0.017266202832261764, 0.0, 0.0, 0.07071993264415252, 0.04567425634491729, 0.3943633692304047, 0.0, 0.15861807151929683, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05848946000294637, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0002825960061227963, 0.18362488935764, 0.0, 0.0, 0.0, 0.16115194339742445, 0.5974334511167834, 0.0, 0.04024167944046561, 0.33382381844541353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6087942383002851, 0.0, 0.0, 0.7807479082612564, 0.3974967659255351, 1.5487027213337248, 0.03765559375444021, 0.17692333788215128, 0.23426356503546714, 0.0, 0.22388851869475412, 0.04007956243736881, 1.0262040210122396, 0.0, 0.7972348331243138, 1.0049259227957363, 0.051433760809377164, 0.0, 0.8283458296766737, 0.0, 0.7561556098507006, 0.0, 0.0, 0.0, 0.24907496608552146, 0.0, 0.5011916402324644, 0.1747189814102487, 0.9899595982235516, 0.6164188940137083, 0.11559029428984438, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.593849645786338, 0.0, 0.0, 0.0, 0.0, 0.13660851844466146, 0.030970403587585463, 0.23157107362581741, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13749939723080143, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7110898100418397, 0.0, 0.0, 0.0, 0.46081314114808425, 0.7109856621285974, 0.2884350146527927, 0.0, 0.0, 0.30337236072169704, 0.06739814120009983, 0.0, 0.7785291912169707, 0.0, 0.0, 0.13287789565404662, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3593823347180517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4624623396236477, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024288127378650458, 0.0, 0.793430084070044, 0.0, 0.0, 0.07003161792104487, 0.0, 0.390525043836891, 0.7636039620295462, 0.06950112764284275, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019321154098251664, 0.0, 0.18651557279691772, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05792018405822482, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18265223354453325, 0.12454723256219671, 0.0, 0.0002798455087109209, 0.0, 0.0, 0.11611318457978691, 0.05398921986713746, 0.0, 0.8752290763913787, 0.0, 0.0, 0.0, 0.0, 0.13107895831547542, 0.0, 0.0, 0.45521248179167606, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11685661958627322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44522459151527227, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26435993065610003, 0.021758541234477975, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2146225974372531, 0.0, 0.13433790512658406, 0.030902835047588124, 0.0, 0.0, 0.03139052000724026, 0.0, 0.0, 0.7200671942026683, 0.0, 0.0, 0.05946084260814123, 0.0, 0.14663715828117352, 0.03324398818324252, 0.008023890708802274, 0.05967012120515395, 0.0, 0.0, 0.0, 0.10840406404650259, 0.0, 0.0, 0.0, 0.23002554078955822, 0.0, 0.21539070760796813, 0.0, 0.15809846801096508, 0.0, 0.0, 0.023379160669788793, 0.06919243918501702, 0.0, 0.4184368340983768, 0.0, 0.0, 0.0, 0.025499570825031452, 0.0, 0.054632252014063354, 0.12383722685885369, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08100665304577326, 0.0, 0.0, 0.0, 1.137863121002405, 0.0, 0.0, 0.18114724382910816, 1.0175246627827756, 0.0, 0.8319051265218698, 0.7408406030496757, 0.0, 0.0, 0.4793159619581115, 0.26457362772519605, 0.7690370272987045, 0.02989528102721059, 0.0, 0.0, 0.0, 0.34236772701032764, 0.22777638147070367, 0.20013535639204974, 0.7445351180710278, 0.0, 0.0, 0.32085924041220354, 0.0, 0.0, 0.0, 0.5922078604864867, 0.0, 0.0, 0.18324289767504168, 0.0, 0.0, 0.05247138335835671, 0.48819810625100485, 0.0, 0.4058243472838195, 0.0, 0.0, 0.6944817185701155, 0.21684000174364465, 0.0, 0.12484778900119534, 0.0, 0.5314768166536554, 0.0, 0.0, 0.0, 0.0, 0.11888353831534566, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4539474830982486, 0.0, 0.12931153719488578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23351697026332235, 0.0, 0.0, 0.0, 0.0, 0.2291429172294868, 0.4258647172142874, 0.0, 0.0, 0.0, 0.22479141295616378, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05853977920447593, 0.0, 0.16715127001769428, 0.0, 0.3712702672746247, 0.13508244363790445, 0.06392604260421969, 0.5912989058272206, 0.4437811402099471, 0.0, 0.0, 0.0, 0.0, 0.005827558703832637, 0.6627845677844773, 0.0, 0.0, 0.0, 0.30904361146056275, 0.6816789334794826, 0.1436649772664439, 0.16334297841077242, 0.0, 0.0, 0.7882286990025497, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7753056808075847, 0.035768380854023674, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44917737188494944, 0.0, 0.18937379890511313, 0.06882274914369534, 0.0, 0.0, 0.0, 0.009287224889287345, 0.06906497794085972, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26624227636214304, 0.0, 0.0, 0.0, 0.1829905316954157, 0.0, 0.06582989974405196, 0.2036305981722081, 0.08008655235603437, 0.0, 0.0, 0.0, 0.0, 0.0, 0.41729810318329397, 0.0, 0.0, 0.0, 0.0, 0.0, 0.037951024806923965, 0.0, 0.0, 0.0, 0.0, 0.41651629143780994, 0.0, 0.2053954771505675, 0.29245802337269494, 0.0368786813523771, 0.0, 0.38440171693921094, 0.0, 1.0326219326945734, 0.0, 0.02604768015780454, 0.9153304998316836, 0.0, 0.0, 0.0, 0.021211537114504155, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3418184639399641, 0.9530187941808128, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5415447555144779, 0.0, 0.16269589476095273, 0.0, 0.2704207942532035, 0.6129742049927741, 0.0, 0.0, 0.0, 0.0, 0.07078757436524862, 0.0, 0.7077105145475581, 0.34262729838304967, 0.5909987644476324, 0.0, 0.11177131716736882, 0.4359653240929594, 0.04723041930008277, 0.45146742793812006, 0.896649501844373, 0.0, 0.0, 0.6373803605539574, 0.5201327056992473, 0.10980024784153514, 0.07242729038979111, 0.0, 0.25075448235999825, 0.0, 0.14797679652158516, 0.0, 0.0, 0.0, 0.0, 0.6939917265420261, 0.9906375819864492, 0.9257686181712707, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37693205342755914, 0.5232740229070877, 0.0, 0.060081052233409524, 0.0, 0.3350366056610552, 0.0, 0.20067269510180097, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5308540215160052, 0.0, 0.0, 0.04969049276705586, 0.4729163126308979, 0.17206519556589087, 0.08142765800071788, 0.0, 0.6342266715653555, 0.28491323267898133, 0.0, 0.0, 0.0, 0.0, 0.3249315382934875, 0.27888197557292793, 0.0, 0.00024008316707892853, 0.0, 0.0, 0.4429941865632019, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08351200806876206, 0.0, 0.0, 0.3644144714136701, 0.0, 0.25850208657414453, 0.3044172676246769, 0.0, 0.0, 0.0, 0.42064905943656483, 0.5410542431685182, 0.0, 0.0, 0.0235250266831074, 0.17227146855360218, 0.0, 0.0, 0.11190852746246406, 0.0, 0.0, 0.0, 0.0, 0.14411942384547258, 0.0, 0.0, 0.4237744107411662, 0.0, 0.2972186074559964, 0.0, 0.5397314242092841, 0.4632402464002124, 0.0, 0.5786606886346829, 0.0, 0.0, 0.47789419623388885, 0.0, 0.0, 0.0, 0.10580588439167306, 0.0, 0.0, 0.7377357792707717, 0.0, 0.10579038780894534, 0.0, 0.0, 0.0, 0.045139981578782816, 0.0, 0.0, 0.11584045845995011, 0.0, 0.0, 0.019771431213384455, 0.36490249309768175, 0.35310751173091487, 1.1149184594112733, 0.0, 0.0, 0.0, 0.5857527772485364, 0.44959707967332574, 0.1008560617277558, 0.018841708015473753, 0.7001531183110316, 0.0, 0.0, 0.0, 0.5946141177445001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22666010948008636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014782177343261123, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02695824533437821, 0.0, 0.0, 0.0, 0.016838719448302838, 0.0, 0.0, 0.0, 0.08231997245985787, 0.0, 0.0, 0.0, 0.01058107131899943, 0.0, 0.05814319369896461, 0.0, 0.0, 0.0, 0.0, 0.047348036509572325, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019563192629043182, 0.027295875565384514, 0.0, 0.0, 0.0, 0.0, 0.33629932740438834, 0.0, 0.0, 0.0, 0.32394391802925426, 0.5704070896351352, 0.0, 0.14173474826201982, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6013127745008292, 0.0, 0.7561985490243033, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2959306772073682, 0.2539911401819114, 0.0, 0.0, 0.7300245622804938, 0.0, 0.2367914528076339, 0.0, 0.0, 0.0, 0.33628005236128533, 0.025934644678944505, 0.0, 0.06026134263178236, 0.0, 0.0, 0.03662105153743461, 0.06883834406728133, 0.0, 0.0, 0.21279478779844782, 0.0, 0.0, 0.07129569822964256, 0.0, 0.09738688258412649, 0.06246220065656896, 0.0, 0.059045957391141574, 0.0, 0.006923917652553149, 0.051490108793371445, 0.0, 0.16644468958034975, 0.19015418061029377, 0.10882376279915186, 0.06400227485256132, 0.2644778831991808, 0.0, 0.19849197355887785, 0.0, 0.0010725148648749981, 0.12198011848273568, 0.1548472256356659, 0.0, 0.009305339154304988, 0.0, 0.10589543929012146, 0.0, 0.0, 0.17178211974814458, 0.0, 0.23085925511580369, 0.0, 0.0, 0.0, 0.31374773386794685, 0.0, 0.4234572393895347, 0.0, 0.0, 0.0, 0.045888478821935044, 0.0, 0.0, 0.13665231391596233, 0.0, 0.05546447903889757, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15847133453228077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3148706017686264, 0.2655048691833022, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28333620483908656, 0.0, 0.0, 0.0, 0.3208604446677971, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25931056270601605, 0.0, 0.0, 0.20842242702467234, 0.0, 0.0, 0.28681902268117093, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2978149918318377, 0.0, 0.0, 0.09574337597144772, 0.0, 0.0, 0.38017827065442816, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03128644702667798, 0.0, 0.0, 0.0]), 42), 42)),)

