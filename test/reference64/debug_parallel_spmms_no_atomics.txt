julia> @finch begin
        CR .= 0
        for i = _
            for j = _
                for k = _
                    CR[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(CR = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0, 0.0, 0.0, 0.01855804465451247, 0.0, 0.0, 0.03641425883740431, 0.0568080930379847, 0.10109716846875504, 0.0, 0.0, 0.0, 0.0, 0.005424838537552436, 0.0, 0.0, 0.5251896042062963, 0.0, 0.0, 0.0, 0.00515342598991506, 0.045980273053842245, 0.0, 0.4033688458623361, 0.0510673466163073, 0.0, 0.0, 0.02134197385615031, 0.01687808266981965, 0.0, 0.4932447371732914, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7178045198025068, 0.2106682695954306, 0.0, 0.3115118572371521, 0.5170963380295581, 0.39595498771521426, 0.1432663403816299, 0.5698542189659496, 0.0, 0.0, 0.0, 0.5317966816929195, 0.0, 0.5315818882594147, 0.8168445783005969, 0.0, 0.0, 0.0, 0.01222733860627223, 0.0, 0.716756885645577, 0.0, 0.8217284580618865, 0.48234209517721377, 0.15545502627894098, 0.04826321760798536, 0.0, 0.0, 0.1394038038151463, 0.0, 0.056802678199824076, 0.0, 0.0, 0.0, 0.3128541399180126, 0.0, 0.0, 0.596593418834028, 0.0, 0.2846892571697076, 0.0, 0.0, 0.0, 0.23365792107187477, 0.0, 0.0, 0.4584793374366712, 0.7152510496888798, 0.0, 0.0, 0.0, 0.8190896825195866, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11127793558853107, 0.0, 0.5789217135796446, 0.0, 0.0, 0.6429712972008511, 0.0, 0.0, 0.661506988212563, 0.7629272979371657, 0.0, 0.4234710523361336, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4257443064600869, 0.0, 0.20425497460318195, 0.0, 0.5184318202957809, 0.5906121758765186, 0.0, 0.9091367634018797, 0.0, 0.30652125262867885, 0.0, 0.7954054080966687, 0.0, 0.0, 0.0, 1.0559083316811368, 0.0, 0.04427154393293609, 0.8228955354751877, 0.46658038808648383, 0.0, 0.02688606839966265, 0.0, 0.0, 0.6276611460113729, 0.22522673327345408, 0.0, 0.12308651028821192, 0.3657688390111318, 0.0, 0.046115245088609334, 0.0, 0.5342436705087195, 0.0, 0.1923886832463953, 0.07621763959827882, 0.0, 0.0, 0.0, 0.6725528112574624, 0.7483589153917343, 0.0, 0.0, 0.0, 0.0, 0.38949567271561986, 0.16492548861911044, 0.10519585180914466, 0.8853743380662012, 0.957787822214843, 0.0, 0.05459549529270359, 0.25656057396140347, 0.14167223917141744, 0.03949155262056029, 0.0, 0.0, 0.2703834370419471, 0.0, 0.723362221877522, 0.0, 0.0, 0.0, 0.7883757417725614, 0.0, 0.03340589062163017, 0.18896261922152233, 0.0, 0.0, 0.8122657251866786, 0.029607718570353176, 0.0, 0.008213735995679807, 0.0, 0.08605746445745502, 0.0, 0.3070376563830936, 0.10398717208817806, 0.0, 0.0, 0.0, 0.383984202548332, 0.0, 0.0, 0.3036396360576457, 0.0, 0.0, 0.0, 0.0, 0.11965354383371518, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04825455862354496, 0.3335108938296942, 0.0, 0.0, 0.0, 0.0, 0.04121745447265487, 0.30594446336043757, 0.0, 0.33550494159326405, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029116195807012437, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020041236855252602, 0.0, 0.4850188795993097, 0.0, 0.16403823255879735, 0.0, 0.60023572778736, 0.0, 0.1787344157826609, 0.0, 0.0, 0.17233073276918714, 0.17532212882841147, 0.6209016656456683, 0.5984929062073807, 0.8843163905991084, 0.3337919143714591, 0.0, 1.0919707374354302, 0.8166713304287454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8330317657834686, 0.27201200741148646, 0.0, 0.8995473587201992, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6388227016456749, 0.0, 0.3682672817259989, 0.0, 0.5422113050459496, 0.0, 0.028602469161704683, 0.0, 0.0, 0.3611222515381231, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.031337192831751, 0.0, 0.0, 0.7687924758178234, 0.012617011188267763, 0.0, 0.0, 0.37315715294884366, 0.0, 0.0, 0.0, 0.0, 0.6888779809403613, 0.121001715761302, 0.0, 0.0, 0.3680175749172942, 0.47420475938210593, 0.0, 0.7604201795169553, 0.0, 0.11917670920609563, 0.0, 0.20848198246903302, 0.24926285864223433, 0.0, 0.5553692000145817, 0.4169252107806106, 0.16597755096800731, 0.7334771026528798, 0.0, 0.0, 0.09908991746260937, 0.0, 0.14652270264303283, 0.006759647910977647, 0.0, 0.9351545954529237, 1.2960299208884896, 0.0, 0.0, 0.21191619141709428, 0.0, 0.0, 0.0, 0.01204540426582532, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06493914883934329, 0.7726981572397796, 0.0, 0.4370686273737238, 0.1778273534538074, 0.055209023869020324, 0.0, 0.0, 0.0, 0.05709969797354255, 0.0, 0.0, 0.0, 0.0, 0.44808598581285264, 0.0, 0.0, 0.0, 0.21141909545938053, 0.13390642572615394, 0.0, 0.0, 0.0, 0.25844994675864025, 0.0, 0.51617125876173, 0.2544268105667549, 0.0, 0.17576058376064335, 0.29087109088619356, 0.0, 0.0, 0.3286889977164017, 0.0, 0.0644707079973061, 0.0, 0.018682819036845075, 0.008522941045952347, 0.0, 0.0, 0.2520720928277996, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.146066293561638, 0.08856351358209161, 0.0, 0.0, 0.0, 0.1407746062228793, 0.5246446552397386, 0.0, 0.0, 0.0, 0.3279179855015388, 0.46137944574560436, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14330339619658897, 0.11500988018505275, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7444467047874328, 0.0, 0.0, 0.40713437972280675, 0.0, 0.0, 0.0, 0.5717679971406524, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4877440937322302, 0.6991654372087778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2369682176872117, 0.0, 0.0, 0.6757556051622259, 0.0, 0.0, 0.0, 0.576368321209737, 0.04986303698751877, 0.0, 0.5384684335273707, 0.0, 0.21524971756673417, 0.4963922980972075, 0.0, 0.0, 0.0, 0.0, 0.18940210229325644, 0.5277692784584271, 0.7718932464687741, 0.7210929692294604, 0.0, 0.0, 0.0, 0.0, 0.48286176484120297, 0.8517937989915767, 0.3931693184679991, 0.0, 0.0, 0.06755455710060922, 0.0, 0.026695131792948763, 1.4844324548606433, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4524914530619723, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4694831016582333, 0.0, 0.42831526212260856, 0.6074590079807168, 0.30066361967157035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6129938591206631, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6008436638333436, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43882326364474644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8591062832739853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6928708028466394, 0.2819039698547807, 0.08752108546971657, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05246236611269018, 0.0, 0.0, 0.5428804251437249, 0.022819504909203123, 0.3729965675892053, 0.0, 0.5741583158437262, 0.0, 0.1935811346150516, 0.0, 0.5023321549740235, 0.2611826745866744, 0.0, 0.056899028261467276, 0.37961815387770637, 0.22527271986010186, 0.05171423855571594, 0.058968538904135746, 0.0, 0.02892983665470066, 0.003313037290954743, 0.0, 0.0, 0.15785301822506514, 0.0, 0.0, 0.07773433689527456, 0.1049811110510813, 0.0, 0.02912372760696772, 0.0, 0.3114315285072337, 0.5703037753007745, 0.0, 0.04813466284828647, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02241933195791073, 0.0, 0.0, 0.0, 0.0, 0.23199999736075347, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1927795141790272, 0.2407119020891451, 0.0, 0.21960445663579467, 0.5770903406753464, 0.15415530724008072, 0.23155572792080611, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3142922871488459, 0.0, 0.0, 0.0, 0.0, 0.3042889785067072, 0.0, 0.5129566600174419, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3746354155106774, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4789154917746846, 0.17517544375428362, 0.0, 0.0, 0.0, 0.2637662382252967, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2102700997979013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.126545263019808, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.332133943612647, 0.0, 0.0, 0.0, 0.06184093060913505, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3212574037659458, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24673989573277713, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3017167941185386, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6233354196759152, 0.0, 0.0, 0.0, 0.5447611694646652, 0.0, 0.0, 0.216176776357065, 0.0, 0.19855221622967104, 0.8686764576101142, 0.16350923987929902, 0.0, 0.0, 0.02334951098786776, 0.17107621812888604, 0.48682879160279025, 0.7109808359466632, 0.6651558420918936, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6192268761337199, 0.31206921876898447, 0.0, 0.0, 0.24595989001208304, 0.11068536977374843, 0.0, 0.6618395820988949, 0.11362642836904417, 0.0, 0.0, 0.23765422406147768, 0.0, 0.0, 0.8073352785417658, 0.0, 0.0, 0.48485062583878286, 1.055137179889336, 0.2671063045394923, 0.0, 0.8476204266733893, 0.473385026956602, 0.5030912229948215, 0.0, 0.0, 0.0, 0.17678971866264329, 0.4076988147485064, 0.0, 0.6776909084507561, 0.13779930009477406, 0.5691281649873949, 0.16622169517158358, 0.433469475882236, 0.818379113845578, 0.9106844436778354, 0.0, 0.499393081506665, 0.0, 0.5725309586159512, 0.6160013948015824, 0.5125607980557373, 0.2778645861379971, 0.0, 0.0, 0.0, 0.0, 0.0, 0.58929804834674, 0.0, 0.8046233984476477, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3967677816937078, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11707526901952116, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.383435421983313, 0.11707115655719894, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07864429232511502, 0.0, 0.0, 0.3002587300128159, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2266804769859352, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24215921984980293, 0.09852579316681935, 0.030588729804574537, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18874358875246125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3978422171614565, 0.1936032919321414, 0.0, 0.0, 0.2542147176776134, 0.0, 0.0, 0.0, 0.11057553114503467, 0.0, 6.012518808232222e-5, 0.0008825884581384356, 0.6460696181307314, 0.0, 0.0, 0.06879188140017427, 0.0, 0.0, 0.04716298956935701, 0.1540138997690347, 0.1854894997474166, 0.003124358567077274, 0.35109153378778224, 0.0, 0.0, 0.016293742009993128, 0.0, 0.0, 0.3747805868672189, 0.025859295570033313, 0.214029422856965, 0.003742959361061961, 0.0002933458280023473, 0.0, 0.0, 0.0, 0.030501494612752977, 0.0, 0.6996784734474946, 0.0, 0.0, 0.05616866413207192, 0.0, 0.0018184995370819535, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020171782722387992, 0.0, 0.0, 0.33279196588498333, 0.0, 0.0, 0.6323890570808377, 0.0, 0.0, 0.0, 0.03594525644730097, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004527887543653786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17039388977135247, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6309056496928738, 0.0, 0.0, 0.0, 0.0, 0.260280820884557, 0.0, 0.0, 0.5107183088816271, 0.7967464980324664, 0.14485492608681305, 0.0, 0.009558078807310686, 0.754701551784825, 0.0, 0.0, 0.0, 0.0, 0.7598764239846039, 0.0, 0.029177780358136646, 0.10253044625069853, 0.03763219091970284, 0.8115624154017409, 0.0, 0.5779584654853126, 0.7162312164421211, 0.0, 0.0, 0.9107472399622886, 0.7029540573396418, 0.0, 1.0969174796040306, 0.0, 0.0, 0.0, 0.0, 0.0038591832437520454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15366266383288085, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.558080611304664, 0.0, 0.0, 0.0, 0.0, 0.30736701150515244, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04311886842935436, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32751189175201845, 0.0, 0.20963216887545572, 0.1760259950046517, 0.0, 0.23753864749234863, 0.24093292162050975, 0.024684548149881243, 0.4936183719527589, 0.7064246796060082, 0.0, 0.06677453327599289, 0.8542035190842152, 0.0, 0.025534429812218047, 0.011379900282647532, 0.039800411165778926, 0.013727111201321645, 0.27843394547131894, 0.24161261590501448, 0.2499939278392831, 0.0, 0.0, 0.3450248607837782, 0.0, 0.0, 0.15272914557490577, 0.10495111476871369, 0.0, 0.0, 0.0, 0.18866875753074405, 0.0, 0.2225813946448797, 0.0, 0.0, 0.3661590836002555, 0.0, 0.0, 0.0, 0.8998957690359772, 0.30889178773893766, 0.6078813570942335, 0.0, 0.6840699471933163, 0.0, 0.0, 0.0, 0.36308986165522683, 0.752528074611531, 0.7191770460546526, 0.49508503625401296, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7784554319844289, 0.16352298326175332, 0.0, 0.042948548224867954, 0.375190336410357, 0.0, 0.6830715485727474, 0.6403864588932121, 0.7831097846364896, 0.46112164648188003, 0.14556009485746954, 0.0, 0.0, 0.0, 0.8585580212631712, 0.0, 0.253359163441029, 0.0, 0.0, 0.0, 0.45902026051340783, 0.0, 0.0, 0.0, 0.0, 0.166881895984427, 0.0, 0.0, 0.837727301005, 0.0, 0.0, 0.7629510291223568, 0.0, 0.34201996071392393, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3670598185817387, 0.0, 0.9533138689466589, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8365046398061449, 0.0, 0.9590136929209965, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.801491142663501, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16957493050977235, 0.0, 0.2507477829737011, 0.0, 0.0, 0.11532054524043628, 0.07735685692161769, 0.5247858743947099, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02100484631395434, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8201592179395716, 0.0, 0.0, 0.04562437343117348, 0.0, 0.22915724847477928, 0.0, 0.0, 0.7159205767218768, 0.0, 0.0, 0.0, 0.0, 0.29228977882295926, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17928588753235497, 0.10348218971035608, 0.8147006956016374, 0.08438351505236888, 0.0, 0.0, 0.0, 0.1070859593707194, 0.7148756921758321, 0.0, 0.8195717571774336, 0.12249576952187838, 0.0, 0.2397374334824377, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05225556550152024, 0.0, 0.0, 0.0, 0.1707240047839942, 0.0, 0.11920584928810601, 0.0, 0.14514911616615112, 0.4284534617057244, 0.04403067305952579, 0.0, 0.012688229227290206, 0.3654383338493288, 0.04642776495318284, 0.0, 0.0, 0.31371261026073766, 0.1364760956744218, 0.5008825042361101, 0.0, 0.04802625911306818, 0.021403808257108425, 0.0, 0.12191697007589085, 0.33632693219045084, 0.49073784844916374, 0.5066599946239788, 0.0, 0.0, 0.033354950181247504, 0.0, 0.15690710970564234, 0.3397139005999492, 0.4535410643500978, 0.0, 0.0, 0.49050280987387757, 0.0, 0.0, 0.45491953624514714, 0.0, 0.0, 0.33864218894794285, 0.0, 0.0, 0.041024172335819876, 0.0, 0.0, 0.0, 0.36175943329836924, 0.0, 0.39749358702806525, 0.0, 0.5595492530984049, 0.0, 0.0, 0.2573403608635483, 0.17262354625512744, 0.0, 0.0, 0.0, 0.3776391490751126, 0.0, 0.0, 0.1445911005474468, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.45961437114556436, 0.08073146338811468, 0.0, 0.0, 0.0, 0.5680662673212686, 0.0, 0.14301647595845865, 0.4278925860448857, 0.2624303541096862, 0.0, 0.5619603114708978, 0.5488835922357254, 0.0, 0.10445139145624531, 0.9180806504163392, 0.5113694952971799, 0.0, 0.4737323723594051, 0.4474251824766889, 0.10616343839228443, 0.0, 0.02745363380188561, 0.0, 0.28312699798757224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6983886824821985, 0.38943652344243257, 1.2223890565530993, 0.05586381199212052, 0.0, 0.07686093310040262, 0.10701921729681718, 0.0, 0.4467721663826107, 0.0, 0.5122035249872549, 1.008045581843232, 0.10571972464185753, 0.6235175832869809, 0.0, 0.0, 0.0, 0.0, 0.7054625658357997, 0.0, 0.0, 0.17326833225856653, 0.0, 0.0, 0.0, 1.0813156328128786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0927477810868, 0.0, 0.599657110852462, 0.22383250773975968, 0.19685563542122056, 0.0, 0.05605157239206448, 0.41837305013296433, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18996852120778085, 0.0, 0.28856245320282303, 0.0, 0.0, 0.0, 0.26772611418599673, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1616947156362974, 0.0, 0.0, 0.0, 0.0, 0.36168541397352055, 0.0, 0.0, 0.0, 0.1573219565054934, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13761515434984456, 0.07943022011077468, 0.0, 0.21912413885604493, 0.0, 0.0, 0.40654019224262405, 0.0, 0.0, 0.022840701875983912, 0.0, 0.09402454628178203, 0.5332211799090736, 0.14433851006651358, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.043396172364084024, 0.04011000425692938, 0.0, 0.0, 0.0, 0.13104327726482687, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10232284486723907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10523174379677519, 0.0, 0.10079612361252216, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04415728315159431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11805369977437472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6523402818477722, 0.04871612610613401, 0.0, 0.19287524873967893, 0.0, 0.3592812200503388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.050401634126662875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8285325211040351, 0.0, 0.0, 0.0, 0.42701794948426475, 0.33826599655789313, 0.0, 0.0, 0.0063200711569476415, 0.0, 0.0, 0.4430530345512502, 0.0, 0.6935424923816391, 0.9428504267370209, 0.42746265456576077, 0.0025457381888520196, 0.0, 0.1593559778661365, 0.0, 0.8273232796667221, 0.04988580847200336, 0.9484876902255591, 0.5784846962232053, 0.0, 0.4437739313478008, 0.0, 0.0, 0.29414816951171024, 0.0, 0.022355194883774517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02069724212749366, 0.0, 0.0, 0.0, 0.0, 0.7860463101009033, 0.0, 0.0, 0.0, 0.8499064420418566, 0.0, 0.014821364995218649, 0.0, 0.004694386740689857, 0.25038082548050006, 0.5774089497757802, 0.0, 0.0, 0.0, 0.0, 0.006270331329794731, 0.6139069964758506, 0.8351435200462407, 1.0242327730345495, 0.0, 0.029307532281045708, 0.0, 0.0, 0.0, 0.5726801471740839, 0.4962238989829506, 0.0, 0.07225356949876899, 0.10746103390935668, 0.0, 0.0, 0.8346013157058231, 0.0, 0.0, 0.12871551917858068, 0.0, 0.0, 0.05722051841888945, 0.0, 0.08248659071056784, 0.08634381582594033, 0.0, 0.14691645141749232, 0.0, 0.0, 0.0, 0.12530859946217338, 0.0, 0.10215231917559951, 0.07032216186532725, 0.0, 0.8790426858900641, 0.10792096194290268, 0.054388076691659265, 0.0, 0.0, 0.0, 0.05001396409640395, 0.114742650990913, 0.28339886512623463, 0.15677327627325063, 0.0, 0.03258480102579557, 0.09096085908673467, 0.0, 0.0, 0.15530131741375458, 0.4726591715297998, 0.7751806988017736, 0.0, 0.51208607063286, 0.0, 0.0, 0.8142774797804637, 0.037795557638802935, 0.0, 0.03542525158598919, 0.07905092197782795, 0.0, 0.0, 0.1322231578644953, 0.0, 0.0, 0.008286484320060336, 0.670492082894006, 0.0, 0.0, 0.2274177128809876, 0.20430769763068007, 0.27374263149272277, 0.0, 0.0, 0.0, 0.0, 0.22105379716709922, 0.08605636125516496, 0.023058224273634728, 0.03343036365050954, 0.7905997431838073, 0.031091378094046798, 0.32331562706461314, 0.0, 0.048595513480849374, 0.0, 0.6695135011372569, 0.01079155413526415, 0.7675661133629853, 0.01950494389053299, 0.0, 0.06609079415505228, 0.15915608286937477, 0.0, 0.17610090955799163, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05278940542005462, 0.0, 0.04363140001862599, 0.3747258422202424, 0.18235409289517635]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = ((ex.bodies[1]).bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    result = nothing
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = (Finch).moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = (Finch).moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            phase_start_2 = max(1, 1 + fld(A_lvl.shape[1] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                for i_7 = phase_start_2:phase_stop_2
                    B_lvl_q = B_lvl_ptr[1]
                    B_lvl_q_stop = B_lvl_ptr[1 + 1]
                    if B_lvl_q < B_lvl_q_stop
                        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                    else
                        B_lvl_i_stop = 0
                    end
                    phase_stop_3 = min(B_lvl.shape[2], B_lvl_i_stop)
                    if phase_stop_3 >= 1
                        if B_lvl_tbl2[B_lvl_q] < 1
                            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        while true
                            B_lvl_i = B_lvl_tbl2[B_lvl_q]
                            B_lvl_q_step = B_lvl_q
                            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                            end
                            if B_lvl_i < phase_stop_3
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_5 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_5 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_5
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_6 = min(B_lvl_i_2, phase_stop_5, A_lvl_i)
                                        if A_lvl_i == phase_stop_6 && B_lvl_i_2 == phase_stop_6
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_7 = min(i_7, A_lvl_i_stop_2)
                                            if phase_stop_7 >= i_7
                                                if A_lvl_tbl1[A_lvl_q] < i_7
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_7
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_9 = min(A_lvl_i_2, phase_stop_7)
                                                        if A_lvl_i_2 == phase_stop_9
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_6
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_6
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_6 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            else
                                phase_stop_14 = min(B_lvl_i, phase_stop_3)
                                if B_lvl_i == phase_stop_14
                                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_14
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_15 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_15 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_15
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_16 = min(B_lvl_i_2, A_lvl_i, phase_stop_15)
                                            if A_lvl_i == phase_stop_16 && B_lvl_i_2 == phase_stop_16
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_17 = min(i_7, A_lvl_i_stop_4)
                                                if phase_stop_17 >= i_7
                                                    if A_lvl_tbl1[A_lvl_q] < i_7
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_17
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_19 = min(A_lvl_i_4, phase_stop_17)
                                                            if A_lvl_i_4 == phase_stop_19
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_16
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_16
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_16 + 1
                                        end
                                    end
                                    B_lvl_q = B_lvl_q_step
                                end
                                break
                            end
                        end
                    end
                end
            end
        end
    resize!(val, A_lvl.shape[1] * B_lvl.shape[2])
    result = (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
    result
end
julia> @finch begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0, 0.0, 0.0, 0.01855804465451247, 0.0, 0.0, 0.03641425883740431, 0.0568080930379847, 0.10109716846875504, 0.0, 0.0, 0.0, 0.0, 0.005424838537552436, 0.0, 0.0, 0.5251896042062963, 0.0, 0.0, 0.0, 0.00515342598991506, 0.045980273053842245, 0.0, 0.4033688458623361, 0.0510673466163073, 0.0, 0.0, 0.02134197385615031, 0.01687808266981965, 0.0, 0.4932447371732914, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7178045198025068, 0.2106682695954306, 0.0, 0.3115118572371521, 0.5170963380295581, 0.39595498771521426, 0.1432663403816299, 0.5698542189659496, 0.0, 0.0, 0.0, 0.5317966816929195, 0.0, 0.5315818882594147, 0.8168445783005969, 0.0, 0.0, 0.0, 0.01222733860627223, 0.0, 0.716756885645577, 0.0, 0.8217284580618865, 0.48234209517721377, 0.15545502627894098, 0.04826321760798536, 0.0, 0.0, 0.1394038038151463, 0.0, 0.056802678199824076, 0.0, 0.0, 0.0, 0.3128541399180126, 0.0, 0.0, 0.596593418834028, 0.0, 0.2846892571697076, 0.0, 0.0, 0.0, 0.23365792107187477, 0.0, 0.0, 0.4584793374366712, 0.7152510496888798, 0.0, 0.0, 0.0, 0.8190896825195866, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11127793558853107, 0.0, 0.5789217135796446, 0.0, 0.0, 0.6429712972008511, 0.0, 0.0, 0.661506988212563, 0.7629272979371657, 0.0, 0.4234710523361336, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4257443064600869, 0.0, 0.20425497460318195, 0.0, 0.5184318202957809, 0.5906121758765186, 0.0, 0.9091367634018797, 0.0, 0.30652125262867885, 0.0, 0.7954054080966687, 0.0, 0.0, 0.0, 1.0559083316811368, 0.0, 0.04427154393293609, 0.8228955354751877, 0.46658038808648383, 0.0, 0.02688606839966265, 0.0, 0.0, 0.6276611460113729, 0.22522673327345408, 0.0, 0.12308651028821192, 0.3657688390111318, 0.0, 0.046115245088609334, 0.0, 0.5342436705087195, 0.0, 0.1923886832463953, 0.07621763959827882, 0.0, 0.0, 0.0, 0.6725528112574624, 0.7483589153917343, 0.0, 0.0, 0.0, 0.0, 0.38949567271561986, 0.16492548861911044, 0.10519585180914466, 0.8853743380662012, 0.957787822214843, 0.0, 0.05459549529270359, 0.25656057396140347, 0.14167223917141744, 0.03949155262056029, 0.0, 0.0, 0.2703834370419471, 0.0, 0.723362221877522, 0.0, 0.0, 0.0, 0.7883757417725614, 0.0, 0.03340589062163017, 0.18896261922152233, 0.0, 0.0, 0.8122657251866786, 0.029607718570353176, 0.0, 0.008213735995679807, 0.0, 0.08605746445745502, 0.0, 0.3070376563830936, 0.10398717208817806, 0.0, 0.0, 0.0, 0.383984202548332, 0.0, 0.0, 0.3036396360576457, 0.0, 0.0, 0.0, 0.0, 0.11965354383371518, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04825455862354496, 0.3335108938296942, 0.0, 0.0, 0.0, 0.0, 0.04121745447265487, 0.30594446336043757, 0.0, 0.33550494159326405, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029116195807012437, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020041236855252602, 0.0, 0.4850188795993097, 0.0, 0.16403823255879735, 0.0, 0.60023572778736, 0.0, 0.1787344157826609, 0.0, 0.0, 0.17233073276918714, 0.17532212882841147, 0.6209016656456683, 0.5984929062073807, 0.8843163905991084, 0.3337919143714591, 0.0, 1.0919707374354302, 0.8166713304287454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8330317657834686, 0.27201200741148646, 0.0, 0.8995473587201992, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6388227016456749, 0.0, 0.3682672817259989, 0.0, 0.5422113050459496, 0.0, 0.028602469161704683, 0.0, 0.0, 0.3611222515381231, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.031337192831751, 0.0, 0.0, 0.7687924758178234, 0.012617011188267763, 0.0, 0.0, 0.37315715294884366, 0.0, 0.0, 0.0, 0.0, 0.6888779809403613, 0.121001715761302, 0.0, 0.0, 0.3680175749172942, 0.47420475938210593, 0.0, 0.7604201795169553, 0.0, 0.11917670920609563, 0.0, 0.20848198246903302, 0.24926285864223433, 0.0, 0.5553692000145817, 0.4169252107806106, 0.16597755096800731, 0.7334771026528798, 0.0, 0.0, 0.09908991746260937, 0.0, 0.14652270264303283, 0.006759647910977647, 0.0, 0.9351545954529237, 1.2960299208884896, 0.0, 0.0, 0.21191619141709428, 0.0, 0.0, 0.0, 0.01204540426582532, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06493914883934329, 0.7726981572397796, 0.0, 0.4370686273737238, 0.1778273534538074, 0.055209023869020324, 0.0, 0.0, 0.0, 0.05709969797354255, 0.0, 0.0, 0.0, 0.0, 0.44808598581285264, 0.0, 0.0, 0.0, 0.21141909545938053, 0.13390642572615394, 0.0, 0.0, 0.0, 0.25844994675864025, 0.0, 0.51617125876173, 0.2544268105667549, 0.0, 0.17576058376064335, 0.29087109088619356, 0.0, 0.0, 0.3286889977164017, 0.0, 0.0644707079973061, 0.0, 0.018682819036845075, 0.008522941045952347, 0.0, 0.0, 0.2520720928277996, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.146066293561638, 0.08856351358209161, 0.0, 0.0, 0.0, 0.1407746062228793, 0.5246446552397386, 0.0, 0.0, 0.0, 0.3279179855015388, 0.46137944574560436, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14330339619658897, 0.11500988018505275, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7444467047874328, 0.0, 0.0, 0.40713437972280675, 0.0, 0.0, 0.0, 0.5717679971406524, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4877440937322302, 0.6991654372087778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2369682176872117, 0.0, 0.0, 0.6757556051622259, 0.0, 0.0, 0.0, 0.576368321209737, 0.04986303698751877, 0.0, 0.5384684335273707, 0.0, 0.21524971756673417, 0.4963922980972075, 0.0, 0.0, 0.0, 0.0, 0.18940210229325644, 0.5277692784584271, 0.7718932464687741, 0.7210929692294604, 0.0, 0.0, 0.0, 0.0, 0.48286176484120297, 0.8517937989915767, 0.3931693184679991, 0.0, 0.0, 0.06755455710060922, 0.0, 0.026695131792948763, 1.4844324548606433, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4524914530619723, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4694831016582333, 0.0, 0.42831526212260856, 0.6074590079807168, 0.30066361967157035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6129938591206631, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6008436638333436, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43882326364474644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8591062832739853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6928708028466394, 0.2819039698547807, 0.08752108546971657, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05246236611269018, 0.0, 0.0, 0.5428804251437249, 0.022819504909203123, 0.3729965675892053, 0.0, 0.5741583158437262, 0.0, 0.1935811346150516, 0.0, 0.5023321549740235, 0.2611826745866744, 0.0, 0.056899028261467276, 0.37961815387770637, 0.22527271986010186, 0.05171423855571594, 0.058968538904135746, 0.0, 0.02892983665470066, 0.003313037290954743, 0.0, 0.0, 0.15785301822506514, 0.0, 0.0, 0.07773433689527456, 0.1049811110510813, 0.0, 0.02912372760696772, 0.0, 0.3114315285072337, 0.5703037753007745, 0.0, 0.04813466284828647, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02241933195791073, 0.0, 0.0, 0.0, 0.0, 0.23199999736075347, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1927795141790272, 0.2407119020891451, 0.0, 0.21960445663579467, 0.5770903406753464, 0.15415530724008072, 0.23155572792080611, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3142922871488459, 0.0, 0.0, 0.0, 0.0, 0.3042889785067072, 0.0, 0.5129566600174419, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3746354155106774, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4789154917746846, 0.17517544375428362, 0.0, 0.0, 0.0, 0.2637662382252967, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2102700997979013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.126545263019808, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.332133943612647, 0.0, 0.0, 0.0, 0.06184093060913505, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3212574037659458, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24673989573277713, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3017167941185386, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6233354196759152, 0.0, 0.0, 0.0, 0.5447611694646652, 0.0, 0.0, 0.216176776357065, 0.0, 0.19855221622967104, 0.8686764576101142, 0.16350923987929902, 0.0, 0.0, 0.02334951098786776, 0.17107621812888604, 0.48682879160279025, 0.7109808359466632, 0.6651558420918936, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6192268761337199, 0.31206921876898447, 0.0, 0.0, 0.24595989001208304, 0.11068536977374843, 0.0, 0.6618395820988949, 0.11362642836904417, 0.0, 0.0, 0.23765422406147768, 0.0, 0.0, 0.8073352785417658, 0.0, 0.0, 0.48485062583878286, 1.055137179889336, 0.2671063045394923, 0.0, 0.8476204266733893, 0.473385026956602, 0.5030912229948215, 0.0, 0.0, 0.0, 0.17678971866264329, 0.4076988147485064, 0.0, 0.6776909084507561, 0.13779930009477406, 0.5691281649873949, 0.16622169517158358, 0.433469475882236, 0.818379113845578, 0.9106844436778354, 0.0, 0.499393081506665, 0.0, 0.5725309586159512, 0.6160013948015824, 0.5125607980557373, 0.2778645861379971, 0.0, 0.0, 0.0, 0.0, 0.0, 0.58929804834674, 0.0, 0.8046233984476477, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3967677816937078, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11707526901952116, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.383435421983313, 0.11707115655719894, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07864429232511502, 0.0, 0.0, 0.3002587300128159, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2266804769859352, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24215921984980293, 0.09852579316681935, 0.030588729804574537, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18874358875246125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3978422171614565, 0.1936032919321414, 0.0, 0.0, 0.2542147176776134, 0.0, 0.0, 0.0, 0.11057553114503467, 0.0, 6.012518808232222e-5, 0.0008825884581384356, 0.6460696181307314, 0.0, 0.0, 0.06879188140017427, 0.0, 0.0, 0.04716298956935701, 0.1540138997690347, 0.1854894997474166, 0.003124358567077274, 0.35109153378778224, 0.0, 0.0, 0.016293742009993128, 0.0, 0.0, 0.3747805868672189, 0.025859295570033313, 0.214029422856965, 0.003742959361061961, 0.0002933458280023473, 0.0, 0.0, 0.0, 0.030501494612752977, 0.0, 0.6996784734474946, 0.0, 0.0, 0.05616866413207192, 0.0, 0.0018184995370819535, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020171782722387992, 0.0, 0.0, 0.33279196588498333, 0.0, 0.0, 0.6323890570808377, 0.0, 0.0, 0.0, 0.03594525644730097, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004527887543653786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17039388977135247, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6309056496928738, 0.0, 0.0, 0.0, 0.0, 0.260280820884557, 0.0, 0.0, 0.5107183088816271, 0.7967464980324664, 0.14485492608681305, 0.0, 0.009558078807310686, 0.754701551784825, 0.0, 0.0, 0.0, 0.0, 0.7598764239846039, 0.0, 0.029177780358136646, 0.10253044625069853, 0.03763219091970284, 0.8115624154017409, 0.0, 0.5779584654853126, 0.7162312164421211, 0.0, 0.0, 0.9107472399622886, 0.7029540573396418, 0.0, 1.0969174796040306, 0.0, 0.0, 0.0, 0.0, 0.0038591832437520454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15366266383288085, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.558080611304664, 0.0, 0.0, 0.0, 0.0, 0.30736701150515244, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04311886842935436, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32751189175201845, 0.0, 0.20963216887545572, 0.1760259950046517, 0.0, 0.23753864749234863, 0.24093292162050975, 0.024684548149881243, 0.4936183719527589, 0.7064246796060082, 0.0, 0.06677453327599289, 0.8542035190842152, 0.0, 0.025534429812218047, 0.011379900282647532, 0.039800411165778926, 0.013727111201321645, 0.27843394547131894, 0.24161261590501448, 0.2499939278392831, 0.0, 0.0, 0.3450248607837782, 0.0, 0.0, 0.15272914557490577, 0.10495111476871369, 0.0, 0.0, 0.0, 0.18866875753074405, 0.0, 0.2225813946448797, 0.0, 0.0, 0.3661590836002555, 0.0, 0.0, 0.0, 0.8998957690359772, 0.30889178773893766, 0.6078813570942335, 0.0, 0.6840699471933163, 0.0, 0.0, 0.0, 0.36308986165522683, 0.752528074611531, 0.7191770460546526, 0.49508503625401296, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7784554319844289, 0.16352298326175332, 0.0, 0.042948548224867954, 0.375190336410357, 0.0, 0.6830715485727474, 0.6403864588932121, 0.7831097846364896, 0.46112164648188003, 0.14556009485746954, 0.0, 0.0, 0.0, 0.8585580212631712, 0.0, 0.253359163441029, 0.0, 0.0, 0.0, 0.45902026051340783, 0.0, 0.0, 0.0, 0.0, 0.166881895984427, 0.0, 0.0, 0.837727301005, 0.0, 0.0, 0.7629510291223568, 0.0, 0.34201996071392393, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3670598185817387, 0.0, 0.9533138689466589, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8365046398061449, 0.0, 0.9590136929209965, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.801491142663501, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16957493050977235, 0.0, 0.2507477829737011, 0.0, 0.0, 0.11532054524043628, 0.07735685692161769, 0.5247858743947099, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02100484631395434, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8201592179395716, 0.0, 0.0, 0.04562437343117348, 0.0, 0.22915724847477928, 0.0, 0.0, 0.7159205767218768, 0.0, 0.0, 0.0, 0.0, 0.29228977882295926, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17928588753235497, 0.10348218971035608, 0.8147006956016374, 0.08438351505236888, 0.0, 0.0, 0.0, 0.1070859593707194, 0.7148756921758321, 0.0, 0.8195717571774336, 0.12249576952187838, 0.0, 0.2397374334824377, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05225556550152024, 0.0, 0.0, 0.0, 0.1707240047839942, 0.0, 0.11920584928810601, 0.0, 0.14514911616615112, 0.4284534617057244, 0.04403067305952579, 0.0, 0.012688229227290206, 0.3654383338493288, 0.04642776495318284, 0.0, 0.0, 0.31371261026073766, 0.1364760956744218, 0.5008825042361101, 0.0, 0.04802625911306818, 0.021403808257108425, 0.0, 0.12191697007589085, 0.33632693219045084, 0.49073784844916374, 0.5066599946239788, 0.0, 0.0, 0.033354950181247504, 0.0, 0.15690710970564234, 0.3397139005999492, 0.4535410643500978, 0.0, 0.0, 0.49050280987387757, 0.0, 0.0, 0.45491953624514714, 0.0, 0.0, 0.33864218894794285, 0.0, 0.0, 0.041024172335819876, 0.0, 0.0, 0.0, 0.36175943329836924, 0.0, 0.39749358702806525, 0.0, 0.5595492530984049, 0.0, 0.0, 0.2573403608635483, 0.17262354625512744, 0.0, 0.0, 0.0, 0.3776391490751126, 0.0, 0.0, 0.1445911005474468, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.45961437114556436, 0.08073146338811468, 0.0, 0.0, 0.0, 0.5680662673212686, 0.0, 0.14301647595845865, 0.4278925860448857, 0.2624303541096862, 0.0, 0.5619603114708978, 0.5488835922357254, 0.0, 0.10445139145624531, 0.9180806504163392, 0.5113694952971799, 0.0, 0.4737323723594051, 0.4474251824766889, 0.10616343839228443, 0.0, 0.02745363380188561, 0.0, 0.28312699798757224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6983886824821985, 0.38943652344243257, 1.2223890565530993, 0.05586381199212052, 0.0, 0.07686093310040262, 0.10701921729681718, 0.0, 0.4467721663826107, 0.0, 0.5122035249872549, 1.008045581843232, 0.10571972464185753, 0.6235175832869809, 0.0, 0.0, 0.0, 0.0, 0.7054625658357997, 0.0, 0.0, 0.17326833225856653, 0.0, 0.0, 0.0, 1.0813156328128786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0927477810868, 0.0, 0.599657110852462, 0.22383250773975968, 0.19685563542122056, 0.0, 0.05605157239206448, 0.41837305013296433, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18996852120778085, 0.0, 0.28856245320282303, 0.0, 0.0, 0.0, 0.26772611418599673, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1616947156362974, 0.0, 0.0, 0.0, 0.0, 0.36168541397352055, 0.0, 0.0, 0.0, 0.1573219565054934, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13761515434984456, 0.07943022011077468, 0.0, 0.21912413885604493, 0.0, 0.0, 0.40654019224262405, 0.0, 0.0, 0.022840701875983912, 0.0, 0.09402454628178203, 0.5332211799090736, 0.14433851006651358, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.043396172364084024, 0.04011000425692938, 0.0, 0.0, 0.0, 0.13104327726482687, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10232284486723907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10523174379677519, 0.0, 0.10079612361252216, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04415728315159431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11805369977437472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6523402818477722, 0.04871612610613401, 0.0, 0.19287524873967893, 0.0, 0.3592812200503388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.050401634126662875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8285325211040351, 0.0, 0.0, 0.0, 0.42701794948426475, 0.33826599655789313, 0.0, 0.0, 0.0063200711569476415, 0.0, 0.0, 0.4430530345512502, 0.0, 0.6935424923816391, 0.9428504267370209, 0.42746265456576077, 0.0025457381888520196, 0.0, 0.1593559778661365, 0.0, 0.8273232796667221, 0.04988580847200336, 0.9484876902255591, 0.5784846962232053, 0.0, 0.4437739313478008, 0.0, 0.0, 0.29414816951171024, 0.0, 0.022355194883774517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02069724212749366, 0.0, 0.0, 0.0, 0.0, 0.7860463101009033, 0.0, 0.0, 0.0, 0.8499064420418566, 0.0, 0.014821364995218649, 0.0, 0.004694386740689857, 0.25038082548050006, 0.5774089497757802, 0.0, 0.0, 0.0, 0.0, 0.006270331329794731, 0.6139069964758506, 0.8351435200462407, 1.0242327730345495, 0.0, 0.029307532281045708, 0.0, 0.0, 0.0, 0.5726801471740839, 0.4962238989829506, 0.0, 0.07225356949876899, 0.10746103390935668, 0.0, 0.0, 0.8346013157058231, 0.0, 0.0, 0.12871551917858068, 0.0, 0.0, 0.05722051841888945, 0.0, 0.08248659071056784, 0.08634381582594033, 0.0, 0.14691645141749232, 0.0, 0.0, 0.0, 0.12530859946217338, 0.0, 0.10215231917559951, 0.07032216186532725, 0.0, 0.8790426858900641, 0.10792096194290268, 0.054388076691659265, 0.0, 0.0, 0.0, 0.05001396409640395, 0.114742650990913, 0.28339886512623463, 0.15677327627325063, 0.0, 0.03258480102579557, 0.09096085908673467, 0.0, 0.0, 0.15530131741375458, 0.4726591715297998, 0.7751806988017736, 0.0, 0.51208607063286, 0.0, 0.0, 0.8142774797804637, 0.037795557638802935, 0.0, 0.03542525158598919, 0.07905092197782795, 0.0, 0.0, 0.1322231578644953, 0.0, 0.0, 0.008286484320060336, 0.670492082894006, 0.0, 0.0, 0.2274177128809876, 0.20430769763068007, 0.27374263149272277, 0.0, 0.0, 0.0, 0.0, 0.22105379716709922, 0.08605636125516496, 0.023058224273634728, 0.03343036365050954, 0.7905997431838073, 0.031091378094046798, 0.32331562706461314, 0.0, 0.048595513480849374, 0.0, 0.6695135011372569, 0.01079155413526415, 0.7675661133629853, 0.01950494389053299, 0.0, 0.06609079415505228, 0.15915608286937477, 0.0, 0.17610090955799163, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05278940542005462, 0.0, 0.04363140001862599, 0.3747258422202424, 0.18235409289517635]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = ((ex.bodies[1]).bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    result = nothing
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    for i_4 = 1:A_lvl.shape[1]
        val = Ct_lvl_2_val
        Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
        B_lvl_ptr_2 = B_lvl_ptr
        B_lvl_ptr = (Finch).moveto(B_lvl_ptr, CPU(Threads.nthreads()))
        B_lvl_tbl1_2 = B_lvl_tbl1
        B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
        B_lvl_tbl2_2 = B_lvl_tbl2
        B_lvl_tbl2 = (Finch).moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
        val_2 = B_lvl_val
        B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
        A_lvl_ptr_2 = A_lvl_ptr
        A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
        A_lvl_tbl1_2 = A_lvl_tbl1
        A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
        A_lvl_tbl2_2 = A_lvl_tbl2
        A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
        val_3 = A_lvl_val
        A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
        Threads.@threads for i_5 = 1:Threads.nthreads()
                B_lvl_q = B_lvl_ptr[1]
                B_lvl_q_stop = B_lvl_ptr[1 + 1]
                if B_lvl_q < B_lvl_q_stop
                    B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                else
                    B_lvl_i_stop = 0
                end
                phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_5 + -1), Threads.nthreads()))
                phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_5, Threads.nthreads()))
                if phase_stop_2 >= phase_start_2
                    if B_lvl_tbl2[B_lvl_q] < phase_start_2
                        B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    while true
                        B_lvl_i = B_lvl_tbl2[B_lvl_q]
                        B_lvl_q_step = B_lvl_q
                        if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                            B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        if B_lvl_i < phase_stop_2
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_4, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_4
                                            if A_lvl_tbl1[A_lvl_q] < i_4
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        else
                            phase_stop_13 = min(B_lvl_i, phase_stop_2)
                            if B_lvl_i == phase_stop_13
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_4, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_4
                                                if A_lvl_tbl1[A_lvl_q] < i_4
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            end
                            break
                        end
                    end
                end
            end
        Ct_lvl_2_val = val
        B_lvl_ptr = B_lvl_ptr_2
        B_lvl_tbl1 = B_lvl_tbl1_2
        B_lvl_tbl2 = B_lvl_tbl2_2
        B_lvl_val = val_2
        A_lvl_ptr = A_lvl_ptr_2
        A_lvl_tbl1 = A_lvl_tbl1_2
        A_lvl_tbl2 = A_lvl_tbl2_2
        A_lvl_val = val_3
    end
    resize!(Ct_lvl_2_val, A_lvl.shape[1] * B_lvl.shape[2])
    result = (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
    result
end
julia> @finch begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0, 0.0, 0.0, 0.01855804465451247, 0.0, 0.0, 0.03641425883740431, 0.0568080930379847, 0.10109716846875504, 0.0, 0.0, 0.0, 0.0, 0.005424838537552436, 0.0, 0.0, 0.5251896042062963, 0.0, 0.0, 0.0, 0.00515342598991506, 0.045980273053842245, 0.0, 0.4033688458623361, 0.0510673466163073, 0.0, 0.0, 0.02134197385615031, 0.01687808266981965, 0.0, 0.4932447371732914, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7178045198025068, 0.2106682695954306, 0.0, 0.3115118572371521, 0.5170963380295581, 0.39595498771521426, 0.1432663403816299, 0.5698542189659496, 0.0, 0.0, 0.0, 0.5317966816929195, 0.0, 0.5315818882594147, 0.8168445783005969, 0.0, 0.0, 0.0, 0.01222733860627223, 0.0, 0.716756885645577, 0.0, 0.8217284580618865, 0.48234209517721377, 0.15545502627894098, 0.04826321760798536, 0.0, 0.0, 0.1394038038151463, 0.0, 0.056802678199824076, 0.0, 0.0, 0.0, 0.3128541399180126, 0.0, 0.0, 0.596593418834028, 0.0, 0.2846892571697076, 0.0, 0.0, 0.0, 0.23365792107187477, 0.0, 0.0, 0.4584793374366712, 0.7152510496888798, 0.0, 0.0, 0.0, 0.8190896825195866, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11127793558853107, 0.0, 0.5789217135796446, 0.0, 0.0, 0.6429712972008511, 0.0, 0.0, 0.661506988212563, 0.7629272979371657, 0.0, 0.4234710523361336, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4257443064600869, 0.0, 0.20425497460318195, 0.0, 0.5184318202957809, 0.5906121758765186, 0.0, 0.9091367634018797, 0.0, 0.30652125262867885, 0.0, 0.7954054080966687, 0.0, 0.0, 0.0, 1.0559083316811368, 0.0, 0.04427154393293609, 0.8228955354751877, 0.46658038808648383, 0.0, 0.02688606839966265, 0.0, 0.0, 0.6276611460113729, 0.22522673327345408, 0.0, 0.12308651028821192, 0.3657688390111318, 0.0, 0.046115245088609334, 0.0, 0.5342436705087195, 0.0, 0.1923886832463953, 0.07621763959827882, 0.0, 0.0, 0.0, 0.6725528112574624, 0.7483589153917343, 0.0, 0.0, 0.0, 0.0, 0.38949567271561986, 0.16492548861911044, 0.10519585180914466, 0.8853743380662012, 0.957787822214843, 0.0, 0.05459549529270359, 0.25656057396140347, 0.14167223917141744, 0.03949155262056029, 0.0, 0.0, 0.2703834370419471, 0.0, 0.723362221877522, 0.0, 0.0, 0.0, 0.7883757417725614, 0.0, 0.03340589062163017, 0.18896261922152233, 0.0, 0.0, 0.8122657251866786, 0.029607718570353176, 0.0, 0.008213735995679807, 0.0, 0.08605746445745502, 0.0, 0.3070376563830936, 0.10398717208817806, 0.0, 0.0, 0.0, 0.383984202548332, 0.0, 0.0, 0.3036396360576457, 0.0, 0.0, 0.0, 0.0, 0.11965354383371518, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04825455862354496, 0.3335108938296942, 0.0, 0.0, 0.0, 0.0, 0.04121745447265487, 0.30594446336043757, 0.0, 0.33550494159326405, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029116195807012437, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020041236855252602, 0.0, 0.4850188795993097, 0.0, 0.16403823255879735, 0.0, 0.60023572778736, 0.0, 0.1787344157826609, 0.0, 0.0, 0.17233073276918714, 0.17532212882841147, 0.6209016656456683, 0.5984929062073807, 0.8843163905991084, 0.3337919143714591, 0.0, 1.0919707374354302, 0.8166713304287454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8330317657834686, 0.27201200741148646, 0.0, 0.8995473587201992, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6388227016456749, 0.0, 0.3682672817259989, 0.0, 0.5422113050459496, 0.0, 0.028602469161704683, 0.0, 0.0, 0.3611222515381231, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.031337192831751, 0.0, 0.0, 0.7687924758178234, 0.012617011188267763, 0.0, 0.0, 0.37315715294884366, 0.0, 0.0, 0.0, 0.0, 0.6888779809403613, 0.121001715761302, 0.0, 0.0, 0.3680175749172942, 0.47420475938210593, 0.0, 0.7604201795169553, 0.0, 0.11917670920609563, 0.0, 0.20848198246903302, 0.24926285864223433, 0.0, 0.5553692000145817, 0.4169252107806106, 0.16597755096800731, 0.7334771026528798, 0.0, 0.0, 0.09908991746260937, 0.0, 0.14652270264303283, 0.006759647910977647, 0.0, 0.9351545954529237, 1.2960299208884896, 0.0, 0.0, 0.21191619141709428, 0.0, 0.0, 0.0, 0.01204540426582532, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06493914883934329, 0.7726981572397796, 0.0, 0.4370686273737238, 0.1778273534538074, 0.055209023869020324, 0.0, 0.0, 0.0, 0.05709969797354255, 0.0, 0.0, 0.0, 0.0, 0.44808598581285264, 0.0, 0.0, 0.0, 0.21141909545938053, 0.13390642572615394, 0.0, 0.0, 0.0, 0.25844994675864025, 0.0, 0.51617125876173, 0.2544268105667549, 0.0, 0.17576058376064335, 0.29087109088619356, 0.0, 0.0, 0.3286889977164017, 0.0, 0.0644707079973061, 0.0, 0.018682819036845075, 0.008522941045952347, 0.0, 0.0, 0.2520720928277996, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.146066293561638, 0.08856351358209161, 0.0, 0.0, 0.0, 0.1407746062228793, 0.5246446552397386, 0.0, 0.0, 0.0, 0.3279179855015388, 0.46137944574560436, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14330339619658897, 0.11500988018505275, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7444467047874328, 0.0, 0.0, 0.40713437972280675, 0.0, 0.0, 0.0, 0.5717679971406524, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4877440937322302, 0.6991654372087778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2369682176872117, 0.0, 0.0, 0.6757556051622259, 0.0, 0.0, 0.0, 0.576368321209737, 0.04986303698751877, 0.0, 0.5384684335273707, 0.0, 0.21524971756673417, 0.4963922980972075, 0.0, 0.0, 0.0, 0.0, 0.18940210229325644, 0.5277692784584271, 0.7718932464687741, 0.7210929692294604, 0.0, 0.0, 0.0, 0.0, 0.48286176484120297, 0.8517937989915767, 0.3931693184679991, 0.0, 0.0, 0.06755455710060922, 0.0, 0.026695131792948763, 1.4844324548606433, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4524914530619723, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4694831016582333, 0.0, 0.42831526212260856, 0.6074590079807168, 0.30066361967157035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6129938591206631, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6008436638333436, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43882326364474644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8591062832739853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6928708028466394, 0.2819039698547807, 0.08752108546971657, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05246236611269018, 0.0, 0.0, 0.5428804251437249, 0.022819504909203123, 0.3729965675892053, 0.0, 0.5741583158437262, 0.0, 0.1935811346150516, 0.0, 0.5023321549740235, 0.2611826745866744, 0.0, 0.056899028261467276, 0.37961815387770637, 0.22527271986010186, 0.05171423855571594, 0.058968538904135746, 0.0, 0.02892983665470066, 0.003313037290954743, 0.0, 0.0, 0.15785301822506514, 0.0, 0.0, 0.07773433689527456, 0.1049811110510813, 0.0, 0.02912372760696772, 0.0, 0.3114315285072337, 0.5703037753007745, 0.0, 0.04813466284828647, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02241933195791073, 0.0, 0.0, 0.0, 0.0, 0.23199999736075347, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1927795141790272, 0.2407119020891451, 0.0, 0.21960445663579467, 0.5770903406753464, 0.15415530724008072, 0.23155572792080611, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3142922871488459, 0.0, 0.0, 0.0, 0.0, 0.3042889785067072, 0.0, 0.5129566600174419, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3746354155106774, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4789154917746846, 0.17517544375428362, 0.0, 0.0, 0.0, 0.2637662382252967, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2102700997979013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.126545263019808, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.332133943612647, 0.0, 0.0, 0.0, 0.06184093060913505, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3212574037659458, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24673989573277713, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3017167941185386, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6233354196759152, 0.0, 0.0, 0.0, 0.5447611694646652, 0.0, 0.0, 0.216176776357065, 0.0, 0.19855221622967104, 0.8686764576101142, 0.16350923987929902, 0.0, 0.0, 0.02334951098786776, 0.17107621812888604, 0.48682879160279025, 0.7109808359466632, 0.6651558420918936, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6192268761337199, 0.31206921876898447, 0.0, 0.0, 0.24595989001208304, 0.11068536977374843, 0.0, 0.6618395820988949, 0.11362642836904417, 0.0, 0.0, 0.23765422406147768, 0.0, 0.0, 0.8073352785417658, 0.0, 0.0, 0.48485062583878286, 1.055137179889336, 0.2671063045394923, 0.0, 0.8476204266733893, 0.473385026956602, 0.5030912229948215, 0.0, 0.0, 0.0, 0.17678971866264329, 0.4076988147485064, 0.0, 0.6776909084507561, 0.13779930009477406, 0.5691281649873949, 0.16622169517158358, 0.433469475882236, 0.818379113845578, 0.9106844436778354, 0.0, 0.499393081506665, 0.0, 0.5725309586159512, 0.6160013948015824, 0.5125607980557373, 0.2778645861379971, 0.0, 0.0, 0.0, 0.0, 0.0, 0.58929804834674, 0.0, 0.8046233984476477, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3967677816937078, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11707526901952116, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.383435421983313, 0.11707115655719894, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07864429232511502, 0.0, 0.0, 0.3002587300128159, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2266804769859352, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24215921984980293, 0.09852579316681935, 0.030588729804574537, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18874358875246125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3978422171614565, 0.1936032919321414, 0.0, 0.0, 0.2542147176776134, 0.0, 0.0, 0.0, 0.11057553114503467, 0.0, 6.012518808232222e-5, 0.0008825884581384356, 0.6460696181307314, 0.0, 0.0, 0.06879188140017427, 0.0, 0.0, 0.04716298956935701, 0.1540138997690347, 0.1854894997474166, 0.003124358567077274, 0.35109153378778224, 0.0, 0.0, 0.016293742009993128, 0.0, 0.0, 0.3747805868672189, 0.025859295570033313, 0.214029422856965, 0.003742959361061961, 0.0002933458280023473, 0.0, 0.0, 0.0, 0.030501494612752977, 0.0, 0.6996784734474946, 0.0, 0.0, 0.05616866413207192, 0.0, 0.0018184995370819535, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020171782722387992, 0.0, 0.0, 0.33279196588498333, 0.0, 0.0, 0.6323890570808377, 0.0, 0.0, 0.0, 0.03594525644730097, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004527887543653786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17039388977135247, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6309056496928738, 0.0, 0.0, 0.0, 0.0, 0.260280820884557, 0.0, 0.0, 0.5107183088816271, 0.7967464980324664, 0.14485492608681305, 0.0, 0.009558078807310686, 0.754701551784825, 0.0, 0.0, 0.0, 0.0, 0.7598764239846039, 0.0, 0.029177780358136646, 0.10253044625069853, 0.03763219091970284, 0.8115624154017409, 0.0, 0.5779584654853126, 0.7162312164421211, 0.0, 0.0, 0.9107472399622886, 0.7029540573396418, 0.0, 1.0969174796040306, 0.0, 0.0, 0.0, 0.0, 0.0038591832437520454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15366266383288085, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.558080611304664, 0.0, 0.0, 0.0, 0.0, 0.30736701150515244, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04311886842935436, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32751189175201845, 0.0, 0.20963216887545572, 0.1760259950046517, 0.0, 0.23753864749234863, 0.24093292162050975, 0.024684548149881243, 0.4936183719527589, 0.7064246796060082, 0.0, 0.06677453327599289, 0.8542035190842152, 0.0, 0.025534429812218047, 0.011379900282647532, 0.039800411165778926, 0.013727111201321645, 0.27843394547131894, 0.24161261590501448, 0.2499939278392831, 0.0, 0.0, 0.3450248607837782, 0.0, 0.0, 0.15272914557490577, 0.10495111476871369, 0.0, 0.0, 0.0, 0.18866875753074405, 0.0, 0.2225813946448797, 0.0, 0.0, 0.3661590836002555, 0.0, 0.0, 0.0, 0.8998957690359772, 0.30889178773893766, 0.6078813570942335, 0.0, 0.6840699471933163, 0.0, 0.0, 0.0, 0.36308986165522683, 0.752528074611531, 0.7191770460546526, 0.49508503625401296, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7784554319844289, 0.16352298326175332, 0.0, 0.042948548224867954, 0.375190336410357, 0.0, 0.6830715485727474, 0.6403864588932121, 0.7831097846364896, 0.46112164648188003, 0.14556009485746954, 0.0, 0.0, 0.0, 0.8585580212631712, 0.0, 0.253359163441029, 0.0, 0.0, 0.0, 0.45902026051340783, 0.0, 0.0, 0.0, 0.0, 0.166881895984427, 0.0, 0.0, 0.837727301005, 0.0, 0.0, 0.7629510291223568, 0.0, 0.34201996071392393, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3670598185817387, 0.0, 0.9533138689466589, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8365046398061449, 0.0, 0.9590136929209965, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.801491142663501, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16957493050977235, 0.0, 0.2507477829737011, 0.0, 0.0, 0.11532054524043628, 0.07735685692161769, 0.5247858743947099, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02100484631395434, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8201592179395716, 0.0, 0.0, 0.04562437343117348, 0.0, 0.22915724847477928, 0.0, 0.0, 0.7159205767218768, 0.0, 0.0, 0.0, 0.0, 0.29228977882295926, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17928588753235497, 0.10348218971035608, 0.8147006956016374, 0.08438351505236888, 0.0, 0.0, 0.0, 0.1070859593707194, 0.7148756921758321, 0.0, 0.8195717571774336, 0.12249576952187838, 0.0, 0.2397374334824377, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05225556550152024, 0.0, 0.0, 0.0, 0.1707240047839942, 0.0, 0.11920584928810601, 0.0, 0.14514911616615112, 0.4284534617057244, 0.04403067305952579, 0.0, 0.012688229227290206, 0.3654383338493288, 0.04642776495318284, 0.0, 0.0, 0.31371261026073766, 0.1364760956744218, 0.5008825042361101, 0.0, 0.04802625911306818, 0.021403808257108425, 0.0, 0.12191697007589085, 0.33632693219045084, 0.49073784844916374, 0.5066599946239788, 0.0, 0.0, 0.033354950181247504, 0.0, 0.15690710970564234, 0.3397139005999492, 0.4535410643500978, 0.0, 0.0, 0.49050280987387757, 0.0, 0.0, 0.45491953624514714, 0.0, 0.0, 0.33864218894794285, 0.0, 0.0, 0.041024172335819876, 0.0, 0.0, 0.0, 0.36175943329836924, 0.0, 0.39749358702806525, 0.0, 0.5595492530984049, 0.0, 0.0, 0.2573403608635483, 0.17262354625512744, 0.0, 0.0, 0.0, 0.3776391490751126, 0.0, 0.0, 0.1445911005474468, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.45961437114556436, 0.08073146338811468, 0.0, 0.0, 0.0, 0.5680662673212686, 0.0, 0.14301647595845865, 0.4278925860448857, 0.2624303541096862, 0.0, 0.5619603114708978, 0.5488835922357254, 0.0, 0.10445139145624531, 0.9180806504163392, 0.5113694952971799, 0.0, 0.4737323723594051, 0.4474251824766889, 0.10616343839228443, 0.0, 0.02745363380188561, 0.0, 0.28312699798757224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6983886824821985, 0.38943652344243257, 1.2223890565530993, 0.05586381199212052, 0.0, 0.07686093310040262, 0.10701921729681718, 0.0, 0.4467721663826107, 0.0, 0.5122035249872549, 1.008045581843232, 0.10571972464185753, 0.6235175832869809, 0.0, 0.0, 0.0, 0.0, 0.7054625658357997, 0.0, 0.0, 0.17326833225856653, 0.0, 0.0, 0.0, 1.0813156328128786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0927477810868, 0.0, 0.599657110852462, 0.22383250773975968, 0.19685563542122056, 0.0, 0.05605157239206448, 0.41837305013296433, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18996852120778085, 0.0, 0.28856245320282303, 0.0, 0.0, 0.0, 0.26772611418599673, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1616947156362974, 0.0, 0.0, 0.0, 0.0, 0.36168541397352055, 0.0, 0.0, 0.0, 0.1573219565054934, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13761515434984456, 0.07943022011077468, 0.0, 0.21912413885604493, 0.0, 0.0, 0.40654019224262405, 0.0, 0.0, 0.022840701875983912, 0.0, 0.09402454628178203, 0.5332211799090736, 0.14433851006651358, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.043396172364084024, 0.04011000425692938, 0.0, 0.0, 0.0, 0.13104327726482687, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10232284486723907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10523174379677519, 0.0, 0.10079612361252216, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04415728315159431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11805369977437472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6523402818477722, 0.04871612610613401, 0.0, 0.19287524873967893, 0.0, 0.3592812200503388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.050401634126662875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8285325211040351, 0.0, 0.0, 0.0, 0.42701794948426475, 0.33826599655789313, 0.0, 0.0, 0.0063200711569476415, 0.0, 0.0, 0.4430530345512502, 0.0, 0.6935424923816391, 0.9428504267370209, 0.42746265456576077, 0.0025457381888520196, 0.0, 0.1593559778661365, 0.0, 0.8273232796667221, 0.04988580847200336, 0.9484876902255591, 0.5784846962232053, 0.0, 0.4437739313478008, 0.0, 0.0, 0.29414816951171024, 0.0, 0.022355194883774517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02069724212749366, 0.0, 0.0, 0.0, 0.0, 0.7860463101009033, 0.0, 0.0, 0.0, 0.8499064420418566, 0.0, 0.014821364995218649, 0.0, 0.004694386740689857, 0.25038082548050006, 0.5774089497757802, 0.0, 0.0, 0.0, 0.0, 0.006270331329794731, 0.6139069964758506, 0.8351435200462407, 1.0242327730345495, 0.0, 0.029307532281045708, 0.0, 0.0, 0.0, 0.5726801471740839, 0.4962238989829506, 0.0, 0.07225356949876899, 0.10746103390935668, 0.0, 0.0, 0.8346013157058231, 0.0, 0.0, 0.12871551917858068, 0.0, 0.0, 0.05722051841888945, 0.0, 0.08248659071056784, 0.08634381582594033, 0.0, 0.14691645141749232, 0.0, 0.0, 0.0, 0.12530859946217338, 0.0, 0.10215231917559951, 0.07032216186532725, 0.0, 0.8790426858900641, 0.10792096194290268, 0.054388076691659265, 0.0, 0.0, 0.0, 0.05001396409640395, 0.114742650990913, 0.28339886512623463, 0.15677327627325063, 0.0, 0.03258480102579557, 0.09096085908673467, 0.0, 0.0, 0.15530131741375458, 0.4726591715297998, 0.7751806988017736, 0.0, 0.51208607063286, 0.0, 0.0, 0.8142774797804637, 0.037795557638802935, 0.0, 0.03542525158598919, 0.07905092197782795, 0.0, 0.0, 0.1322231578644953, 0.0, 0.0, 0.008286484320060336, 0.670492082894006, 0.0, 0.0, 0.2274177128809876, 0.20430769763068007, 0.27374263149272277, 0.0, 0.0, 0.0, 0.0, 0.22105379716709922, 0.08605636125516496, 0.023058224273634728, 0.03343036365050954, 0.7905997431838073, 0.031091378094046798, 0.32331562706461314, 0.0, 0.048595513480849374, 0.0, 0.6695135011372569, 0.01079155413526415, 0.7675661133629853, 0.01950494389053299, 0.0, 0.06609079415505228, 0.15915608286937477, 0.0, 0.17610090955799163, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05278940542005462, 0.0, 0.04363140001862599, 0.3747258422202424, 0.18235409289517635]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = ((ex.bodies[1]).bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    result = nothing
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = (Finch).moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = (Finch).moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            B_lvl_q = B_lvl_ptr[1]
            B_lvl_q_stop = B_lvl_ptr[1 + 1]
            if B_lvl_q < B_lvl_q_stop
                B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
            else
                B_lvl_i_stop = 0
            end
            phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                if B_lvl_tbl2[B_lvl_q] < phase_start_2
                    B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                end
                while true
                    B_lvl_i = B_lvl_tbl2[B_lvl_q]
                    B_lvl_q_step = B_lvl_q
                    if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                        B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    if B_lvl_i < phase_stop_2
                        Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                        for i_6 = 1:A_lvl.shape[1]
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_6
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_6, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_6
                                            if A_lvl_tbl1[A_lvl_q] < i_6
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_6, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                        end
                        B_lvl_q = B_lvl_q_step
                    else
                        phase_stop_13 = min(B_lvl_i, phase_stop_2)
                        if B_lvl_i == phase_stop_13
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                            for i_8 = 1:A_lvl.shape[1]
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_8
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_8, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_8
                                                if A_lvl_tbl1[A_lvl_q] < i_8
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_8, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        end
                        break
                    end
                end
            end
        end
    resize!(val, A_lvl.shape[1] * B_lvl.shape[2])
    result = (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
    result
end
julia> @finch begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0, 0.0, 0.0, 0.01855804465451247, 0.0, 0.0, 0.03641425883740431, 0.0568080930379847, 0.10109716846875504, 0.0, 0.0, 0.0, 0.0, 0.005424838537552436, 0.0, 0.0, 0.5251896042062963, 0.0, 0.0, 0.0, 0.00515342598991506, 0.045980273053842245, 0.0, 0.4033688458623361, 0.0510673466163073, 0.0, 0.0, 0.02134197385615031, 0.01687808266981965, 0.0, 0.4932447371732914, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7178045198025068, 0.2106682695954306, 0.0, 0.3115118572371521, 0.5170963380295581, 0.39595498771521426, 0.1432663403816299, 0.5698542189659496, 0.0, 0.0, 0.0, 0.5317966816929195, 0.0, 0.5315818882594147, 0.8168445783005969, 0.0, 0.0, 0.0, 0.01222733860627223, 0.0, 0.716756885645577, 0.0, 0.8217284580618865, 0.48234209517721377, 0.15545502627894098, 0.04826321760798536, 0.0, 0.0, 0.1394038038151463, 0.0, 0.056802678199824076, 0.0, 0.0, 0.0, 0.3128541399180126, 0.0, 0.0, 0.596593418834028, 0.0, 0.2846892571697076, 0.0, 0.0, 0.0, 0.23365792107187477, 0.0, 0.0, 0.4584793374366712, 0.7152510496888798, 0.0, 0.0, 0.0, 0.8190896825195866, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11127793558853107, 0.0, 0.5789217135796446, 0.0, 0.0, 0.6429712972008511, 0.0, 0.0, 0.661506988212563, 0.7629272979371657, 0.0, 0.4234710523361336, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4257443064600869, 0.0, 0.20425497460318195, 0.0, 0.5184318202957809, 0.5906121758765186, 0.0, 0.9091367634018797, 0.0, 0.30652125262867885, 0.0, 0.7954054080966687, 0.0, 0.0, 0.0, 1.0559083316811368, 0.0, 0.04427154393293609, 0.8228955354751877, 0.46658038808648383, 0.0, 0.02688606839966265, 0.0, 0.0, 0.6276611460113729, 0.22522673327345408, 0.0, 0.12308651028821192, 0.3657688390111318, 0.0, 0.046115245088609334, 0.0, 0.5342436705087195, 0.0, 0.1923886832463953, 0.07621763959827882, 0.0, 0.0, 0.0, 0.6725528112574624, 0.7483589153917343, 0.0, 0.0, 0.0, 0.0, 0.38949567271561986, 0.16492548861911044, 0.10519585180914466, 0.8853743380662012, 0.957787822214843, 0.0, 0.05459549529270359, 0.25656057396140347, 0.14167223917141744, 0.03949155262056029, 0.0, 0.0, 0.2703834370419471, 0.0, 0.723362221877522, 0.0, 0.0, 0.0, 0.7883757417725614, 0.0, 0.03340589062163017, 0.18896261922152233, 0.0, 0.0, 0.8122657251866786, 0.029607718570353176, 0.0, 0.008213735995679807, 0.0, 0.08605746445745502, 0.0, 0.3070376563830936, 0.10398717208817806, 0.0, 0.0, 0.0, 0.383984202548332, 0.0, 0.0, 0.3036396360576457, 0.0, 0.0, 0.0, 0.0, 0.11965354383371518, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04825455862354496, 0.3335108938296942, 0.0, 0.0, 0.0, 0.0, 0.04121745447265487, 0.30594446336043757, 0.0, 0.33550494159326405, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029116195807012437, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020041236855252602, 0.0, 0.4850188795993097, 0.0, 0.16403823255879735, 0.0, 0.60023572778736, 0.0, 0.1787344157826609, 0.0, 0.0, 0.17233073276918714, 0.17532212882841147, 0.6209016656456683, 0.5984929062073807, 0.8843163905991084, 0.3337919143714591, 0.0, 1.0919707374354302, 0.8166713304287454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8330317657834686, 0.27201200741148646, 0.0, 0.8995473587201992, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6388227016456749, 0.0, 0.3682672817259989, 0.0, 0.5422113050459496, 0.0, 0.028602469161704683, 0.0, 0.0, 0.3611222515381231, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.031337192831751, 0.0, 0.0, 0.7687924758178234, 0.012617011188267763, 0.0, 0.0, 0.37315715294884366, 0.0, 0.0, 0.0, 0.0, 0.6888779809403613, 0.121001715761302, 0.0, 0.0, 0.3680175749172942, 0.47420475938210593, 0.0, 0.7604201795169553, 0.0, 0.11917670920609563, 0.0, 0.20848198246903302, 0.24926285864223433, 0.0, 0.5553692000145817, 0.4169252107806106, 0.16597755096800731, 0.7334771026528798, 0.0, 0.0, 0.09908991746260937, 0.0, 0.14652270264303283, 0.006759647910977647, 0.0, 0.9351545954529237, 1.2960299208884896, 0.0, 0.0, 0.21191619141709428, 0.0, 0.0, 0.0, 0.01204540426582532, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06493914883934329, 0.7726981572397796, 0.0, 0.4370686273737238, 0.1778273534538074, 0.055209023869020324, 0.0, 0.0, 0.0, 0.05709969797354255, 0.0, 0.0, 0.0, 0.0, 0.44808598581285264, 0.0, 0.0, 0.0, 0.21141909545938053, 0.13390642572615394, 0.0, 0.0, 0.0, 0.25844994675864025, 0.0, 0.51617125876173, 0.2544268105667549, 0.0, 0.17576058376064335, 0.29087109088619356, 0.0, 0.0, 0.3286889977164017, 0.0, 0.0644707079973061, 0.0, 0.018682819036845075, 0.008522941045952347, 0.0, 0.0, 0.2520720928277996, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.146066293561638, 0.08856351358209161, 0.0, 0.0, 0.0, 0.1407746062228793, 0.5246446552397386, 0.0, 0.0, 0.0, 0.3279179855015388, 0.46137944574560436, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14330339619658897, 0.11500988018505275, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7444467047874328, 0.0, 0.0, 0.40713437972280675, 0.0, 0.0, 0.0, 0.5717679971406524, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4877440937322302, 0.6991654372087778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2369682176872117, 0.0, 0.0, 0.6757556051622259, 0.0, 0.0, 0.0, 0.576368321209737, 0.04986303698751877, 0.0, 0.5384684335273707, 0.0, 0.21524971756673417, 0.4963922980972075, 0.0, 0.0, 0.0, 0.0, 0.18940210229325644, 0.5277692784584271, 0.7718932464687741, 0.7210929692294604, 0.0, 0.0, 0.0, 0.0, 0.48286176484120297, 0.8517937989915767, 0.3931693184679991, 0.0, 0.0, 0.06755455710060922, 0.0, 0.026695131792948763, 1.4844324548606433, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4524914530619723, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4694831016582333, 0.0, 0.42831526212260856, 0.6074590079807168, 0.30066361967157035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6129938591206631, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6008436638333436, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43882326364474644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8591062832739853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6928708028466394, 0.2819039698547807, 0.08752108546971657, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05246236611269018, 0.0, 0.0, 0.5428804251437249, 0.022819504909203123, 0.3729965675892053, 0.0, 0.5741583158437262, 0.0, 0.1935811346150516, 0.0, 0.5023321549740235, 0.2611826745866744, 0.0, 0.056899028261467276, 0.37961815387770637, 0.22527271986010186, 0.05171423855571594, 0.058968538904135746, 0.0, 0.02892983665470066, 0.003313037290954743, 0.0, 0.0, 0.15785301822506514, 0.0, 0.0, 0.07773433689527456, 0.1049811110510813, 0.0, 0.02912372760696772, 0.0, 0.3114315285072337, 0.5703037753007745, 0.0, 0.04813466284828647, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02241933195791073, 0.0, 0.0, 0.0, 0.0, 0.23199999736075347, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1927795141790272, 0.2407119020891451, 0.0, 0.21960445663579467, 0.5770903406753464, 0.15415530724008072, 0.23155572792080611, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3142922871488459, 0.0, 0.0, 0.0, 0.0, 0.3042889785067072, 0.0, 0.5129566600174419, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3746354155106774, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4789154917746846, 0.17517544375428362, 0.0, 0.0, 0.0, 0.2637662382252967, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2102700997979013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.126545263019808, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.332133943612647, 0.0, 0.0, 0.0, 0.06184093060913505, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3212574037659458, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24673989573277713, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3017167941185386, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6233354196759152, 0.0, 0.0, 0.0, 0.5447611694646652, 0.0, 0.0, 0.216176776357065, 0.0, 0.19855221622967104, 0.8686764576101142, 0.16350923987929902, 0.0, 0.0, 0.02334951098786776, 0.17107621812888604, 0.48682879160279025, 0.7109808359466632, 0.6651558420918936, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6192268761337199, 0.31206921876898447, 0.0, 0.0, 0.24595989001208304, 0.11068536977374843, 0.0, 0.6618395820988949, 0.11362642836904417, 0.0, 0.0, 0.23765422406147768, 0.0, 0.0, 0.8073352785417658, 0.0, 0.0, 0.48485062583878286, 1.055137179889336, 0.2671063045394923, 0.0, 0.8476204266733893, 0.473385026956602, 0.5030912229948215, 0.0, 0.0, 0.0, 0.17678971866264329, 0.4076988147485064, 0.0, 0.6776909084507561, 0.13779930009477406, 0.5691281649873949, 0.16622169517158358, 0.433469475882236, 0.818379113845578, 0.9106844436778354, 0.0, 0.499393081506665, 0.0, 0.5725309586159512, 0.6160013948015824, 0.5125607980557373, 0.2778645861379971, 0.0, 0.0, 0.0, 0.0, 0.0, 0.58929804834674, 0.0, 0.8046233984476477, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3967677816937078, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11707526901952116, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.383435421983313, 0.11707115655719894, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07864429232511502, 0.0, 0.0, 0.3002587300128159, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2266804769859352, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24215921984980293, 0.09852579316681935, 0.030588729804574537, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18874358875246125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3978422171614565, 0.1936032919321414, 0.0, 0.0, 0.2542147176776134, 0.0, 0.0, 0.0, 0.11057553114503467, 0.0, 6.012518808232222e-5, 0.0008825884581384356, 0.6460696181307314, 0.0, 0.0, 0.06879188140017427, 0.0, 0.0, 0.04716298956935701, 0.1540138997690347, 0.1854894997474166, 0.003124358567077274, 0.35109153378778224, 0.0, 0.0, 0.016293742009993128, 0.0, 0.0, 0.3747805868672189, 0.025859295570033313, 0.214029422856965, 0.003742959361061961, 0.0002933458280023473, 0.0, 0.0, 0.0, 0.030501494612752977, 0.0, 0.6996784734474946, 0.0, 0.0, 0.05616866413207192, 0.0, 0.0018184995370819535, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020171782722387992, 0.0, 0.0, 0.33279196588498333, 0.0, 0.0, 0.6323890570808377, 0.0, 0.0, 0.0, 0.03594525644730097, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004527887543653786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17039388977135247, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6309056496928738, 0.0, 0.0, 0.0, 0.0, 0.260280820884557, 0.0, 0.0, 0.5107183088816271, 0.7967464980324664, 0.14485492608681305, 0.0, 0.009558078807310686, 0.754701551784825, 0.0, 0.0, 0.0, 0.0, 0.7598764239846039, 0.0, 0.029177780358136646, 0.10253044625069853, 0.03763219091970284, 0.8115624154017409, 0.0, 0.5779584654853126, 0.7162312164421211, 0.0, 0.0, 0.9107472399622886, 0.7029540573396418, 0.0, 1.0969174796040306, 0.0, 0.0, 0.0, 0.0, 0.0038591832437520454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15366266383288085, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.558080611304664, 0.0, 0.0, 0.0, 0.0, 0.30736701150515244, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04311886842935436, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32751189175201845, 0.0, 0.20963216887545572, 0.1760259950046517, 0.0, 0.23753864749234863, 0.24093292162050975, 0.024684548149881243, 0.4936183719527589, 0.7064246796060082, 0.0, 0.06677453327599289, 0.8542035190842152, 0.0, 0.025534429812218047, 0.011379900282647532, 0.039800411165778926, 0.013727111201321645, 0.27843394547131894, 0.24161261590501448, 0.2499939278392831, 0.0, 0.0, 0.3450248607837782, 0.0, 0.0, 0.15272914557490577, 0.10495111476871369, 0.0, 0.0, 0.0, 0.18866875753074405, 0.0, 0.2225813946448797, 0.0, 0.0, 0.3661590836002555, 0.0, 0.0, 0.0, 0.8998957690359772, 0.30889178773893766, 0.6078813570942335, 0.0, 0.6840699471933163, 0.0, 0.0, 0.0, 0.36308986165522683, 0.752528074611531, 0.7191770460546526, 0.49508503625401296, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7784554319844289, 0.16352298326175332, 0.0, 0.042948548224867954, 0.375190336410357, 0.0, 0.6830715485727474, 0.6403864588932121, 0.7831097846364896, 0.46112164648188003, 0.14556009485746954, 0.0, 0.0, 0.0, 0.8585580212631712, 0.0, 0.253359163441029, 0.0, 0.0, 0.0, 0.45902026051340783, 0.0, 0.0, 0.0, 0.0, 0.166881895984427, 0.0, 0.0, 0.837727301005, 0.0, 0.0, 0.7629510291223568, 0.0, 0.34201996071392393, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3670598185817387, 0.0, 0.9533138689466589, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8365046398061449, 0.0, 0.9590136929209965, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.801491142663501, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16957493050977235, 0.0, 0.2507477829737011, 0.0, 0.0, 0.11532054524043628, 0.07735685692161769, 0.5247858743947099, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02100484631395434, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8201592179395716, 0.0, 0.0, 0.04562437343117348, 0.0, 0.22915724847477928, 0.0, 0.0, 0.7159205767218768, 0.0, 0.0, 0.0, 0.0, 0.29228977882295926, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17928588753235497, 0.10348218971035608, 0.8147006956016374, 0.08438351505236888, 0.0, 0.0, 0.0, 0.1070859593707194, 0.7148756921758321, 0.0, 0.8195717571774336, 0.12249576952187838, 0.0, 0.2397374334824377, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05225556550152024, 0.0, 0.0, 0.0, 0.1707240047839942, 0.0, 0.11920584928810601, 0.0, 0.14514911616615112, 0.4284534617057244, 0.04403067305952579, 0.0, 0.012688229227290206, 0.3654383338493288, 0.04642776495318284, 0.0, 0.0, 0.31371261026073766, 0.1364760956744218, 0.5008825042361101, 0.0, 0.04802625911306818, 0.021403808257108425, 0.0, 0.12191697007589085, 0.33632693219045084, 0.49073784844916374, 0.5066599946239788, 0.0, 0.0, 0.033354950181247504, 0.0, 0.15690710970564234, 0.3397139005999492, 0.4535410643500978, 0.0, 0.0, 0.49050280987387757, 0.0, 0.0, 0.45491953624514714, 0.0, 0.0, 0.33864218894794285, 0.0, 0.0, 0.041024172335819876, 0.0, 0.0, 0.0, 0.36175943329836924, 0.0, 0.39749358702806525, 0.0, 0.5595492530984049, 0.0, 0.0, 0.2573403608635483, 0.17262354625512744, 0.0, 0.0, 0.0, 0.3776391490751126, 0.0, 0.0, 0.1445911005474468, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.45961437114556436, 0.08073146338811468, 0.0, 0.0, 0.0, 0.5680662673212686, 0.0, 0.14301647595845865, 0.4278925860448857, 0.2624303541096862, 0.0, 0.5619603114708978, 0.5488835922357254, 0.0, 0.10445139145624531, 0.9180806504163392, 0.5113694952971799, 0.0, 0.4737323723594051, 0.4474251824766889, 0.10616343839228443, 0.0, 0.02745363380188561, 0.0, 0.28312699798757224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6983886824821985, 0.38943652344243257, 1.2223890565530993, 0.05586381199212052, 0.0, 0.07686093310040262, 0.10701921729681718, 0.0, 0.4467721663826107, 0.0, 0.5122035249872549, 1.008045581843232, 0.10571972464185753, 0.6235175832869809, 0.0, 0.0, 0.0, 0.0, 0.7054625658357997, 0.0, 0.0, 0.17326833225856653, 0.0, 0.0, 0.0, 1.0813156328128786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0927477810868, 0.0, 0.599657110852462, 0.22383250773975968, 0.19685563542122056, 0.0, 0.05605157239206448, 0.41837305013296433, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18996852120778085, 0.0, 0.28856245320282303, 0.0, 0.0, 0.0, 0.26772611418599673, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1616947156362974, 0.0, 0.0, 0.0, 0.0, 0.36168541397352055, 0.0, 0.0, 0.0, 0.1573219565054934, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13761515434984456, 0.07943022011077468, 0.0, 0.21912413885604493, 0.0, 0.0, 0.40654019224262405, 0.0, 0.0, 0.022840701875983912, 0.0, 0.09402454628178203, 0.5332211799090736, 0.14433851006651358, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.043396172364084024, 0.04011000425692938, 0.0, 0.0, 0.0, 0.13104327726482687, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10232284486723907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10523174379677519, 0.0, 0.10079612361252216, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04415728315159431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11805369977437472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6523402818477722, 0.04871612610613401, 0.0, 0.19287524873967893, 0.0, 0.3592812200503388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.050401634126662875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8285325211040351, 0.0, 0.0, 0.0, 0.42701794948426475, 0.33826599655789313, 0.0, 0.0, 0.0063200711569476415, 0.0, 0.0, 0.4430530345512502, 0.0, 0.6935424923816391, 0.9428504267370209, 0.42746265456576077, 0.0025457381888520196, 0.0, 0.1593559778661365, 0.0, 0.8273232796667221, 0.04988580847200336, 0.9484876902255591, 0.5784846962232053, 0.0, 0.4437739313478008, 0.0, 0.0, 0.29414816951171024, 0.0, 0.022355194883774517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02069724212749366, 0.0, 0.0, 0.0, 0.0, 0.7860463101009033, 0.0, 0.0, 0.0, 0.8499064420418566, 0.0, 0.014821364995218649, 0.0, 0.004694386740689857, 0.25038082548050006, 0.5774089497757802, 0.0, 0.0, 0.0, 0.0, 0.006270331329794731, 0.6139069964758506, 0.8351435200462407, 1.0242327730345495, 0.0, 0.029307532281045708, 0.0, 0.0, 0.0, 0.5726801471740839, 0.4962238989829506, 0.0, 0.07225356949876899, 0.10746103390935668, 0.0, 0.0, 0.8346013157058231, 0.0, 0.0, 0.12871551917858068, 0.0, 0.0, 0.05722051841888945, 0.0, 0.08248659071056784, 0.08634381582594033, 0.0, 0.14691645141749232, 0.0, 0.0, 0.0, 0.12530859946217338, 0.0, 0.10215231917559951, 0.07032216186532725, 0.0, 0.8790426858900641, 0.10792096194290268, 0.054388076691659265, 0.0, 0.0, 0.0, 0.05001396409640395, 0.114742650990913, 0.28339886512623463, 0.15677327627325063, 0.0, 0.03258480102579557, 0.09096085908673467, 0.0, 0.0, 0.15530131741375458, 0.4726591715297998, 0.7751806988017736, 0.0, 0.51208607063286, 0.0, 0.0, 0.8142774797804637, 0.037795557638802935, 0.0, 0.03542525158598919, 0.07905092197782795, 0.0, 0.0, 0.1322231578644953, 0.0, 0.0, 0.008286484320060336, 0.670492082894006, 0.0, 0.0, 0.2274177128809876, 0.20430769763068007, 0.27374263149272277, 0.0, 0.0, 0.0, 0.0, 0.22105379716709922, 0.08605636125516496, 0.023058224273634728, 0.03343036365050954, 0.7905997431838073, 0.031091378094046798, 0.32331562706461314, 0.0, 0.048595513480849374, 0.0, 0.6695135011372569, 0.01079155413526415, 0.7675661133629853, 0.01950494389053299, 0.0, 0.06609079415505228, 0.15915608286937477, 0.0, 0.17610090955799163, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05278940542005462, 0.0, 0.04363140001862599, 0.3747258422202424, 0.18235409289517635]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = ((ex.bodies[1]).bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    result = nothing
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    B_lvl_q = B_lvl_ptr[1]
    B_lvl_q_stop = B_lvl_ptr[1 + 1]
    if B_lvl_q < B_lvl_q_stop
        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
    else
        B_lvl_i_stop = 0
    end
    phase_stop = min(B_lvl.shape[2], B_lvl_i_stop)
    if phase_stop >= 1
        if B_lvl_tbl2[B_lvl_q] < 1
            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
        end
        while true
            B_lvl_i = B_lvl_tbl2[B_lvl_q]
            B_lvl_q_step = B_lvl_q
            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
            end
            if B_lvl_i < phase_stop
                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                val = Ct_lvl_2_val
                Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                B_lvl_tbl1_2 = B_lvl_tbl1
                B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                B_lvl_tbl2_2 = B_lvl_tbl2
                val_2 = B_lvl_val
                B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
                A_lvl_ptr_2 = A_lvl_ptr
                A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                A_lvl_tbl1_2 = A_lvl_tbl1
                A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                A_lvl_tbl2_2 = A_lvl_tbl2
                A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                val_3 = A_lvl_val
                A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
                Threads.@threads for i_9 = 1:Threads.nthreads()
                        phase_start_6 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_9), Threads.nthreads()))
                        phase_stop_7 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_9, Threads.nthreads()))
                        if phase_stop_7 >= phase_start_6
                            for i_12 = phase_start_6:phase_stop_7
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_12
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_8 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_8 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_8
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_9 = min(B_lvl_i_2, phase_stop_8, A_lvl_i)
                                        if A_lvl_i == phase_stop_9 && B_lvl_i_2 == phase_stop_9
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_10 = min(i_12, A_lvl_i_stop_2)
                                            if phase_stop_10 >= i_12
                                                if A_lvl_tbl1[A_lvl_q] < i_12
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_12, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_10
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_12 = min(A_lvl_i_2, phase_stop_10)
                                                        if A_lvl_i_2 == phase_stop_12
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_9
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_9
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_9 + 1
                                    end
                                end
                            end
                        end
                    end
                Ct_lvl_2_val = val
                B_lvl_tbl1 = B_lvl_tbl1_2
                B_lvl_tbl2 = B_lvl_tbl2_2
                B_lvl_val = val_2
                A_lvl_ptr = A_lvl_ptr_2
                A_lvl_tbl1 = A_lvl_tbl1_2
                A_lvl_tbl2 = A_lvl_tbl2_2
                A_lvl_val = val_3
                B_lvl_q = B_lvl_q_step
            else
                phase_stop_18 = min(B_lvl_i, phase_stop)
                if B_lvl_i == phase_stop_18
                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_18
                    val_4 = Ct_lvl_2_val
                    Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                    B_lvl_tbl1_3 = B_lvl_tbl1
                    B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                    B_lvl_tbl2_3 = B_lvl_tbl2
                    val_5 = B_lvl_val
                    B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
                    A_lvl_ptr_3 = A_lvl_ptr
                    A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                    A_lvl_tbl1_3 = A_lvl_tbl1
                    A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                    A_lvl_tbl2_3 = A_lvl_tbl2
                    A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                    val_6 = A_lvl_val
                    A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
                    Threads.@threads for i_19 = 1:Threads.nthreads()
                            phase_start_21 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_19), Threads.nthreads()))
                            phase_stop_23 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_19, Threads.nthreads()))
                            if phase_stop_23 >= phase_start_21
                                for i_22 = phase_start_21:phase_stop_23
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_22
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_24 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_24 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_24
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_25 = min(B_lvl_i_2, A_lvl_i, phase_stop_24)
                                            if A_lvl_i == phase_stop_25 && B_lvl_i_2 == phase_stop_25
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_26 = min(i_22, A_lvl_i_stop_4)
                                                if phase_stop_26 >= i_22
                                                    if A_lvl_tbl1[A_lvl_q] < i_22
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_22, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_26
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_28 = min(A_lvl_i_4, phase_stop_26)
                                                            if A_lvl_i_4 == phase_stop_28
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_25
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_25
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_25 + 1
                                        end
                                    end
                                end
                            end
                        end
                    Ct_lvl_2_val = val_4
                    B_lvl_tbl1 = B_lvl_tbl1_3
                    B_lvl_tbl2 = B_lvl_tbl2_3
                    B_lvl_val = val_5
                    A_lvl_ptr = A_lvl_ptr_3
                    A_lvl_tbl1 = A_lvl_tbl1_3
                    A_lvl_tbl2 = A_lvl_tbl2_3
                    A_lvl_val = val_6
                    B_lvl_q = B_lvl_q_step
                end
                break
            end
        end
    end
    resize!(Ct_lvl_2_val, A_lvl.shape[1] * B_lvl.shape[2])
    result = (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
    result
end
julia> @finch begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0, 0.0, 0.0, 0.01855804465451247, 0.0, 0.0, 0.03641425883740431, 0.0568080930379847, 0.10109716846875504, 0.0, 0.0, 0.0, 0.0, 0.005424838537552436, 0.0, 0.0, 0.5251896042062963, 0.0, 0.0, 0.0, 0.00515342598991506, 0.045980273053842245, 0.0, 0.4033688458623361, 0.0510673466163073, 0.0, 0.0, 0.02134197385615031, 0.01687808266981965, 0.0, 0.4932447371732914, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7178045198025068, 0.2106682695954306, 0.0, 0.3115118572371521, 0.5170963380295581, 0.39595498771521426, 0.1432663403816299, 0.5698542189659496, 0.0, 0.0, 0.0, 0.5317966816929195, 0.0, 0.5315818882594147, 0.8168445783005969, 0.0, 0.0, 0.0, 0.01222733860627223, 0.0, 0.716756885645577, 0.0, 0.8217284580618865, 0.48234209517721377, 0.15545502627894098, 0.04826321760798536, 0.0, 0.0, 0.1394038038151463, 0.0, 0.056802678199824076, 0.0, 0.0, 0.0, 0.3128541399180126, 0.0, 0.0, 0.596593418834028, 0.0, 0.2846892571697076, 0.0, 0.0, 0.0, 0.23365792107187477, 0.0, 0.0, 0.4584793374366712, 0.7152510496888798, 0.0, 0.0, 0.0, 0.8190896825195866, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11127793558853107, 0.0, 0.5789217135796446, 0.0, 0.0, 0.6429712972008511, 0.0, 0.0, 0.661506988212563, 0.7629272979371657, 0.0, 0.4234710523361336, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4257443064600869, 0.0, 0.20425497460318195, 0.0, 0.5184318202957809, 0.5906121758765186, 0.0, 0.9091367634018797, 0.0, 0.30652125262867885, 0.0, 0.7954054080966687, 0.0, 0.0, 0.0, 1.0559083316811368, 0.0, 0.04427154393293609, 0.8228955354751877, 0.46658038808648383, 0.0, 0.02688606839966265, 0.0, 0.0, 0.6276611460113729, 0.22522673327345408, 0.0, 0.12308651028821192, 0.3657688390111318, 0.0, 0.046115245088609334, 0.0, 0.5342436705087195, 0.0, 0.1923886832463953, 0.07621763959827882, 0.0, 0.0, 0.0, 0.6725528112574624, 0.7483589153917343, 0.0, 0.0, 0.0, 0.0, 0.38949567271561986, 0.16492548861911044, 0.10519585180914466, 0.8853743380662012, 0.957787822214843, 0.0, 0.05459549529270359, 0.25656057396140347, 0.14167223917141744, 0.03949155262056029, 0.0, 0.0, 0.2703834370419471, 0.0, 0.723362221877522, 0.0, 0.0, 0.0, 0.7883757417725614, 0.0, 0.03340589062163017, 0.18896261922152233, 0.0, 0.0, 0.8122657251866786, 0.029607718570353176, 0.0, 0.008213735995679807, 0.0, 0.08605746445745502, 0.0, 0.3070376563830936, 0.10398717208817806, 0.0, 0.0, 0.0, 0.383984202548332, 0.0, 0.0, 0.3036396360576457, 0.0, 0.0, 0.0, 0.0, 0.11965354383371518, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04825455862354496, 0.3335108938296942, 0.0, 0.0, 0.0, 0.0, 0.04121745447265487, 0.30594446336043757, 0.0, 0.33550494159326405, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029116195807012437, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020041236855252602, 0.0, 0.4850188795993097, 0.0, 0.16403823255879735, 0.0, 0.60023572778736, 0.0, 0.1787344157826609, 0.0, 0.0, 0.17233073276918714, 0.17532212882841147, 0.6209016656456683, 0.5984929062073807, 0.8843163905991084, 0.3337919143714591, 0.0, 1.0919707374354302, 0.8166713304287454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8330317657834686, 0.27201200741148646, 0.0, 0.8995473587201992, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6388227016456749, 0.0, 0.3682672817259989, 0.0, 0.5422113050459496, 0.0, 0.028602469161704683, 0.0, 0.0, 0.3611222515381231, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.031337192831751, 0.0, 0.0, 0.7687924758178234, 0.012617011188267763, 0.0, 0.0, 0.37315715294884366, 0.0, 0.0, 0.0, 0.0, 0.6888779809403613, 0.121001715761302, 0.0, 0.0, 0.3680175749172942, 0.47420475938210593, 0.0, 0.7604201795169553, 0.0, 0.11917670920609563, 0.0, 0.20848198246903302, 0.24926285864223433, 0.0, 0.5553692000145817, 0.4169252107806106, 0.16597755096800731, 0.7334771026528798, 0.0, 0.0, 0.09908991746260937, 0.0, 0.14652270264303283, 0.006759647910977647, 0.0, 0.9351545954529237, 1.2960299208884896, 0.0, 0.0, 0.21191619141709428, 0.0, 0.0, 0.0, 0.01204540426582532, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06493914883934329, 0.7726981572397796, 0.0, 0.4370686273737238, 0.1778273534538074, 0.055209023869020324, 0.0, 0.0, 0.0, 0.05709969797354255, 0.0, 0.0, 0.0, 0.0, 0.44808598581285264, 0.0, 0.0, 0.0, 0.21141909545938053, 0.13390642572615394, 0.0, 0.0, 0.0, 0.25844994675864025, 0.0, 0.51617125876173, 0.2544268105667549, 0.0, 0.17576058376064335, 0.29087109088619356, 0.0, 0.0, 0.3286889977164017, 0.0, 0.0644707079973061, 0.0, 0.018682819036845075, 0.008522941045952347, 0.0, 0.0, 0.2520720928277996, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.146066293561638, 0.08856351358209161, 0.0, 0.0, 0.0, 0.1407746062228793, 0.5246446552397386, 0.0, 0.0, 0.0, 0.3279179855015388, 0.46137944574560436, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14330339619658897, 0.11500988018505275, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7444467047874328, 0.0, 0.0, 0.40713437972280675, 0.0, 0.0, 0.0, 0.5717679971406524, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4877440937322302, 0.6991654372087778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2369682176872117, 0.0, 0.0, 0.6757556051622259, 0.0, 0.0, 0.0, 0.576368321209737, 0.04986303698751877, 0.0, 0.5384684335273707, 0.0, 0.21524971756673417, 0.4963922980972075, 0.0, 0.0, 0.0, 0.0, 0.18940210229325644, 0.5277692784584271, 0.7718932464687741, 0.7210929692294604, 0.0, 0.0, 0.0, 0.0, 0.48286176484120297, 0.8517937989915767, 0.3931693184679991, 0.0, 0.0, 0.06755455710060922, 0.0, 0.026695131792948763, 1.4844324548606433, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4524914530619723, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4694831016582333, 0.0, 0.42831526212260856, 0.6074590079807168, 0.30066361967157035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6129938591206631, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6008436638333436, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43882326364474644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8591062832739853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6928708028466394, 0.2819039698547807, 0.08752108546971657, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05246236611269018, 0.0, 0.0, 0.5428804251437249, 0.022819504909203123, 0.3729965675892053, 0.0, 0.5741583158437262, 0.0, 0.1935811346150516, 0.0, 0.5023321549740235, 0.2611826745866744, 0.0, 0.056899028261467276, 0.37961815387770637, 0.22527271986010186, 0.05171423855571594, 0.058968538904135746, 0.0, 0.02892983665470066, 0.003313037290954743, 0.0, 0.0, 0.15785301822506514, 0.0, 0.0, 0.07773433689527456, 0.1049811110510813, 0.0, 0.02912372760696772, 0.0, 0.3114315285072337, 0.5703037753007745, 0.0, 0.04813466284828647, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02241933195791073, 0.0, 0.0, 0.0, 0.0, 0.23199999736075347, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1927795141790272, 0.2407119020891451, 0.0, 0.21960445663579467, 0.5770903406753464, 0.15415530724008072, 0.23155572792080611, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3142922871488459, 0.0, 0.0, 0.0, 0.0, 0.3042889785067072, 0.0, 0.5129566600174419, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3746354155106774, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4789154917746846, 0.17517544375428362, 0.0, 0.0, 0.0, 0.2637662382252967, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2102700997979013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.126545263019808, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.332133943612647, 0.0, 0.0, 0.0, 0.06184093060913505, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3212574037659458, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24673989573277713, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3017167941185386, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6233354196759152, 0.0, 0.0, 0.0, 0.5447611694646652, 0.0, 0.0, 0.216176776357065, 0.0, 0.19855221622967104, 0.8686764576101142, 0.16350923987929902, 0.0, 0.0, 0.02334951098786776, 0.17107621812888604, 0.48682879160279025, 0.7109808359466632, 0.6651558420918936, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6192268761337199, 0.31206921876898447, 0.0, 0.0, 0.24595989001208304, 0.11068536977374843, 0.0, 0.6618395820988949, 0.11362642836904417, 0.0, 0.0, 0.23765422406147768, 0.0, 0.0, 0.8073352785417658, 0.0, 0.0, 0.48485062583878286, 1.055137179889336, 0.2671063045394923, 0.0, 0.8476204266733893, 0.473385026956602, 0.5030912229948215, 0.0, 0.0, 0.0, 0.17678971866264329, 0.4076988147485064, 0.0, 0.6776909084507561, 0.13779930009477406, 0.5691281649873949, 0.16622169517158358, 0.433469475882236, 0.818379113845578, 0.9106844436778354, 0.0, 0.499393081506665, 0.0, 0.5725309586159512, 0.6160013948015824, 0.5125607980557373, 0.2778645861379971, 0.0, 0.0, 0.0, 0.0, 0.0, 0.58929804834674, 0.0, 0.8046233984476477, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3967677816937078, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11707526901952116, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.383435421983313, 0.11707115655719894, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07864429232511502, 0.0, 0.0, 0.3002587300128159, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2266804769859352, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24215921984980293, 0.09852579316681935, 0.030588729804574537, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18874358875246125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3978422171614565, 0.1936032919321414, 0.0, 0.0, 0.2542147176776134, 0.0, 0.0, 0.0, 0.11057553114503467, 0.0, 6.012518808232222e-5, 0.0008825884581384356, 0.6460696181307314, 0.0, 0.0, 0.06879188140017427, 0.0, 0.0, 0.04716298956935701, 0.1540138997690347, 0.1854894997474166, 0.003124358567077274, 0.35109153378778224, 0.0, 0.0, 0.016293742009993128, 0.0, 0.0, 0.3747805868672189, 0.025859295570033313, 0.214029422856965, 0.003742959361061961, 0.0002933458280023473, 0.0, 0.0, 0.0, 0.030501494612752977, 0.0, 0.6996784734474946, 0.0, 0.0, 0.05616866413207192, 0.0, 0.0018184995370819535, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020171782722387992, 0.0, 0.0, 0.33279196588498333, 0.0, 0.0, 0.6323890570808377, 0.0, 0.0, 0.0, 0.03594525644730097, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004527887543653786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17039388977135247, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6309056496928738, 0.0, 0.0, 0.0, 0.0, 0.260280820884557, 0.0, 0.0, 0.5107183088816271, 0.7967464980324664, 0.14485492608681305, 0.0, 0.009558078807310686, 0.754701551784825, 0.0, 0.0, 0.0, 0.0, 0.7598764239846039, 0.0, 0.029177780358136646, 0.10253044625069853, 0.03763219091970284, 0.8115624154017409, 0.0, 0.5779584654853126, 0.7162312164421211, 0.0, 0.0, 0.9107472399622886, 0.7029540573396418, 0.0, 1.0969174796040306, 0.0, 0.0, 0.0, 0.0, 0.0038591832437520454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15366266383288085, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.558080611304664, 0.0, 0.0, 0.0, 0.0, 0.30736701150515244, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04311886842935436, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32751189175201845, 0.0, 0.20963216887545572, 0.1760259950046517, 0.0, 0.23753864749234863, 0.24093292162050975, 0.024684548149881243, 0.4936183719527589, 0.7064246796060082, 0.0, 0.06677453327599289, 0.8542035190842152, 0.0, 0.025534429812218047, 0.011379900282647532, 0.039800411165778926, 0.013727111201321645, 0.27843394547131894, 0.24161261590501448, 0.2499939278392831, 0.0, 0.0, 0.3450248607837782, 0.0, 0.0, 0.15272914557490577, 0.10495111476871369, 0.0, 0.0, 0.0, 0.18866875753074405, 0.0, 0.2225813946448797, 0.0, 0.0, 0.3661590836002555, 0.0, 0.0, 0.0, 0.8998957690359772, 0.30889178773893766, 0.6078813570942335, 0.0, 0.6840699471933163, 0.0, 0.0, 0.0, 0.36308986165522683, 0.752528074611531, 0.7191770460546526, 0.49508503625401296, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7784554319844289, 0.16352298326175332, 0.0, 0.042948548224867954, 0.375190336410357, 0.0, 0.6830715485727474, 0.6403864588932121, 0.7831097846364896, 0.46112164648188003, 0.14556009485746954, 0.0, 0.0, 0.0, 0.8585580212631712, 0.0, 0.253359163441029, 0.0, 0.0, 0.0, 0.45902026051340783, 0.0, 0.0, 0.0, 0.0, 0.166881895984427, 0.0, 0.0, 0.837727301005, 0.0, 0.0, 0.7629510291223568, 0.0, 0.34201996071392393, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3670598185817387, 0.0, 0.9533138689466589, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8365046398061449, 0.0, 0.9590136929209965, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.801491142663501, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16957493050977235, 0.0, 0.2507477829737011, 0.0, 0.0, 0.11532054524043628, 0.07735685692161769, 0.5247858743947099, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02100484631395434, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8201592179395716, 0.0, 0.0, 0.04562437343117348, 0.0, 0.22915724847477928, 0.0, 0.0, 0.7159205767218768, 0.0, 0.0, 0.0, 0.0, 0.29228977882295926, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17928588753235497, 0.10348218971035608, 0.8147006956016374, 0.08438351505236888, 0.0, 0.0, 0.0, 0.1070859593707194, 0.7148756921758321, 0.0, 0.8195717571774336, 0.12249576952187838, 0.0, 0.2397374334824377, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05225556550152024, 0.0, 0.0, 0.0, 0.1707240047839942, 0.0, 0.11920584928810601, 0.0, 0.14514911616615112, 0.4284534617057244, 0.04403067305952579, 0.0, 0.012688229227290206, 0.3654383338493288, 0.04642776495318284, 0.0, 0.0, 0.31371261026073766, 0.1364760956744218, 0.5008825042361101, 0.0, 0.04802625911306818, 0.021403808257108425, 0.0, 0.12191697007589085, 0.33632693219045084, 0.49073784844916374, 0.5066599946239788, 0.0, 0.0, 0.033354950181247504, 0.0, 0.15690710970564234, 0.3397139005999492, 0.4535410643500978, 0.0, 0.0, 0.49050280987387757, 0.0, 0.0, 0.45491953624514714, 0.0, 0.0, 0.33864218894794285, 0.0, 0.0, 0.041024172335819876, 0.0, 0.0, 0.0, 0.36175943329836924, 0.0, 0.39749358702806525, 0.0, 0.5595492530984049, 0.0, 0.0, 0.2573403608635483, 0.17262354625512744, 0.0, 0.0, 0.0, 0.3776391490751126, 0.0, 0.0, 0.1445911005474468, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.45961437114556436, 0.08073146338811468, 0.0, 0.0, 0.0, 0.5680662673212686, 0.0, 0.14301647595845865, 0.4278925860448857, 0.2624303541096862, 0.0, 0.5619603114708978, 0.5488835922357254, 0.0, 0.10445139145624531, 0.9180806504163392, 0.5113694952971799, 0.0, 0.4737323723594051, 0.4474251824766889, 0.10616343839228443, 0.0, 0.02745363380188561, 0.0, 0.28312699798757224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6983886824821985, 0.38943652344243257, 1.2223890565530993, 0.05586381199212052, 0.0, 0.07686093310040262, 0.10701921729681718, 0.0, 0.4467721663826107, 0.0, 0.5122035249872549, 1.008045581843232, 0.10571972464185753, 0.6235175832869809, 0.0, 0.0, 0.0, 0.0, 0.7054625658357997, 0.0, 0.0, 0.17326833225856653, 0.0, 0.0, 0.0, 1.0813156328128786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0927477810868, 0.0, 0.599657110852462, 0.22383250773975968, 0.19685563542122056, 0.0, 0.05605157239206448, 0.41837305013296433, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18996852120778085, 0.0, 0.28856245320282303, 0.0, 0.0, 0.0, 0.26772611418599673, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1616947156362974, 0.0, 0.0, 0.0, 0.0, 0.36168541397352055, 0.0, 0.0, 0.0, 0.1573219565054934, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13761515434984456, 0.07943022011077468, 0.0, 0.21912413885604493, 0.0, 0.0, 0.40654019224262405, 0.0, 0.0, 0.022840701875983912, 0.0, 0.09402454628178203, 0.5332211799090736, 0.14433851006651358, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.043396172364084024, 0.04011000425692938, 0.0, 0.0, 0.0, 0.13104327726482687, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10232284486723907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10523174379677519, 0.0, 0.10079612361252216, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04415728315159431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11805369977437472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6523402818477722, 0.04871612610613401, 0.0, 0.19287524873967893, 0.0, 0.3592812200503388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.050401634126662875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8285325211040351, 0.0, 0.0, 0.0, 0.42701794948426475, 0.33826599655789313, 0.0, 0.0, 0.0063200711569476415, 0.0, 0.0, 0.4430530345512502, 0.0, 0.6935424923816391, 0.9428504267370209, 0.42746265456576077, 0.0025457381888520196, 0.0, 0.1593559778661365, 0.0, 0.8273232796667221, 0.04988580847200336, 0.9484876902255591, 0.5784846962232053, 0.0, 0.4437739313478008, 0.0, 0.0, 0.29414816951171024, 0.0, 0.022355194883774517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02069724212749366, 0.0, 0.0, 0.0, 0.0, 0.7860463101009033, 0.0, 0.0, 0.0, 0.8499064420418566, 0.0, 0.014821364995218649, 0.0, 0.004694386740689857, 0.25038082548050006, 0.5774089497757802, 0.0, 0.0, 0.0, 0.0, 0.006270331329794731, 0.6139069964758506, 0.8351435200462407, 1.0242327730345495, 0.0, 0.029307532281045708, 0.0, 0.0, 0.0, 0.5726801471740839, 0.4962238989829506, 0.0, 0.07225356949876899, 0.10746103390935668, 0.0, 0.0, 0.8346013157058231, 0.0, 0.0, 0.12871551917858068, 0.0, 0.0, 0.05722051841888945, 0.0, 0.08248659071056784, 0.08634381582594033, 0.0, 0.14691645141749232, 0.0, 0.0, 0.0, 0.12530859946217338, 0.0, 0.10215231917559951, 0.07032216186532725, 0.0, 0.8790426858900641, 0.10792096194290268, 0.054388076691659265, 0.0, 0.0, 0.0, 0.05001396409640395, 0.114742650990913, 0.28339886512623463, 0.15677327627325063, 0.0, 0.03258480102579557, 0.09096085908673467, 0.0, 0.0, 0.15530131741375458, 0.4726591715297998, 0.7751806988017736, 0.0, 0.51208607063286, 0.0, 0.0, 0.8142774797804637, 0.037795557638802935, 0.0, 0.03542525158598919, 0.07905092197782795, 0.0, 0.0, 0.1322231578644953, 0.0, 0.0, 0.008286484320060336, 0.670492082894006, 0.0, 0.0, 0.2274177128809876, 0.20430769763068007, 0.27374263149272277, 0.0, 0.0, 0.0, 0.0, 0.22105379716709922, 0.08605636125516496, 0.023058224273634728, 0.03343036365050954, 0.7905997431838073, 0.031091378094046798, 0.32331562706461314, 0.0, 0.048595513480849374, 0.0, 0.6695135011372569, 0.01079155413526415, 0.7675661133629853, 0.01950494389053299, 0.0, 0.06609079415505228, 0.15915608286937477, 0.0, 0.17610090955799163, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05278940542005462, 0.0, 0.04363140001862599, 0.3747258422202424, 0.18235409289517635]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = parallel(_)
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = ((ex.bodies[1]).bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    result = nothing
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = (Finch).moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = (Finch).moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            B_lvl_q = B_lvl_ptr[1]
            B_lvl_q_stop = B_lvl_ptr[1 + 1]
            if B_lvl_q < B_lvl_q_stop
                B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
            else
                B_lvl_i_stop = 0
            end
            phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                if B_lvl_tbl2[B_lvl_q] < phase_start_2
                    B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                end
                while true
                    B_lvl_i = B_lvl_tbl2[B_lvl_q]
                    B_lvl_q_step = B_lvl_q
                    if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                        B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    if B_lvl_i < phase_stop_2
                        Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                        val_4 = Ct_lvl_2_val
                        Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                        A_lvl_ptr_3 = A_lvl_ptr
                        A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                        A_lvl_tbl1_3 = A_lvl_tbl1
                        A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                        A_lvl_tbl2_3 = A_lvl_tbl2
                        A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                        val_5 = A_lvl_val
                        A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
                        B_lvl_ptr_3 = B_lvl_ptr
                        B_lvl_tbl1_3 = B_lvl_tbl1
                        B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                        B_lvl_tbl2_3 = B_lvl_tbl2
                        val_6 = B_lvl_val
                        B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
                        Threads.@threads for i_10 = 1:Threads.nthreads()
                                phase_start_7 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_10), Threads.nthreads()))
                                phase_stop_8 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_10, Threads.nthreads()))
                                if phase_stop_8 >= phase_start_7
                                    for i_13 = phase_start_7:phase_stop_8
                                        Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_13
                                        A_lvl_q = A_lvl_ptr[1]
                                        A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                        if A_lvl_q < A_lvl_q_stop
                                            A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                        else
                                            A_lvl_i_stop = 0
                                        end
                                        B_lvl_q_3 = B_lvl_q
                                        if B_lvl_q < B_lvl_q_step
                                            B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                        else
                                            B_lvl_i_stop_3 = 0
                                        end
                                        phase_stop_9 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                        if phase_stop_9 >= 1
                                            k = 1
                                            if A_lvl_tbl2[A_lvl_q] < 1
                                                A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            if B_lvl_tbl1[B_lvl_q] < 1
                                                B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                            end
                                            while k <= phase_stop_9
                                                A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                                A_lvl_q_step = A_lvl_q
                                                if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                    A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                                end
                                                B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                                phase_stop_10 = min(B_lvl_i_3, phase_stop_9, A_lvl_i)
                                                if A_lvl_i == phase_stop_10 && B_lvl_i_3 == phase_stop_10
                                                    B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                                    A_lvl_q_2 = A_lvl_q
                                                    if A_lvl_q < A_lvl_q_step
                                                        A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                    else
                                                        A_lvl_i_stop_2 = 0
                                                    end
                                                    phase_stop_11 = min(i_13, A_lvl_i_stop_2)
                                                    if phase_stop_11 >= i_13
                                                        if A_lvl_tbl1[A_lvl_q] < i_13
                                                            A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_13, A_lvl_q, A_lvl_q_step - 1)
                                                        end
                                                        while true
                                                            A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                            if A_lvl_i_2 < phase_stop_11
                                                                A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                                Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                                A_lvl_q_2 += 1
                                                            else
                                                                phase_stop_13 = min(A_lvl_i_2, phase_stop_11)
                                                                if A_lvl_i_2 == phase_stop_13
                                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                                    Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                                    A_lvl_q_2 += 1
                                                                end
                                                                break
                                                            end
                                                        end
                                                    end
                                                    A_lvl_q = A_lvl_q_step
                                                    B_lvl_q_3 += 1
                                                elseif B_lvl_i_3 == phase_stop_10
                                                    B_lvl_q_3 += 1
                                                elseif A_lvl_i == phase_stop_10
                                                    A_lvl_q = A_lvl_q_step
                                                end
                                                k = phase_stop_10 + 1
                                            end
                                        end
                                    end
                                end
                            end
                        Ct_lvl_2_val = val_4
                        A_lvl_ptr = A_lvl_ptr_3
                        A_lvl_tbl1 = A_lvl_tbl1_3
                        A_lvl_tbl2 = A_lvl_tbl2_3
                        A_lvl_val = val_5
                        B_lvl_ptr = B_lvl_ptr_3
                        B_lvl_tbl1 = B_lvl_tbl1_3
                        B_lvl_tbl2 = B_lvl_tbl2_3
                        B_lvl_val = val_6
                        B_lvl_q = B_lvl_q_step
                    else
                        phase_stop_19 = min(B_lvl_i, phase_stop_2)
                        if B_lvl_i == phase_stop_19
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_19
                            val_7 = Ct_lvl_2_val
                            Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                            A_lvl_ptr_4 = A_lvl_ptr
                            A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                            A_lvl_tbl1_4 = A_lvl_tbl1
                            A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                            A_lvl_tbl2_4 = A_lvl_tbl2
                            A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                            val_8 = A_lvl_val
                            A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
                            B_lvl_ptr_4 = B_lvl_ptr
                            B_lvl_tbl1_4 = B_lvl_tbl1
                            B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                            B_lvl_tbl2_4 = B_lvl_tbl2
                            val_9 = B_lvl_val
                            B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
                            Threads.@threads for i_20 = 1:Threads.nthreads()
                                    phase_start_22 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_20), Threads.nthreads()))
                                    phase_stop_24 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_20, Threads.nthreads()))
                                    if phase_stop_24 >= phase_start_22
                                        for i_23 = phase_start_22:phase_stop_24
                                            Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_23
                                            A_lvl_q = A_lvl_ptr[1]
                                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                            if A_lvl_q < A_lvl_q_stop
                                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                            else
                                                A_lvl_i_stop = 0
                                            end
                                            B_lvl_q_3 = B_lvl_q
                                            if B_lvl_q < B_lvl_q_step
                                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                            else
                                                B_lvl_i_stop_3 = 0
                                            end
                                            phase_stop_25 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                            if phase_stop_25 >= 1
                                                k = 1
                                                if A_lvl_tbl2[A_lvl_q] < 1
                                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                                end
                                                if B_lvl_tbl1[B_lvl_q] < 1
                                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                                end
                                                while k <= phase_stop_25
                                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                                    A_lvl_q_step = A_lvl_q
                                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                                    end
                                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                                    phase_stop_26 = min(B_lvl_i_3, A_lvl_i, phase_stop_25)
                                                    if A_lvl_i == phase_stop_26 && B_lvl_i_3 == phase_stop_26
                                                        B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                                        A_lvl_q_4 = A_lvl_q
                                                        if A_lvl_q < A_lvl_q_step
                                                            A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                        else
                                                            A_lvl_i_stop_4 = 0
                                                        end
                                                        phase_stop_27 = min(i_23, A_lvl_i_stop_4)
                                                        if phase_stop_27 >= i_23
                                                            if A_lvl_tbl1[A_lvl_q] < i_23
                                                                A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_23, A_lvl_q, A_lvl_q_step - 1)
                                                            end
                                                            while true
                                                                A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                                if A_lvl_i_4 < phase_stop_27
                                                                    A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                    Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                                    A_lvl_q_4 += 1
                                                                else
                                                                    phase_stop_29 = min(A_lvl_i_4, phase_stop_27)
                                                                    if A_lvl_i_4 == phase_stop_29
                                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                                        A_lvl_q_4 += 1
                                                                    end
                                                                    break
                                                                end
                                                            end
                                                        end
                                                        A_lvl_q = A_lvl_q_step
                                                        B_lvl_q_3 += 1
                                                    elseif B_lvl_i_3 == phase_stop_26
                                                        B_lvl_q_3 += 1
                                                    elseif A_lvl_i == phase_stop_26
                                                        A_lvl_q = A_lvl_q_step
                                                    end
                                                    k = phase_stop_26 + 1
                                                end
                                            end
                                        end
                                    end
                                end
                            Ct_lvl_2_val = val_7
                            A_lvl_ptr = A_lvl_ptr_4
                            A_lvl_tbl1 = A_lvl_tbl1_4
                            A_lvl_tbl2 = A_lvl_tbl2_4
                            A_lvl_val = val_8
                            B_lvl_ptr = B_lvl_ptr_4
                            B_lvl_tbl1 = B_lvl_tbl1_4
                            B_lvl_tbl2 = B_lvl_tbl2_4
                            B_lvl_val = val_9
                            B_lvl_q = B_lvl_q_step
                        end
                        break
                    end
                end
            end
        end
    resize!(val, A_lvl.shape[1] * B_lvl.shape[2])
    result = (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
    result
end
julia> @finch begin
        Ct .= 0
        for j = parallel(_)
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0, 0.0, 0.0, 0.01855804465451247, 0.0, 0.0, 0.03641425883740431, 0.0568080930379847, 0.10109716846875504, 0.0, 0.0, 0.0, 0.0, 0.005424838537552436, 0.0, 0.0, 0.5251896042062963, 0.0, 0.0, 0.0, 0.00515342598991506, 0.045980273053842245, 0.0, 0.4033688458623361, 0.0510673466163073, 0.0, 0.0, 0.02134197385615031, 0.01687808266981965, 0.0, 0.4932447371732914, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7178045198025068, 0.2106682695954306, 0.0, 0.3115118572371521, 0.5170963380295581, 0.39595498771521426, 0.1432663403816299, 0.5698542189659496, 0.0, 0.0, 0.0, 0.5317966816929195, 0.0, 0.5315818882594147, 0.8168445783005969, 0.0, 0.0, 0.0, 0.01222733860627223, 0.0, 0.716756885645577, 0.0, 0.8217284580618865, 0.48234209517721377, 0.15545502627894098, 0.04826321760798536, 0.0, 0.0, 0.1394038038151463, 0.0, 0.056802678199824076, 0.0, 0.0, 0.0, 0.3128541399180126, 0.0, 0.0, 0.596593418834028, 0.0, 0.2846892571697076, 0.0, 0.0, 0.0, 0.23365792107187477, 0.0, 0.0, 0.4584793374366712, 0.7152510496888798, 0.0, 0.0, 0.0, 0.8190896825195866, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11127793558853107, 0.0, 0.5789217135796446, 0.0, 0.0, 0.6429712972008511, 0.0, 0.0, 0.661506988212563, 0.7629272979371657, 0.0, 0.4234710523361336, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4257443064600869, 0.0, 0.20425497460318195, 0.0, 0.5184318202957809, 0.5906121758765186, 0.0, 0.9091367634018797, 0.0, 0.30652125262867885, 0.0, 0.7954054080966687, 0.0, 0.0, 0.0, 1.0559083316811368, 0.0, 0.04427154393293609, 0.8228955354751877, 0.46658038808648383, 0.0, 0.02688606839966265, 0.0, 0.0, 0.6276611460113729, 0.22522673327345408, 0.0, 0.12308651028821192, 0.3657688390111318, 0.0, 0.046115245088609334, 0.0, 0.5342436705087195, 0.0, 0.1923886832463953, 0.07621763959827882, 0.0, 0.0, 0.0, 0.6725528112574624, 0.7483589153917343, 0.0, 0.0, 0.0, 0.0, 0.38949567271561986, 0.16492548861911044, 0.10519585180914466, 0.8853743380662012, 0.957787822214843, 0.0, 0.05459549529270359, 0.25656057396140347, 0.14167223917141744, 0.03949155262056029, 0.0, 0.0, 0.2703834370419471, 0.0, 0.723362221877522, 0.0, 0.0, 0.0, 0.7883757417725614, 0.0, 0.03340589062163017, 0.18896261922152233, 0.0, 0.0, 0.8122657251866786, 0.029607718570353176, 0.0, 0.008213735995679807, 0.0, 0.08605746445745502, 0.0, 0.3070376563830936, 0.10398717208817806, 0.0, 0.0, 0.0, 0.383984202548332, 0.0, 0.0, 0.3036396360576457, 0.0, 0.0, 0.0, 0.0, 0.11965354383371518, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04825455862354496, 0.3335108938296942, 0.0, 0.0, 0.0, 0.0, 0.04121745447265487, 0.30594446336043757, 0.0, 0.33550494159326405, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029116195807012437, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020041236855252602, 0.0, 0.4850188795993097, 0.0, 0.16403823255879735, 0.0, 0.60023572778736, 0.0, 0.1787344157826609, 0.0, 0.0, 0.17233073276918714, 0.17532212882841147, 0.6209016656456683, 0.5984929062073807, 0.8843163905991084, 0.3337919143714591, 0.0, 1.0919707374354302, 0.8166713304287454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8330317657834686, 0.27201200741148646, 0.0, 0.8995473587201992, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6388227016456749, 0.0, 0.3682672817259989, 0.0, 0.5422113050459496, 0.0, 0.028602469161704683, 0.0, 0.0, 0.3611222515381231, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.031337192831751, 0.0, 0.0, 0.7687924758178234, 0.012617011188267763, 0.0, 0.0, 0.37315715294884366, 0.0, 0.0, 0.0, 0.0, 0.6888779809403613, 0.121001715761302, 0.0, 0.0, 0.3680175749172942, 0.47420475938210593, 0.0, 0.7604201795169553, 0.0, 0.11917670920609563, 0.0, 0.20848198246903302, 0.24926285864223433, 0.0, 0.5553692000145817, 0.4169252107806106, 0.16597755096800731, 0.7334771026528798, 0.0, 0.0, 0.09908991746260937, 0.0, 0.14652270264303283, 0.006759647910977647, 0.0, 0.9351545954529237, 1.2960299208884896, 0.0, 0.0, 0.21191619141709428, 0.0, 0.0, 0.0, 0.01204540426582532, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06493914883934329, 0.7726981572397796, 0.0, 0.4370686273737238, 0.1778273534538074, 0.055209023869020324, 0.0, 0.0, 0.0, 0.05709969797354255, 0.0, 0.0, 0.0, 0.0, 0.44808598581285264, 0.0, 0.0, 0.0, 0.21141909545938053, 0.13390642572615394, 0.0, 0.0, 0.0, 0.25844994675864025, 0.0, 0.51617125876173, 0.2544268105667549, 0.0, 0.17576058376064335, 0.29087109088619356, 0.0, 0.0, 0.3286889977164017, 0.0, 0.0644707079973061, 0.0, 0.018682819036845075, 0.008522941045952347, 0.0, 0.0, 0.2520720928277996, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.146066293561638, 0.08856351358209161, 0.0, 0.0, 0.0, 0.1407746062228793, 0.5246446552397386, 0.0, 0.0, 0.0, 0.3279179855015388, 0.46137944574560436, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14330339619658897, 0.11500988018505275, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7444467047874328, 0.0, 0.0, 0.40713437972280675, 0.0, 0.0, 0.0, 0.5717679971406524, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4877440937322302, 0.6991654372087778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2369682176872117, 0.0, 0.0, 0.6757556051622259, 0.0, 0.0, 0.0, 0.576368321209737, 0.04986303698751877, 0.0, 0.5384684335273707, 0.0, 0.21524971756673417, 0.4963922980972075, 0.0, 0.0, 0.0, 0.0, 0.18940210229325644, 0.5277692784584271, 0.7718932464687741, 0.7210929692294604, 0.0, 0.0, 0.0, 0.0, 0.48286176484120297, 0.8517937989915767, 0.3931693184679991, 0.0, 0.0, 0.06755455710060922, 0.0, 0.026695131792948763, 1.4844324548606433, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4524914530619723, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4694831016582333, 0.0, 0.42831526212260856, 0.6074590079807168, 0.30066361967157035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6129938591206631, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6008436638333436, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43882326364474644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8591062832739853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6928708028466394, 0.2819039698547807, 0.08752108546971657, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05246236611269018, 0.0, 0.0, 0.5428804251437249, 0.022819504909203123, 0.3729965675892053, 0.0, 0.5741583158437262, 0.0, 0.1935811346150516, 0.0, 0.5023321549740235, 0.2611826745866744, 0.0, 0.056899028261467276, 0.37961815387770637, 0.22527271986010186, 0.05171423855571594, 0.058968538904135746, 0.0, 0.02892983665470066, 0.003313037290954743, 0.0, 0.0, 0.15785301822506514, 0.0, 0.0, 0.07773433689527456, 0.1049811110510813, 0.0, 0.02912372760696772, 0.0, 0.3114315285072337, 0.5703037753007745, 0.0, 0.04813466284828647, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02241933195791073, 0.0, 0.0, 0.0, 0.0, 0.23199999736075347, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1927795141790272, 0.2407119020891451, 0.0, 0.21960445663579467, 0.5770903406753464, 0.15415530724008072, 0.23155572792080611, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3142922871488459, 0.0, 0.0, 0.0, 0.0, 0.3042889785067072, 0.0, 0.5129566600174419, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3746354155106774, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4789154917746846, 0.17517544375428362, 0.0, 0.0, 0.0, 0.2637662382252967, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2102700997979013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.126545263019808, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.332133943612647, 0.0, 0.0, 0.0, 0.06184093060913505, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3212574037659458, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24673989573277713, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3017167941185386, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6233354196759152, 0.0, 0.0, 0.0, 0.5447611694646652, 0.0, 0.0, 0.216176776357065, 0.0, 0.19855221622967104, 0.8686764576101142, 0.16350923987929902, 0.0, 0.0, 0.02334951098786776, 0.17107621812888604, 0.48682879160279025, 0.7109808359466632, 0.6651558420918936, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6192268761337199, 0.31206921876898447, 0.0, 0.0, 0.24595989001208304, 0.11068536977374843, 0.0, 0.6618395820988949, 0.11362642836904417, 0.0, 0.0, 0.23765422406147768, 0.0, 0.0, 0.8073352785417658, 0.0, 0.0, 0.48485062583878286, 1.055137179889336, 0.2671063045394923, 0.0, 0.8476204266733893, 0.473385026956602, 0.5030912229948215, 0.0, 0.0, 0.0, 0.17678971866264329, 0.4076988147485064, 0.0, 0.6776909084507561, 0.13779930009477406, 0.5691281649873949, 0.16622169517158358, 0.433469475882236, 0.818379113845578, 0.9106844436778354, 0.0, 0.499393081506665, 0.0, 0.5725309586159512, 0.6160013948015824, 0.5125607980557373, 0.2778645861379971, 0.0, 0.0, 0.0, 0.0, 0.0, 0.58929804834674, 0.0, 0.8046233984476477, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3967677816937078, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11707526901952116, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.383435421983313, 0.11707115655719894, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07864429232511502, 0.0, 0.0, 0.3002587300128159, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2266804769859352, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24215921984980293, 0.09852579316681935, 0.030588729804574537, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18874358875246125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3978422171614565, 0.1936032919321414, 0.0, 0.0, 0.2542147176776134, 0.0, 0.0, 0.0, 0.11057553114503467, 0.0, 6.012518808232222e-5, 0.0008825884581384356, 0.6460696181307314, 0.0, 0.0, 0.06879188140017427, 0.0, 0.0, 0.04716298956935701, 0.1540138997690347, 0.1854894997474166, 0.003124358567077274, 0.35109153378778224, 0.0, 0.0, 0.016293742009993128, 0.0, 0.0, 0.3747805868672189, 0.025859295570033313, 0.214029422856965, 0.003742959361061961, 0.0002933458280023473, 0.0, 0.0, 0.0, 0.030501494612752977, 0.0, 0.6996784734474946, 0.0, 0.0, 0.05616866413207192, 0.0, 0.0018184995370819535, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020171782722387992, 0.0, 0.0, 0.33279196588498333, 0.0, 0.0, 0.6323890570808377, 0.0, 0.0, 0.0, 0.03594525644730097, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004527887543653786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17039388977135247, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6309056496928738, 0.0, 0.0, 0.0, 0.0, 0.260280820884557, 0.0, 0.0, 0.5107183088816271, 0.7967464980324664, 0.14485492608681305, 0.0, 0.009558078807310686, 0.754701551784825, 0.0, 0.0, 0.0, 0.0, 0.7598764239846039, 0.0, 0.029177780358136646, 0.10253044625069853, 0.03763219091970284, 0.8115624154017409, 0.0, 0.5779584654853126, 0.7162312164421211, 0.0, 0.0, 0.9107472399622886, 0.7029540573396418, 0.0, 1.0969174796040306, 0.0, 0.0, 0.0, 0.0, 0.0038591832437520454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15366266383288085, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.558080611304664, 0.0, 0.0, 0.0, 0.0, 0.30736701150515244, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04311886842935436, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32751189175201845, 0.0, 0.20963216887545572, 0.1760259950046517, 0.0, 0.23753864749234863, 0.24093292162050975, 0.024684548149881243, 0.4936183719527589, 0.7064246796060082, 0.0, 0.06677453327599289, 0.8542035190842152, 0.0, 0.025534429812218047, 0.011379900282647532, 0.039800411165778926, 0.013727111201321645, 0.27843394547131894, 0.24161261590501448, 0.2499939278392831, 0.0, 0.0, 0.3450248607837782, 0.0, 0.0, 0.15272914557490577, 0.10495111476871369, 0.0, 0.0, 0.0, 0.18866875753074405, 0.0, 0.2225813946448797, 0.0, 0.0, 0.3661590836002555, 0.0, 0.0, 0.0, 0.8998957690359772, 0.30889178773893766, 0.6078813570942335, 0.0, 0.6840699471933163, 0.0, 0.0, 0.0, 0.36308986165522683, 0.752528074611531, 0.7191770460546526, 0.49508503625401296, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7784554319844289, 0.16352298326175332, 0.0, 0.042948548224867954, 0.375190336410357, 0.0, 0.6830715485727474, 0.6403864588932121, 0.7831097846364896, 0.46112164648188003, 0.14556009485746954, 0.0, 0.0, 0.0, 0.8585580212631712, 0.0, 0.253359163441029, 0.0, 0.0, 0.0, 0.45902026051340783, 0.0, 0.0, 0.0, 0.0, 0.166881895984427, 0.0, 0.0, 0.837727301005, 0.0, 0.0, 0.7629510291223568, 0.0, 0.34201996071392393, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3670598185817387, 0.0, 0.9533138689466589, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8365046398061449, 0.0, 0.9590136929209965, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.801491142663501, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16957493050977235, 0.0, 0.2507477829737011, 0.0, 0.0, 0.11532054524043628, 0.07735685692161769, 0.5247858743947099, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02100484631395434, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8201592179395716, 0.0, 0.0, 0.04562437343117348, 0.0, 0.22915724847477928, 0.0, 0.0, 0.7159205767218768, 0.0, 0.0, 0.0, 0.0, 0.29228977882295926, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17928588753235497, 0.10348218971035608, 0.8147006956016374, 0.08438351505236888, 0.0, 0.0, 0.0, 0.1070859593707194, 0.7148756921758321, 0.0, 0.8195717571774336, 0.12249576952187838, 0.0, 0.2397374334824377, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05225556550152024, 0.0, 0.0, 0.0, 0.1707240047839942, 0.0, 0.11920584928810601, 0.0, 0.14514911616615112, 0.4284534617057244, 0.04403067305952579, 0.0, 0.012688229227290206, 0.3654383338493288, 0.04642776495318284, 0.0, 0.0, 0.31371261026073766, 0.1364760956744218, 0.5008825042361101, 0.0, 0.04802625911306818, 0.021403808257108425, 0.0, 0.12191697007589085, 0.33632693219045084, 0.49073784844916374, 0.5066599946239788, 0.0, 0.0, 0.033354950181247504, 0.0, 0.15690710970564234, 0.3397139005999492, 0.4535410643500978, 0.0, 0.0, 0.49050280987387757, 0.0, 0.0, 0.45491953624514714, 0.0, 0.0, 0.33864218894794285, 0.0, 0.0, 0.041024172335819876, 0.0, 0.0, 0.0, 0.36175943329836924, 0.0, 0.39749358702806525, 0.0, 0.5595492530984049, 0.0, 0.0, 0.2573403608635483, 0.17262354625512744, 0.0, 0.0, 0.0, 0.3776391490751126, 0.0, 0.0, 0.1445911005474468, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.45961437114556436, 0.08073146338811468, 0.0, 0.0, 0.0, 0.5680662673212686, 0.0, 0.14301647595845865, 0.4278925860448857, 0.2624303541096862, 0.0, 0.5619603114708978, 0.5488835922357254, 0.0, 0.10445139145624531, 0.9180806504163392, 0.5113694952971799, 0.0, 0.4737323723594051, 0.4474251824766889, 0.10616343839228443, 0.0, 0.02745363380188561, 0.0, 0.28312699798757224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6983886824821985, 0.38943652344243257, 1.2223890565530993, 0.05586381199212052, 0.0, 0.07686093310040262, 0.10701921729681718, 0.0, 0.4467721663826107, 0.0, 0.5122035249872549, 1.008045581843232, 0.10571972464185753, 0.6235175832869809, 0.0, 0.0, 0.0, 0.0, 0.7054625658357997, 0.0, 0.0, 0.17326833225856653, 0.0, 0.0, 0.0, 1.0813156328128786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0927477810868, 0.0, 0.599657110852462, 0.22383250773975968, 0.19685563542122056, 0.0, 0.05605157239206448, 0.41837305013296433, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18996852120778085, 0.0, 0.28856245320282303, 0.0, 0.0, 0.0, 0.26772611418599673, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1616947156362974, 0.0, 0.0, 0.0, 0.0, 0.36168541397352055, 0.0, 0.0, 0.0, 0.1573219565054934, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13761515434984456, 0.07943022011077468, 0.0, 0.21912413885604493, 0.0, 0.0, 0.40654019224262405, 0.0, 0.0, 0.022840701875983912, 0.0, 0.09402454628178203, 0.5332211799090736, 0.14433851006651358, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.043396172364084024, 0.04011000425692938, 0.0, 0.0, 0.0, 0.13104327726482687, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10232284486723907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10523174379677519, 0.0, 0.10079612361252216, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04415728315159431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11805369977437472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6523402818477722, 0.04871612610613401, 0.0, 0.19287524873967893, 0.0, 0.3592812200503388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.050401634126662875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8285325211040351, 0.0, 0.0, 0.0, 0.42701794948426475, 0.33826599655789313, 0.0, 0.0, 0.0063200711569476415, 0.0, 0.0, 0.4430530345512502, 0.0, 0.6935424923816391, 0.9428504267370209, 0.42746265456576077, 0.0025457381888520196, 0.0, 0.1593559778661365, 0.0, 0.8273232796667221, 0.04988580847200336, 0.9484876902255591, 0.5784846962232053, 0.0, 0.4437739313478008, 0.0, 0.0, 0.29414816951171024, 0.0, 0.022355194883774517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02069724212749366, 0.0, 0.0, 0.0, 0.0, 0.7860463101009033, 0.0, 0.0, 0.0, 0.8499064420418566, 0.0, 0.014821364995218649, 0.0, 0.004694386740689857, 0.25038082548050006, 0.5774089497757802, 0.0, 0.0, 0.0, 0.0, 0.006270331329794731, 0.6139069964758506, 0.8351435200462407, 1.0242327730345495, 0.0, 0.029307532281045708, 0.0, 0.0, 0.0, 0.5726801471740839, 0.4962238989829506, 0.0, 0.07225356949876899, 0.10746103390935668, 0.0, 0.0, 0.8346013157058231, 0.0, 0.0, 0.12871551917858068, 0.0, 0.0, 0.05722051841888945, 0.0, 0.08248659071056784, 0.08634381582594033, 0.0, 0.14691645141749232, 0.0, 0.0, 0.0, 0.12530859946217338, 0.0, 0.10215231917559951, 0.07032216186532725, 0.0, 0.8790426858900641, 0.10792096194290268, 0.054388076691659265, 0.0, 0.0, 0.0, 0.05001396409640395, 0.114742650990913, 0.28339886512623463, 0.15677327627325063, 0.0, 0.03258480102579557, 0.09096085908673467, 0.0, 0.0, 0.15530131741375458, 0.4726591715297998, 0.7751806988017736, 0.0, 0.51208607063286, 0.0, 0.0, 0.8142774797804637, 0.037795557638802935, 0.0, 0.03542525158598919, 0.07905092197782795, 0.0, 0.0, 0.1322231578644953, 0.0, 0.0, 0.008286484320060336, 0.670492082894006, 0.0, 0.0, 0.2274177128809876, 0.20430769763068007, 0.27374263149272277, 0.0, 0.0, 0.0, 0.0, 0.22105379716709922, 0.08605636125516496, 0.023058224273634728, 0.03343036365050954, 0.7905997431838073, 0.031091378094046798, 0.32331562706461314, 0.0, 0.048595513480849374, 0.0, 0.6695135011372569, 0.01079155413526415, 0.7675661133629853, 0.01950494389053299, 0.0, 0.06609079415505228, 0.15915608286937477, 0.0, 0.17610090955799163, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05278940542005462, 0.0, 0.04363140001862599, 0.3747258422202424, 0.18235409289517635]), 42), 42)),)

