julia> @finch begin
        CR .= 0
        for i = _
            for j = _
                for k = _
                    CR[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(CR = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0, 0.0, 0.40798272794029244, 0.0, 0.05552236202954211, 0.0, 0.0, 0.0, 1.1508129348205518, 0.0, 0.19395356472348033, 0.2307481734593406, 0.0, 0.0, 0.0, 0.0, 0.7400687115958035, 0.0, 0.0, 0.4711198057610666, 0.0, 0.07015028050595504, 0.08663540769774136, 0.0, 0.0, 0.46359958748433167, 0.0, 0.035823254967012344, 0.0, 0.21605429332119105, 0.050389255833200065, 0.0, 0.0, 0.08982343514212215, 0.0, 0.11508828733565957, 0.4022461237733094, 0.0, 0.5232411380476881, 0.0, 0.17677960337865342, 0.6347058701128525, 0.0, 0.0, 0.0, 0.080760767105778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.530417945120141, 0.036947581649449654, 0.0, 0.42172014534542845, 0.0, 0.0, 0.0, 0.04046090790460827, 0.0, 0.0, 0.0, 0.0, 0.016730358390191713, 0.11216899138664496, 0.10090277285890621, 0.3770798811678483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06022016698365358, 0.0, 0.3347277070064448, 0.08319806447593037, 0.0, 0.0, 0.15734979027583865, 0.6034963214197537, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8559793301235343, 0.0, 0.0221818618958444, 0.0, 0.0, 0.0, 0.0, 0.6249640468245943, 0.0, 0.706457127780651, 0.8381987455683932, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05075027829145945, 0.0, 0.0, 0.28298860880763127, 0.0, 0.46775948880043733, 0.3276758676884306, 0.23162154119323575, 0.0, 0.0, 0.7097058534293814, 0.0, 0.01925480712564137, 0.6617464495997725, 0.5510119129346432, 0.9503030089786222, 0.19493828703065708, 0.4234771306495736, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11491839349038546, 0.0, 0.0, 0.0, 0.0, 0.0, 0.952344347120024, 0.31481029761912926, 0.0, 0.0, 0.3371532262514988, 0.0, 0.6461986748500718, 0.0, 0.20388394461232018, 0.6689077519036521, 0.008065708303756284, 0.026774581755143884, 0.0, 0.0, 0.0, 0.0, 0.20246692989511542, 0.0, 0.0, 0.0, 0.0, 0.6964472745009185, 0.0, 0.3001098015546565, 0.0, 0.002810485733837209, 0.0, 0.0, 0.2833439267406652, 0.0, 0.0, 0.0, 0.01365478232442864, 0.0, 0.07752931191170997, 0.005400497114468351, 0.01008472602077043, 0.7305457354717977, 0.0, 0.0, 0.3791767135274968, 0.009729638043940761, 0.14133243937397577, 0.0, 0.6050992861887972, 0.0, 0.0, 0.0, 0.0, 0.32748073574326575, 0.29782974023215064, 0.0, 0.0, 0.0, 0.0, 0.008516959572353846, 0.0, 0.0, 0.0, 0.5188617620854502, 0.0, 0.12960309538886133, 0.0, 0.2159060099855789, 0.0, 0.0, 0.0, 0.5916658876154497, 0.0, 0.08410324034674674, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7242114416300933, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43295426032070555, 0.1879551877902947, 0.09140698995628445, 0.0, 0.0, 0.0, 0.0, 0.053834983759932134, 0.014210556502435533, 0.023024626002393696, 0.0, 0.0, 0.08510060429095491, 0.15442202399960203, 0.0, 0.5391171356753139, 0.0, 0.44855253786204935, 0.048493606647816025, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22138370024934187, 0.0, 0.08746644969030876, 0.0, 0.0, 0.0, 0.1616519416528851, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.035084084514750494, 0.0, 0.11688517594508105, 0.2089391042079727, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05598779403983851, 0.014778823268485956, 0.0, 0.0, 0.0, 0.0, 0.057780108315529474, 0.0, 0.201721527067654, 0.0, 0.3341648455181398, 0.0, 0.0, 0.7601477683176978, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8032144106029788, 0.0, 0.18178069418497272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6936206835397086, 0.0, 0.36283080196789974, 0.4415514891805875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.041591186808888826, 0.3770004419200786, 0.0, 0.0, 0.0, 0.17252416859375463, 0.0, 0.0, 0.6527400000425327, 0.0, 0.3340575108846618, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7763472218770214, 0.0, 0.0, 0.0, 0.0, 0.8376247582699287, 0.0, 0.014460886244840168, 0.6266762938745922, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.46397397404593055, 0.0, 0.23926875754339813, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.46347904589364963, 0.05902942262309747, 0.0, 0.03877718275846287, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2950495413884076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06597380264866397, 1.0898197013430582, 0.0, 0.012501862454917015, 0.0, 0.0, 0.05385941027253928, 0.0, 0.3522343881391282, 0.0, 0.398164495085241, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004630463685504157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25876266416388005, 0.0, 0.0, 0.3103356146935319, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6074148238597709, 0.09425255656440318, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0032593418386170935, 0.0, 0.2297346969390008, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12055792386342427, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25324185315889564, 0.0, 0.05731281617489229, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21868854065637486, 0.0, 0.0, 0.13921477990069736, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013113097926933236, 0.11886277100269102, 0.0, 0.0, 0.0, 0.0, 0.0, 0.52224991418746, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4173873832595858, 0.08858709037486684, 0.0, 0.0, 0.0, 0.0, 0.15667266254329507, 0.0, 0.40730912125206437, 0.5164653515440787, 0.0, 0.2032159032369745, 0.40587691627032874, 0.0, 0.0, 0.0, 0.0340034080482809, 0.0, 0.0, 0.15427161497649788, 0.44428262792688017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014262992619462008, 0.38024333666677773, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1612065141710269, 0.0, 0.03925137009723705, 0.0, 0.0078366311533161, 0.0, 0.014460397168610369, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06667255880132161, 0.0, 0.14952781464402123, 0.0, 0.0, 0.0, 0.0, 0.2779234999305725, 0.0, 0.008746314891451676, 0.0, 0.0, 0.06465282563196756, 0.0, 0.2464235137262255, 0.006899580599861792, 0.278556260330773, 0.007445268802069765, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18845739307946788, 0.01641249348500929, 0.5317378389887557, 0.0, 0.0, 0.14988438489574235, 0.0, 0.010375471289624015, 1.3557213333649594, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18808146093167488, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37253241298417417, 0.0, 0.1111794727253734, 0.16963583085055012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10986305912788819, 0.0, 0.0, 0.11949895822624457, 0.0, 0.0, 0.15821760502340373, 0.0, 0.5523682427180556, 0.0, 0.4595776329669528, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33581328744582783, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24138228903328196, 0.0, 0.3691306800612154, 0.0, 0.0, 0.0, 0.0, 0.0, 1.232394871494877, 0.0, 0.0, 0.0, 0.5293521525003586, 0.0, 0.48349454590588176, 0.0, 0.0, 0.6981988327823653, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33600003819418794, 0.10493510524858034, 0.1002766625939797, 0.0, 0.4760848715940903, 0.30327936688968893, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15549653856407664, 0.053555423141422365, 0.33124643300151746, 0.1452133754699813, 0.29863037601205833, 0.5069679636197618, 0.0, 1.2650777504013886, 0.0, 0.0, 0.8010376870067246, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8336854027224526, 0.0, 0.0, 0.0, 0.0, 0.013135866634354858, 0.7693584995036987, 0.0, 0.0, 0.0, 0.1809386811488775, 0.18409266358857004, 0.0, 0.8209089663283375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6269731623222802, 0.0, 0.0, 0.0, 0.0, 0.8578109001691112, 0.13549697762155272, 0.8892025809188543, 0.0, 0.17753144998442447, 0.0, 0.3275865900116729, 0.37045604078207667, 0.0, 0.0, 0.45942273296787417, 0.0, 0.0, 0.1496601554416231, 0.0, 0.4896166566389182, 0.09296161452877622, 0.0, 0.15295686770265157, 0.0, 0.0, 0.0, 0.18412880463476564, 0.0, 0.19671721658454625, 0.0, 0.0, 0.15630345798009787, 0.0, 0.16866550691184853, 0.0, 0.3962565901298767, 0.0, 0.0, 0.009054605400028368, 0.0, 0.0, 0.0, 0.0, 0.6323717354885464, 0.0, 0.27367929286054155, 1.074929147630902, 0.1458067334724375, 0.22417700682510228, 0.0, 0.10951695376531846, 0.0, 0.007581110002282966, 0.24931277842808883, 0.0, 0.6883954376611309, 0.6336690200937254, 0.2666286675338324, 0.0, 0.0, 0.0, 0.0, 0.2876738228687025, 0.0, 0.0, 0.21522568832904387, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0438812752886428, 0.6525143726026617, 0.0, 0.0, 0.0, 0.0, 0.0, 1.1869853573228195, 0.11631516482088484, 0.0, 0.0, 0.0, 0.0, 0.06506946683877823, 0.0, 0.22717008671076838, 0.10242247240104177, 0.18900849588618046, 0.11442201255265518, 0.5664450740439114, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019408932182682588, 0.20321698947174538, 1.233551723482737, 0.3258945612095407, 0.0, 0.13004560449904765, 0.0, 0.5146863326888748, 0.0, 0.0, 0.26506893163091566, 0.6961290235591785, 0.7308179758735578, 0.019108835994024305, 0.0, 0.0, 0.12318551581994355, 0.0, 0.6633736020985646, 0.09733981959320877, 0.6085861256804133, 0.0, 0.3702803270823883, 0.0, 0.31915877170632656, 0.0, 0.0048098299833409025, 0.08330917183364514, 0.03767139221212642, 0.0, 0.0, 0.7595521482230864, 0.0, 0.40492576086384785, 0.9767048148851073, 0.0, 0.7118250358317957, 1.4759259446598505, 0.6177251878074594, 0.0, 0.0, 0.5578474164235181, 0.0, 0.0, 0.0, 0.8466209961003106, 0.0, 0.0, 0.0, 0.0, 0.9134452822413822, 0.0, 0.0, 1.1506817448958997, 0.0, 0.0, 0.5717156136201251, 0.0, 0.0, 0.0, 0.0, 0.22301492152906585, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13134314230830874, 0.20762859161099162, 0.0, 0.4768286902241477, 0.0, 0.3252201513410915, 0.5728390197892637, 0.07274257736444308, 0.0946619273240044, 0.0, 0.08735695093663028, 0.0, 0.006047124502145837, 0.0, 0.0, 0.03396077178506113, 0.056902676891892164, 0.05446213354503342, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03284486748482166, 0.0, 0.03394228891521373, 0.0, 0.0, 0.0, 0.15913459349815823, 0.0988686644243417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06403136572671528, 0.0008539041494970381, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4996043959310575, 0.03466871274471111, 0.0, 0.0, 0.0, 0.5916091668585121, 0.0, 0.0, 0.18522959478004286, 0.0, 0.0, 0.6748263739107161, 0.0, 0.0, 0.5617854630926186, 0.0, 0.7191769350887419, 0.22919875305609166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07421634147547619, 0.2082330820676049, 0.0, 0.33671550924577986, 0.0, 0.0, 0.8216886233224614, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9288637557541211, 0.0, 0.0, 1.2410393190221276, 0.024834505618206743, 0.6697889027661909, 0.39618825585335343, 0.37106318592637155, 0.321662062811288, 0.0, 0.8625204846142658, 0.4737807718760894, 0.8171062776340833, 0.16785258111865808, 1.931465974538034, 0.1820623956052182, 0.0, 0.40831479446929375, 0.0, 1.6290713896950666, 0.0, 0.0, 1.0235643660149811, 0.0, 0.0, 0.0, 0.0, 0.996334540430692, 0.0, 0.0, 0.44156504893800563, 0.289897027676234, 0.0, 0.5556259307084835, 0.0, 0.0, 0.5751570179061616, 0.0, 0.15465163640644544, 0.0, 0.0, 0.0, 0.0, 0.1740886831542383, 0.8206341762594166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8467869185435447, 0.05898504962060997, 0.03624481511042625, 0.6732560722166093, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.201694060644539, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4141926752107277, 0.14795154825402113, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21427949262749374, 0.46809092516056994, 0.0, 0.16828675961297507, 0.14194253863939424, 0.0, 0.46436829494386733, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13445486993921502, 0.019720618408469233, 0.2722006890254698, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06550905522135163, 0.04806777087710764, 0.0, 0.0, 0.0, 0.599761835300619, 0.0, 0.0, 0.5741431736534577, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17101973439245724, 0.0, 0.0, 0.0749875087919759, 0.0, 0.0, 0.28776812087676407, 0.8309016545414952, 0.0, 0.0, 0.0, 0.016199069272106216, 0.0, 0.0, 0.0, 0.0, 0.6545099569258361, 0.22702193109653576, 0.04569906480983916, 0.2833580027667076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11138860511768055, 0.062440751035711506, 0.5182620734767335, 0.0, 0.3481758497366363, 0.0, 0.4677994729760326, 0.6992526088901763, 0.01746909322126967, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4416823398234807, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08074443292335906, 0.005624454367769537, 0.4953841958649777, 0.06419759041251974, 0.0, 0.0, 0.0, 0.03378063280221251, 0.27793271609589054, 0.0, 0.0, 0.0, 0.013968102118739474, 0.0, 0.08424327814659714, 0.057402094804332435, 0.0, 0.0, 0.0333773112405404, 0.0, 0.0, 0.0, 0.020814415925578544, 0.050277550690626754, 0.07266714943835642, 0.05420980100927718, 0.1299217183065535, 0.0, 0.0, 0.3197221256849075, 0.0, 0.13800911480490768, 0.0, 0.0, 0.0, 0.0, 1.01268183989861, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7358814757212789, 0.30707785521832553, 0.5276131242565835, 0.0, 0.1743689886750783, 0.0, 0.25821885501181596, 0.5805464694386301, 0.7164948773890167, 0.0, 0.0, 0.0, 0.37021123955626156, 0.12525001349038106, 0.608077535649827, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0938347119534369, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17530069799945608, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6839828466860547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0036702000371577168, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.030804733252794536, 0.6085396823865649, 0.3664826840226463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.052310575793324585, 0.23438841535865984, 0.027151249553211434, 0.42665810243765545, 0.060892623345148826, 0.0, 0.0, 0.08445468462850356, 0.05478879427510317, 0.5501718854551394, 0.0, 0.11343808432438283, 0.0, 0.09259962343643532, 0.2732186452567139, 0.0, 0.42011946077495216, 0.2576342392674693, 0.0, 0.5090094423403475, 0.0, 0.0, 0.0, 0.0010952054350690443, 0.08086253983246877, 0.0, 0.0, 0.23985944027024297, 0.0, 0.09028722397131896, 0.0, 0.07674602267622682, 0.43900489287844074, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3006474050100551, 0.0, 0.0, 0.0709455536446502, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24893625570967048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2768131690025365, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0014853584531852684, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2394367041087018, 0.8202130158779322, 0.0, 0.0, 0.28871220094126887, 0.0, 0.0, 0.04358483751824937, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.052964062286445145, 0.0, 0.2958896434064324, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.695162332188878, 0.037163387811150035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0679762361139716, 0.0, 0.0, 0.24678121036961095, 0.0, 0.0, 0.2964710587865061, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0759598903032082, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07295585235760724, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0603439559536301, 0.0, 0.0, 0.0, 0.17528000283667142, 0.0, 0.12675745187805368, 0.328557971512483, 0.028687306018592266, 0.0, 0.0, 0.8516002254500735, 0.0, 0.4371253184691764, 0.13991121920676916, 0.0, 0.3475458448096685, 0.46894433815651543, 0.0, 0.0, 0.6068029321085694, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28015080493647854, 0.3107571390829928, 0.0, 0.0, 0.0, 0.0, 0.006563618369995558, 0.5906679530337167, 0.0, 0.0, 0.0, 0.6525869879262292, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00921986700494424, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04479484649111107, 0.0, 0.3151409942990884, 0.021951930030038837, 0.0, 0.2505595957731255, 0.0, 0.0, 0.0, 0.03191831493444459, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22403715744289487, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027940093628006875, 0.0, 0.0, 0.0, 0.26553429437558146, 0.0, 0.030379801994009328, 0.0, 1.223521302648351, 0.3114612281119979, 0.3934859798468234, 0.0, 0.3688885410084021, 0.0, 0.5211677391443227, 0.318605826264122, 0.11794887163110916, 0.0, 0.0, 0.4256936617495534, 0.0, 0.49172025370622324, 0.48430956473476544, 0.0, 1.3234474260067457, 0.28650182105087124, 0.006913853673799038, 0.28826288884375384, 0.3033256150127764, 0.0, 0.19416551553442815, 0.19479473776455275, 0.0, 0.2201952762677155, 0.0, 0.0, 0.34956927181897907, 0.0, 0.0, 0.0, 0.5335997378219544, 0.1831214497933391, 0.5101375543135913, 0.0, 0.7417718286797346, 0.0, 0.23966647500006097, 0.8693502490764229, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21247306643180602, 0.1378389271943791, 0.0, 0.0, 0.2853901797463675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20343586472727235, 0.29392458170687635, 0.011188169832657642, 0.0, 0.0, 0.28953305486076913, 0.0, 0.0, 0.07223789467137894, 0.0, 0.33051308113923733, 0.4540345661588786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9537370190824367, 0.0, 0.6089477015288718, 0.0, 0.07722630190963325, 0.0, 0.0, 1.0000222254545443, 0.8637206744494375, 0.0, 0.7751920658363071, 0.524298363554463, 0.0, 0.0, 0.01676078910810196, 0.0, 0.0, 0.07604278960261779, 0.0, 0.0, 0.0, 0.0, 0.40940877803475156, 0.0, 0.0, 0.0, 0.0, 0.09252661041066143, 0.44765043171933794, 0.0, 0.24055976047324873, 0.0, 0.15100552643910237, 0.35800810608226774]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            phase_start_2 = max(1, 1 + fld(A_lvl.shape[1] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                for i_7 = phase_start_2:phase_stop_2
                    B_lvl_q = B_lvl_ptr[1]
                    B_lvl_q_stop = B_lvl_ptr[1 + 1]
                    if B_lvl_q < B_lvl_q_stop
                        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                    else
                        B_lvl_i_stop = 0
                    end
                    phase_stop_3 = min(B_lvl.shape[2], B_lvl_i_stop)
                    if phase_stop_3 >= 1
                        if B_lvl_tbl2[B_lvl_q] < 1
                            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        while true
                            B_lvl_i = B_lvl_tbl2[B_lvl_q]
                            B_lvl_q_step = B_lvl_q
                            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                            end
                            if B_lvl_i < phase_stop_3
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_5 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_5 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_5
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_6 = min(B_lvl_i_2, phase_stop_5, A_lvl_i)
                                        if A_lvl_i == phase_stop_6 && B_lvl_i_2 == phase_stop_6
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_7 = min(i_7, A_lvl_i_stop_2)
                                            if phase_stop_7 >= i_7
                                                if A_lvl_tbl1[A_lvl_q] < i_7
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_7
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_9 = min(A_lvl_i_2, phase_stop_7)
                                                        if A_lvl_i_2 == phase_stop_9
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_6
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_6
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_6 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            else
                                phase_stop_14 = min(B_lvl_i, phase_stop_3)
                                if B_lvl_i == phase_stop_14
                                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_14
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_15 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_15 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_15
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_16 = min(B_lvl_i_2, A_lvl_i, phase_stop_15)
                                            if A_lvl_i == phase_stop_16 && B_lvl_i_2 == phase_stop_16
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_17 = min(i_7, A_lvl_i_stop_4)
                                                if phase_stop_17 >= i_7
                                                    if A_lvl_tbl1[A_lvl_q] < i_7
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_17
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_19 = min(A_lvl_i_4, phase_stop_17)
                                                            if A_lvl_i_4 == phase_stop_19
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_16
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_16
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_16 + 1
                                        end
                                    end
                                    B_lvl_q = B_lvl_q_step
                                end
                                break
                            end
                        end
                    end
                end
            end
        end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0, 0.0, 0.40798272794029244, 0.0, 0.05552236202954211, 0.0, 0.0, 0.0, 1.1508129348205518, 0.0, 0.19395356472348033, 0.2307481734593406, 0.0, 0.0, 0.0, 0.0, 0.7400687115958035, 0.0, 0.0, 0.4711198057610666, 0.0, 0.07015028050595504, 0.08663540769774136, 0.0, 0.0, 0.46359958748433167, 0.0, 0.035823254967012344, 0.0, 0.21605429332119105, 0.050389255833200065, 0.0, 0.0, 0.08982343514212215, 0.0, 0.11508828733565957, 0.4022461237733094, 0.0, 0.5232411380476881, 0.0, 0.17677960337865342, 0.6347058701128525, 0.0, 0.0, 0.0, 0.080760767105778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.530417945120141, 0.036947581649449654, 0.0, 0.42172014534542845, 0.0, 0.0, 0.0, 0.04046090790460827, 0.0, 0.0, 0.0, 0.0, 0.016730358390191713, 0.11216899138664496, 0.10090277285890621, 0.3770798811678483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06022016698365358, 0.0, 0.3347277070064448, 0.08319806447593037, 0.0, 0.0, 0.15734979027583865, 0.6034963214197537, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8559793301235343, 0.0, 0.0221818618958444, 0.0, 0.0, 0.0, 0.0, 0.6249640468245943, 0.0, 0.706457127780651, 0.8381987455683932, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05075027829145945, 0.0, 0.0, 0.28298860880763127, 0.0, 0.46775948880043733, 0.3276758676884306, 0.23162154119323575, 0.0, 0.0, 0.7097058534293814, 0.0, 0.01925480712564137, 0.6617464495997725, 0.5510119129346432, 0.9503030089786222, 0.19493828703065708, 0.4234771306495736, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11491839349038546, 0.0, 0.0, 0.0, 0.0, 0.0, 0.952344347120024, 0.31481029761912926, 0.0, 0.0, 0.3371532262514988, 0.0, 0.6461986748500718, 0.0, 0.20388394461232018, 0.6689077519036521, 0.008065708303756284, 0.026774581755143884, 0.0, 0.0, 0.0, 0.0, 0.20246692989511542, 0.0, 0.0, 0.0, 0.0, 0.6964472745009185, 0.0, 0.3001098015546565, 0.0, 0.002810485733837209, 0.0, 0.0, 0.2833439267406652, 0.0, 0.0, 0.0, 0.01365478232442864, 0.0, 0.07752931191170997, 0.005400497114468351, 0.01008472602077043, 0.7305457354717977, 0.0, 0.0, 0.3791767135274968, 0.009729638043940761, 0.14133243937397577, 0.0, 0.6050992861887972, 0.0, 0.0, 0.0, 0.0, 0.32748073574326575, 0.29782974023215064, 0.0, 0.0, 0.0, 0.0, 0.008516959572353846, 0.0, 0.0, 0.0, 0.5188617620854502, 0.0, 0.12960309538886133, 0.0, 0.2159060099855789, 0.0, 0.0, 0.0, 0.5916658876154497, 0.0, 0.08410324034674674, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7242114416300933, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43295426032070555, 0.1879551877902947, 0.09140698995628445, 0.0, 0.0, 0.0, 0.0, 0.053834983759932134, 0.014210556502435533, 0.023024626002393696, 0.0, 0.0, 0.08510060429095491, 0.15442202399960203, 0.0, 0.5391171356753139, 0.0, 0.44855253786204935, 0.048493606647816025, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22138370024934187, 0.0, 0.08746644969030876, 0.0, 0.0, 0.0, 0.1616519416528851, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.035084084514750494, 0.0, 0.11688517594508105, 0.2089391042079727, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05598779403983851, 0.014778823268485956, 0.0, 0.0, 0.0, 0.0, 0.057780108315529474, 0.0, 0.201721527067654, 0.0, 0.3341648455181398, 0.0, 0.0, 0.7601477683176978, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8032144106029788, 0.0, 0.18178069418497272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6936206835397086, 0.0, 0.36283080196789974, 0.4415514891805875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.041591186808888826, 0.3770004419200786, 0.0, 0.0, 0.0, 0.17252416859375463, 0.0, 0.0, 0.6527400000425327, 0.0, 0.3340575108846618, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7763472218770214, 0.0, 0.0, 0.0, 0.0, 0.8376247582699287, 0.0, 0.014460886244840168, 0.6266762938745922, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.46397397404593055, 0.0, 0.23926875754339813, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.46347904589364963, 0.05902942262309747, 0.0, 0.03877718275846287, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2950495413884076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06597380264866397, 1.0898197013430582, 0.0, 0.012501862454917015, 0.0, 0.0, 0.05385941027253928, 0.0, 0.3522343881391282, 0.0, 0.398164495085241, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004630463685504157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25876266416388005, 0.0, 0.0, 0.3103356146935319, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6074148238597709, 0.09425255656440318, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0032593418386170935, 0.0, 0.2297346969390008, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12055792386342427, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25324185315889564, 0.0, 0.05731281617489229, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21868854065637486, 0.0, 0.0, 0.13921477990069736, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013113097926933236, 0.11886277100269102, 0.0, 0.0, 0.0, 0.0, 0.0, 0.52224991418746, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4173873832595858, 0.08858709037486684, 0.0, 0.0, 0.0, 0.0, 0.15667266254329507, 0.0, 0.40730912125206437, 0.5164653515440787, 0.0, 0.2032159032369745, 0.40587691627032874, 0.0, 0.0, 0.0, 0.0340034080482809, 0.0, 0.0, 0.15427161497649788, 0.44428262792688017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014262992619462008, 0.38024333666677773, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1612065141710269, 0.0, 0.03925137009723705, 0.0, 0.0078366311533161, 0.0, 0.014460397168610369, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06667255880132161, 0.0, 0.14952781464402123, 0.0, 0.0, 0.0, 0.0, 0.2779234999305725, 0.0, 0.008746314891451676, 0.0, 0.0, 0.06465282563196756, 0.0, 0.2464235137262255, 0.006899580599861792, 0.278556260330773, 0.007445268802069765, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18845739307946788, 0.01641249348500929, 0.5317378389887557, 0.0, 0.0, 0.14988438489574235, 0.0, 0.010375471289624015, 1.3557213333649594, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18808146093167488, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37253241298417417, 0.0, 0.1111794727253734, 0.16963583085055012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10986305912788819, 0.0, 0.0, 0.11949895822624457, 0.0, 0.0, 0.15821760502340373, 0.0, 0.5523682427180556, 0.0, 0.4595776329669528, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33581328744582783, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24138228903328196, 0.0, 0.3691306800612154, 0.0, 0.0, 0.0, 0.0, 0.0, 1.232394871494877, 0.0, 0.0, 0.0, 0.5293521525003586, 0.0, 0.48349454590588176, 0.0, 0.0, 0.6981988327823653, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33600003819418794, 0.10493510524858034, 0.1002766625939797, 0.0, 0.4760848715940903, 0.30327936688968893, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15549653856407664, 0.053555423141422365, 0.33124643300151746, 0.1452133754699813, 0.29863037601205833, 0.5069679636197618, 0.0, 1.2650777504013886, 0.0, 0.0, 0.8010376870067246, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8336854027224526, 0.0, 0.0, 0.0, 0.0, 0.013135866634354858, 0.7693584995036987, 0.0, 0.0, 0.0, 0.1809386811488775, 0.18409266358857004, 0.0, 0.8209089663283375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6269731623222802, 0.0, 0.0, 0.0, 0.0, 0.8578109001691112, 0.13549697762155272, 0.8892025809188543, 0.0, 0.17753144998442447, 0.0, 0.3275865900116729, 0.37045604078207667, 0.0, 0.0, 0.45942273296787417, 0.0, 0.0, 0.1496601554416231, 0.0, 0.4896166566389182, 0.09296161452877622, 0.0, 0.15295686770265157, 0.0, 0.0, 0.0, 0.18412880463476564, 0.0, 0.19671721658454625, 0.0, 0.0, 0.15630345798009787, 0.0, 0.16866550691184853, 0.0, 0.3962565901298767, 0.0, 0.0, 0.009054605400028368, 0.0, 0.0, 0.0, 0.0, 0.6323717354885464, 0.0, 0.27367929286054155, 1.074929147630902, 0.1458067334724375, 0.22417700682510228, 0.0, 0.10951695376531846, 0.0, 0.007581110002282966, 0.24931277842808883, 0.0, 0.6883954376611309, 0.6336690200937254, 0.2666286675338324, 0.0, 0.0, 0.0, 0.0, 0.2876738228687025, 0.0, 0.0, 0.21522568832904387, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0438812752886428, 0.6525143726026617, 0.0, 0.0, 0.0, 0.0, 0.0, 1.1869853573228195, 0.11631516482088484, 0.0, 0.0, 0.0, 0.0, 0.06506946683877823, 0.0, 0.22717008671076838, 0.10242247240104177, 0.18900849588618046, 0.11442201255265518, 0.5664450740439114, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019408932182682588, 0.20321698947174538, 1.233551723482737, 0.3258945612095407, 0.0, 0.13004560449904765, 0.0, 0.5146863326888748, 0.0, 0.0, 0.26506893163091566, 0.6961290235591785, 0.7308179758735578, 0.019108835994024305, 0.0, 0.0, 0.12318551581994355, 0.0, 0.6633736020985646, 0.09733981959320877, 0.6085861256804133, 0.0, 0.3702803270823883, 0.0, 0.31915877170632656, 0.0, 0.0048098299833409025, 0.08330917183364514, 0.03767139221212642, 0.0, 0.0, 0.7595521482230864, 0.0, 0.40492576086384785, 0.9767048148851073, 0.0, 0.7118250358317957, 1.4759259446598505, 0.6177251878074594, 0.0, 0.0, 0.5578474164235181, 0.0, 0.0, 0.0, 0.8466209961003106, 0.0, 0.0, 0.0, 0.0, 0.9134452822413822, 0.0, 0.0, 1.1506817448958997, 0.0, 0.0, 0.5717156136201251, 0.0, 0.0, 0.0, 0.0, 0.22301492152906585, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13134314230830874, 0.20762859161099162, 0.0, 0.4768286902241477, 0.0, 0.3252201513410915, 0.5728390197892637, 0.07274257736444308, 0.0946619273240044, 0.0, 0.08735695093663028, 0.0, 0.006047124502145837, 0.0, 0.0, 0.03396077178506113, 0.056902676891892164, 0.05446213354503342, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03284486748482166, 0.0, 0.03394228891521373, 0.0, 0.0, 0.0, 0.15913459349815823, 0.0988686644243417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06403136572671528, 0.0008539041494970381, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4996043959310575, 0.03466871274471111, 0.0, 0.0, 0.0, 0.5916091668585121, 0.0, 0.0, 0.18522959478004286, 0.0, 0.0, 0.6748263739107161, 0.0, 0.0, 0.5617854630926186, 0.0, 0.7191769350887419, 0.22919875305609166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07421634147547619, 0.2082330820676049, 0.0, 0.33671550924577986, 0.0, 0.0, 0.8216886233224614, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9288637557541211, 0.0, 0.0, 1.2410393190221276, 0.024834505618206743, 0.6697889027661909, 0.39618825585335343, 0.37106318592637155, 0.321662062811288, 0.0, 0.8625204846142658, 0.4737807718760894, 0.8171062776340833, 0.16785258111865808, 1.931465974538034, 0.1820623956052182, 0.0, 0.40831479446929375, 0.0, 1.6290713896950666, 0.0, 0.0, 1.0235643660149811, 0.0, 0.0, 0.0, 0.0, 0.996334540430692, 0.0, 0.0, 0.44156504893800563, 0.289897027676234, 0.0, 0.5556259307084835, 0.0, 0.0, 0.5751570179061616, 0.0, 0.15465163640644544, 0.0, 0.0, 0.0, 0.0, 0.1740886831542383, 0.8206341762594166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8467869185435447, 0.05898504962060997, 0.03624481511042625, 0.6732560722166093, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.201694060644539, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4141926752107277, 0.14795154825402113, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21427949262749374, 0.46809092516056994, 0.0, 0.16828675961297507, 0.14194253863939424, 0.0, 0.46436829494386733, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13445486993921502, 0.019720618408469233, 0.2722006890254698, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06550905522135163, 0.04806777087710764, 0.0, 0.0, 0.0, 0.599761835300619, 0.0, 0.0, 0.5741431736534577, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17101973439245724, 0.0, 0.0, 0.0749875087919759, 0.0, 0.0, 0.28776812087676407, 0.8309016545414952, 0.0, 0.0, 0.0, 0.016199069272106216, 0.0, 0.0, 0.0, 0.0, 0.6545099569258361, 0.22702193109653576, 0.04569906480983916, 0.2833580027667076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11138860511768055, 0.062440751035711506, 0.5182620734767335, 0.0, 0.3481758497366363, 0.0, 0.4677994729760326, 0.6992526088901763, 0.01746909322126967, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4416823398234807, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08074443292335906, 0.005624454367769537, 0.4953841958649777, 0.06419759041251974, 0.0, 0.0, 0.0, 0.03378063280221251, 0.27793271609589054, 0.0, 0.0, 0.0, 0.013968102118739474, 0.0, 0.08424327814659714, 0.057402094804332435, 0.0, 0.0, 0.0333773112405404, 0.0, 0.0, 0.0, 0.020814415925578544, 0.050277550690626754, 0.07266714943835642, 0.05420980100927718, 0.1299217183065535, 0.0, 0.0, 0.3197221256849075, 0.0, 0.13800911480490768, 0.0, 0.0, 0.0, 0.0, 1.01268183989861, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7358814757212789, 0.30707785521832553, 0.5276131242565835, 0.0, 0.1743689886750783, 0.0, 0.25821885501181596, 0.5805464694386301, 0.7164948773890167, 0.0, 0.0, 0.0, 0.37021123955626156, 0.12525001349038106, 0.608077535649827, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0938347119534369, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17530069799945608, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6839828466860547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0036702000371577168, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.030804733252794536, 0.6085396823865649, 0.3664826840226463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.052310575793324585, 0.23438841535865984, 0.027151249553211434, 0.42665810243765545, 0.060892623345148826, 0.0, 0.0, 0.08445468462850356, 0.05478879427510317, 0.5501718854551394, 0.0, 0.11343808432438283, 0.0, 0.09259962343643532, 0.2732186452567139, 0.0, 0.42011946077495216, 0.2576342392674693, 0.0, 0.5090094423403475, 0.0, 0.0, 0.0, 0.0010952054350690443, 0.08086253983246877, 0.0, 0.0, 0.23985944027024297, 0.0, 0.09028722397131896, 0.0, 0.07674602267622682, 0.43900489287844074, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3006474050100551, 0.0, 0.0, 0.0709455536446502, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24893625570967048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2768131690025365, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0014853584531852684, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2394367041087018, 0.8202130158779322, 0.0, 0.0, 0.28871220094126887, 0.0, 0.0, 0.04358483751824937, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.052964062286445145, 0.0, 0.2958896434064324, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.695162332188878, 0.037163387811150035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0679762361139716, 0.0, 0.0, 0.24678121036961095, 0.0, 0.0, 0.2964710587865061, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0759598903032082, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07295585235760724, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0603439559536301, 0.0, 0.0, 0.0, 0.17528000283667142, 0.0, 0.12675745187805368, 0.328557971512483, 0.028687306018592266, 0.0, 0.0, 0.8516002254500735, 0.0, 0.4371253184691764, 0.13991121920676916, 0.0, 0.3475458448096685, 0.46894433815651543, 0.0, 0.0, 0.6068029321085694, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28015080493647854, 0.3107571390829928, 0.0, 0.0, 0.0, 0.0, 0.006563618369995558, 0.5906679530337167, 0.0, 0.0, 0.0, 0.6525869879262292, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00921986700494424, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04479484649111107, 0.0, 0.3151409942990884, 0.021951930030038837, 0.0, 0.2505595957731255, 0.0, 0.0, 0.0, 0.03191831493444459, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22403715744289487, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027940093628006875, 0.0, 0.0, 0.0, 0.26553429437558146, 0.0, 0.030379801994009328, 0.0, 1.223521302648351, 0.3114612281119979, 0.3934859798468234, 0.0, 0.3688885410084021, 0.0, 0.5211677391443227, 0.318605826264122, 0.11794887163110916, 0.0, 0.0, 0.4256936617495534, 0.0, 0.49172025370622324, 0.48430956473476544, 0.0, 1.3234474260067457, 0.28650182105087124, 0.006913853673799038, 0.28826288884375384, 0.3033256150127764, 0.0, 0.19416551553442815, 0.19479473776455275, 0.0, 0.2201952762677155, 0.0, 0.0, 0.34956927181897907, 0.0, 0.0, 0.0, 0.5335997378219544, 0.1831214497933391, 0.5101375543135913, 0.0, 0.7417718286797346, 0.0, 0.23966647500006097, 0.8693502490764229, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21247306643180602, 0.1378389271943791, 0.0, 0.0, 0.2853901797463675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20343586472727235, 0.29392458170687635, 0.011188169832657642, 0.0, 0.0, 0.28953305486076913, 0.0, 0.0, 0.07223789467137894, 0.0, 0.33051308113923733, 0.4540345661588786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9537370190824367, 0.0, 0.6089477015288718, 0.0, 0.07722630190963325, 0.0, 0.0, 1.0000222254545443, 0.8637206744494375, 0.0, 0.7751920658363071, 0.524298363554463, 0.0, 0.0, 0.01676078910810196, 0.0, 0.0, 0.07604278960261779, 0.0, 0.0, 0.0, 0.0, 0.40940877803475156, 0.0, 0.0, 0.0, 0.0, 0.09252661041066143, 0.44765043171933794, 0.0, 0.24055976047324873, 0.0, 0.15100552643910237, 0.35800810608226774]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    for i_4 = 1:A_lvl.shape[1]
        val = Ct_lvl_2_val
        Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
        B_lvl_ptr_2 = B_lvl_ptr
        B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
        B_lvl_tbl1_2 = B_lvl_tbl1
        B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
        B_lvl_tbl2_2 = B_lvl_tbl2
        B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
        val_2 = B_lvl_val
        B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
        A_lvl_ptr_2 = A_lvl_ptr
        A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
        A_lvl_tbl1_2 = A_lvl_tbl1
        A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
        A_lvl_tbl2_2 = A_lvl_tbl2
        A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
        val_3 = A_lvl_val
        A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
        Threads.@threads for i_5 = 1:Threads.nthreads()
                B_lvl_q = B_lvl_ptr[1]
                B_lvl_q_stop = B_lvl_ptr[1 + 1]
                if B_lvl_q < B_lvl_q_stop
                    B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                else
                    B_lvl_i_stop = 0
                end
                phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_5 + -1), Threads.nthreads()))
                phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_5, Threads.nthreads()))
                if phase_stop_2 >= phase_start_2
                    if B_lvl_tbl2[B_lvl_q] < phase_start_2
                        B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    while true
                        B_lvl_i = B_lvl_tbl2[B_lvl_q]
                        B_lvl_q_step = B_lvl_q
                        if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                            B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        if B_lvl_i < phase_stop_2
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_4, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_4
                                            if A_lvl_tbl1[A_lvl_q] < i_4
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        else
                            phase_stop_13 = min(B_lvl_i, phase_stop_2)
                            if B_lvl_i == phase_stop_13
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_4, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_4
                                                if A_lvl_tbl1[A_lvl_q] < i_4
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            end
                            break
                        end
                    end
                end
            end
        Ct_lvl_2_val = val
        B_lvl_ptr = B_lvl_ptr_2
        B_lvl_tbl1 = B_lvl_tbl1_2
        B_lvl_tbl2 = B_lvl_tbl2_2
        B_lvl_val = val_2
        A_lvl_ptr = A_lvl_ptr_2
        A_lvl_tbl1 = A_lvl_tbl1_2
        A_lvl_tbl2 = A_lvl_tbl2_2
        A_lvl_val = val_3
    end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(Ct_lvl_2_val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0, 0.0, 0.40798272794029244, 0.0, 0.05552236202954211, 0.0, 0.0, 0.0, 1.1508129348205518, 0.0, 0.19395356472348033, 0.2307481734593406, 0.0, 0.0, 0.0, 0.0, 0.7400687115958035, 0.0, 0.0, 0.4711198057610666, 0.0, 0.07015028050595504, 0.08663540769774136, 0.0, 0.0, 0.46359958748433167, 0.0, 0.035823254967012344, 0.0, 0.21605429332119105, 0.050389255833200065, 0.0, 0.0, 0.08982343514212215, 0.0, 0.11508828733565957, 0.4022461237733094, 0.0, 0.5232411380476881, 0.0, 0.17677960337865342, 0.6347058701128525, 0.0, 0.0, 0.0, 0.080760767105778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.530417945120141, 0.036947581649449654, 0.0, 0.42172014534542845, 0.0, 0.0, 0.0, 0.04046090790460827, 0.0, 0.0, 0.0, 0.0, 0.016730358390191713, 0.11216899138664496, 0.10090277285890621, 0.3770798811678483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06022016698365358, 0.0, 0.3347277070064448, 0.08319806447593037, 0.0, 0.0, 0.15734979027583865, 0.6034963214197537, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8559793301235343, 0.0, 0.0221818618958444, 0.0, 0.0, 0.0, 0.0, 0.6249640468245943, 0.0, 0.706457127780651, 0.8381987455683932, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05075027829145945, 0.0, 0.0, 0.28298860880763127, 0.0, 0.46775948880043733, 0.3276758676884306, 0.23162154119323575, 0.0, 0.0, 0.7097058534293814, 0.0, 0.01925480712564137, 0.6617464495997725, 0.5510119129346432, 0.9503030089786222, 0.19493828703065708, 0.4234771306495736, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11491839349038546, 0.0, 0.0, 0.0, 0.0, 0.0, 0.952344347120024, 0.31481029761912926, 0.0, 0.0, 0.3371532262514988, 0.0, 0.6461986748500718, 0.0, 0.20388394461232018, 0.6689077519036521, 0.008065708303756284, 0.026774581755143884, 0.0, 0.0, 0.0, 0.0, 0.20246692989511542, 0.0, 0.0, 0.0, 0.0, 0.6964472745009185, 0.0, 0.3001098015546565, 0.0, 0.002810485733837209, 0.0, 0.0, 0.2833439267406652, 0.0, 0.0, 0.0, 0.01365478232442864, 0.0, 0.07752931191170997, 0.005400497114468351, 0.01008472602077043, 0.7305457354717977, 0.0, 0.0, 0.3791767135274968, 0.009729638043940761, 0.14133243937397577, 0.0, 0.6050992861887972, 0.0, 0.0, 0.0, 0.0, 0.32748073574326575, 0.29782974023215064, 0.0, 0.0, 0.0, 0.0, 0.008516959572353846, 0.0, 0.0, 0.0, 0.5188617620854502, 0.0, 0.12960309538886133, 0.0, 0.2159060099855789, 0.0, 0.0, 0.0, 0.5916658876154497, 0.0, 0.08410324034674674, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7242114416300933, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43295426032070555, 0.1879551877902947, 0.09140698995628445, 0.0, 0.0, 0.0, 0.0, 0.053834983759932134, 0.014210556502435533, 0.023024626002393696, 0.0, 0.0, 0.08510060429095491, 0.15442202399960203, 0.0, 0.5391171356753139, 0.0, 0.44855253786204935, 0.048493606647816025, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22138370024934187, 0.0, 0.08746644969030876, 0.0, 0.0, 0.0, 0.1616519416528851, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.035084084514750494, 0.0, 0.11688517594508105, 0.2089391042079727, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05598779403983851, 0.014778823268485956, 0.0, 0.0, 0.0, 0.0, 0.057780108315529474, 0.0, 0.201721527067654, 0.0, 0.3341648455181398, 0.0, 0.0, 0.7601477683176978, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8032144106029788, 0.0, 0.18178069418497272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6936206835397086, 0.0, 0.36283080196789974, 0.4415514891805875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.041591186808888826, 0.3770004419200786, 0.0, 0.0, 0.0, 0.17252416859375463, 0.0, 0.0, 0.6527400000425327, 0.0, 0.3340575108846618, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7763472218770214, 0.0, 0.0, 0.0, 0.0, 0.8376247582699287, 0.0, 0.014460886244840168, 0.6266762938745922, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.46397397404593055, 0.0, 0.23926875754339813, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.46347904589364963, 0.05902942262309747, 0.0, 0.03877718275846287, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2950495413884076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06597380264866397, 1.0898197013430582, 0.0, 0.012501862454917015, 0.0, 0.0, 0.05385941027253928, 0.0, 0.3522343881391282, 0.0, 0.398164495085241, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004630463685504157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25876266416388005, 0.0, 0.0, 0.3103356146935319, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6074148238597709, 0.09425255656440318, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0032593418386170935, 0.0, 0.2297346969390008, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12055792386342427, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25324185315889564, 0.0, 0.05731281617489229, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21868854065637486, 0.0, 0.0, 0.13921477990069736, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013113097926933236, 0.11886277100269102, 0.0, 0.0, 0.0, 0.0, 0.0, 0.52224991418746, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4173873832595858, 0.08858709037486684, 0.0, 0.0, 0.0, 0.0, 0.15667266254329507, 0.0, 0.40730912125206437, 0.5164653515440787, 0.0, 0.2032159032369745, 0.40587691627032874, 0.0, 0.0, 0.0, 0.0340034080482809, 0.0, 0.0, 0.15427161497649788, 0.44428262792688017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014262992619462008, 0.38024333666677773, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1612065141710269, 0.0, 0.03925137009723705, 0.0, 0.0078366311533161, 0.0, 0.014460397168610369, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06667255880132161, 0.0, 0.14952781464402123, 0.0, 0.0, 0.0, 0.0, 0.2779234999305725, 0.0, 0.008746314891451676, 0.0, 0.0, 0.06465282563196756, 0.0, 0.2464235137262255, 0.006899580599861792, 0.278556260330773, 0.007445268802069765, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18845739307946788, 0.01641249348500929, 0.5317378389887557, 0.0, 0.0, 0.14988438489574235, 0.0, 0.010375471289624015, 1.3557213333649594, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18808146093167488, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37253241298417417, 0.0, 0.1111794727253734, 0.16963583085055012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10986305912788819, 0.0, 0.0, 0.11949895822624457, 0.0, 0.0, 0.15821760502340373, 0.0, 0.5523682427180556, 0.0, 0.4595776329669528, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33581328744582783, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24138228903328196, 0.0, 0.3691306800612154, 0.0, 0.0, 0.0, 0.0, 0.0, 1.232394871494877, 0.0, 0.0, 0.0, 0.5293521525003586, 0.0, 0.48349454590588176, 0.0, 0.0, 0.6981988327823653, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33600003819418794, 0.10493510524858034, 0.1002766625939797, 0.0, 0.4760848715940903, 0.30327936688968893, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15549653856407664, 0.053555423141422365, 0.33124643300151746, 0.1452133754699813, 0.29863037601205833, 0.5069679636197618, 0.0, 1.2650777504013886, 0.0, 0.0, 0.8010376870067246, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8336854027224526, 0.0, 0.0, 0.0, 0.0, 0.013135866634354858, 0.7693584995036987, 0.0, 0.0, 0.0, 0.1809386811488775, 0.18409266358857004, 0.0, 0.8209089663283375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6269731623222802, 0.0, 0.0, 0.0, 0.0, 0.8578109001691112, 0.13549697762155272, 0.8892025809188543, 0.0, 0.17753144998442447, 0.0, 0.3275865900116729, 0.37045604078207667, 0.0, 0.0, 0.45942273296787417, 0.0, 0.0, 0.1496601554416231, 0.0, 0.4896166566389182, 0.09296161452877622, 0.0, 0.15295686770265157, 0.0, 0.0, 0.0, 0.18412880463476564, 0.0, 0.19671721658454625, 0.0, 0.0, 0.15630345798009787, 0.0, 0.16866550691184853, 0.0, 0.3962565901298767, 0.0, 0.0, 0.009054605400028368, 0.0, 0.0, 0.0, 0.0, 0.6323717354885464, 0.0, 0.27367929286054155, 1.074929147630902, 0.1458067334724375, 0.22417700682510228, 0.0, 0.10951695376531846, 0.0, 0.007581110002282966, 0.24931277842808883, 0.0, 0.6883954376611309, 0.6336690200937254, 0.2666286675338324, 0.0, 0.0, 0.0, 0.0, 0.2876738228687025, 0.0, 0.0, 0.21522568832904387, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0438812752886428, 0.6525143726026617, 0.0, 0.0, 0.0, 0.0, 0.0, 1.1869853573228195, 0.11631516482088484, 0.0, 0.0, 0.0, 0.0, 0.06506946683877823, 0.0, 0.22717008671076838, 0.10242247240104177, 0.18900849588618046, 0.11442201255265518, 0.5664450740439114, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019408932182682588, 0.20321698947174538, 1.233551723482737, 0.3258945612095407, 0.0, 0.13004560449904765, 0.0, 0.5146863326888748, 0.0, 0.0, 0.26506893163091566, 0.6961290235591785, 0.7308179758735578, 0.019108835994024305, 0.0, 0.0, 0.12318551581994355, 0.0, 0.6633736020985646, 0.09733981959320877, 0.6085861256804133, 0.0, 0.3702803270823883, 0.0, 0.31915877170632656, 0.0, 0.0048098299833409025, 0.08330917183364514, 0.03767139221212642, 0.0, 0.0, 0.7595521482230864, 0.0, 0.40492576086384785, 0.9767048148851073, 0.0, 0.7118250358317957, 1.4759259446598505, 0.6177251878074594, 0.0, 0.0, 0.5578474164235181, 0.0, 0.0, 0.0, 0.8466209961003106, 0.0, 0.0, 0.0, 0.0, 0.9134452822413822, 0.0, 0.0, 1.1506817448958997, 0.0, 0.0, 0.5717156136201251, 0.0, 0.0, 0.0, 0.0, 0.22301492152906585, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13134314230830874, 0.20762859161099162, 0.0, 0.4768286902241477, 0.0, 0.3252201513410915, 0.5728390197892637, 0.07274257736444308, 0.0946619273240044, 0.0, 0.08735695093663028, 0.0, 0.006047124502145837, 0.0, 0.0, 0.03396077178506113, 0.056902676891892164, 0.05446213354503342, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03284486748482166, 0.0, 0.03394228891521373, 0.0, 0.0, 0.0, 0.15913459349815823, 0.0988686644243417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06403136572671528, 0.0008539041494970381, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4996043959310575, 0.03466871274471111, 0.0, 0.0, 0.0, 0.5916091668585121, 0.0, 0.0, 0.18522959478004286, 0.0, 0.0, 0.6748263739107161, 0.0, 0.0, 0.5617854630926186, 0.0, 0.7191769350887419, 0.22919875305609166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07421634147547619, 0.2082330820676049, 0.0, 0.33671550924577986, 0.0, 0.0, 0.8216886233224614, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9288637557541211, 0.0, 0.0, 1.2410393190221276, 0.024834505618206743, 0.6697889027661909, 0.39618825585335343, 0.37106318592637155, 0.321662062811288, 0.0, 0.8625204846142658, 0.4737807718760894, 0.8171062776340833, 0.16785258111865808, 1.931465974538034, 0.1820623956052182, 0.0, 0.40831479446929375, 0.0, 1.6290713896950666, 0.0, 0.0, 1.0235643660149811, 0.0, 0.0, 0.0, 0.0, 0.996334540430692, 0.0, 0.0, 0.44156504893800563, 0.289897027676234, 0.0, 0.5556259307084835, 0.0, 0.0, 0.5751570179061616, 0.0, 0.15465163640644544, 0.0, 0.0, 0.0, 0.0, 0.1740886831542383, 0.8206341762594166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8467869185435447, 0.05898504962060997, 0.03624481511042625, 0.6732560722166093, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.201694060644539, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4141926752107277, 0.14795154825402113, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21427949262749374, 0.46809092516056994, 0.0, 0.16828675961297507, 0.14194253863939424, 0.0, 0.46436829494386733, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13445486993921502, 0.019720618408469233, 0.2722006890254698, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06550905522135163, 0.04806777087710764, 0.0, 0.0, 0.0, 0.599761835300619, 0.0, 0.0, 0.5741431736534577, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17101973439245724, 0.0, 0.0, 0.0749875087919759, 0.0, 0.0, 0.28776812087676407, 0.8309016545414952, 0.0, 0.0, 0.0, 0.016199069272106216, 0.0, 0.0, 0.0, 0.0, 0.6545099569258361, 0.22702193109653576, 0.04569906480983916, 0.2833580027667076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11138860511768055, 0.062440751035711506, 0.5182620734767335, 0.0, 0.3481758497366363, 0.0, 0.4677994729760326, 0.6992526088901763, 0.01746909322126967, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4416823398234807, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08074443292335906, 0.005624454367769537, 0.4953841958649777, 0.06419759041251974, 0.0, 0.0, 0.0, 0.03378063280221251, 0.27793271609589054, 0.0, 0.0, 0.0, 0.013968102118739474, 0.0, 0.08424327814659714, 0.057402094804332435, 0.0, 0.0, 0.0333773112405404, 0.0, 0.0, 0.0, 0.020814415925578544, 0.050277550690626754, 0.07266714943835642, 0.05420980100927718, 0.1299217183065535, 0.0, 0.0, 0.3197221256849075, 0.0, 0.13800911480490768, 0.0, 0.0, 0.0, 0.0, 1.01268183989861, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7358814757212789, 0.30707785521832553, 0.5276131242565835, 0.0, 0.1743689886750783, 0.0, 0.25821885501181596, 0.5805464694386301, 0.7164948773890167, 0.0, 0.0, 0.0, 0.37021123955626156, 0.12525001349038106, 0.608077535649827, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0938347119534369, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17530069799945608, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6839828466860547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0036702000371577168, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.030804733252794536, 0.6085396823865649, 0.3664826840226463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.052310575793324585, 0.23438841535865984, 0.027151249553211434, 0.42665810243765545, 0.060892623345148826, 0.0, 0.0, 0.08445468462850356, 0.05478879427510317, 0.5501718854551394, 0.0, 0.11343808432438283, 0.0, 0.09259962343643532, 0.2732186452567139, 0.0, 0.42011946077495216, 0.2576342392674693, 0.0, 0.5090094423403475, 0.0, 0.0, 0.0, 0.0010952054350690443, 0.08086253983246877, 0.0, 0.0, 0.23985944027024297, 0.0, 0.09028722397131896, 0.0, 0.07674602267622682, 0.43900489287844074, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3006474050100551, 0.0, 0.0, 0.0709455536446502, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24893625570967048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2768131690025365, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0014853584531852684, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2394367041087018, 0.8202130158779322, 0.0, 0.0, 0.28871220094126887, 0.0, 0.0, 0.04358483751824937, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.052964062286445145, 0.0, 0.2958896434064324, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.695162332188878, 0.037163387811150035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0679762361139716, 0.0, 0.0, 0.24678121036961095, 0.0, 0.0, 0.2964710587865061, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0759598903032082, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07295585235760724, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0603439559536301, 0.0, 0.0, 0.0, 0.17528000283667142, 0.0, 0.12675745187805368, 0.328557971512483, 0.028687306018592266, 0.0, 0.0, 0.8516002254500735, 0.0, 0.4371253184691764, 0.13991121920676916, 0.0, 0.3475458448096685, 0.46894433815651543, 0.0, 0.0, 0.6068029321085694, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28015080493647854, 0.3107571390829928, 0.0, 0.0, 0.0, 0.0, 0.006563618369995558, 0.5906679530337167, 0.0, 0.0, 0.0, 0.6525869879262292, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00921986700494424, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04479484649111107, 0.0, 0.3151409942990884, 0.021951930030038837, 0.0, 0.2505595957731255, 0.0, 0.0, 0.0, 0.03191831493444459, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22403715744289487, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027940093628006875, 0.0, 0.0, 0.0, 0.26553429437558146, 0.0, 0.030379801994009328, 0.0, 1.223521302648351, 0.3114612281119979, 0.3934859798468234, 0.0, 0.3688885410084021, 0.0, 0.5211677391443227, 0.318605826264122, 0.11794887163110916, 0.0, 0.0, 0.4256936617495534, 0.0, 0.49172025370622324, 0.48430956473476544, 0.0, 1.3234474260067457, 0.28650182105087124, 0.006913853673799038, 0.28826288884375384, 0.3033256150127764, 0.0, 0.19416551553442815, 0.19479473776455275, 0.0, 0.2201952762677155, 0.0, 0.0, 0.34956927181897907, 0.0, 0.0, 0.0, 0.5335997378219544, 0.1831214497933391, 0.5101375543135913, 0.0, 0.7417718286797346, 0.0, 0.23966647500006097, 0.8693502490764229, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21247306643180602, 0.1378389271943791, 0.0, 0.0, 0.2853901797463675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20343586472727235, 0.29392458170687635, 0.011188169832657642, 0.0, 0.0, 0.28953305486076913, 0.0, 0.0, 0.07223789467137894, 0.0, 0.33051308113923733, 0.4540345661588786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9537370190824367, 0.0, 0.6089477015288718, 0.0, 0.07722630190963325, 0.0, 0.0, 1.0000222254545443, 0.8637206744494375, 0.0, 0.7751920658363071, 0.524298363554463, 0.0, 0.0, 0.01676078910810196, 0.0, 0.0, 0.07604278960261779, 0.0, 0.0, 0.0, 0.0, 0.40940877803475156, 0.0, 0.0, 0.0, 0.0, 0.09252661041066143, 0.44765043171933794, 0.0, 0.24055976047324873, 0.0, 0.15100552643910237, 0.35800810608226774]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            B_lvl_q = B_lvl_ptr[1]
            B_lvl_q_stop = B_lvl_ptr[1 + 1]
            if B_lvl_q < B_lvl_q_stop
                B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
            else
                B_lvl_i_stop = 0
            end
            phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                if B_lvl_tbl2[B_lvl_q] < phase_start_2
                    B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                end
                while true
                    B_lvl_i = B_lvl_tbl2[B_lvl_q]
                    B_lvl_q_step = B_lvl_q
                    if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                        B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    if B_lvl_i < phase_stop_2
                        Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                        for i_6 = 1:A_lvl.shape[1]
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_6
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_6, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_6
                                            if A_lvl_tbl1[A_lvl_q] < i_6
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_6, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                        end
                        B_lvl_q = B_lvl_q_step
                    else
                        phase_stop_13 = min(B_lvl_i, phase_stop_2)
                        if B_lvl_i == phase_stop_13
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                            for i_8 = 1:A_lvl.shape[1]
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_8
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_8, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_8
                                                if A_lvl_tbl1[A_lvl_q] < i_8
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_8, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        end
                        break
                    end
                end
            end
        end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0, 0.0, 0.40798272794029244, 0.0, 0.05552236202954211, 0.0, 0.0, 0.0, 1.1508129348205518, 0.0, 0.19395356472348033, 0.2307481734593406, 0.0, 0.0, 0.0, 0.0, 0.7400687115958035, 0.0, 0.0, 0.4711198057610666, 0.0, 0.07015028050595504, 0.08663540769774136, 0.0, 0.0, 0.46359958748433167, 0.0, 0.035823254967012344, 0.0, 0.21605429332119105, 0.050389255833200065, 0.0, 0.0, 0.08982343514212215, 0.0, 0.11508828733565957, 0.4022461237733094, 0.0, 0.5232411380476881, 0.0, 0.17677960337865342, 0.6347058701128525, 0.0, 0.0, 0.0, 0.080760767105778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.530417945120141, 0.036947581649449654, 0.0, 0.42172014534542845, 0.0, 0.0, 0.0, 0.04046090790460827, 0.0, 0.0, 0.0, 0.0, 0.016730358390191713, 0.11216899138664496, 0.10090277285890621, 0.3770798811678483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06022016698365358, 0.0, 0.3347277070064448, 0.08319806447593037, 0.0, 0.0, 0.15734979027583865, 0.6034963214197537, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8559793301235343, 0.0, 0.0221818618958444, 0.0, 0.0, 0.0, 0.0, 0.6249640468245943, 0.0, 0.706457127780651, 0.8381987455683932, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05075027829145945, 0.0, 0.0, 0.28298860880763127, 0.0, 0.46775948880043733, 0.3276758676884306, 0.23162154119323575, 0.0, 0.0, 0.7097058534293814, 0.0, 0.01925480712564137, 0.6617464495997725, 0.5510119129346432, 0.9503030089786222, 0.19493828703065708, 0.4234771306495736, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11491839349038546, 0.0, 0.0, 0.0, 0.0, 0.0, 0.952344347120024, 0.31481029761912926, 0.0, 0.0, 0.3371532262514988, 0.0, 0.6461986748500718, 0.0, 0.20388394461232018, 0.6689077519036521, 0.008065708303756284, 0.026774581755143884, 0.0, 0.0, 0.0, 0.0, 0.20246692989511542, 0.0, 0.0, 0.0, 0.0, 0.6964472745009185, 0.0, 0.3001098015546565, 0.0, 0.002810485733837209, 0.0, 0.0, 0.2833439267406652, 0.0, 0.0, 0.0, 0.01365478232442864, 0.0, 0.07752931191170997, 0.005400497114468351, 0.01008472602077043, 0.7305457354717977, 0.0, 0.0, 0.3791767135274968, 0.009729638043940761, 0.14133243937397577, 0.0, 0.6050992861887972, 0.0, 0.0, 0.0, 0.0, 0.32748073574326575, 0.29782974023215064, 0.0, 0.0, 0.0, 0.0, 0.008516959572353846, 0.0, 0.0, 0.0, 0.5188617620854502, 0.0, 0.12960309538886133, 0.0, 0.2159060099855789, 0.0, 0.0, 0.0, 0.5916658876154497, 0.0, 0.08410324034674674, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7242114416300933, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43295426032070555, 0.1879551877902947, 0.09140698995628445, 0.0, 0.0, 0.0, 0.0, 0.053834983759932134, 0.014210556502435533, 0.023024626002393696, 0.0, 0.0, 0.08510060429095491, 0.15442202399960203, 0.0, 0.5391171356753139, 0.0, 0.44855253786204935, 0.048493606647816025, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22138370024934187, 0.0, 0.08746644969030876, 0.0, 0.0, 0.0, 0.1616519416528851, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.035084084514750494, 0.0, 0.11688517594508105, 0.2089391042079727, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05598779403983851, 0.014778823268485956, 0.0, 0.0, 0.0, 0.0, 0.057780108315529474, 0.0, 0.201721527067654, 0.0, 0.3341648455181398, 0.0, 0.0, 0.7601477683176978, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8032144106029788, 0.0, 0.18178069418497272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6936206835397086, 0.0, 0.36283080196789974, 0.4415514891805875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.041591186808888826, 0.3770004419200786, 0.0, 0.0, 0.0, 0.17252416859375463, 0.0, 0.0, 0.6527400000425327, 0.0, 0.3340575108846618, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7763472218770214, 0.0, 0.0, 0.0, 0.0, 0.8376247582699287, 0.0, 0.014460886244840168, 0.6266762938745922, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.46397397404593055, 0.0, 0.23926875754339813, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.46347904589364963, 0.05902942262309747, 0.0, 0.03877718275846287, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2950495413884076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06597380264866397, 1.0898197013430582, 0.0, 0.012501862454917015, 0.0, 0.0, 0.05385941027253928, 0.0, 0.3522343881391282, 0.0, 0.398164495085241, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004630463685504157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25876266416388005, 0.0, 0.0, 0.3103356146935319, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6074148238597709, 0.09425255656440318, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0032593418386170935, 0.0, 0.2297346969390008, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12055792386342427, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25324185315889564, 0.0, 0.05731281617489229, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21868854065637486, 0.0, 0.0, 0.13921477990069736, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013113097926933236, 0.11886277100269102, 0.0, 0.0, 0.0, 0.0, 0.0, 0.52224991418746, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4173873832595858, 0.08858709037486684, 0.0, 0.0, 0.0, 0.0, 0.15667266254329507, 0.0, 0.40730912125206437, 0.5164653515440787, 0.0, 0.2032159032369745, 0.40587691627032874, 0.0, 0.0, 0.0, 0.0340034080482809, 0.0, 0.0, 0.15427161497649788, 0.44428262792688017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014262992619462008, 0.38024333666677773, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1612065141710269, 0.0, 0.03925137009723705, 0.0, 0.0078366311533161, 0.0, 0.014460397168610369, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06667255880132161, 0.0, 0.14952781464402123, 0.0, 0.0, 0.0, 0.0, 0.2779234999305725, 0.0, 0.008746314891451676, 0.0, 0.0, 0.06465282563196756, 0.0, 0.2464235137262255, 0.006899580599861792, 0.278556260330773, 0.007445268802069765, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18845739307946788, 0.01641249348500929, 0.5317378389887557, 0.0, 0.0, 0.14988438489574235, 0.0, 0.010375471289624015, 1.3557213333649594, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18808146093167488, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37253241298417417, 0.0, 0.1111794727253734, 0.16963583085055012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10986305912788819, 0.0, 0.0, 0.11949895822624457, 0.0, 0.0, 0.15821760502340373, 0.0, 0.5523682427180556, 0.0, 0.4595776329669528, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33581328744582783, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24138228903328196, 0.0, 0.3691306800612154, 0.0, 0.0, 0.0, 0.0, 0.0, 1.232394871494877, 0.0, 0.0, 0.0, 0.5293521525003586, 0.0, 0.48349454590588176, 0.0, 0.0, 0.6981988327823653, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33600003819418794, 0.10493510524858034, 0.1002766625939797, 0.0, 0.4760848715940903, 0.30327936688968893, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15549653856407664, 0.053555423141422365, 0.33124643300151746, 0.1452133754699813, 0.29863037601205833, 0.5069679636197618, 0.0, 1.2650777504013886, 0.0, 0.0, 0.8010376870067246, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8336854027224526, 0.0, 0.0, 0.0, 0.0, 0.013135866634354858, 0.7693584995036987, 0.0, 0.0, 0.0, 0.1809386811488775, 0.18409266358857004, 0.0, 0.8209089663283375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6269731623222802, 0.0, 0.0, 0.0, 0.0, 0.8578109001691112, 0.13549697762155272, 0.8892025809188543, 0.0, 0.17753144998442447, 0.0, 0.3275865900116729, 0.37045604078207667, 0.0, 0.0, 0.45942273296787417, 0.0, 0.0, 0.1496601554416231, 0.0, 0.4896166566389182, 0.09296161452877622, 0.0, 0.15295686770265157, 0.0, 0.0, 0.0, 0.18412880463476564, 0.0, 0.19671721658454625, 0.0, 0.0, 0.15630345798009787, 0.0, 0.16866550691184853, 0.0, 0.3962565901298767, 0.0, 0.0, 0.009054605400028368, 0.0, 0.0, 0.0, 0.0, 0.6323717354885464, 0.0, 0.27367929286054155, 1.074929147630902, 0.1458067334724375, 0.22417700682510228, 0.0, 0.10951695376531846, 0.0, 0.007581110002282966, 0.24931277842808883, 0.0, 0.6883954376611309, 0.6336690200937254, 0.2666286675338324, 0.0, 0.0, 0.0, 0.0, 0.2876738228687025, 0.0, 0.0, 0.21522568832904387, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0438812752886428, 0.6525143726026617, 0.0, 0.0, 0.0, 0.0, 0.0, 1.1869853573228195, 0.11631516482088484, 0.0, 0.0, 0.0, 0.0, 0.06506946683877823, 0.0, 0.22717008671076838, 0.10242247240104177, 0.18900849588618046, 0.11442201255265518, 0.5664450740439114, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019408932182682588, 0.20321698947174538, 1.233551723482737, 0.3258945612095407, 0.0, 0.13004560449904765, 0.0, 0.5146863326888748, 0.0, 0.0, 0.26506893163091566, 0.6961290235591785, 0.7308179758735578, 0.019108835994024305, 0.0, 0.0, 0.12318551581994355, 0.0, 0.6633736020985646, 0.09733981959320877, 0.6085861256804133, 0.0, 0.3702803270823883, 0.0, 0.31915877170632656, 0.0, 0.0048098299833409025, 0.08330917183364514, 0.03767139221212642, 0.0, 0.0, 0.7595521482230864, 0.0, 0.40492576086384785, 0.9767048148851073, 0.0, 0.7118250358317957, 1.4759259446598505, 0.6177251878074594, 0.0, 0.0, 0.5578474164235181, 0.0, 0.0, 0.0, 0.8466209961003106, 0.0, 0.0, 0.0, 0.0, 0.9134452822413822, 0.0, 0.0, 1.1506817448958997, 0.0, 0.0, 0.5717156136201251, 0.0, 0.0, 0.0, 0.0, 0.22301492152906585, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13134314230830874, 0.20762859161099162, 0.0, 0.4768286902241477, 0.0, 0.3252201513410915, 0.5728390197892637, 0.07274257736444308, 0.0946619273240044, 0.0, 0.08735695093663028, 0.0, 0.006047124502145837, 0.0, 0.0, 0.03396077178506113, 0.056902676891892164, 0.05446213354503342, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03284486748482166, 0.0, 0.03394228891521373, 0.0, 0.0, 0.0, 0.15913459349815823, 0.0988686644243417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06403136572671528, 0.0008539041494970381, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4996043959310575, 0.03466871274471111, 0.0, 0.0, 0.0, 0.5916091668585121, 0.0, 0.0, 0.18522959478004286, 0.0, 0.0, 0.6748263739107161, 0.0, 0.0, 0.5617854630926186, 0.0, 0.7191769350887419, 0.22919875305609166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07421634147547619, 0.2082330820676049, 0.0, 0.33671550924577986, 0.0, 0.0, 0.8216886233224614, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9288637557541211, 0.0, 0.0, 1.2410393190221276, 0.024834505618206743, 0.6697889027661909, 0.39618825585335343, 0.37106318592637155, 0.321662062811288, 0.0, 0.8625204846142658, 0.4737807718760894, 0.8171062776340833, 0.16785258111865808, 1.931465974538034, 0.1820623956052182, 0.0, 0.40831479446929375, 0.0, 1.6290713896950666, 0.0, 0.0, 1.0235643660149811, 0.0, 0.0, 0.0, 0.0, 0.996334540430692, 0.0, 0.0, 0.44156504893800563, 0.289897027676234, 0.0, 0.5556259307084835, 0.0, 0.0, 0.5751570179061616, 0.0, 0.15465163640644544, 0.0, 0.0, 0.0, 0.0, 0.1740886831542383, 0.8206341762594166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8467869185435447, 0.05898504962060997, 0.03624481511042625, 0.6732560722166093, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.201694060644539, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4141926752107277, 0.14795154825402113, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21427949262749374, 0.46809092516056994, 0.0, 0.16828675961297507, 0.14194253863939424, 0.0, 0.46436829494386733, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13445486993921502, 0.019720618408469233, 0.2722006890254698, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06550905522135163, 0.04806777087710764, 0.0, 0.0, 0.0, 0.599761835300619, 0.0, 0.0, 0.5741431736534577, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17101973439245724, 0.0, 0.0, 0.0749875087919759, 0.0, 0.0, 0.28776812087676407, 0.8309016545414952, 0.0, 0.0, 0.0, 0.016199069272106216, 0.0, 0.0, 0.0, 0.0, 0.6545099569258361, 0.22702193109653576, 0.04569906480983916, 0.2833580027667076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11138860511768055, 0.062440751035711506, 0.5182620734767335, 0.0, 0.3481758497366363, 0.0, 0.4677994729760326, 0.6992526088901763, 0.01746909322126967, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4416823398234807, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08074443292335906, 0.005624454367769537, 0.4953841958649777, 0.06419759041251974, 0.0, 0.0, 0.0, 0.03378063280221251, 0.27793271609589054, 0.0, 0.0, 0.0, 0.013968102118739474, 0.0, 0.08424327814659714, 0.057402094804332435, 0.0, 0.0, 0.0333773112405404, 0.0, 0.0, 0.0, 0.020814415925578544, 0.050277550690626754, 0.07266714943835642, 0.05420980100927718, 0.1299217183065535, 0.0, 0.0, 0.3197221256849075, 0.0, 0.13800911480490768, 0.0, 0.0, 0.0, 0.0, 1.01268183989861, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7358814757212789, 0.30707785521832553, 0.5276131242565835, 0.0, 0.1743689886750783, 0.0, 0.25821885501181596, 0.5805464694386301, 0.7164948773890167, 0.0, 0.0, 0.0, 0.37021123955626156, 0.12525001349038106, 0.608077535649827, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0938347119534369, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17530069799945608, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6839828466860547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0036702000371577168, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.030804733252794536, 0.6085396823865649, 0.3664826840226463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.052310575793324585, 0.23438841535865984, 0.027151249553211434, 0.42665810243765545, 0.060892623345148826, 0.0, 0.0, 0.08445468462850356, 0.05478879427510317, 0.5501718854551394, 0.0, 0.11343808432438283, 0.0, 0.09259962343643532, 0.2732186452567139, 0.0, 0.42011946077495216, 0.2576342392674693, 0.0, 0.5090094423403475, 0.0, 0.0, 0.0, 0.0010952054350690443, 0.08086253983246877, 0.0, 0.0, 0.23985944027024297, 0.0, 0.09028722397131896, 0.0, 0.07674602267622682, 0.43900489287844074, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3006474050100551, 0.0, 0.0, 0.0709455536446502, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24893625570967048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2768131690025365, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0014853584531852684, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2394367041087018, 0.8202130158779322, 0.0, 0.0, 0.28871220094126887, 0.0, 0.0, 0.04358483751824937, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.052964062286445145, 0.0, 0.2958896434064324, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.695162332188878, 0.037163387811150035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0679762361139716, 0.0, 0.0, 0.24678121036961095, 0.0, 0.0, 0.2964710587865061, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0759598903032082, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07295585235760724, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0603439559536301, 0.0, 0.0, 0.0, 0.17528000283667142, 0.0, 0.12675745187805368, 0.328557971512483, 0.028687306018592266, 0.0, 0.0, 0.8516002254500735, 0.0, 0.4371253184691764, 0.13991121920676916, 0.0, 0.3475458448096685, 0.46894433815651543, 0.0, 0.0, 0.6068029321085694, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28015080493647854, 0.3107571390829928, 0.0, 0.0, 0.0, 0.0, 0.006563618369995558, 0.5906679530337167, 0.0, 0.0, 0.0, 0.6525869879262292, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00921986700494424, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04479484649111107, 0.0, 0.3151409942990884, 0.021951930030038837, 0.0, 0.2505595957731255, 0.0, 0.0, 0.0, 0.03191831493444459, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22403715744289487, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027940093628006875, 0.0, 0.0, 0.0, 0.26553429437558146, 0.0, 0.030379801994009328, 0.0, 1.223521302648351, 0.3114612281119979, 0.3934859798468234, 0.0, 0.3688885410084021, 0.0, 0.5211677391443227, 0.318605826264122, 0.11794887163110916, 0.0, 0.0, 0.4256936617495534, 0.0, 0.49172025370622324, 0.48430956473476544, 0.0, 1.3234474260067457, 0.28650182105087124, 0.006913853673799038, 0.28826288884375384, 0.3033256150127764, 0.0, 0.19416551553442815, 0.19479473776455275, 0.0, 0.2201952762677155, 0.0, 0.0, 0.34956927181897907, 0.0, 0.0, 0.0, 0.5335997378219544, 0.1831214497933391, 0.5101375543135913, 0.0, 0.7417718286797346, 0.0, 0.23966647500006097, 0.8693502490764229, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21247306643180602, 0.1378389271943791, 0.0, 0.0, 0.2853901797463675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20343586472727235, 0.29392458170687635, 0.011188169832657642, 0.0, 0.0, 0.28953305486076913, 0.0, 0.0, 0.07223789467137894, 0.0, 0.33051308113923733, 0.4540345661588786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9537370190824367, 0.0, 0.6089477015288718, 0.0, 0.07722630190963325, 0.0, 0.0, 1.0000222254545443, 0.8637206744494375, 0.0, 0.7751920658363071, 0.524298363554463, 0.0, 0.0, 0.01676078910810196, 0.0, 0.0, 0.07604278960261779, 0.0, 0.0, 0.0, 0.0, 0.40940877803475156, 0.0, 0.0, 0.0, 0.0, 0.09252661041066143, 0.44765043171933794, 0.0, 0.24055976047324873, 0.0, 0.15100552643910237, 0.35800810608226774]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    B_lvl_q = B_lvl_ptr[1]
    B_lvl_q_stop = B_lvl_ptr[1 + 1]
    if B_lvl_q < B_lvl_q_stop
        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
    else
        B_lvl_i_stop = 0
    end
    phase_stop = min(B_lvl.shape[2], B_lvl_i_stop)
    if phase_stop >= 1
        if B_lvl_tbl2[B_lvl_q] < 1
            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
        end
        while true
            B_lvl_i = B_lvl_tbl2[B_lvl_q]
            B_lvl_q_step = B_lvl_q
            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
            end
            if B_lvl_i < phase_stop
                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                val = Ct_lvl_2_val
                Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                B_lvl_tbl1_2 = B_lvl_tbl1
                B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                B_lvl_tbl2_2 = B_lvl_tbl2
                val_2 = B_lvl_val
                B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                A_lvl_ptr_2 = A_lvl_ptr
                A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                A_lvl_tbl1_2 = A_lvl_tbl1
                A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                A_lvl_tbl2_2 = A_lvl_tbl2
                A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                val_3 = A_lvl_val
                A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                Threads.@threads for i_9 = 1:Threads.nthreads()
                        phase_start_6 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_9), Threads.nthreads()))
                        phase_stop_7 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_9, Threads.nthreads()))
                        if phase_stop_7 >= phase_start_6
                            for i_12 = phase_start_6:phase_stop_7
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_12
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_8 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_8 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_8
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_9 = min(B_lvl_i_2, phase_stop_8, A_lvl_i)
                                        if A_lvl_i == phase_stop_9 && B_lvl_i_2 == phase_stop_9
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_10 = min(i_12, A_lvl_i_stop_2)
                                            if phase_stop_10 >= i_12
                                                if A_lvl_tbl1[A_lvl_q] < i_12
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_12, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_10
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_12 = min(A_lvl_i_2, phase_stop_10)
                                                        if A_lvl_i_2 == phase_stop_12
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_9
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_9
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_9 + 1
                                    end
                                end
                            end
                        end
                    end
                Ct_lvl_2_val = val
                B_lvl_tbl1 = B_lvl_tbl1_2
                B_lvl_tbl2 = B_lvl_tbl2_2
                B_lvl_val = val_2
                A_lvl_ptr = A_lvl_ptr_2
                A_lvl_tbl1 = A_lvl_tbl1_2
                A_lvl_tbl2 = A_lvl_tbl2_2
                A_lvl_val = val_3
                B_lvl_q = B_lvl_q_step
            else
                phase_stop_18 = min(B_lvl_i, phase_stop)
                if B_lvl_i == phase_stop_18
                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_18
                    val_4 = Ct_lvl_2_val
                    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                    B_lvl_tbl1_3 = B_lvl_tbl1
                    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                    B_lvl_tbl2_3 = B_lvl_tbl2
                    val_5 = B_lvl_val
                    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                    A_lvl_ptr_3 = A_lvl_ptr
                    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                    A_lvl_tbl1_3 = A_lvl_tbl1
                    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                    A_lvl_tbl2_3 = A_lvl_tbl2
                    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                    val_6 = A_lvl_val
                    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                    Threads.@threads for i_19 = 1:Threads.nthreads()
                            phase_start_21 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_19), Threads.nthreads()))
                            phase_stop_23 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_19, Threads.nthreads()))
                            if phase_stop_23 >= phase_start_21
                                for i_22 = phase_start_21:phase_stop_23
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_22
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_24 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_24 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_24
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_25 = min(B_lvl_i_2, A_lvl_i, phase_stop_24)
                                            if A_lvl_i == phase_stop_25 && B_lvl_i_2 == phase_stop_25
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_26 = min(i_22, A_lvl_i_stop_4)
                                                if phase_stop_26 >= i_22
                                                    if A_lvl_tbl1[A_lvl_q] < i_22
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_22, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_26
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_28 = min(A_lvl_i_4, phase_stop_26)
                                                            if A_lvl_i_4 == phase_stop_28
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_25
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_25
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_25 + 1
                                        end
                                    end
                                end
                            end
                        end
                    Ct_lvl_2_val = val_4
                    B_lvl_tbl1 = B_lvl_tbl1_3
                    B_lvl_tbl2 = B_lvl_tbl2_3
                    B_lvl_val = val_5
                    A_lvl_ptr = A_lvl_ptr_3
                    A_lvl_tbl1 = A_lvl_tbl1_3
                    A_lvl_tbl2 = A_lvl_tbl2_3
                    A_lvl_val = val_6
                    B_lvl_q = B_lvl_q_step
                end
                break
            end
        end
    end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(Ct_lvl_2_val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0, 0.0, 0.40798272794029244, 0.0, 0.05552236202954211, 0.0, 0.0, 0.0, 1.1508129348205518, 0.0, 0.19395356472348033, 0.2307481734593406, 0.0, 0.0, 0.0, 0.0, 0.7400687115958035, 0.0, 0.0, 0.4711198057610666, 0.0, 0.07015028050595504, 0.08663540769774136, 0.0, 0.0, 0.46359958748433167, 0.0, 0.035823254967012344, 0.0, 0.21605429332119105, 0.050389255833200065, 0.0, 0.0, 0.08982343514212215, 0.0, 0.11508828733565957, 0.4022461237733094, 0.0, 0.5232411380476881, 0.0, 0.17677960337865342, 0.6347058701128525, 0.0, 0.0, 0.0, 0.080760767105778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.530417945120141, 0.036947581649449654, 0.0, 0.42172014534542845, 0.0, 0.0, 0.0, 0.04046090790460827, 0.0, 0.0, 0.0, 0.0, 0.016730358390191713, 0.11216899138664496, 0.10090277285890621, 0.3770798811678483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06022016698365358, 0.0, 0.3347277070064448, 0.08319806447593037, 0.0, 0.0, 0.15734979027583865, 0.6034963214197537, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8559793301235343, 0.0, 0.0221818618958444, 0.0, 0.0, 0.0, 0.0, 0.6249640468245943, 0.0, 0.706457127780651, 0.8381987455683932, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05075027829145945, 0.0, 0.0, 0.28298860880763127, 0.0, 0.46775948880043733, 0.3276758676884306, 0.23162154119323575, 0.0, 0.0, 0.7097058534293814, 0.0, 0.01925480712564137, 0.6617464495997725, 0.5510119129346432, 0.9503030089786222, 0.19493828703065708, 0.4234771306495736, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11491839349038546, 0.0, 0.0, 0.0, 0.0, 0.0, 0.952344347120024, 0.31481029761912926, 0.0, 0.0, 0.3371532262514988, 0.0, 0.6461986748500718, 0.0, 0.20388394461232018, 0.6689077519036521, 0.008065708303756284, 0.026774581755143884, 0.0, 0.0, 0.0, 0.0, 0.20246692989511542, 0.0, 0.0, 0.0, 0.0, 0.6964472745009185, 0.0, 0.3001098015546565, 0.0, 0.002810485733837209, 0.0, 0.0, 0.2833439267406652, 0.0, 0.0, 0.0, 0.01365478232442864, 0.0, 0.07752931191170997, 0.005400497114468351, 0.01008472602077043, 0.7305457354717977, 0.0, 0.0, 0.3791767135274968, 0.009729638043940761, 0.14133243937397577, 0.0, 0.6050992861887972, 0.0, 0.0, 0.0, 0.0, 0.32748073574326575, 0.29782974023215064, 0.0, 0.0, 0.0, 0.0, 0.008516959572353846, 0.0, 0.0, 0.0, 0.5188617620854502, 0.0, 0.12960309538886133, 0.0, 0.2159060099855789, 0.0, 0.0, 0.0, 0.5916658876154497, 0.0, 0.08410324034674674, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7242114416300933, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43295426032070555, 0.1879551877902947, 0.09140698995628445, 0.0, 0.0, 0.0, 0.0, 0.053834983759932134, 0.014210556502435533, 0.023024626002393696, 0.0, 0.0, 0.08510060429095491, 0.15442202399960203, 0.0, 0.5391171356753139, 0.0, 0.44855253786204935, 0.048493606647816025, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22138370024934187, 0.0, 0.08746644969030876, 0.0, 0.0, 0.0, 0.1616519416528851, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.035084084514750494, 0.0, 0.11688517594508105, 0.2089391042079727, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05598779403983851, 0.014778823268485956, 0.0, 0.0, 0.0, 0.0, 0.057780108315529474, 0.0, 0.201721527067654, 0.0, 0.3341648455181398, 0.0, 0.0, 0.7601477683176978, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8032144106029788, 0.0, 0.18178069418497272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6936206835397086, 0.0, 0.36283080196789974, 0.4415514891805875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.041591186808888826, 0.3770004419200786, 0.0, 0.0, 0.0, 0.17252416859375463, 0.0, 0.0, 0.6527400000425327, 0.0, 0.3340575108846618, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7763472218770214, 0.0, 0.0, 0.0, 0.0, 0.8376247582699287, 0.0, 0.014460886244840168, 0.6266762938745922, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.46397397404593055, 0.0, 0.23926875754339813, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.46347904589364963, 0.05902942262309747, 0.0, 0.03877718275846287, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2950495413884076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06597380264866397, 1.0898197013430582, 0.0, 0.012501862454917015, 0.0, 0.0, 0.05385941027253928, 0.0, 0.3522343881391282, 0.0, 0.398164495085241, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004630463685504157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25876266416388005, 0.0, 0.0, 0.3103356146935319, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6074148238597709, 0.09425255656440318, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0032593418386170935, 0.0, 0.2297346969390008, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12055792386342427, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25324185315889564, 0.0, 0.05731281617489229, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21868854065637486, 0.0, 0.0, 0.13921477990069736, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013113097926933236, 0.11886277100269102, 0.0, 0.0, 0.0, 0.0, 0.0, 0.52224991418746, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4173873832595858, 0.08858709037486684, 0.0, 0.0, 0.0, 0.0, 0.15667266254329507, 0.0, 0.40730912125206437, 0.5164653515440787, 0.0, 0.2032159032369745, 0.40587691627032874, 0.0, 0.0, 0.0, 0.0340034080482809, 0.0, 0.0, 0.15427161497649788, 0.44428262792688017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014262992619462008, 0.38024333666677773, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1612065141710269, 0.0, 0.03925137009723705, 0.0, 0.0078366311533161, 0.0, 0.014460397168610369, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06667255880132161, 0.0, 0.14952781464402123, 0.0, 0.0, 0.0, 0.0, 0.2779234999305725, 0.0, 0.008746314891451676, 0.0, 0.0, 0.06465282563196756, 0.0, 0.2464235137262255, 0.006899580599861792, 0.278556260330773, 0.007445268802069765, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18845739307946788, 0.01641249348500929, 0.5317378389887557, 0.0, 0.0, 0.14988438489574235, 0.0, 0.010375471289624015, 1.3557213333649594, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18808146093167488, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37253241298417417, 0.0, 0.1111794727253734, 0.16963583085055012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10986305912788819, 0.0, 0.0, 0.11949895822624457, 0.0, 0.0, 0.15821760502340373, 0.0, 0.5523682427180556, 0.0, 0.4595776329669528, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33581328744582783, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24138228903328196, 0.0, 0.3691306800612154, 0.0, 0.0, 0.0, 0.0, 0.0, 1.232394871494877, 0.0, 0.0, 0.0, 0.5293521525003586, 0.0, 0.48349454590588176, 0.0, 0.0, 0.6981988327823653, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33600003819418794, 0.10493510524858034, 0.1002766625939797, 0.0, 0.4760848715940903, 0.30327936688968893, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15549653856407664, 0.053555423141422365, 0.33124643300151746, 0.1452133754699813, 0.29863037601205833, 0.5069679636197618, 0.0, 1.2650777504013886, 0.0, 0.0, 0.8010376870067246, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8336854027224526, 0.0, 0.0, 0.0, 0.0, 0.013135866634354858, 0.7693584995036987, 0.0, 0.0, 0.0, 0.1809386811488775, 0.18409266358857004, 0.0, 0.8209089663283375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6269731623222802, 0.0, 0.0, 0.0, 0.0, 0.8578109001691112, 0.13549697762155272, 0.8892025809188543, 0.0, 0.17753144998442447, 0.0, 0.3275865900116729, 0.37045604078207667, 0.0, 0.0, 0.45942273296787417, 0.0, 0.0, 0.1496601554416231, 0.0, 0.4896166566389182, 0.09296161452877622, 0.0, 0.15295686770265157, 0.0, 0.0, 0.0, 0.18412880463476564, 0.0, 0.19671721658454625, 0.0, 0.0, 0.15630345798009787, 0.0, 0.16866550691184853, 0.0, 0.3962565901298767, 0.0, 0.0, 0.009054605400028368, 0.0, 0.0, 0.0, 0.0, 0.6323717354885464, 0.0, 0.27367929286054155, 1.074929147630902, 0.1458067334724375, 0.22417700682510228, 0.0, 0.10951695376531846, 0.0, 0.007581110002282966, 0.24931277842808883, 0.0, 0.6883954376611309, 0.6336690200937254, 0.2666286675338324, 0.0, 0.0, 0.0, 0.0, 0.2876738228687025, 0.0, 0.0, 0.21522568832904387, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0438812752886428, 0.6525143726026617, 0.0, 0.0, 0.0, 0.0, 0.0, 1.1869853573228195, 0.11631516482088484, 0.0, 0.0, 0.0, 0.0, 0.06506946683877823, 0.0, 0.22717008671076838, 0.10242247240104177, 0.18900849588618046, 0.11442201255265518, 0.5664450740439114, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019408932182682588, 0.20321698947174538, 1.233551723482737, 0.3258945612095407, 0.0, 0.13004560449904765, 0.0, 0.5146863326888748, 0.0, 0.0, 0.26506893163091566, 0.6961290235591785, 0.7308179758735578, 0.019108835994024305, 0.0, 0.0, 0.12318551581994355, 0.0, 0.6633736020985646, 0.09733981959320877, 0.6085861256804133, 0.0, 0.3702803270823883, 0.0, 0.31915877170632656, 0.0, 0.0048098299833409025, 0.08330917183364514, 0.03767139221212642, 0.0, 0.0, 0.7595521482230864, 0.0, 0.40492576086384785, 0.9767048148851073, 0.0, 0.7118250358317957, 1.4759259446598505, 0.6177251878074594, 0.0, 0.0, 0.5578474164235181, 0.0, 0.0, 0.0, 0.8466209961003106, 0.0, 0.0, 0.0, 0.0, 0.9134452822413822, 0.0, 0.0, 1.1506817448958997, 0.0, 0.0, 0.5717156136201251, 0.0, 0.0, 0.0, 0.0, 0.22301492152906585, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13134314230830874, 0.20762859161099162, 0.0, 0.4768286902241477, 0.0, 0.3252201513410915, 0.5728390197892637, 0.07274257736444308, 0.0946619273240044, 0.0, 0.08735695093663028, 0.0, 0.006047124502145837, 0.0, 0.0, 0.03396077178506113, 0.056902676891892164, 0.05446213354503342, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03284486748482166, 0.0, 0.03394228891521373, 0.0, 0.0, 0.0, 0.15913459349815823, 0.0988686644243417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06403136572671528, 0.0008539041494970381, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4996043959310575, 0.03466871274471111, 0.0, 0.0, 0.0, 0.5916091668585121, 0.0, 0.0, 0.18522959478004286, 0.0, 0.0, 0.6748263739107161, 0.0, 0.0, 0.5617854630926186, 0.0, 0.7191769350887419, 0.22919875305609166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07421634147547619, 0.2082330820676049, 0.0, 0.33671550924577986, 0.0, 0.0, 0.8216886233224614, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9288637557541211, 0.0, 0.0, 1.2410393190221276, 0.024834505618206743, 0.6697889027661909, 0.39618825585335343, 0.37106318592637155, 0.321662062811288, 0.0, 0.8625204846142658, 0.4737807718760894, 0.8171062776340833, 0.16785258111865808, 1.931465974538034, 0.1820623956052182, 0.0, 0.40831479446929375, 0.0, 1.6290713896950666, 0.0, 0.0, 1.0235643660149811, 0.0, 0.0, 0.0, 0.0, 0.996334540430692, 0.0, 0.0, 0.44156504893800563, 0.289897027676234, 0.0, 0.5556259307084835, 0.0, 0.0, 0.5751570179061616, 0.0, 0.15465163640644544, 0.0, 0.0, 0.0, 0.0, 0.1740886831542383, 0.8206341762594166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8467869185435447, 0.05898504962060997, 0.03624481511042625, 0.6732560722166093, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.201694060644539, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4141926752107277, 0.14795154825402113, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21427949262749374, 0.46809092516056994, 0.0, 0.16828675961297507, 0.14194253863939424, 0.0, 0.46436829494386733, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13445486993921502, 0.019720618408469233, 0.2722006890254698, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06550905522135163, 0.04806777087710764, 0.0, 0.0, 0.0, 0.599761835300619, 0.0, 0.0, 0.5741431736534577, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17101973439245724, 0.0, 0.0, 0.0749875087919759, 0.0, 0.0, 0.28776812087676407, 0.8309016545414952, 0.0, 0.0, 0.0, 0.016199069272106216, 0.0, 0.0, 0.0, 0.0, 0.6545099569258361, 0.22702193109653576, 0.04569906480983916, 0.2833580027667076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11138860511768055, 0.062440751035711506, 0.5182620734767335, 0.0, 0.3481758497366363, 0.0, 0.4677994729760326, 0.6992526088901763, 0.01746909322126967, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4416823398234807, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08074443292335906, 0.005624454367769537, 0.4953841958649777, 0.06419759041251974, 0.0, 0.0, 0.0, 0.03378063280221251, 0.27793271609589054, 0.0, 0.0, 0.0, 0.013968102118739474, 0.0, 0.08424327814659714, 0.057402094804332435, 0.0, 0.0, 0.0333773112405404, 0.0, 0.0, 0.0, 0.020814415925578544, 0.050277550690626754, 0.07266714943835642, 0.05420980100927718, 0.1299217183065535, 0.0, 0.0, 0.3197221256849075, 0.0, 0.13800911480490768, 0.0, 0.0, 0.0, 0.0, 1.01268183989861, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7358814757212789, 0.30707785521832553, 0.5276131242565835, 0.0, 0.1743689886750783, 0.0, 0.25821885501181596, 0.5805464694386301, 0.7164948773890167, 0.0, 0.0, 0.0, 0.37021123955626156, 0.12525001349038106, 0.608077535649827, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0938347119534369, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17530069799945608, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6839828466860547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0036702000371577168, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.030804733252794536, 0.6085396823865649, 0.3664826840226463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.052310575793324585, 0.23438841535865984, 0.027151249553211434, 0.42665810243765545, 0.060892623345148826, 0.0, 0.0, 0.08445468462850356, 0.05478879427510317, 0.5501718854551394, 0.0, 0.11343808432438283, 0.0, 0.09259962343643532, 0.2732186452567139, 0.0, 0.42011946077495216, 0.2576342392674693, 0.0, 0.5090094423403475, 0.0, 0.0, 0.0, 0.0010952054350690443, 0.08086253983246877, 0.0, 0.0, 0.23985944027024297, 0.0, 0.09028722397131896, 0.0, 0.07674602267622682, 0.43900489287844074, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3006474050100551, 0.0, 0.0, 0.0709455536446502, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24893625570967048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2768131690025365, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0014853584531852684, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2394367041087018, 0.8202130158779322, 0.0, 0.0, 0.28871220094126887, 0.0, 0.0, 0.04358483751824937, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.052964062286445145, 0.0, 0.2958896434064324, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.695162332188878, 0.037163387811150035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0679762361139716, 0.0, 0.0, 0.24678121036961095, 0.0, 0.0, 0.2964710587865061, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0759598903032082, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07295585235760724, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0603439559536301, 0.0, 0.0, 0.0, 0.17528000283667142, 0.0, 0.12675745187805368, 0.328557971512483, 0.028687306018592266, 0.0, 0.0, 0.8516002254500735, 0.0, 0.4371253184691764, 0.13991121920676916, 0.0, 0.3475458448096685, 0.46894433815651543, 0.0, 0.0, 0.6068029321085694, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28015080493647854, 0.3107571390829928, 0.0, 0.0, 0.0, 0.0, 0.006563618369995558, 0.5906679530337167, 0.0, 0.0, 0.0, 0.6525869879262292, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00921986700494424, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04479484649111107, 0.0, 0.3151409942990884, 0.021951930030038837, 0.0, 0.2505595957731255, 0.0, 0.0, 0.0, 0.03191831493444459, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22403715744289487, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027940093628006875, 0.0, 0.0, 0.0, 0.26553429437558146, 0.0, 0.030379801994009328, 0.0, 1.223521302648351, 0.3114612281119979, 0.3934859798468234, 0.0, 0.3688885410084021, 0.0, 0.5211677391443227, 0.318605826264122, 0.11794887163110916, 0.0, 0.0, 0.4256936617495534, 0.0, 0.49172025370622324, 0.48430956473476544, 0.0, 1.3234474260067457, 0.28650182105087124, 0.006913853673799038, 0.28826288884375384, 0.3033256150127764, 0.0, 0.19416551553442815, 0.19479473776455275, 0.0, 0.2201952762677155, 0.0, 0.0, 0.34956927181897907, 0.0, 0.0, 0.0, 0.5335997378219544, 0.1831214497933391, 0.5101375543135913, 0.0, 0.7417718286797346, 0.0, 0.23966647500006097, 0.8693502490764229, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21247306643180602, 0.1378389271943791, 0.0, 0.0, 0.2853901797463675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20343586472727235, 0.29392458170687635, 0.011188169832657642, 0.0, 0.0, 0.28953305486076913, 0.0, 0.0, 0.07223789467137894, 0.0, 0.33051308113923733, 0.4540345661588786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9537370190824367, 0.0, 0.6089477015288718, 0.0, 0.07722630190963325, 0.0, 0.0, 1.0000222254545443, 0.8637206744494375, 0.0, 0.7751920658363071, 0.524298363554463, 0.0, 0.0, 0.01676078910810196, 0.0, 0.0, 0.07604278960261779, 0.0, 0.0, 0.0, 0.0, 0.40940877803475156, 0.0, 0.0, 0.0, 0.0, 0.09252661041066143, 0.44765043171933794, 0.0, 0.24055976047324873, 0.0, 0.15100552643910237, 0.35800810608226774]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = parallel(_)
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            B_lvl_q = B_lvl_ptr[1]
            B_lvl_q_stop = B_lvl_ptr[1 + 1]
            if B_lvl_q < B_lvl_q_stop
                B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
            else
                B_lvl_i_stop = 0
            end
            phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                if B_lvl_tbl2[B_lvl_q] < phase_start_2
                    B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                end
                while true
                    B_lvl_i = B_lvl_tbl2[B_lvl_q]
                    B_lvl_q_step = B_lvl_q
                    if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                        B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    if B_lvl_i < phase_stop_2
                        Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                        val_4 = Ct_lvl_2_val
                        Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                        A_lvl_ptr_3 = A_lvl_ptr
                        A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                        A_lvl_tbl1_3 = A_lvl_tbl1
                        A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                        A_lvl_tbl2_3 = A_lvl_tbl2
                        A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                        val_5 = A_lvl_val
                        A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                        B_lvl_ptr_3 = B_lvl_ptr
                        B_lvl_tbl1_3 = B_lvl_tbl1
                        B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                        B_lvl_tbl2_3 = B_lvl_tbl2
                        val_6 = B_lvl_val
                        B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                        Threads.@threads for i_10 = 1:Threads.nthreads()
                                phase_start_7 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_10), Threads.nthreads()))
                                phase_stop_8 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_10, Threads.nthreads()))
                                if phase_stop_8 >= phase_start_7
                                    for i_13 = phase_start_7:phase_stop_8
                                        Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_13
                                        A_lvl_q = A_lvl_ptr[1]
                                        A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                        if A_lvl_q < A_lvl_q_stop
                                            A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                        else
                                            A_lvl_i_stop = 0
                                        end
                                        B_lvl_q_3 = B_lvl_q
                                        if B_lvl_q < B_lvl_q_step
                                            B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                        else
                                            B_lvl_i_stop_3 = 0
                                        end
                                        phase_stop_9 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                        if phase_stop_9 >= 1
                                            k = 1
                                            if A_lvl_tbl2[A_lvl_q] < 1
                                                A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            if B_lvl_tbl1[B_lvl_q] < 1
                                                B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                            end
                                            while k <= phase_stop_9
                                                A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                                A_lvl_q_step = A_lvl_q
                                                if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                    A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                                end
                                                B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                                phase_stop_10 = min(B_lvl_i_3, phase_stop_9, A_lvl_i)
                                                if A_lvl_i == phase_stop_10 && B_lvl_i_3 == phase_stop_10
                                                    B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                                    A_lvl_q_2 = A_lvl_q
                                                    if A_lvl_q < A_lvl_q_step
                                                        A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                    else
                                                        A_lvl_i_stop_2 = 0
                                                    end
                                                    phase_stop_11 = min(i_13, A_lvl_i_stop_2)
                                                    if phase_stop_11 >= i_13
                                                        if A_lvl_tbl1[A_lvl_q] < i_13
                                                            A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_13, A_lvl_q, A_lvl_q_step - 1)
                                                        end
                                                        while true
                                                            A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                            if A_lvl_i_2 < phase_stop_11
                                                                A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                                Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                                A_lvl_q_2 += 1
                                                            else
                                                                phase_stop_13 = min(A_lvl_i_2, phase_stop_11)
                                                                if A_lvl_i_2 == phase_stop_13
                                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                                    Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                                    A_lvl_q_2 += 1
                                                                end
                                                                break
                                                            end
                                                        end
                                                    end
                                                    A_lvl_q = A_lvl_q_step
                                                    B_lvl_q_3 += 1
                                                elseif B_lvl_i_3 == phase_stop_10
                                                    B_lvl_q_3 += 1
                                                elseif A_lvl_i == phase_stop_10
                                                    A_lvl_q = A_lvl_q_step
                                                end
                                                k = phase_stop_10 + 1
                                            end
                                        end
                                    end
                                end
                            end
                        Ct_lvl_2_val = val_4
                        A_lvl_ptr = A_lvl_ptr_3
                        A_lvl_tbl1 = A_lvl_tbl1_3
                        A_lvl_tbl2 = A_lvl_tbl2_3
                        A_lvl_val = val_5
                        B_lvl_ptr = B_lvl_ptr_3
                        B_lvl_tbl1 = B_lvl_tbl1_3
                        B_lvl_tbl2 = B_lvl_tbl2_3
                        B_lvl_val = val_6
                        B_lvl_q = B_lvl_q_step
                    else
                        phase_stop_19 = min(B_lvl_i, phase_stop_2)
                        if B_lvl_i == phase_stop_19
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_19
                            val_7 = Ct_lvl_2_val
                            Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                            A_lvl_ptr_4 = A_lvl_ptr
                            A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                            A_lvl_tbl1_4 = A_lvl_tbl1
                            A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                            A_lvl_tbl2_4 = A_lvl_tbl2
                            A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                            val_8 = A_lvl_val
                            A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                            B_lvl_ptr_4 = B_lvl_ptr
                            B_lvl_tbl1_4 = B_lvl_tbl1
                            B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                            B_lvl_tbl2_4 = B_lvl_tbl2
                            val_9 = B_lvl_val
                            B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                            Threads.@threads for i_20 = 1:Threads.nthreads()
                                    phase_start_22 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_20), Threads.nthreads()))
                                    phase_stop_24 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_20, Threads.nthreads()))
                                    if phase_stop_24 >= phase_start_22
                                        for i_23 = phase_start_22:phase_stop_24
                                            Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_23
                                            A_lvl_q = A_lvl_ptr[1]
                                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                            if A_lvl_q < A_lvl_q_stop
                                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                            else
                                                A_lvl_i_stop = 0
                                            end
                                            B_lvl_q_3 = B_lvl_q
                                            if B_lvl_q < B_lvl_q_step
                                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                            else
                                                B_lvl_i_stop_3 = 0
                                            end
                                            phase_stop_25 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                            if phase_stop_25 >= 1
                                                k = 1
                                                if A_lvl_tbl2[A_lvl_q] < 1
                                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                                end
                                                if B_lvl_tbl1[B_lvl_q] < 1
                                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                                end
                                                while k <= phase_stop_25
                                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                                    A_lvl_q_step = A_lvl_q
                                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                                    end
                                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                                    phase_stop_26 = min(B_lvl_i_3, A_lvl_i, phase_stop_25)
                                                    if A_lvl_i == phase_stop_26 && B_lvl_i_3 == phase_stop_26
                                                        B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                                        A_lvl_q_4 = A_lvl_q
                                                        if A_lvl_q < A_lvl_q_step
                                                            A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                        else
                                                            A_lvl_i_stop_4 = 0
                                                        end
                                                        phase_stop_27 = min(i_23, A_lvl_i_stop_4)
                                                        if phase_stop_27 >= i_23
                                                            if A_lvl_tbl1[A_lvl_q] < i_23
                                                                A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_23, A_lvl_q, A_lvl_q_step - 1)
                                                            end
                                                            while true
                                                                A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                                if A_lvl_i_4 < phase_stop_27
                                                                    A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                    Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                    A_lvl_q_4 += 1
                                                                else
                                                                    phase_stop_29 = min(A_lvl_i_4, phase_stop_27)
                                                                    if A_lvl_i_4 == phase_stop_29
                                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                        A_lvl_q_4 += 1
                                                                    end
                                                                    break
                                                                end
                                                            end
                                                        end
                                                        A_lvl_q = A_lvl_q_step
                                                        B_lvl_q_3 += 1
                                                    elseif B_lvl_i_3 == phase_stop_26
                                                        B_lvl_q_3 += 1
                                                    elseif A_lvl_i == phase_stop_26
                                                        A_lvl_q = A_lvl_q_step
                                                    end
                                                    k = phase_stop_26 + 1
                                                end
                                            end
                                        end
                                    end
                                end
                            Ct_lvl_2_val = val_7
                            A_lvl_ptr = A_lvl_ptr_4
                            A_lvl_tbl1 = A_lvl_tbl1_4
                            A_lvl_tbl2 = A_lvl_tbl2_4
                            A_lvl_val = val_8
                            B_lvl_ptr = B_lvl_ptr_4
                            B_lvl_tbl1 = B_lvl_tbl1_4
                            B_lvl_tbl2 = B_lvl_tbl2_4
                            B_lvl_val = val_9
                            B_lvl_q = B_lvl_q_step
                        end
                        break
                    end
                end
            end
        end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for j = parallel(_)
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0, 0.0, 0.40798272794029244, 0.0, 0.05552236202954211, 0.0, 0.0, 0.0, 1.1508129348205518, 0.0, 0.19395356472348033, 0.2307481734593406, 0.0, 0.0, 0.0, 0.0, 0.7400687115958035, 0.0, 0.0, 0.4711198057610666, 0.0, 0.07015028050595504, 0.08663540769774136, 0.0, 0.0, 0.46359958748433167, 0.0, 0.035823254967012344, 0.0, 0.21605429332119105, 0.050389255833200065, 0.0, 0.0, 0.08982343514212215, 0.0, 0.11508828733565957, 0.4022461237733094, 0.0, 0.5232411380476881, 0.0, 0.17677960337865342, 0.6347058701128525, 0.0, 0.0, 0.0, 0.080760767105778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.530417945120141, 0.036947581649449654, 0.0, 0.42172014534542845, 0.0, 0.0, 0.0, 0.04046090790460827, 0.0, 0.0, 0.0, 0.0, 0.016730358390191713, 0.11216899138664496, 0.10090277285890621, 0.3770798811678483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06022016698365358, 0.0, 0.3347277070064448, 0.08319806447593037, 0.0, 0.0, 0.15734979027583865, 0.6034963214197537, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8559793301235343, 0.0, 0.0221818618958444, 0.0, 0.0, 0.0, 0.0, 0.6249640468245943, 0.0, 0.706457127780651, 0.8381987455683932, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05075027829145945, 0.0, 0.0, 0.28298860880763127, 0.0, 0.46775948880043733, 0.3276758676884306, 0.23162154119323575, 0.0, 0.0, 0.7097058534293814, 0.0, 0.01925480712564137, 0.6617464495997725, 0.5510119129346432, 0.9503030089786222, 0.19493828703065708, 0.4234771306495736, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11491839349038546, 0.0, 0.0, 0.0, 0.0, 0.0, 0.952344347120024, 0.31481029761912926, 0.0, 0.0, 0.3371532262514988, 0.0, 0.6461986748500718, 0.0, 0.20388394461232018, 0.6689077519036521, 0.008065708303756284, 0.026774581755143884, 0.0, 0.0, 0.0, 0.0, 0.20246692989511542, 0.0, 0.0, 0.0, 0.0, 0.6964472745009185, 0.0, 0.3001098015546565, 0.0, 0.002810485733837209, 0.0, 0.0, 0.2833439267406652, 0.0, 0.0, 0.0, 0.01365478232442864, 0.0, 0.07752931191170997, 0.005400497114468351, 0.01008472602077043, 0.7305457354717977, 0.0, 0.0, 0.3791767135274968, 0.009729638043940761, 0.14133243937397577, 0.0, 0.6050992861887972, 0.0, 0.0, 0.0, 0.0, 0.32748073574326575, 0.29782974023215064, 0.0, 0.0, 0.0, 0.0, 0.008516959572353846, 0.0, 0.0, 0.0, 0.5188617620854502, 0.0, 0.12960309538886133, 0.0, 0.2159060099855789, 0.0, 0.0, 0.0, 0.5916658876154497, 0.0, 0.08410324034674674, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7242114416300933, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43295426032070555, 0.1879551877902947, 0.09140698995628445, 0.0, 0.0, 0.0, 0.0, 0.053834983759932134, 0.014210556502435533, 0.023024626002393696, 0.0, 0.0, 0.08510060429095491, 0.15442202399960203, 0.0, 0.5391171356753139, 0.0, 0.44855253786204935, 0.048493606647816025, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22138370024934187, 0.0, 0.08746644969030876, 0.0, 0.0, 0.0, 0.1616519416528851, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.035084084514750494, 0.0, 0.11688517594508105, 0.2089391042079727, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05598779403983851, 0.014778823268485956, 0.0, 0.0, 0.0, 0.0, 0.057780108315529474, 0.0, 0.201721527067654, 0.0, 0.3341648455181398, 0.0, 0.0, 0.7601477683176978, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8032144106029788, 0.0, 0.18178069418497272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6936206835397086, 0.0, 0.36283080196789974, 0.4415514891805875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.041591186808888826, 0.3770004419200786, 0.0, 0.0, 0.0, 0.17252416859375463, 0.0, 0.0, 0.6527400000425327, 0.0, 0.3340575108846618, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7763472218770214, 0.0, 0.0, 0.0, 0.0, 0.8376247582699287, 0.0, 0.014460886244840168, 0.6266762938745922, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.46397397404593055, 0.0, 0.23926875754339813, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.46347904589364963, 0.05902942262309747, 0.0, 0.03877718275846287, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2950495413884076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06597380264866397, 1.0898197013430582, 0.0, 0.012501862454917015, 0.0, 0.0, 0.05385941027253928, 0.0, 0.3522343881391282, 0.0, 0.398164495085241, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004630463685504157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25876266416388005, 0.0, 0.0, 0.3103356146935319, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6074148238597709, 0.09425255656440318, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0032593418386170935, 0.0, 0.2297346969390008, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12055792386342427, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25324185315889564, 0.0, 0.05731281617489229, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21868854065637486, 0.0, 0.0, 0.13921477990069736, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013113097926933236, 0.11886277100269102, 0.0, 0.0, 0.0, 0.0, 0.0, 0.52224991418746, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4173873832595858, 0.08858709037486684, 0.0, 0.0, 0.0, 0.0, 0.15667266254329507, 0.0, 0.40730912125206437, 0.5164653515440787, 0.0, 0.2032159032369745, 0.40587691627032874, 0.0, 0.0, 0.0, 0.0340034080482809, 0.0, 0.0, 0.15427161497649788, 0.44428262792688017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014262992619462008, 0.38024333666677773, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1612065141710269, 0.0, 0.03925137009723705, 0.0, 0.0078366311533161, 0.0, 0.014460397168610369, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06667255880132161, 0.0, 0.14952781464402123, 0.0, 0.0, 0.0, 0.0, 0.2779234999305725, 0.0, 0.008746314891451676, 0.0, 0.0, 0.06465282563196756, 0.0, 0.2464235137262255, 0.006899580599861792, 0.278556260330773, 0.007445268802069765, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18845739307946788, 0.01641249348500929, 0.5317378389887557, 0.0, 0.0, 0.14988438489574235, 0.0, 0.010375471289624015, 1.3557213333649594, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18808146093167488, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37253241298417417, 0.0, 0.1111794727253734, 0.16963583085055012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10986305912788819, 0.0, 0.0, 0.11949895822624457, 0.0, 0.0, 0.15821760502340373, 0.0, 0.5523682427180556, 0.0, 0.4595776329669528, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33581328744582783, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24138228903328196, 0.0, 0.3691306800612154, 0.0, 0.0, 0.0, 0.0, 0.0, 1.232394871494877, 0.0, 0.0, 0.0, 0.5293521525003586, 0.0, 0.48349454590588176, 0.0, 0.0, 0.6981988327823653, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33600003819418794, 0.10493510524858034, 0.1002766625939797, 0.0, 0.4760848715940903, 0.30327936688968893, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15549653856407664, 0.053555423141422365, 0.33124643300151746, 0.1452133754699813, 0.29863037601205833, 0.5069679636197618, 0.0, 1.2650777504013886, 0.0, 0.0, 0.8010376870067246, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8336854027224526, 0.0, 0.0, 0.0, 0.0, 0.013135866634354858, 0.7693584995036987, 0.0, 0.0, 0.0, 0.1809386811488775, 0.18409266358857004, 0.0, 0.8209089663283375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6269731623222802, 0.0, 0.0, 0.0, 0.0, 0.8578109001691112, 0.13549697762155272, 0.8892025809188543, 0.0, 0.17753144998442447, 0.0, 0.3275865900116729, 0.37045604078207667, 0.0, 0.0, 0.45942273296787417, 0.0, 0.0, 0.1496601554416231, 0.0, 0.4896166566389182, 0.09296161452877622, 0.0, 0.15295686770265157, 0.0, 0.0, 0.0, 0.18412880463476564, 0.0, 0.19671721658454625, 0.0, 0.0, 0.15630345798009787, 0.0, 0.16866550691184853, 0.0, 0.3962565901298767, 0.0, 0.0, 0.009054605400028368, 0.0, 0.0, 0.0, 0.0, 0.6323717354885464, 0.0, 0.27367929286054155, 1.074929147630902, 0.1458067334724375, 0.22417700682510228, 0.0, 0.10951695376531846, 0.0, 0.007581110002282966, 0.24931277842808883, 0.0, 0.6883954376611309, 0.6336690200937254, 0.2666286675338324, 0.0, 0.0, 0.0, 0.0, 0.2876738228687025, 0.0, 0.0, 0.21522568832904387, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0438812752886428, 0.6525143726026617, 0.0, 0.0, 0.0, 0.0, 0.0, 1.1869853573228195, 0.11631516482088484, 0.0, 0.0, 0.0, 0.0, 0.06506946683877823, 0.0, 0.22717008671076838, 0.10242247240104177, 0.18900849588618046, 0.11442201255265518, 0.5664450740439114, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019408932182682588, 0.20321698947174538, 1.233551723482737, 0.3258945612095407, 0.0, 0.13004560449904765, 0.0, 0.5146863326888748, 0.0, 0.0, 0.26506893163091566, 0.6961290235591785, 0.7308179758735578, 0.019108835994024305, 0.0, 0.0, 0.12318551581994355, 0.0, 0.6633736020985646, 0.09733981959320877, 0.6085861256804133, 0.0, 0.3702803270823883, 0.0, 0.31915877170632656, 0.0, 0.0048098299833409025, 0.08330917183364514, 0.03767139221212642, 0.0, 0.0, 0.7595521482230864, 0.0, 0.40492576086384785, 0.9767048148851073, 0.0, 0.7118250358317957, 1.4759259446598505, 0.6177251878074594, 0.0, 0.0, 0.5578474164235181, 0.0, 0.0, 0.0, 0.8466209961003106, 0.0, 0.0, 0.0, 0.0, 0.9134452822413822, 0.0, 0.0, 1.1506817448958997, 0.0, 0.0, 0.5717156136201251, 0.0, 0.0, 0.0, 0.0, 0.22301492152906585, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13134314230830874, 0.20762859161099162, 0.0, 0.4768286902241477, 0.0, 0.3252201513410915, 0.5728390197892637, 0.07274257736444308, 0.0946619273240044, 0.0, 0.08735695093663028, 0.0, 0.006047124502145837, 0.0, 0.0, 0.03396077178506113, 0.056902676891892164, 0.05446213354503342, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03284486748482166, 0.0, 0.03394228891521373, 0.0, 0.0, 0.0, 0.15913459349815823, 0.0988686644243417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06403136572671528, 0.0008539041494970381, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4996043959310575, 0.03466871274471111, 0.0, 0.0, 0.0, 0.5916091668585121, 0.0, 0.0, 0.18522959478004286, 0.0, 0.0, 0.6748263739107161, 0.0, 0.0, 0.5617854630926186, 0.0, 0.7191769350887419, 0.22919875305609166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07421634147547619, 0.2082330820676049, 0.0, 0.33671550924577986, 0.0, 0.0, 0.8216886233224614, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9288637557541211, 0.0, 0.0, 1.2410393190221276, 0.024834505618206743, 0.6697889027661909, 0.39618825585335343, 0.37106318592637155, 0.321662062811288, 0.0, 0.8625204846142658, 0.4737807718760894, 0.8171062776340833, 0.16785258111865808, 1.931465974538034, 0.1820623956052182, 0.0, 0.40831479446929375, 0.0, 1.6290713896950666, 0.0, 0.0, 1.0235643660149811, 0.0, 0.0, 0.0, 0.0, 0.996334540430692, 0.0, 0.0, 0.44156504893800563, 0.289897027676234, 0.0, 0.5556259307084835, 0.0, 0.0, 0.5751570179061616, 0.0, 0.15465163640644544, 0.0, 0.0, 0.0, 0.0, 0.1740886831542383, 0.8206341762594166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8467869185435447, 0.05898504962060997, 0.03624481511042625, 0.6732560722166093, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.201694060644539, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4141926752107277, 0.14795154825402113, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21427949262749374, 0.46809092516056994, 0.0, 0.16828675961297507, 0.14194253863939424, 0.0, 0.46436829494386733, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13445486993921502, 0.019720618408469233, 0.2722006890254698, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06550905522135163, 0.04806777087710764, 0.0, 0.0, 0.0, 0.599761835300619, 0.0, 0.0, 0.5741431736534577, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17101973439245724, 0.0, 0.0, 0.0749875087919759, 0.0, 0.0, 0.28776812087676407, 0.8309016545414952, 0.0, 0.0, 0.0, 0.016199069272106216, 0.0, 0.0, 0.0, 0.0, 0.6545099569258361, 0.22702193109653576, 0.04569906480983916, 0.2833580027667076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11138860511768055, 0.062440751035711506, 0.5182620734767335, 0.0, 0.3481758497366363, 0.0, 0.4677994729760326, 0.6992526088901763, 0.01746909322126967, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4416823398234807, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08074443292335906, 0.005624454367769537, 0.4953841958649777, 0.06419759041251974, 0.0, 0.0, 0.0, 0.03378063280221251, 0.27793271609589054, 0.0, 0.0, 0.0, 0.013968102118739474, 0.0, 0.08424327814659714, 0.057402094804332435, 0.0, 0.0, 0.0333773112405404, 0.0, 0.0, 0.0, 0.020814415925578544, 0.050277550690626754, 0.07266714943835642, 0.05420980100927718, 0.1299217183065535, 0.0, 0.0, 0.3197221256849075, 0.0, 0.13800911480490768, 0.0, 0.0, 0.0, 0.0, 1.01268183989861, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7358814757212789, 0.30707785521832553, 0.5276131242565835, 0.0, 0.1743689886750783, 0.0, 0.25821885501181596, 0.5805464694386301, 0.7164948773890167, 0.0, 0.0, 0.0, 0.37021123955626156, 0.12525001349038106, 0.608077535649827, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0938347119534369, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17530069799945608, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6839828466860547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0036702000371577168, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.030804733252794536, 0.6085396823865649, 0.3664826840226463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.052310575793324585, 0.23438841535865984, 0.027151249553211434, 0.42665810243765545, 0.060892623345148826, 0.0, 0.0, 0.08445468462850356, 0.05478879427510317, 0.5501718854551394, 0.0, 0.11343808432438283, 0.0, 0.09259962343643532, 0.2732186452567139, 0.0, 0.42011946077495216, 0.2576342392674693, 0.0, 0.5090094423403475, 0.0, 0.0, 0.0, 0.0010952054350690443, 0.08086253983246877, 0.0, 0.0, 0.23985944027024297, 0.0, 0.09028722397131896, 0.0, 0.07674602267622682, 0.43900489287844074, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3006474050100551, 0.0, 0.0, 0.0709455536446502, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24893625570967048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2768131690025365, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0014853584531852684, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2394367041087018, 0.8202130158779322, 0.0, 0.0, 0.28871220094126887, 0.0, 0.0, 0.04358483751824937, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.052964062286445145, 0.0, 0.2958896434064324, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.695162332188878, 0.037163387811150035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0679762361139716, 0.0, 0.0, 0.24678121036961095, 0.0, 0.0, 0.2964710587865061, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0759598903032082, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07295585235760724, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0603439559536301, 0.0, 0.0, 0.0, 0.17528000283667142, 0.0, 0.12675745187805368, 0.328557971512483, 0.028687306018592266, 0.0, 0.0, 0.8516002254500735, 0.0, 0.4371253184691764, 0.13991121920676916, 0.0, 0.3475458448096685, 0.46894433815651543, 0.0, 0.0, 0.6068029321085694, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28015080493647854, 0.3107571390829928, 0.0, 0.0, 0.0, 0.0, 0.006563618369995558, 0.5906679530337167, 0.0, 0.0, 0.0, 0.6525869879262292, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00921986700494424, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04479484649111107, 0.0, 0.3151409942990884, 0.021951930030038837, 0.0, 0.2505595957731255, 0.0, 0.0, 0.0, 0.03191831493444459, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22403715744289487, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027940093628006875, 0.0, 0.0, 0.0, 0.26553429437558146, 0.0, 0.030379801994009328, 0.0, 1.223521302648351, 0.3114612281119979, 0.3934859798468234, 0.0, 0.3688885410084021, 0.0, 0.5211677391443227, 0.318605826264122, 0.11794887163110916, 0.0, 0.0, 0.4256936617495534, 0.0, 0.49172025370622324, 0.48430956473476544, 0.0, 1.3234474260067457, 0.28650182105087124, 0.006913853673799038, 0.28826288884375384, 0.3033256150127764, 0.0, 0.19416551553442815, 0.19479473776455275, 0.0, 0.2201952762677155, 0.0, 0.0, 0.34956927181897907, 0.0, 0.0, 0.0, 0.5335997378219544, 0.1831214497933391, 0.5101375543135913, 0.0, 0.7417718286797346, 0.0, 0.23966647500006097, 0.8693502490764229, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21247306643180602, 0.1378389271943791, 0.0, 0.0, 0.2853901797463675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20343586472727235, 0.29392458170687635, 0.011188169832657642, 0.0, 0.0, 0.28953305486076913, 0.0, 0.0, 0.07223789467137894, 0.0, 0.33051308113923733, 0.4540345661588786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9537370190824367, 0.0, 0.6089477015288718, 0.0, 0.07722630190963325, 0.0, 0.0, 1.0000222254545443, 0.8637206744494375, 0.0, 0.7751920658363071, 0.524298363554463, 0.0, 0.0, 0.01676078910810196, 0.0, 0.0, 0.07604278960261779, 0.0, 0.0, 0.0, 0.0, 0.40940877803475156, 0.0, 0.0, 0.0, 0.0, 0.09252661041066143, 0.44765043171933794, 0.0, 0.24055976047324873, 0.0, 0.15100552643910237, 0.35800810608226774]), 42), 42)),)

