julia> @finch begin
        CR .= 0
        for i = _
            for j = _
                for k = _
                    CR[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(CR = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0, 0.0, 0.1619393094464903, 0.0, 0.0, 0.0, 0.0, 0.11373910752308043, 0.0, 0.32533429668145175, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5405512925681031, 0.0, 0.0, 0.5409817971066755, 0.6726532010052559, 0.0, 0.0, 0.0, 0.04498001780375231, 0.0, 0.7446527460595233, 0.0, 0.0, 0.0, 0.0648053115514859, 0.0, 0.20542126000594346, 0.0, 0.2489342378383824, 0.0, 0.0, 0.2446702787341799, 0.0, 0.0, 0.4075132486743765, 0.372693911014397, 0.0, 0.03990868671702989, 0.32865292999993995, 0.0, 0.0, 0.0, 0.037570880212435515, 0.0, 0.0, 0.2585440952996821, 0.0, 0.0, 0.5429032184680616, 0.0, 0.0, 0.0, 0.7310331654112876, 1.6974310666890076, 0.13434284643730737, 0.0, 0.15388431962676646, 0.0, 0.018799628425153446, 0.026448056346556916, 0.020943024178172204, 0.08671614507629592, 0.19272805440861934, 0.07015243227726202, 0.0, 0.1267665206913561, 0.1014023431949165, 0.494724963965631, 0.0, 0.6317423070706343, 0.0, 0.4351905463586081, 0.019451367524410516, 0.11392026534019284, 0.0, 0.7674183972620773, 0.0, 0.0, 0.009728790198518009, 0.0, 0.0, 0.03627546054320975, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0140288190005624, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4229718175600567, 0.0752980218883111, 0.013070459322559402, 0.0, 0.4793455258509991, 0.0, 0.0, 0.0, 0.0, 0.08811633730455204, 0.08467555208376226, 0.0, 0.0, 1.4824413586764027, 0.27442978968992093, 0.0, 0.0028104905637611507, 0.1480443868451215, 0.6744061467564826, 0.043998508948338316, 0.8697701045107844, 0.8469834954000914, 0.0, 0.02425613445444166, 0.0, 0.0, 0.0, 0.9679232213808633, 0.6774894982888419, 0.35310753741950685, 0.0, 0.2841919624705342, 0.0, 0.0, 0.7349602036514661, 0.0, 0.2186035505612502, 0.04416466424315345, 0.0, 0.0, 0.25133653271826356, 0.11160837067772013, 0.0, 0.14048007630402903, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.035048636482553235, 0.038653696504083, 0.10014641420794519, 0.0, 0.0837599459565703, 0.10942537757376403, 0.0, 0.0, 0.0, 0.08204362658289192, 0.0, 0.0, 0.011591138384550418, 0.0, 0.038004261053570694, 0.0066189468466051875, 0.06306237088337384, 0.0, 0.0, 0.03121664575636558, 0.0, 0.0, 0.02910081181829685, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08506408857544866, 0.0, 0.0, 0.0, 0.7712870388819855, 0.2285263236705729, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6966121466049721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6540220401149287, 0.0, 0.0, 0.2336670223432043, 0.0, 0.0, 0.005339227289024509, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4930665741229495, 0.0, 0.029559863815954657, 0.0, 0.0, 0.10760363562562023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15061269543615943, 0.6107211710602042, 0.0, 0.0, 0.733057860494869, 0.057362855000467054, 0.17650444882735677, 0.0, 0.0, 0.014677698725170102, 0.0, 0.17821555119599314, 0.0025555492556658746, 0.14808219427948163, 0.0, 0.11979690859781277, 0.0, 0.0, 0.13110894332781436, 0.04449679654819995, 0.0, 0.1133902344080122, 0.0, 0.0, 0.0017495044850376442, 0.013900993801975209, 0.03729472110086185, 0.19173933242455474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1709634942172136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13051636625786148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4357713541613571, 0.0, 0.0, 0.0, 0.0, 0.07073267416682687, 0.0, 0.0, 0.0, 0.0, 0.06443354393704691, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28238918313491895, 0.0, 0.0, 0.0, 0.28219752132848275, 0.0, 0.0, 0.0, 0.3045252869011713, 0.0, 0.0, 0.019083741024338748, 0.0, 0.06257051306836219, 0.7837995208867943, 0.05061887468267648, 0.0, 0.0, 0.05139533007879884, 0.0, 0.0, 0.04791180451081585, 0.0, 0.5281077501137272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7571506519409951, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.026654196669850944, 0.0, 0.0, 0.0, 0.0, 0.23843473688023176, 0.0, 0.0, 0.6051386294410329, 0.0, 0.0, 0.0, 0.0, 0.04194764240896927, 0.13628497764809444, 0.0, 0.0, 0.0, 0.0, 0.06043639304510901, 0.04344269031549197, 0.0, 0.7296328391677986, 0.6312346675367022, 0.0, 0.0, 0.22817557354519388, 0.0, 0.05073205279897938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1594527848545218, 0.1381227206057285, 0.0, 0.0, 0.0, 0.0, 0.4217049712314433, 0.0, 0.0, 0.31027130374464373, 0.0, 0.0, 0.08118863628632204, 0.8304208532182848, 0.983137215831116, 0.0, 0.24897028906526258, 0.14034260442091134, 0.0, 0.691731307052974, 0.7500207108166317, 0.833859795824118, 0.0, 0.05053828036951065, 0.8139843305766208, 0.0, 0.0, 1.3856822465181273, 1.0343729450047436, 0.0, 0.0, 1.09811525132565, 0.0, 0.0, 0.005748580563865145, 0.0, 0.0, 0.8634025361175751, 0.0, 0.14616020357939458, 0.7514377111323272, 0.0, 0.0, 0.1732108049991939, 0.0, 0.0, 0.0, 0.1879649588465491, 0.0, 0.0, 0.0, 0.00817965091901661, 0.0, 0.0, 0.0, 0.5779500058438248, 0.0, 0.0, 0.0, 0.005249336837927155, 0.0, 0.0, 0.0, 0.3064350538312218, 0.0, 0.001890320177282059, 0.45976772975798436, 0.40475012946470096, 0.3442380471075082, 0.017185108496187197, 0.288035784498987, 0.0, 0.0, 0.2827566402954316, 0.0, 0.0, 0.2635916697723135, 0.0, 0.0, 0.0, 0.3588181656747819, 0.0, 0.0, 0.0, 0.17570163169455902, 0.0, 0.2076868163288207, 0.0, 0.0, 0.0, 0.5097341557191223, 0.14587016095283192, 0.0, 0.4172405362928458, 0.0, 0.13951021052730417, 0.0, 0.6033798708626249, 0.0, 0.0, 0.0, 0.0, 0.4512081190066747, 1.1915177253624374, 0.0, 0.53626862172996, 0.26267190873847124, 0.024946529615964698, 0.630282563668714, 0.0, 0.7072072219187049, 0.2557442098535857, 0.0, 0.0, 0.0, 0.027439980552710016, 0.0, 0.2634523244708942, 0.750190928083991, 0.31925762502388877, 0.0, 0.0, 0.0, 0.17667054707398389, 1.689395274403252, 0.0, 0.5386556936239477, 0.21507940395745773, 0.30698899212442304, 0.11255638742238075, 0.12042909174810443, 0.1983506487023661, 0.0, 0.2156156778918424, 0.0, 0.6179686549930762, 0.0, 0.0, 0.0, 0.011162873807723371, 0.003307472295366656, 0.5417377310206691, 0.1038404263557644, 0.0, 0.0, 0.0, 0.010082100558503525, 0.5759566406752274, 0.5373731999014957, 0.24497595431253502, 0.3628321361534925, 0.4271139218413104, 0.0888079037091352, 0.013607634508588288, 0.5254304747208627, 0.0, 0.0, 0.0033818738704913948, 0.0, 0.3894178985059293, 7.727488918331624e-5, 0.4719056234120719, 0.0, 0.0, 0.7548965169480935, 0.0, 0.0, 0.0, 0.0, 0.04295271297559638, 0.3048341885265102, 0.0, 0.16015654700904233, 0.0, 0.03255288417829475, 0.0, 0.0, 0.0, 0.0, 0.16019347860889097, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3426629556463706, 0.0, 0.33591507158653605, 0.0, 0.0, 0.04025140372585128, 0.0, 0.5756230062735465, 0.0, 0.0, 0.0, 0.0, 0.0017523850398583757, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32421546356710995, 0.0, 0.011282618188498251, 0.6937716630434633, 0.2529924781123921, 0.2596235239889639, 0.0, 0.043120476743740394, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2793513283874615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7775441119273626, 0.14515455310675088, 0.0, 0.13510821830370037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44716225485327055, 0.0, 0.0, 0.0, 0.8291790553392753, 0.0, 0.0, 0.26334889102262177, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15479667093058366, 0.0, 0.0, 0.0, 0.36993623982653, 0.0, 0.0, 0.0, 0.0, 0.3623559014178924, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029233403628440607, 0.1427335193249464, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051251957334763225, 0.691166524717783, 0.29938181148714105, 0.5349464228115411, 0.06157899241792075, 0.0, 0.0, 0.2102727260204982, 0.0, 0.60145477594229, 0.0, 0.0, 0.03062034252430008, 0.1635016794080344, 0.7915679370358879, 0.0, 0.07317711878131836, 0.9809574403621903, 0.0, 0.0, 0.44926990137304107, 0.21608537407152922, 0.0, 0.05777727499408966, 0.039929965496851884, 0.0, 0.29915326656993935, 0.31099294960838475, 0.13414679708282617, 0.10949528327933863, 0.0, 0.10753728810972799, 0.0, 0.3797681313373244, 0.5019126906002389, 0.8077076191871431, 0.02575673641821252, 0.0, 0.0, 0.0, 0.0, 0.08748647908145589, 0.0, 0.0, 0.2775216739406583, 0.0, 0.0, 0.0, 0.0, 0.19491911889837882, 0.0, 0.626470449065492, 0.0, 0.1610321350235478, 0.0, 0.6250193931691284, 0.18518836301307043, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5645059221744597, 0.0, 0.30319370972913895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5299926460898494, 0.0, 0.0, 0.18935417444624433, 0.0, 0.3520383786660897, 0.004326690884129795, 0.4266082560323472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04753484047689839, 0.0, 0.0, 0.0, 0.2507515224933452, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011126595947621928, 0.0, 0.0, 0.0, 0.0, 0.07883374294223121, 0.0, 0.0, 0.06158319631443835, 0.0, 0.0, 0.0, 0.0, 0.18599923723367912, 0.0, 0.0, 0.0, 0.0, 0.07225540391146344, 0.0, 0.17222213577613948, 0.0, 0.07109258970520838, 0.30622054435699597, 0.0, 0.0, 0.021600783197372013, 0.0, 0.0, 0.0, 0.0, 0.08747962563792125, 0.0, 0.0, 0.6661127453751955, 0.20906059268184987, 0.0, 0.14050781836020196, 0.0, 0.0, 0.38775347026983786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016520556870922822, 0.08066242485362363, 0.0, 0.0, 0.008507877750707929, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19963971549104562, 0.0, 0.0, 0.0, 0.0, 0.1517009343251733, 0.5312750255062002, 0.0, 0.0, 0.1147421120346578, 0.0, 0.0, 0.0, 0.0, 0.35500190337758014, 0.0, 0.045597713545061126, 0.0, 0.5209586802357347, 0.021862434298379425, 0.0, 0.0, 0.0, 1.0378291015579089, 0.0, 0.0, 0.0, 0.08585205631703301, 0.0483968201096062, 0.0, 0.025471357792783925, 0.0, 0.22080976260076224, 1.166729214929465, 0.0, 0.677323308302816, 0.18535083381370177, 0.0, 0.0, 0.8704056171514745, 0.0, 0.8935663748182991, 0.015577429284835683, 0.13855228421849528, 0.0, 0.0, 0.1744139155099896, 0.15132491229634354, 0.0, 0.02513332482807071, 0.0, 0.17525145412541498, 0.0, 0.0, 0.0, 0.0, 0.02694350993020697, 0.162823516985682, 0.0, 0.0, 0.6457319525792288, 0.07238426196350153, 0.05745965468198706, 0.0, 0.0, 0.46532195926191927, 0.08460519940877058, 0.22064747420300995, 0.07874956387307873, 0.02873453791303352, 0.04102153493571428, 0.0, 0.0, 0.0, 0.0, 0.49780157894039, 0.0, 0.0, 0.301057479311315, 0.0, 0.0, 0.0016911663416444849, 0.0, 0.2606342751929628, 0.0, 0.0, 0.0, 0.4832976838898257, 0.2663912296088851, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16074512474220395, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0418820657562929, 0.0, 0.04820286684177523, 0.0, 0.1580443847885599, 0.0, 0.1278562139832909, 0.0, 0.0, 0.12981743196566292, 0.0, 0.0, 0.12101853247948517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13204408757174144, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024933507022188058, 0.0, 0.0, 0.0, 0.22607531711845172, 0.06698435016959209, 0.0, 0.0, 0.38137808720681626, 0.04452538198512276, 0.13682477248336336, 0.20418703285947667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19170326048233127, 0.0, 0.0, 0.0684911628398921, 0.0, 0.0, 0.0015650042612973304, 0.0, 0.0, 0.0, 0.0, 0.3663518735291969, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27889642218469224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09171777647431381, 0.0, 0.031206336760942092, 0.04657190125204274, 0.0, 0.21168995340334643, 0.0, 0.0, 0.2715554536912592, 0.0, 0.0, 0.0476752915188213, 0.0, 0.0, 0.0, 0.0, 0.36410628949886475, 0.021118814521733104, 0.0, 0.25524059151149175, 0.22081888489064375, 0.0, 0.0008303397905156581, 0.0, 0.0, 0.0, 0.0, 0.07229346662213895, 0.0, 0.0, 0.820696989198259, 0.541933064129384, 0.0, 0.6976372073851678, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6559268620540256, 0.0, 0.0, 0.00593765430305966, 0.0, 0.03489392321475427, 0.14772313877299487, 0.20501704987333305, 0.04206924698360076, 0.15444861072003166, 0.0, 0.8226903093195739, 0.0, 0.15641473972332146, 0.790869654262485, 0.6842130952365706, 0.03633117370294634, 0.0, 0.18980663716791113, 0.0, 0.0, 0.038558748063994376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.40339029653363123, 0.0, 0.0, 0.0, 0.0, 0.43960663038458647, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08136147927839638, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16190486570077617, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2992216626110986, 0.0, 0.0, 0.0, 0.17882109545054858, 0.0, 0.0796930212837386, 0.0, 0.620119218817229, 0.06539128749913568, 0.09575080072237417, 0.0, 0.0, 0.0, 0.31047674992819635, 0.0, 0.0, 0.3258789812741783, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03198183278340688, 0.0, 0.0, 0.05159425685765124, 0.22600144774467618, 0.0, 0.17025710615993656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27035127710875223, 0.6595462305837041, 0.0, 0.0, 0.0, 0.5146584726996519, 0.0, 0.4337121439657544, 0.0, 0.0, 0.006091807521893999, 0.0, 0.0, 0.0, 0.0, 0.012888743397998564, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.336652909816104, 0.0, 0.0, 0.33692102610329455, 0.41892538326879675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4637663755690806, 0.004049689433712162, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2537974154183676, 0.0, 0.0, 0.0, 0.01941920682276314, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2625647477298457, 0.0, 0.0, 0.0, 0.28333914754514417, 0.0, 0.009497623447163565, 0.0, 0.0, 0.015321911880061054, 0.7292697770823527, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4913667474115824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8188140725936274, 0.23538609489199455, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004251660745273512, 0.2915534718195699, 0.8234990979680312, 0.15784851698528854, 0.31856705993041073, 0.0, 0.0, 0.0, 0.0, 0.8168645454274115, 0.2887084121601819, 0.0, 0.0, 0.0, 0.020685054965967595, 0.784321469374656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5719438351419199, 0.0, 0.0, 0.0, 0.0, 0.23810583942791758, 0.0, 0.0, 0.13190421809771516, 0.2355353163505223, 0.21414939137204708, 0.0, 0.13824109301205073, 0.0, 0.0, 0.4059138329547745, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3839879400284894, 0.7642606292584734, 0.17337082964447778, 0.43085204172578645, 0.5071846921436626, 0.0, 0.0, 0.0, 0.0, 0.0064224562854441995, 0.45947466920879537, 0.0, 0.048013668076727356, 0.008826989297459209, 0.0, 0.10254269883954115, 0.0, 0.449628221574638, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3130138916400048, 0.0, 0.0, 0.0, 0.0, 0.2198473045188841, 0.0, 0.7854658854954149, 0.0, 0.0, 0.0, 0.27482511606730614, 0.0, 0.0, 0.0, 0.10051459327881847, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03619595572786272, 0.2919384825550201, 0.0, 0.27020767480793234, 0.32906141180848986, 0.4014761358710767, 0.0, 0.0, 0.22194819820968317, 0.0, 0.397060529897469, 0.20690476484626705, 0.4811671410392035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10602724763101144, 0.0, 0.9180290974697203, 0.32581440887276836, 0.11505868376784396, 0.5289668394733107, 0.0, 0.5268986674479775, 0.2483018891728648, 0.0, 0.0, 0.014289568621041568, 0.0, 0.0, 0.0, 0.0, 1.010481071564644, 0.06469540823739603, 0.19880648111582097, 0.580377081863938, 0.3701240890884847, 0.00033976241032705203, 0.5120222312956857, 0.54263963710411, 0.24775903677393285, 0.13070835967347635, 0.9915175816548458, 0.2899273698579516, 0.0, 0.0, 1.1703372022756595, 0.02329003991510475, 0.11859776061629382, 0.69977713771051, 0.6054053013517017, 0.707408250006164, 0.0, 0.2196427786747048, 0.5323095043727097, 0.027197936547729912, 0.0, 0.0, 0.6744130710240275, 0.0, 0.0, 0.009472629890788111, 0.0, 0.0, 0.0, 0.0, 0.11454144616825819, 0.0, 0.0, 0.0, 1.3982199856641506, 0.2538753491205552, 0.20096391172061562, 0.0, 0.0, 0.04239072289345119, 0.0, 0.773883065549572, 0.0, 0.0, 0.41373255777296347, 0.0, 0.0, 0.0, 0.0, 0.7328658795072667, 0.0, 0.0, 0.8143261048934798, 0.0, 0.0, 0.005931475071506779, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5479468542821576, 0.0, 0.32433512795769415, 0.7968405650512658, 0.0, 0.0, 0.2823749754870304, 0.0, 0.08888138290002096, 0.0, 0.0, 0.0, 0.17925820791427197, 0.2000876096746871, 0.0, 0.0, 0.40815111826084144, 0.16385977895936904, 0.06986036185280928, 0.0, 0.0, 0.0, 0.0, 0.05375441115395932, 0.0, 0.1762464226118615, 0.0, 0.3587964347388992, 0.0, 0.8015475518237312, 0.175514484083294, 0.0, 0.0, 0.13495628742382593, 0.0, 0.3105505922199702, 0.7248528773530782, 0.0, 0.18705292898123585, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1748071953278316, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7043431257504623, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13971141189132677, 0.0, 0.0, 0.0, 0.0, 0.0, 0.031464801155366616, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16845401890236414, 0.14573633601571054, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13998624971109908, 0.0, 0.07426203609200491, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2049851695927226, 0.0, 0.0, 0.17338949919280155, 0.0, 0.0, 0.18945472552918335, 0.0, 0.0, 0.0, 0.0, 0.0, 0.036320374032721404, 0.15648907735824633, 0.0, 0.05859334953381536, 0.0, 0.0, 0.0, 0.0, 0.5224897007715829, 0.0, 0.0, 0.0, 0.0, 0.08480845151311803, 0.0, 0.0, 0.0, 0.0, 0.07725579657733653, 0.0, 0.14447765770967222, 0.0, 0.0, 0.0, 0.15678431253622274, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.255601945951803, 0.0, 0.0, 0.2867970808516741, 0.3376079838842544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.29929546323980694, 0.0, 0.0, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = ((ex.bodies[1]).bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    result = nothing
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = (Finch).moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = (Finch).moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            phase_start_2 = max(1, 1 + fld(A_lvl.shape[1] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                for i_7 = phase_start_2:phase_stop_2
                    B_lvl_q = B_lvl_ptr[1]
                    B_lvl_q_stop = B_lvl_ptr[1 + 1]
                    if B_lvl_q < B_lvl_q_stop
                        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                    else
                        B_lvl_i_stop = 0
                    end
                    phase_stop_3 = min(B_lvl.shape[2], B_lvl_i_stop)
                    if phase_stop_3 >= 1
                        if B_lvl_tbl2[B_lvl_q] < 1
                            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        while true
                            B_lvl_i = B_lvl_tbl2[B_lvl_q]
                            B_lvl_q_step = B_lvl_q
                            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                            end
                            if B_lvl_i < phase_stop_3
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_5 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_5 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_5
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_6 = min(B_lvl_i_2, phase_stop_5, A_lvl_i)
                                        if A_lvl_i == phase_stop_6 && B_lvl_i_2 == phase_stop_6
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_7 = min(i_7, A_lvl_i_stop_2)
                                            if phase_stop_7 >= i_7
                                                if A_lvl_tbl1[A_lvl_q] < i_7
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_7
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_9 = min(A_lvl_i_2, phase_stop_7)
                                                        if A_lvl_i_2 == phase_stop_9
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_6
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_6
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_6 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            else
                                phase_stop_14 = min(B_lvl_i, phase_stop_3)
                                if B_lvl_i == phase_stop_14
                                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_14
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_15 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_15 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_15
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_16 = min(B_lvl_i_2, A_lvl_i, phase_stop_15)
                                            if A_lvl_i == phase_stop_16 && B_lvl_i_2 == phase_stop_16
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_17 = min(i_7, A_lvl_i_stop_4)
                                                if phase_stop_17 >= i_7
                                                    if A_lvl_tbl1[A_lvl_q] < i_7
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_17
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_19 = min(A_lvl_i_4, phase_stop_17)
                                                            if A_lvl_i_4 == phase_stop_19
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_16
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_16
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_16 + 1
                                        end
                                    end
                                    B_lvl_q = B_lvl_q_step
                                end
                                break
                            end
                        end
                    end
                end
            end
        end
    resize!(val, A_lvl.shape[1] * B_lvl.shape[2])
    result = (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
    result
end
julia> @finch begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0, 0.0, 0.1619393094464903, 0.0, 0.0, 0.0, 0.0, 0.11373910752308043, 0.0, 0.32533429668145175, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5405512925681031, 0.0, 0.0, 0.5409817971066755, 0.6726532010052559, 0.0, 0.0, 0.0, 0.04498001780375231, 0.0, 0.7446527460595233, 0.0, 0.0, 0.0, 0.0648053115514859, 0.0, 0.20542126000594346, 0.0, 0.2489342378383824, 0.0, 0.0, 0.2446702787341799, 0.0, 0.0, 0.4075132486743765, 0.372693911014397, 0.0, 0.03990868671702989, 0.32865292999993995, 0.0, 0.0, 0.0, 0.037570880212435515, 0.0, 0.0, 0.2585440952996821, 0.0, 0.0, 0.5429032184680616, 0.0, 0.0, 0.0, 0.7310331654112876, 1.6974310666890076, 0.13434284643730737, 0.0, 0.15388431962676646, 0.0, 0.018799628425153446, 0.026448056346556916, 0.020943024178172204, 0.08671614507629592, 0.19272805440861934, 0.07015243227726202, 0.0, 0.1267665206913561, 0.1014023431949165, 0.494724963965631, 0.0, 0.6317423070706343, 0.0, 0.4351905463586081, 0.019451367524410516, 0.11392026534019284, 0.0, 0.7674183972620773, 0.0, 0.0, 0.009728790198518009, 0.0, 0.0, 0.03627546054320975, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0140288190005624, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4229718175600567, 0.0752980218883111, 0.013070459322559402, 0.0, 0.4793455258509991, 0.0, 0.0, 0.0, 0.0, 0.08811633730455204, 0.08467555208376226, 0.0, 0.0, 1.4824413586764027, 0.27442978968992093, 0.0, 0.0028104905637611507, 0.1480443868451215, 0.6744061467564826, 0.043998508948338316, 0.8697701045107844, 0.8469834954000914, 0.0, 0.02425613445444166, 0.0, 0.0, 0.0, 0.9679232213808633, 0.6774894982888419, 0.35310753741950685, 0.0, 0.2841919624705342, 0.0, 0.0, 0.7349602036514661, 0.0, 0.2186035505612502, 0.04416466424315345, 0.0, 0.0, 0.25133653271826356, 0.11160837067772013, 0.0, 0.14048007630402903, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.035048636482553235, 0.038653696504083, 0.10014641420794519, 0.0, 0.0837599459565703, 0.10942537757376403, 0.0, 0.0, 0.0, 0.08204362658289192, 0.0, 0.0, 0.011591138384550418, 0.0, 0.038004261053570694, 0.0066189468466051875, 0.06306237088337384, 0.0, 0.0, 0.03121664575636558, 0.0, 0.0, 0.02910081181829685, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08506408857544866, 0.0, 0.0, 0.0, 0.7712870388819855, 0.2285263236705729, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6966121466049721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6540220401149287, 0.0, 0.0, 0.2336670223432043, 0.0, 0.0, 0.005339227289024509, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4930665741229495, 0.0, 0.029559863815954657, 0.0, 0.0, 0.10760363562562023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15061269543615943, 0.6107211710602042, 0.0, 0.0, 0.733057860494869, 0.057362855000467054, 0.17650444882735677, 0.0, 0.0, 0.014677698725170102, 0.0, 0.17821555119599314, 0.0025555492556658746, 0.14808219427948163, 0.0, 0.11979690859781277, 0.0, 0.0, 0.13110894332781436, 0.04449679654819995, 0.0, 0.1133902344080122, 0.0, 0.0, 0.0017495044850376442, 0.013900993801975209, 0.03729472110086185, 0.19173933242455474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1709634942172136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13051636625786148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4357713541613571, 0.0, 0.0, 0.0, 0.0, 0.07073267416682687, 0.0, 0.0, 0.0, 0.0, 0.06443354393704691, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28238918313491895, 0.0, 0.0, 0.0, 0.28219752132848275, 0.0, 0.0, 0.0, 0.3045252869011713, 0.0, 0.0, 0.019083741024338748, 0.0, 0.06257051306836219, 0.7837995208867943, 0.05061887468267648, 0.0, 0.0, 0.05139533007879884, 0.0, 0.0, 0.04791180451081585, 0.0, 0.5281077501137272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7571506519409951, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.026654196669850944, 0.0, 0.0, 0.0, 0.0, 0.23843473688023176, 0.0, 0.0, 0.6051386294410329, 0.0, 0.0, 0.0, 0.0, 0.04194764240896927, 0.13628497764809444, 0.0, 0.0, 0.0, 0.0, 0.06043639304510901, 0.04344269031549197, 0.0, 0.7296328391677986, 0.6312346675367022, 0.0, 0.0, 0.22817557354519388, 0.0, 0.05073205279897938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1594527848545218, 0.1381227206057285, 0.0, 0.0, 0.0, 0.0, 0.4217049712314433, 0.0, 0.0, 0.31027130374464373, 0.0, 0.0, 0.08118863628632204, 0.8304208532182848, 0.983137215831116, 0.0, 0.24897028906526258, 0.14034260442091134, 0.0, 0.691731307052974, 0.7500207108166317, 0.833859795824118, 0.0, 0.05053828036951065, 0.8139843305766208, 0.0, 0.0, 1.3856822465181273, 1.0343729450047436, 0.0, 0.0, 1.09811525132565, 0.0, 0.0, 0.005748580563865145, 0.0, 0.0, 0.8634025361175751, 0.0, 0.14616020357939458, 0.7514377111323272, 0.0, 0.0, 0.1732108049991939, 0.0, 0.0, 0.0, 0.1879649588465491, 0.0, 0.0, 0.0, 0.00817965091901661, 0.0, 0.0, 0.0, 0.5779500058438248, 0.0, 0.0, 0.0, 0.005249336837927155, 0.0, 0.0, 0.0, 0.3064350538312218, 0.0, 0.001890320177282059, 0.45976772975798436, 0.40475012946470096, 0.3442380471075082, 0.017185108496187197, 0.288035784498987, 0.0, 0.0, 0.2827566402954316, 0.0, 0.0, 0.2635916697723135, 0.0, 0.0, 0.0, 0.3588181656747819, 0.0, 0.0, 0.0, 0.17570163169455902, 0.0, 0.2076868163288207, 0.0, 0.0, 0.0, 0.5097341557191223, 0.14587016095283192, 0.0, 0.4172405362928458, 0.0, 0.13951021052730417, 0.0, 0.6033798708626249, 0.0, 0.0, 0.0, 0.0, 0.4512081190066747, 1.1915177253624374, 0.0, 0.53626862172996, 0.26267190873847124, 0.024946529615964698, 0.630282563668714, 0.0, 0.7072072219187049, 0.2557442098535857, 0.0, 0.0, 0.0, 0.027439980552710016, 0.0, 0.2634523244708942, 0.750190928083991, 0.31925762502388877, 0.0, 0.0, 0.0, 0.17667054707398389, 1.689395274403252, 0.0, 0.5386556936239477, 0.21507940395745773, 0.30698899212442304, 0.11255638742238075, 0.12042909174810443, 0.1983506487023661, 0.0, 0.2156156778918424, 0.0, 0.6179686549930762, 0.0, 0.0, 0.0, 0.011162873807723371, 0.003307472295366656, 0.5417377310206691, 0.1038404263557644, 0.0, 0.0, 0.0, 0.010082100558503525, 0.5759566406752274, 0.5373731999014957, 0.24497595431253502, 0.3628321361534925, 0.4271139218413104, 0.0888079037091352, 0.013607634508588288, 0.5254304747208627, 0.0, 0.0, 0.0033818738704913948, 0.0, 0.3894178985059293, 7.727488918331624e-5, 0.4719056234120719, 0.0, 0.0, 0.7548965169480935, 0.0, 0.0, 0.0, 0.0, 0.04295271297559638, 0.3048341885265102, 0.0, 0.16015654700904233, 0.0, 0.03255288417829475, 0.0, 0.0, 0.0, 0.0, 0.16019347860889097, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3426629556463706, 0.0, 0.33591507158653605, 0.0, 0.0, 0.04025140372585128, 0.0, 0.5756230062735465, 0.0, 0.0, 0.0, 0.0, 0.0017523850398583757, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32421546356710995, 0.0, 0.011282618188498251, 0.6937716630434633, 0.2529924781123921, 0.2596235239889639, 0.0, 0.043120476743740394, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2793513283874615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7775441119273626, 0.14515455310675088, 0.0, 0.13510821830370037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44716225485327055, 0.0, 0.0, 0.0, 0.8291790553392753, 0.0, 0.0, 0.26334889102262177, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15479667093058366, 0.0, 0.0, 0.0, 0.36993623982653, 0.0, 0.0, 0.0, 0.0, 0.3623559014178924, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029233403628440607, 0.1427335193249464, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051251957334763225, 0.691166524717783, 0.29938181148714105, 0.5349464228115411, 0.06157899241792075, 0.0, 0.0, 0.2102727260204982, 0.0, 0.60145477594229, 0.0, 0.0, 0.03062034252430008, 0.1635016794080344, 0.7915679370358879, 0.0, 0.07317711878131836, 0.9809574403621903, 0.0, 0.0, 0.44926990137304107, 0.21608537407152922, 0.0, 0.05777727499408966, 0.039929965496851884, 0.0, 0.29915326656993935, 0.31099294960838475, 0.13414679708282617, 0.10949528327933863, 0.0, 0.10753728810972799, 0.0, 0.3797681313373244, 0.5019126906002389, 0.8077076191871431, 0.02575673641821252, 0.0, 0.0, 0.0, 0.0, 0.08748647908145589, 0.0, 0.0, 0.2775216739406583, 0.0, 0.0, 0.0, 0.0, 0.19491911889837882, 0.0, 0.626470449065492, 0.0, 0.1610321350235478, 0.0, 0.6250193931691284, 0.18518836301307043, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5645059221744597, 0.0, 0.30319370972913895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5299926460898494, 0.0, 0.0, 0.18935417444624433, 0.0, 0.3520383786660897, 0.004326690884129795, 0.4266082560323472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04753484047689839, 0.0, 0.0, 0.0, 0.2507515224933452, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011126595947621928, 0.0, 0.0, 0.0, 0.0, 0.07883374294223121, 0.0, 0.0, 0.06158319631443835, 0.0, 0.0, 0.0, 0.0, 0.18599923723367912, 0.0, 0.0, 0.0, 0.0, 0.07225540391146344, 0.0, 0.17222213577613948, 0.0, 0.07109258970520838, 0.30622054435699597, 0.0, 0.0, 0.021600783197372013, 0.0, 0.0, 0.0, 0.0, 0.08747962563792125, 0.0, 0.0, 0.6661127453751955, 0.20906059268184987, 0.0, 0.14050781836020196, 0.0, 0.0, 0.38775347026983786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016520556870922822, 0.08066242485362363, 0.0, 0.0, 0.008507877750707929, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19963971549104562, 0.0, 0.0, 0.0, 0.0, 0.1517009343251733, 0.5312750255062002, 0.0, 0.0, 0.1147421120346578, 0.0, 0.0, 0.0, 0.0, 0.35500190337758014, 0.0, 0.045597713545061126, 0.0, 0.5209586802357347, 0.021862434298379425, 0.0, 0.0, 0.0, 1.0378291015579089, 0.0, 0.0, 0.0, 0.08585205631703301, 0.0483968201096062, 0.0, 0.025471357792783925, 0.0, 0.22080976260076224, 1.166729214929465, 0.0, 0.677323308302816, 0.18535083381370177, 0.0, 0.0, 0.8704056171514745, 0.0, 0.8935663748182991, 0.015577429284835683, 0.13855228421849528, 0.0, 0.0, 0.1744139155099896, 0.15132491229634354, 0.0, 0.02513332482807071, 0.0, 0.17525145412541498, 0.0, 0.0, 0.0, 0.0, 0.02694350993020697, 0.162823516985682, 0.0, 0.0, 0.6457319525792288, 0.07238426196350153, 0.05745965468198706, 0.0, 0.0, 0.46532195926191927, 0.08460519940877058, 0.22064747420300995, 0.07874956387307873, 0.02873453791303352, 0.04102153493571428, 0.0, 0.0, 0.0, 0.0, 0.49780157894039, 0.0, 0.0, 0.301057479311315, 0.0, 0.0, 0.0016911663416444849, 0.0, 0.2606342751929628, 0.0, 0.0, 0.0, 0.4832976838898257, 0.2663912296088851, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16074512474220395, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0418820657562929, 0.0, 0.04820286684177523, 0.0, 0.1580443847885599, 0.0, 0.1278562139832909, 0.0, 0.0, 0.12981743196566292, 0.0, 0.0, 0.12101853247948517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13204408757174144, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024933507022188058, 0.0, 0.0, 0.0, 0.22607531711845172, 0.06698435016959209, 0.0, 0.0, 0.38137808720681626, 0.04452538198512276, 0.13682477248336336, 0.20418703285947667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19170326048233127, 0.0, 0.0, 0.0684911628398921, 0.0, 0.0, 0.0015650042612973304, 0.0, 0.0, 0.0, 0.0, 0.3663518735291969, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27889642218469224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09171777647431381, 0.0, 0.031206336760942092, 0.04657190125204274, 0.0, 0.21168995340334643, 0.0, 0.0, 0.2715554536912592, 0.0, 0.0, 0.0476752915188213, 0.0, 0.0, 0.0, 0.0, 0.36410628949886475, 0.021118814521733104, 0.0, 0.25524059151149175, 0.22081888489064375, 0.0, 0.0008303397905156581, 0.0, 0.0, 0.0, 0.0, 0.07229346662213895, 0.0, 0.0, 0.820696989198259, 0.541933064129384, 0.0, 0.6976372073851678, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6559268620540256, 0.0, 0.0, 0.00593765430305966, 0.0, 0.03489392321475427, 0.14772313877299487, 0.20501704987333305, 0.04206924698360076, 0.15444861072003166, 0.0, 0.8226903093195739, 0.0, 0.15641473972332146, 0.790869654262485, 0.6842130952365706, 0.03633117370294634, 0.0, 0.18980663716791113, 0.0, 0.0, 0.038558748063994376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.40339029653363123, 0.0, 0.0, 0.0, 0.0, 0.43960663038458647, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08136147927839638, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16190486570077617, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2992216626110986, 0.0, 0.0, 0.0, 0.17882109545054858, 0.0, 0.0796930212837386, 0.0, 0.620119218817229, 0.06539128749913568, 0.09575080072237417, 0.0, 0.0, 0.0, 0.31047674992819635, 0.0, 0.0, 0.3258789812741783, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03198183278340688, 0.0, 0.0, 0.05159425685765124, 0.22600144774467618, 0.0, 0.17025710615993656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27035127710875223, 0.6595462305837041, 0.0, 0.0, 0.0, 0.5146584726996519, 0.0, 0.4337121439657544, 0.0, 0.0, 0.006091807521893999, 0.0, 0.0, 0.0, 0.0, 0.012888743397998564, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.336652909816104, 0.0, 0.0, 0.33692102610329455, 0.41892538326879675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4637663755690806, 0.004049689433712162, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2537974154183676, 0.0, 0.0, 0.0, 0.01941920682276314, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2625647477298457, 0.0, 0.0, 0.0, 0.28333914754514417, 0.0, 0.009497623447163565, 0.0, 0.0, 0.015321911880061054, 0.7292697770823527, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4913667474115824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8188140725936274, 0.23538609489199455, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004251660745273512, 0.2915534718195699, 0.8234990979680312, 0.15784851698528854, 0.31856705993041073, 0.0, 0.0, 0.0, 0.0, 0.8168645454274115, 0.2887084121601819, 0.0, 0.0, 0.0, 0.020685054965967595, 0.784321469374656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5719438351419199, 0.0, 0.0, 0.0, 0.0, 0.23810583942791758, 0.0, 0.0, 0.13190421809771516, 0.2355353163505223, 0.21414939137204708, 0.0, 0.13824109301205073, 0.0, 0.0, 0.4059138329547745, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3839879400284894, 0.7642606292584734, 0.17337082964447778, 0.43085204172578645, 0.5071846921436626, 0.0, 0.0, 0.0, 0.0, 0.0064224562854441995, 0.45947466920879537, 0.0, 0.048013668076727356, 0.008826989297459209, 0.0, 0.10254269883954115, 0.0, 0.449628221574638, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3130138916400048, 0.0, 0.0, 0.0, 0.0, 0.2198473045188841, 0.0, 0.7854658854954149, 0.0, 0.0, 0.0, 0.27482511606730614, 0.0, 0.0, 0.0, 0.10051459327881847, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03619595572786272, 0.2919384825550201, 0.0, 0.27020767480793234, 0.32906141180848986, 0.4014761358710767, 0.0, 0.0, 0.22194819820968317, 0.0, 0.397060529897469, 0.20690476484626705, 0.4811671410392035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10602724763101144, 0.0, 0.9180290974697203, 0.32581440887276836, 0.11505868376784396, 0.5289668394733107, 0.0, 0.5268986674479775, 0.2483018891728648, 0.0, 0.0, 0.014289568621041568, 0.0, 0.0, 0.0, 0.0, 1.010481071564644, 0.06469540823739603, 0.19880648111582097, 0.580377081863938, 0.3701240890884847, 0.00033976241032705203, 0.5120222312956857, 0.54263963710411, 0.24775903677393285, 0.13070835967347635, 0.9915175816548458, 0.2899273698579516, 0.0, 0.0, 1.1703372022756595, 0.02329003991510475, 0.11859776061629382, 0.69977713771051, 0.6054053013517017, 0.707408250006164, 0.0, 0.2196427786747048, 0.5323095043727097, 0.027197936547729912, 0.0, 0.0, 0.6744130710240275, 0.0, 0.0, 0.009472629890788111, 0.0, 0.0, 0.0, 0.0, 0.11454144616825819, 0.0, 0.0, 0.0, 1.3982199856641506, 0.2538753491205552, 0.20096391172061562, 0.0, 0.0, 0.04239072289345119, 0.0, 0.773883065549572, 0.0, 0.0, 0.41373255777296347, 0.0, 0.0, 0.0, 0.0, 0.7328658795072667, 0.0, 0.0, 0.8143261048934798, 0.0, 0.0, 0.005931475071506779, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5479468542821576, 0.0, 0.32433512795769415, 0.7968405650512658, 0.0, 0.0, 0.2823749754870304, 0.0, 0.08888138290002096, 0.0, 0.0, 0.0, 0.17925820791427197, 0.2000876096746871, 0.0, 0.0, 0.40815111826084144, 0.16385977895936904, 0.06986036185280928, 0.0, 0.0, 0.0, 0.0, 0.05375441115395932, 0.0, 0.1762464226118615, 0.0, 0.3587964347388992, 0.0, 0.8015475518237312, 0.175514484083294, 0.0, 0.0, 0.13495628742382593, 0.0, 0.3105505922199702, 0.7248528773530782, 0.0, 0.18705292898123585, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1748071953278316, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7043431257504623, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13971141189132677, 0.0, 0.0, 0.0, 0.0, 0.0, 0.031464801155366616, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16845401890236414, 0.14573633601571054, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13998624971109908, 0.0, 0.07426203609200491, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2049851695927226, 0.0, 0.0, 0.17338949919280155, 0.0, 0.0, 0.18945472552918335, 0.0, 0.0, 0.0, 0.0, 0.0, 0.036320374032721404, 0.15648907735824633, 0.0, 0.05859334953381536, 0.0, 0.0, 0.0, 0.0, 0.5224897007715829, 0.0, 0.0, 0.0, 0.0, 0.08480845151311803, 0.0, 0.0, 0.0, 0.0, 0.07725579657733653, 0.0, 0.14447765770967222, 0.0, 0.0, 0.0, 0.15678431253622274, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.255601945951803, 0.0, 0.0, 0.2867970808516741, 0.3376079838842544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.29929546323980694, 0.0, 0.0, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = ((ex.bodies[1]).bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    result = nothing
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    for i_4 = 1:A_lvl.shape[1]
        val = Ct_lvl_2_val
        Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
        B_lvl_ptr_2 = B_lvl_ptr
        B_lvl_ptr = (Finch).moveto(B_lvl_ptr, CPU(Threads.nthreads()))
        B_lvl_tbl1_2 = B_lvl_tbl1
        B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
        B_lvl_tbl2_2 = B_lvl_tbl2
        B_lvl_tbl2 = (Finch).moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
        val_2 = B_lvl_val
        B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
        A_lvl_ptr_2 = A_lvl_ptr
        A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
        A_lvl_tbl1_2 = A_lvl_tbl1
        A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
        A_lvl_tbl2_2 = A_lvl_tbl2
        A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
        val_3 = A_lvl_val
        A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
        Threads.@threads for i_5 = 1:Threads.nthreads()
                B_lvl_q = B_lvl_ptr[1]
                B_lvl_q_stop = B_lvl_ptr[1 + 1]
                if B_lvl_q < B_lvl_q_stop
                    B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                else
                    B_lvl_i_stop = 0
                end
                phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_5 + -1), Threads.nthreads()))
                phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_5, Threads.nthreads()))
                if phase_stop_2 >= phase_start_2
                    if B_lvl_tbl2[B_lvl_q] < phase_start_2
                        B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    while true
                        B_lvl_i = B_lvl_tbl2[B_lvl_q]
                        B_lvl_q_step = B_lvl_q
                        if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                            B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        if B_lvl_i < phase_stop_2
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_4, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_4
                                            if A_lvl_tbl1[A_lvl_q] < i_4
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        else
                            phase_stop_13 = min(B_lvl_i, phase_stop_2)
                            if B_lvl_i == phase_stop_13
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_4, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_4
                                                if A_lvl_tbl1[A_lvl_q] < i_4
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            end
                            break
                        end
                    end
                end
            end
        Ct_lvl_2_val = val
        B_lvl_ptr = B_lvl_ptr_2
        B_lvl_tbl1 = B_lvl_tbl1_2
        B_lvl_tbl2 = B_lvl_tbl2_2
        B_lvl_val = val_2
        A_lvl_ptr = A_lvl_ptr_2
        A_lvl_tbl1 = A_lvl_tbl1_2
        A_lvl_tbl2 = A_lvl_tbl2_2
        A_lvl_val = val_3
    end
    resize!(Ct_lvl_2_val, A_lvl.shape[1] * B_lvl.shape[2])
    result = (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
    result
end
julia> @finch begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0, 0.0, 0.1619393094464903, 0.0, 0.0, 0.0, 0.0, 0.11373910752308043, 0.0, 0.32533429668145175, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5405512925681031, 0.0, 0.0, 0.5409817971066755, 0.6726532010052559, 0.0, 0.0, 0.0, 0.04498001780375231, 0.0, 0.7446527460595233, 0.0, 0.0, 0.0, 0.0648053115514859, 0.0, 0.20542126000594346, 0.0, 0.2489342378383824, 0.0, 0.0, 0.2446702787341799, 0.0, 0.0, 0.4075132486743765, 0.372693911014397, 0.0, 0.03990868671702989, 0.32865292999993995, 0.0, 0.0, 0.0, 0.037570880212435515, 0.0, 0.0, 0.2585440952996821, 0.0, 0.0, 0.5429032184680616, 0.0, 0.0, 0.0, 0.7310331654112876, 1.6974310666890076, 0.13434284643730737, 0.0, 0.15388431962676646, 0.0, 0.018799628425153446, 0.026448056346556916, 0.020943024178172204, 0.08671614507629592, 0.19272805440861934, 0.07015243227726202, 0.0, 0.1267665206913561, 0.1014023431949165, 0.494724963965631, 0.0, 0.6317423070706343, 0.0, 0.4351905463586081, 0.019451367524410516, 0.11392026534019284, 0.0, 0.7674183972620773, 0.0, 0.0, 0.009728790198518009, 0.0, 0.0, 0.03627546054320975, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0140288190005624, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4229718175600567, 0.0752980218883111, 0.013070459322559402, 0.0, 0.4793455258509991, 0.0, 0.0, 0.0, 0.0, 0.08811633730455204, 0.08467555208376226, 0.0, 0.0, 1.4824413586764027, 0.27442978968992093, 0.0, 0.0028104905637611507, 0.1480443868451215, 0.6744061467564826, 0.043998508948338316, 0.8697701045107844, 0.8469834954000914, 0.0, 0.02425613445444166, 0.0, 0.0, 0.0, 0.9679232213808633, 0.6774894982888419, 0.35310753741950685, 0.0, 0.2841919624705342, 0.0, 0.0, 0.7349602036514661, 0.0, 0.2186035505612502, 0.04416466424315345, 0.0, 0.0, 0.25133653271826356, 0.11160837067772013, 0.0, 0.14048007630402903, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.035048636482553235, 0.038653696504083, 0.10014641420794519, 0.0, 0.0837599459565703, 0.10942537757376403, 0.0, 0.0, 0.0, 0.08204362658289192, 0.0, 0.0, 0.011591138384550418, 0.0, 0.038004261053570694, 0.0066189468466051875, 0.06306237088337384, 0.0, 0.0, 0.03121664575636558, 0.0, 0.0, 0.02910081181829685, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08506408857544866, 0.0, 0.0, 0.0, 0.7712870388819855, 0.2285263236705729, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6966121466049721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6540220401149287, 0.0, 0.0, 0.2336670223432043, 0.0, 0.0, 0.005339227289024509, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4930665741229495, 0.0, 0.029559863815954657, 0.0, 0.0, 0.10760363562562023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15061269543615943, 0.6107211710602042, 0.0, 0.0, 0.733057860494869, 0.057362855000467054, 0.17650444882735677, 0.0, 0.0, 0.014677698725170102, 0.0, 0.17821555119599314, 0.0025555492556658746, 0.14808219427948163, 0.0, 0.11979690859781277, 0.0, 0.0, 0.13110894332781436, 0.04449679654819995, 0.0, 0.1133902344080122, 0.0, 0.0, 0.0017495044850376442, 0.013900993801975209, 0.03729472110086185, 0.19173933242455474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1709634942172136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13051636625786148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4357713541613571, 0.0, 0.0, 0.0, 0.0, 0.07073267416682687, 0.0, 0.0, 0.0, 0.0, 0.06443354393704691, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28238918313491895, 0.0, 0.0, 0.0, 0.28219752132848275, 0.0, 0.0, 0.0, 0.3045252869011713, 0.0, 0.0, 0.019083741024338748, 0.0, 0.06257051306836219, 0.7837995208867943, 0.05061887468267648, 0.0, 0.0, 0.05139533007879884, 0.0, 0.0, 0.04791180451081585, 0.0, 0.5281077501137272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7571506519409951, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.026654196669850944, 0.0, 0.0, 0.0, 0.0, 0.23843473688023176, 0.0, 0.0, 0.6051386294410329, 0.0, 0.0, 0.0, 0.0, 0.04194764240896927, 0.13628497764809444, 0.0, 0.0, 0.0, 0.0, 0.06043639304510901, 0.04344269031549197, 0.0, 0.7296328391677986, 0.6312346675367022, 0.0, 0.0, 0.22817557354519388, 0.0, 0.05073205279897938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1594527848545218, 0.1381227206057285, 0.0, 0.0, 0.0, 0.0, 0.4217049712314433, 0.0, 0.0, 0.31027130374464373, 0.0, 0.0, 0.08118863628632204, 0.8304208532182848, 0.983137215831116, 0.0, 0.24897028906526258, 0.14034260442091134, 0.0, 0.691731307052974, 0.7500207108166317, 0.833859795824118, 0.0, 0.05053828036951065, 0.8139843305766208, 0.0, 0.0, 1.3856822465181273, 1.0343729450047436, 0.0, 0.0, 1.09811525132565, 0.0, 0.0, 0.005748580563865145, 0.0, 0.0, 0.8634025361175751, 0.0, 0.14616020357939458, 0.7514377111323272, 0.0, 0.0, 0.1732108049991939, 0.0, 0.0, 0.0, 0.1879649588465491, 0.0, 0.0, 0.0, 0.00817965091901661, 0.0, 0.0, 0.0, 0.5779500058438248, 0.0, 0.0, 0.0, 0.005249336837927155, 0.0, 0.0, 0.0, 0.3064350538312218, 0.0, 0.001890320177282059, 0.45976772975798436, 0.40475012946470096, 0.3442380471075082, 0.017185108496187197, 0.288035784498987, 0.0, 0.0, 0.2827566402954316, 0.0, 0.0, 0.2635916697723135, 0.0, 0.0, 0.0, 0.3588181656747819, 0.0, 0.0, 0.0, 0.17570163169455902, 0.0, 0.2076868163288207, 0.0, 0.0, 0.0, 0.5097341557191223, 0.14587016095283192, 0.0, 0.4172405362928458, 0.0, 0.13951021052730417, 0.0, 0.6033798708626249, 0.0, 0.0, 0.0, 0.0, 0.4512081190066747, 1.1915177253624374, 0.0, 0.53626862172996, 0.26267190873847124, 0.024946529615964698, 0.630282563668714, 0.0, 0.7072072219187049, 0.2557442098535857, 0.0, 0.0, 0.0, 0.027439980552710016, 0.0, 0.2634523244708942, 0.750190928083991, 0.31925762502388877, 0.0, 0.0, 0.0, 0.17667054707398389, 1.689395274403252, 0.0, 0.5386556936239477, 0.21507940395745773, 0.30698899212442304, 0.11255638742238075, 0.12042909174810443, 0.1983506487023661, 0.0, 0.2156156778918424, 0.0, 0.6179686549930762, 0.0, 0.0, 0.0, 0.011162873807723371, 0.003307472295366656, 0.5417377310206691, 0.1038404263557644, 0.0, 0.0, 0.0, 0.010082100558503525, 0.5759566406752274, 0.5373731999014957, 0.24497595431253502, 0.3628321361534925, 0.4271139218413104, 0.0888079037091352, 0.013607634508588288, 0.5254304747208627, 0.0, 0.0, 0.0033818738704913948, 0.0, 0.3894178985059293, 7.727488918331624e-5, 0.4719056234120719, 0.0, 0.0, 0.7548965169480935, 0.0, 0.0, 0.0, 0.0, 0.04295271297559638, 0.3048341885265102, 0.0, 0.16015654700904233, 0.0, 0.03255288417829475, 0.0, 0.0, 0.0, 0.0, 0.16019347860889097, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3426629556463706, 0.0, 0.33591507158653605, 0.0, 0.0, 0.04025140372585128, 0.0, 0.5756230062735465, 0.0, 0.0, 0.0, 0.0, 0.0017523850398583757, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32421546356710995, 0.0, 0.011282618188498251, 0.6937716630434633, 0.2529924781123921, 0.2596235239889639, 0.0, 0.043120476743740394, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2793513283874615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7775441119273626, 0.14515455310675088, 0.0, 0.13510821830370037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44716225485327055, 0.0, 0.0, 0.0, 0.8291790553392753, 0.0, 0.0, 0.26334889102262177, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15479667093058366, 0.0, 0.0, 0.0, 0.36993623982653, 0.0, 0.0, 0.0, 0.0, 0.3623559014178924, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029233403628440607, 0.1427335193249464, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051251957334763225, 0.691166524717783, 0.29938181148714105, 0.5349464228115411, 0.06157899241792075, 0.0, 0.0, 0.2102727260204982, 0.0, 0.60145477594229, 0.0, 0.0, 0.03062034252430008, 0.1635016794080344, 0.7915679370358879, 0.0, 0.07317711878131836, 0.9809574403621903, 0.0, 0.0, 0.44926990137304107, 0.21608537407152922, 0.0, 0.05777727499408966, 0.039929965496851884, 0.0, 0.29915326656993935, 0.31099294960838475, 0.13414679708282617, 0.10949528327933863, 0.0, 0.10753728810972799, 0.0, 0.3797681313373244, 0.5019126906002389, 0.8077076191871431, 0.02575673641821252, 0.0, 0.0, 0.0, 0.0, 0.08748647908145589, 0.0, 0.0, 0.2775216739406583, 0.0, 0.0, 0.0, 0.0, 0.19491911889837882, 0.0, 0.626470449065492, 0.0, 0.1610321350235478, 0.0, 0.6250193931691284, 0.18518836301307043, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5645059221744597, 0.0, 0.30319370972913895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5299926460898494, 0.0, 0.0, 0.18935417444624433, 0.0, 0.3520383786660897, 0.004326690884129795, 0.4266082560323472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04753484047689839, 0.0, 0.0, 0.0, 0.2507515224933452, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011126595947621928, 0.0, 0.0, 0.0, 0.0, 0.07883374294223121, 0.0, 0.0, 0.06158319631443835, 0.0, 0.0, 0.0, 0.0, 0.18599923723367912, 0.0, 0.0, 0.0, 0.0, 0.07225540391146344, 0.0, 0.17222213577613948, 0.0, 0.07109258970520838, 0.30622054435699597, 0.0, 0.0, 0.021600783197372013, 0.0, 0.0, 0.0, 0.0, 0.08747962563792125, 0.0, 0.0, 0.6661127453751955, 0.20906059268184987, 0.0, 0.14050781836020196, 0.0, 0.0, 0.38775347026983786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016520556870922822, 0.08066242485362363, 0.0, 0.0, 0.008507877750707929, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19963971549104562, 0.0, 0.0, 0.0, 0.0, 0.1517009343251733, 0.5312750255062002, 0.0, 0.0, 0.1147421120346578, 0.0, 0.0, 0.0, 0.0, 0.35500190337758014, 0.0, 0.045597713545061126, 0.0, 0.5209586802357347, 0.021862434298379425, 0.0, 0.0, 0.0, 1.0378291015579089, 0.0, 0.0, 0.0, 0.08585205631703301, 0.0483968201096062, 0.0, 0.025471357792783925, 0.0, 0.22080976260076224, 1.166729214929465, 0.0, 0.677323308302816, 0.18535083381370177, 0.0, 0.0, 0.8704056171514745, 0.0, 0.8935663748182991, 0.015577429284835683, 0.13855228421849528, 0.0, 0.0, 0.1744139155099896, 0.15132491229634354, 0.0, 0.02513332482807071, 0.0, 0.17525145412541498, 0.0, 0.0, 0.0, 0.0, 0.02694350993020697, 0.162823516985682, 0.0, 0.0, 0.6457319525792288, 0.07238426196350153, 0.05745965468198706, 0.0, 0.0, 0.46532195926191927, 0.08460519940877058, 0.22064747420300995, 0.07874956387307873, 0.02873453791303352, 0.04102153493571428, 0.0, 0.0, 0.0, 0.0, 0.49780157894039, 0.0, 0.0, 0.301057479311315, 0.0, 0.0, 0.0016911663416444849, 0.0, 0.2606342751929628, 0.0, 0.0, 0.0, 0.4832976838898257, 0.2663912296088851, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16074512474220395, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0418820657562929, 0.0, 0.04820286684177523, 0.0, 0.1580443847885599, 0.0, 0.1278562139832909, 0.0, 0.0, 0.12981743196566292, 0.0, 0.0, 0.12101853247948517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13204408757174144, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024933507022188058, 0.0, 0.0, 0.0, 0.22607531711845172, 0.06698435016959209, 0.0, 0.0, 0.38137808720681626, 0.04452538198512276, 0.13682477248336336, 0.20418703285947667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19170326048233127, 0.0, 0.0, 0.0684911628398921, 0.0, 0.0, 0.0015650042612973304, 0.0, 0.0, 0.0, 0.0, 0.3663518735291969, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27889642218469224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09171777647431381, 0.0, 0.031206336760942092, 0.04657190125204274, 0.0, 0.21168995340334643, 0.0, 0.0, 0.2715554536912592, 0.0, 0.0, 0.0476752915188213, 0.0, 0.0, 0.0, 0.0, 0.36410628949886475, 0.021118814521733104, 0.0, 0.25524059151149175, 0.22081888489064375, 0.0, 0.0008303397905156581, 0.0, 0.0, 0.0, 0.0, 0.07229346662213895, 0.0, 0.0, 0.820696989198259, 0.541933064129384, 0.0, 0.6976372073851678, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6559268620540256, 0.0, 0.0, 0.00593765430305966, 0.0, 0.03489392321475427, 0.14772313877299487, 0.20501704987333305, 0.04206924698360076, 0.15444861072003166, 0.0, 0.8226903093195739, 0.0, 0.15641473972332146, 0.790869654262485, 0.6842130952365706, 0.03633117370294634, 0.0, 0.18980663716791113, 0.0, 0.0, 0.038558748063994376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.40339029653363123, 0.0, 0.0, 0.0, 0.0, 0.43960663038458647, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08136147927839638, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16190486570077617, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2992216626110986, 0.0, 0.0, 0.0, 0.17882109545054858, 0.0, 0.0796930212837386, 0.0, 0.620119218817229, 0.06539128749913568, 0.09575080072237417, 0.0, 0.0, 0.0, 0.31047674992819635, 0.0, 0.0, 0.3258789812741783, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03198183278340688, 0.0, 0.0, 0.05159425685765124, 0.22600144774467618, 0.0, 0.17025710615993656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27035127710875223, 0.6595462305837041, 0.0, 0.0, 0.0, 0.5146584726996519, 0.0, 0.4337121439657544, 0.0, 0.0, 0.006091807521893999, 0.0, 0.0, 0.0, 0.0, 0.012888743397998564, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.336652909816104, 0.0, 0.0, 0.33692102610329455, 0.41892538326879675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4637663755690806, 0.004049689433712162, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2537974154183676, 0.0, 0.0, 0.0, 0.01941920682276314, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2625647477298457, 0.0, 0.0, 0.0, 0.28333914754514417, 0.0, 0.009497623447163565, 0.0, 0.0, 0.015321911880061054, 0.7292697770823527, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4913667474115824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8188140725936274, 0.23538609489199455, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004251660745273512, 0.2915534718195699, 0.8234990979680312, 0.15784851698528854, 0.31856705993041073, 0.0, 0.0, 0.0, 0.0, 0.8168645454274115, 0.2887084121601819, 0.0, 0.0, 0.0, 0.020685054965967595, 0.784321469374656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5719438351419199, 0.0, 0.0, 0.0, 0.0, 0.23810583942791758, 0.0, 0.0, 0.13190421809771516, 0.2355353163505223, 0.21414939137204708, 0.0, 0.13824109301205073, 0.0, 0.0, 0.4059138329547745, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3839879400284894, 0.7642606292584734, 0.17337082964447778, 0.43085204172578645, 0.5071846921436626, 0.0, 0.0, 0.0, 0.0, 0.0064224562854441995, 0.45947466920879537, 0.0, 0.048013668076727356, 0.008826989297459209, 0.0, 0.10254269883954115, 0.0, 0.449628221574638, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3130138916400048, 0.0, 0.0, 0.0, 0.0, 0.2198473045188841, 0.0, 0.7854658854954149, 0.0, 0.0, 0.0, 0.27482511606730614, 0.0, 0.0, 0.0, 0.10051459327881847, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03619595572786272, 0.2919384825550201, 0.0, 0.27020767480793234, 0.32906141180848986, 0.4014761358710767, 0.0, 0.0, 0.22194819820968317, 0.0, 0.397060529897469, 0.20690476484626705, 0.4811671410392035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10602724763101144, 0.0, 0.9180290974697203, 0.32581440887276836, 0.11505868376784396, 0.5289668394733107, 0.0, 0.5268986674479775, 0.2483018891728648, 0.0, 0.0, 0.014289568621041568, 0.0, 0.0, 0.0, 0.0, 1.010481071564644, 0.06469540823739603, 0.19880648111582097, 0.580377081863938, 0.3701240890884847, 0.00033976241032705203, 0.5120222312956857, 0.54263963710411, 0.24775903677393285, 0.13070835967347635, 0.9915175816548458, 0.2899273698579516, 0.0, 0.0, 1.1703372022756595, 0.02329003991510475, 0.11859776061629382, 0.69977713771051, 0.6054053013517017, 0.707408250006164, 0.0, 0.2196427786747048, 0.5323095043727097, 0.027197936547729912, 0.0, 0.0, 0.6744130710240275, 0.0, 0.0, 0.009472629890788111, 0.0, 0.0, 0.0, 0.0, 0.11454144616825819, 0.0, 0.0, 0.0, 1.3982199856641506, 0.2538753491205552, 0.20096391172061562, 0.0, 0.0, 0.04239072289345119, 0.0, 0.773883065549572, 0.0, 0.0, 0.41373255777296347, 0.0, 0.0, 0.0, 0.0, 0.7328658795072667, 0.0, 0.0, 0.8143261048934798, 0.0, 0.0, 0.005931475071506779, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5479468542821576, 0.0, 0.32433512795769415, 0.7968405650512658, 0.0, 0.0, 0.2823749754870304, 0.0, 0.08888138290002096, 0.0, 0.0, 0.0, 0.17925820791427197, 0.2000876096746871, 0.0, 0.0, 0.40815111826084144, 0.16385977895936904, 0.06986036185280928, 0.0, 0.0, 0.0, 0.0, 0.05375441115395932, 0.0, 0.1762464226118615, 0.0, 0.3587964347388992, 0.0, 0.8015475518237312, 0.175514484083294, 0.0, 0.0, 0.13495628742382593, 0.0, 0.3105505922199702, 0.7248528773530782, 0.0, 0.18705292898123585, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1748071953278316, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7043431257504623, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13971141189132677, 0.0, 0.0, 0.0, 0.0, 0.0, 0.031464801155366616, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16845401890236414, 0.14573633601571054, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13998624971109908, 0.0, 0.07426203609200491, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2049851695927226, 0.0, 0.0, 0.17338949919280155, 0.0, 0.0, 0.18945472552918335, 0.0, 0.0, 0.0, 0.0, 0.0, 0.036320374032721404, 0.15648907735824633, 0.0, 0.05859334953381536, 0.0, 0.0, 0.0, 0.0, 0.5224897007715829, 0.0, 0.0, 0.0, 0.0, 0.08480845151311803, 0.0, 0.0, 0.0, 0.0, 0.07725579657733653, 0.0, 0.14447765770967222, 0.0, 0.0, 0.0, 0.15678431253622274, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.255601945951803, 0.0, 0.0, 0.2867970808516741, 0.3376079838842544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.29929546323980694, 0.0, 0.0, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = ((ex.bodies[1]).bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    result = nothing
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = (Finch).moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = (Finch).moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            B_lvl_q = B_lvl_ptr[1]
            B_lvl_q_stop = B_lvl_ptr[1 + 1]
            if B_lvl_q < B_lvl_q_stop
                B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
            else
                B_lvl_i_stop = 0
            end
            phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                if B_lvl_tbl2[B_lvl_q] < phase_start_2
                    B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                end
                while true
                    B_lvl_i = B_lvl_tbl2[B_lvl_q]
                    B_lvl_q_step = B_lvl_q
                    if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                        B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    if B_lvl_i < phase_stop_2
                        Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                        for i_6 = 1:A_lvl.shape[1]
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_6
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_6, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_6
                                            if A_lvl_tbl1[A_lvl_q] < i_6
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_6, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                        end
                        B_lvl_q = B_lvl_q_step
                    else
                        phase_stop_13 = min(B_lvl_i, phase_stop_2)
                        if B_lvl_i == phase_stop_13
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                            for i_8 = 1:A_lvl.shape[1]
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_8
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_8, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_8
                                                if A_lvl_tbl1[A_lvl_q] < i_8
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_8, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        end
                        break
                    end
                end
            end
        end
    resize!(val, A_lvl.shape[1] * B_lvl.shape[2])
    result = (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
    result
end
julia> @finch begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0, 0.0, 0.1619393094464903, 0.0, 0.0, 0.0, 0.0, 0.11373910752308043, 0.0, 0.32533429668145175, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5405512925681031, 0.0, 0.0, 0.5409817971066755, 0.6726532010052559, 0.0, 0.0, 0.0, 0.04498001780375231, 0.0, 0.7446527460595233, 0.0, 0.0, 0.0, 0.0648053115514859, 0.0, 0.20542126000594346, 0.0, 0.2489342378383824, 0.0, 0.0, 0.2446702787341799, 0.0, 0.0, 0.4075132486743765, 0.372693911014397, 0.0, 0.03990868671702989, 0.32865292999993995, 0.0, 0.0, 0.0, 0.037570880212435515, 0.0, 0.0, 0.2585440952996821, 0.0, 0.0, 0.5429032184680616, 0.0, 0.0, 0.0, 0.7310331654112876, 1.6974310666890076, 0.13434284643730737, 0.0, 0.15388431962676646, 0.0, 0.018799628425153446, 0.026448056346556916, 0.020943024178172204, 0.08671614507629592, 0.19272805440861934, 0.07015243227726202, 0.0, 0.1267665206913561, 0.1014023431949165, 0.494724963965631, 0.0, 0.6317423070706343, 0.0, 0.4351905463586081, 0.019451367524410516, 0.11392026534019284, 0.0, 0.7674183972620773, 0.0, 0.0, 0.009728790198518009, 0.0, 0.0, 0.03627546054320975, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0140288190005624, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4229718175600567, 0.0752980218883111, 0.013070459322559402, 0.0, 0.4793455258509991, 0.0, 0.0, 0.0, 0.0, 0.08811633730455204, 0.08467555208376226, 0.0, 0.0, 1.4824413586764027, 0.27442978968992093, 0.0, 0.0028104905637611507, 0.1480443868451215, 0.6744061467564826, 0.043998508948338316, 0.8697701045107844, 0.8469834954000914, 0.0, 0.02425613445444166, 0.0, 0.0, 0.0, 0.9679232213808633, 0.6774894982888419, 0.35310753741950685, 0.0, 0.2841919624705342, 0.0, 0.0, 0.7349602036514661, 0.0, 0.2186035505612502, 0.04416466424315345, 0.0, 0.0, 0.25133653271826356, 0.11160837067772013, 0.0, 0.14048007630402903, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.035048636482553235, 0.038653696504083, 0.10014641420794519, 0.0, 0.0837599459565703, 0.10942537757376403, 0.0, 0.0, 0.0, 0.08204362658289192, 0.0, 0.0, 0.011591138384550418, 0.0, 0.038004261053570694, 0.0066189468466051875, 0.06306237088337384, 0.0, 0.0, 0.03121664575636558, 0.0, 0.0, 0.02910081181829685, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08506408857544866, 0.0, 0.0, 0.0, 0.7712870388819855, 0.2285263236705729, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6966121466049721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6540220401149287, 0.0, 0.0, 0.2336670223432043, 0.0, 0.0, 0.005339227289024509, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4930665741229495, 0.0, 0.029559863815954657, 0.0, 0.0, 0.10760363562562023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15061269543615943, 0.6107211710602042, 0.0, 0.0, 0.733057860494869, 0.057362855000467054, 0.17650444882735677, 0.0, 0.0, 0.014677698725170102, 0.0, 0.17821555119599314, 0.0025555492556658746, 0.14808219427948163, 0.0, 0.11979690859781277, 0.0, 0.0, 0.13110894332781436, 0.04449679654819995, 0.0, 0.1133902344080122, 0.0, 0.0, 0.0017495044850376442, 0.013900993801975209, 0.03729472110086185, 0.19173933242455474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1709634942172136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13051636625786148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4357713541613571, 0.0, 0.0, 0.0, 0.0, 0.07073267416682687, 0.0, 0.0, 0.0, 0.0, 0.06443354393704691, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28238918313491895, 0.0, 0.0, 0.0, 0.28219752132848275, 0.0, 0.0, 0.0, 0.3045252869011713, 0.0, 0.0, 0.019083741024338748, 0.0, 0.06257051306836219, 0.7837995208867943, 0.05061887468267648, 0.0, 0.0, 0.05139533007879884, 0.0, 0.0, 0.04791180451081585, 0.0, 0.5281077501137272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7571506519409951, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.026654196669850944, 0.0, 0.0, 0.0, 0.0, 0.23843473688023176, 0.0, 0.0, 0.6051386294410329, 0.0, 0.0, 0.0, 0.0, 0.04194764240896927, 0.13628497764809444, 0.0, 0.0, 0.0, 0.0, 0.06043639304510901, 0.04344269031549197, 0.0, 0.7296328391677986, 0.6312346675367022, 0.0, 0.0, 0.22817557354519388, 0.0, 0.05073205279897938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1594527848545218, 0.1381227206057285, 0.0, 0.0, 0.0, 0.0, 0.4217049712314433, 0.0, 0.0, 0.31027130374464373, 0.0, 0.0, 0.08118863628632204, 0.8304208532182848, 0.983137215831116, 0.0, 0.24897028906526258, 0.14034260442091134, 0.0, 0.691731307052974, 0.7500207108166317, 0.833859795824118, 0.0, 0.05053828036951065, 0.8139843305766208, 0.0, 0.0, 1.3856822465181273, 1.0343729450047436, 0.0, 0.0, 1.09811525132565, 0.0, 0.0, 0.005748580563865145, 0.0, 0.0, 0.8634025361175751, 0.0, 0.14616020357939458, 0.7514377111323272, 0.0, 0.0, 0.1732108049991939, 0.0, 0.0, 0.0, 0.1879649588465491, 0.0, 0.0, 0.0, 0.00817965091901661, 0.0, 0.0, 0.0, 0.5779500058438248, 0.0, 0.0, 0.0, 0.005249336837927155, 0.0, 0.0, 0.0, 0.3064350538312218, 0.0, 0.001890320177282059, 0.45976772975798436, 0.40475012946470096, 0.3442380471075082, 0.017185108496187197, 0.288035784498987, 0.0, 0.0, 0.2827566402954316, 0.0, 0.0, 0.2635916697723135, 0.0, 0.0, 0.0, 0.3588181656747819, 0.0, 0.0, 0.0, 0.17570163169455902, 0.0, 0.2076868163288207, 0.0, 0.0, 0.0, 0.5097341557191223, 0.14587016095283192, 0.0, 0.4172405362928458, 0.0, 0.13951021052730417, 0.0, 0.6033798708626249, 0.0, 0.0, 0.0, 0.0, 0.4512081190066747, 1.1915177253624374, 0.0, 0.53626862172996, 0.26267190873847124, 0.024946529615964698, 0.630282563668714, 0.0, 0.7072072219187049, 0.2557442098535857, 0.0, 0.0, 0.0, 0.027439980552710016, 0.0, 0.2634523244708942, 0.750190928083991, 0.31925762502388877, 0.0, 0.0, 0.0, 0.17667054707398389, 1.689395274403252, 0.0, 0.5386556936239477, 0.21507940395745773, 0.30698899212442304, 0.11255638742238075, 0.12042909174810443, 0.1983506487023661, 0.0, 0.2156156778918424, 0.0, 0.6179686549930762, 0.0, 0.0, 0.0, 0.011162873807723371, 0.003307472295366656, 0.5417377310206691, 0.1038404263557644, 0.0, 0.0, 0.0, 0.010082100558503525, 0.5759566406752274, 0.5373731999014957, 0.24497595431253502, 0.3628321361534925, 0.4271139218413104, 0.0888079037091352, 0.013607634508588288, 0.5254304747208627, 0.0, 0.0, 0.0033818738704913948, 0.0, 0.3894178985059293, 7.727488918331624e-5, 0.4719056234120719, 0.0, 0.0, 0.7548965169480935, 0.0, 0.0, 0.0, 0.0, 0.04295271297559638, 0.3048341885265102, 0.0, 0.16015654700904233, 0.0, 0.03255288417829475, 0.0, 0.0, 0.0, 0.0, 0.16019347860889097, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3426629556463706, 0.0, 0.33591507158653605, 0.0, 0.0, 0.04025140372585128, 0.0, 0.5756230062735465, 0.0, 0.0, 0.0, 0.0, 0.0017523850398583757, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32421546356710995, 0.0, 0.011282618188498251, 0.6937716630434633, 0.2529924781123921, 0.2596235239889639, 0.0, 0.043120476743740394, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2793513283874615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7775441119273626, 0.14515455310675088, 0.0, 0.13510821830370037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44716225485327055, 0.0, 0.0, 0.0, 0.8291790553392753, 0.0, 0.0, 0.26334889102262177, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15479667093058366, 0.0, 0.0, 0.0, 0.36993623982653, 0.0, 0.0, 0.0, 0.0, 0.3623559014178924, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029233403628440607, 0.1427335193249464, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051251957334763225, 0.691166524717783, 0.29938181148714105, 0.5349464228115411, 0.06157899241792075, 0.0, 0.0, 0.2102727260204982, 0.0, 0.60145477594229, 0.0, 0.0, 0.03062034252430008, 0.1635016794080344, 0.7915679370358879, 0.0, 0.07317711878131836, 0.9809574403621903, 0.0, 0.0, 0.44926990137304107, 0.21608537407152922, 0.0, 0.05777727499408966, 0.039929965496851884, 0.0, 0.29915326656993935, 0.31099294960838475, 0.13414679708282617, 0.10949528327933863, 0.0, 0.10753728810972799, 0.0, 0.3797681313373244, 0.5019126906002389, 0.8077076191871431, 0.02575673641821252, 0.0, 0.0, 0.0, 0.0, 0.08748647908145589, 0.0, 0.0, 0.2775216739406583, 0.0, 0.0, 0.0, 0.0, 0.19491911889837882, 0.0, 0.626470449065492, 0.0, 0.1610321350235478, 0.0, 0.6250193931691284, 0.18518836301307043, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5645059221744597, 0.0, 0.30319370972913895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5299926460898494, 0.0, 0.0, 0.18935417444624433, 0.0, 0.3520383786660897, 0.004326690884129795, 0.4266082560323472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04753484047689839, 0.0, 0.0, 0.0, 0.2507515224933452, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011126595947621928, 0.0, 0.0, 0.0, 0.0, 0.07883374294223121, 0.0, 0.0, 0.06158319631443835, 0.0, 0.0, 0.0, 0.0, 0.18599923723367912, 0.0, 0.0, 0.0, 0.0, 0.07225540391146344, 0.0, 0.17222213577613948, 0.0, 0.07109258970520838, 0.30622054435699597, 0.0, 0.0, 0.021600783197372013, 0.0, 0.0, 0.0, 0.0, 0.08747962563792125, 0.0, 0.0, 0.6661127453751955, 0.20906059268184987, 0.0, 0.14050781836020196, 0.0, 0.0, 0.38775347026983786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016520556870922822, 0.08066242485362363, 0.0, 0.0, 0.008507877750707929, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19963971549104562, 0.0, 0.0, 0.0, 0.0, 0.1517009343251733, 0.5312750255062002, 0.0, 0.0, 0.1147421120346578, 0.0, 0.0, 0.0, 0.0, 0.35500190337758014, 0.0, 0.045597713545061126, 0.0, 0.5209586802357347, 0.021862434298379425, 0.0, 0.0, 0.0, 1.0378291015579089, 0.0, 0.0, 0.0, 0.08585205631703301, 0.0483968201096062, 0.0, 0.025471357792783925, 0.0, 0.22080976260076224, 1.166729214929465, 0.0, 0.677323308302816, 0.18535083381370177, 0.0, 0.0, 0.8704056171514745, 0.0, 0.8935663748182991, 0.015577429284835683, 0.13855228421849528, 0.0, 0.0, 0.1744139155099896, 0.15132491229634354, 0.0, 0.02513332482807071, 0.0, 0.17525145412541498, 0.0, 0.0, 0.0, 0.0, 0.02694350993020697, 0.162823516985682, 0.0, 0.0, 0.6457319525792288, 0.07238426196350153, 0.05745965468198706, 0.0, 0.0, 0.46532195926191927, 0.08460519940877058, 0.22064747420300995, 0.07874956387307873, 0.02873453791303352, 0.04102153493571428, 0.0, 0.0, 0.0, 0.0, 0.49780157894039, 0.0, 0.0, 0.301057479311315, 0.0, 0.0, 0.0016911663416444849, 0.0, 0.2606342751929628, 0.0, 0.0, 0.0, 0.4832976838898257, 0.2663912296088851, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16074512474220395, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0418820657562929, 0.0, 0.04820286684177523, 0.0, 0.1580443847885599, 0.0, 0.1278562139832909, 0.0, 0.0, 0.12981743196566292, 0.0, 0.0, 0.12101853247948517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13204408757174144, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024933507022188058, 0.0, 0.0, 0.0, 0.22607531711845172, 0.06698435016959209, 0.0, 0.0, 0.38137808720681626, 0.04452538198512276, 0.13682477248336336, 0.20418703285947667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19170326048233127, 0.0, 0.0, 0.0684911628398921, 0.0, 0.0, 0.0015650042612973304, 0.0, 0.0, 0.0, 0.0, 0.3663518735291969, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27889642218469224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09171777647431381, 0.0, 0.031206336760942092, 0.04657190125204274, 0.0, 0.21168995340334643, 0.0, 0.0, 0.2715554536912592, 0.0, 0.0, 0.0476752915188213, 0.0, 0.0, 0.0, 0.0, 0.36410628949886475, 0.021118814521733104, 0.0, 0.25524059151149175, 0.22081888489064375, 0.0, 0.0008303397905156581, 0.0, 0.0, 0.0, 0.0, 0.07229346662213895, 0.0, 0.0, 0.820696989198259, 0.541933064129384, 0.0, 0.6976372073851678, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6559268620540256, 0.0, 0.0, 0.00593765430305966, 0.0, 0.03489392321475427, 0.14772313877299487, 0.20501704987333305, 0.04206924698360076, 0.15444861072003166, 0.0, 0.8226903093195739, 0.0, 0.15641473972332146, 0.790869654262485, 0.6842130952365706, 0.03633117370294634, 0.0, 0.18980663716791113, 0.0, 0.0, 0.038558748063994376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.40339029653363123, 0.0, 0.0, 0.0, 0.0, 0.43960663038458647, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08136147927839638, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16190486570077617, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2992216626110986, 0.0, 0.0, 0.0, 0.17882109545054858, 0.0, 0.0796930212837386, 0.0, 0.620119218817229, 0.06539128749913568, 0.09575080072237417, 0.0, 0.0, 0.0, 0.31047674992819635, 0.0, 0.0, 0.3258789812741783, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03198183278340688, 0.0, 0.0, 0.05159425685765124, 0.22600144774467618, 0.0, 0.17025710615993656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27035127710875223, 0.6595462305837041, 0.0, 0.0, 0.0, 0.5146584726996519, 0.0, 0.4337121439657544, 0.0, 0.0, 0.006091807521893999, 0.0, 0.0, 0.0, 0.0, 0.012888743397998564, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.336652909816104, 0.0, 0.0, 0.33692102610329455, 0.41892538326879675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4637663755690806, 0.004049689433712162, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2537974154183676, 0.0, 0.0, 0.0, 0.01941920682276314, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2625647477298457, 0.0, 0.0, 0.0, 0.28333914754514417, 0.0, 0.009497623447163565, 0.0, 0.0, 0.015321911880061054, 0.7292697770823527, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4913667474115824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8188140725936274, 0.23538609489199455, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004251660745273512, 0.2915534718195699, 0.8234990979680312, 0.15784851698528854, 0.31856705993041073, 0.0, 0.0, 0.0, 0.0, 0.8168645454274115, 0.2887084121601819, 0.0, 0.0, 0.0, 0.020685054965967595, 0.784321469374656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5719438351419199, 0.0, 0.0, 0.0, 0.0, 0.23810583942791758, 0.0, 0.0, 0.13190421809771516, 0.2355353163505223, 0.21414939137204708, 0.0, 0.13824109301205073, 0.0, 0.0, 0.4059138329547745, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3839879400284894, 0.7642606292584734, 0.17337082964447778, 0.43085204172578645, 0.5071846921436626, 0.0, 0.0, 0.0, 0.0, 0.0064224562854441995, 0.45947466920879537, 0.0, 0.048013668076727356, 0.008826989297459209, 0.0, 0.10254269883954115, 0.0, 0.449628221574638, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3130138916400048, 0.0, 0.0, 0.0, 0.0, 0.2198473045188841, 0.0, 0.7854658854954149, 0.0, 0.0, 0.0, 0.27482511606730614, 0.0, 0.0, 0.0, 0.10051459327881847, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03619595572786272, 0.2919384825550201, 0.0, 0.27020767480793234, 0.32906141180848986, 0.4014761358710767, 0.0, 0.0, 0.22194819820968317, 0.0, 0.397060529897469, 0.20690476484626705, 0.4811671410392035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10602724763101144, 0.0, 0.9180290974697203, 0.32581440887276836, 0.11505868376784396, 0.5289668394733107, 0.0, 0.5268986674479775, 0.2483018891728648, 0.0, 0.0, 0.014289568621041568, 0.0, 0.0, 0.0, 0.0, 1.010481071564644, 0.06469540823739603, 0.19880648111582097, 0.580377081863938, 0.3701240890884847, 0.00033976241032705203, 0.5120222312956857, 0.54263963710411, 0.24775903677393285, 0.13070835967347635, 0.9915175816548458, 0.2899273698579516, 0.0, 0.0, 1.1703372022756595, 0.02329003991510475, 0.11859776061629382, 0.69977713771051, 0.6054053013517017, 0.707408250006164, 0.0, 0.2196427786747048, 0.5323095043727097, 0.027197936547729912, 0.0, 0.0, 0.6744130710240275, 0.0, 0.0, 0.009472629890788111, 0.0, 0.0, 0.0, 0.0, 0.11454144616825819, 0.0, 0.0, 0.0, 1.3982199856641506, 0.2538753491205552, 0.20096391172061562, 0.0, 0.0, 0.04239072289345119, 0.0, 0.773883065549572, 0.0, 0.0, 0.41373255777296347, 0.0, 0.0, 0.0, 0.0, 0.7328658795072667, 0.0, 0.0, 0.8143261048934798, 0.0, 0.0, 0.005931475071506779, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5479468542821576, 0.0, 0.32433512795769415, 0.7968405650512658, 0.0, 0.0, 0.2823749754870304, 0.0, 0.08888138290002096, 0.0, 0.0, 0.0, 0.17925820791427197, 0.2000876096746871, 0.0, 0.0, 0.40815111826084144, 0.16385977895936904, 0.06986036185280928, 0.0, 0.0, 0.0, 0.0, 0.05375441115395932, 0.0, 0.1762464226118615, 0.0, 0.3587964347388992, 0.0, 0.8015475518237312, 0.175514484083294, 0.0, 0.0, 0.13495628742382593, 0.0, 0.3105505922199702, 0.7248528773530782, 0.0, 0.18705292898123585, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1748071953278316, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7043431257504623, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13971141189132677, 0.0, 0.0, 0.0, 0.0, 0.0, 0.031464801155366616, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16845401890236414, 0.14573633601571054, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13998624971109908, 0.0, 0.07426203609200491, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2049851695927226, 0.0, 0.0, 0.17338949919280155, 0.0, 0.0, 0.18945472552918335, 0.0, 0.0, 0.0, 0.0, 0.0, 0.036320374032721404, 0.15648907735824633, 0.0, 0.05859334953381536, 0.0, 0.0, 0.0, 0.0, 0.5224897007715829, 0.0, 0.0, 0.0, 0.0, 0.08480845151311803, 0.0, 0.0, 0.0, 0.0, 0.07725579657733653, 0.0, 0.14447765770967222, 0.0, 0.0, 0.0, 0.15678431253622274, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.255601945951803, 0.0, 0.0, 0.2867970808516741, 0.3376079838842544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.29929546323980694, 0.0, 0.0, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = ((ex.bodies[1]).bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    result = nothing
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    B_lvl_q = B_lvl_ptr[1]
    B_lvl_q_stop = B_lvl_ptr[1 + 1]
    if B_lvl_q < B_lvl_q_stop
        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
    else
        B_lvl_i_stop = 0
    end
    phase_stop = min(B_lvl.shape[2], B_lvl_i_stop)
    if phase_stop >= 1
        if B_lvl_tbl2[B_lvl_q] < 1
            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
        end
        while true
            B_lvl_i = B_lvl_tbl2[B_lvl_q]
            B_lvl_q_step = B_lvl_q
            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
            end
            if B_lvl_i < phase_stop
                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                val = Ct_lvl_2_val
                Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                B_lvl_tbl1_2 = B_lvl_tbl1
                B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                B_lvl_tbl2_2 = B_lvl_tbl2
                val_2 = B_lvl_val
                B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
                A_lvl_ptr_2 = A_lvl_ptr
                A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                A_lvl_tbl1_2 = A_lvl_tbl1
                A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                A_lvl_tbl2_2 = A_lvl_tbl2
                A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                val_3 = A_lvl_val
                A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
                Threads.@threads for i_9 = 1:Threads.nthreads()
                        phase_start_6 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_9), Threads.nthreads()))
                        phase_stop_7 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_9, Threads.nthreads()))
                        if phase_stop_7 >= phase_start_6
                            for i_12 = phase_start_6:phase_stop_7
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_12
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_8 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_8 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_8
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_9 = min(B_lvl_i_2, phase_stop_8, A_lvl_i)
                                        if A_lvl_i == phase_stop_9 && B_lvl_i_2 == phase_stop_9
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_10 = min(i_12, A_lvl_i_stop_2)
                                            if phase_stop_10 >= i_12
                                                if A_lvl_tbl1[A_lvl_q] < i_12
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_12, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_10
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_12 = min(A_lvl_i_2, phase_stop_10)
                                                        if A_lvl_i_2 == phase_stop_12
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_9
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_9
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_9 + 1
                                    end
                                end
                            end
                        end
                    end
                Ct_lvl_2_val = val
                B_lvl_tbl1 = B_lvl_tbl1_2
                B_lvl_tbl2 = B_lvl_tbl2_2
                B_lvl_val = val_2
                A_lvl_ptr = A_lvl_ptr_2
                A_lvl_tbl1 = A_lvl_tbl1_2
                A_lvl_tbl2 = A_lvl_tbl2_2
                A_lvl_val = val_3
                B_lvl_q = B_lvl_q_step
            else
                phase_stop_18 = min(B_lvl_i, phase_stop)
                if B_lvl_i == phase_stop_18
                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_18
                    val_4 = Ct_lvl_2_val
                    Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                    B_lvl_tbl1_3 = B_lvl_tbl1
                    B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                    B_lvl_tbl2_3 = B_lvl_tbl2
                    val_5 = B_lvl_val
                    B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
                    A_lvl_ptr_3 = A_lvl_ptr
                    A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                    A_lvl_tbl1_3 = A_lvl_tbl1
                    A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                    A_lvl_tbl2_3 = A_lvl_tbl2
                    A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                    val_6 = A_lvl_val
                    A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
                    Threads.@threads for i_19 = 1:Threads.nthreads()
                            phase_start_21 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_19), Threads.nthreads()))
                            phase_stop_23 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_19, Threads.nthreads()))
                            if phase_stop_23 >= phase_start_21
                                for i_22 = phase_start_21:phase_stop_23
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_22
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_24 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_24 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_24
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_25 = min(B_lvl_i_2, A_lvl_i, phase_stop_24)
                                            if A_lvl_i == phase_stop_25 && B_lvl_i_2 == phase_stop_25
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_26 = min(i_22, A_lvl_i_stop_4)
                                                if phase_stop_26 >= i_22
                                                    if A_lvl_tbl1[A_lvl_q] < i_22
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_22, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_26
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_28 = min(A_lvl_i_4, phase_stop_26)
                                                            if A_lvl_i_4 == phase_stop_28
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_25
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_25
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_25 + 1
                                        end
                                    end
                                end
                            end
                        end
                    Ct_lvl_2_val = val_4
                    B_lvl_tbl1 = B_lvl_tbl1_3
                    B_lvl_tbl2 = B_lvl_tbl2_3
                    B_lvl_val = val_5
                    A_lvl_ptr = A_lvl_ptr_3
                    A_lvl_tbl1 = A_lvl_tbl1_3
                    A_lvl_tbl2 = A_lvl_tbl2_3
                    A_lvl_val = val_6
                    B_lvl_q = B_lvl_q_step
                end
                break
            end
        end
    end
    resize!(Ct_lvl_2_val, A_lvl.shape[1] * B_lvl.shape[2])
    result = (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
    result
end
julia> @finch begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0, 0.0, 0.1619393094464903, 0.0, 0.0, 0.0, 0.0, 0.11373910752308043, 0.0, 0.32533429668145175, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5405512925681031, 0.0, 0.0, 0.5409817971066755, 0.6726532010052559, 0.0, 0.0, 0.0, 0.04498001780375231, 0.0, 0.7446527460595233, 0.0, 0.0, 0.0, 0.0648053115514859, 0.0, 0.20542126000594346, 0.0, 0.2489342378383824, 0.0, 0.0, 0.2446702787341799, 0.0, 0.0, 0.4075132486743765, 0.372693911014397, 0.0, 0.03990868671702989, 0.32865292999993995, 0.0, 0.0, 0.0, 0.037570880212435515, 0.0, 0.0, 0.2585440952996821, 0.0, 0.0, 0.5429032184680616, 0.0, 0.0, 0.0, 0.7310331654112876, 1.6974310666890076, 0.13434284643730737, 0.0, 0.15388431962676646, 0.0, 0.018799628425153446, 0.026448056346556916, 0.020943024178172204, 0.08671614507629592, 0.19272805440861934, 0.07015243227726202, 0.0, 0.1267665206913561, 0.1014023431949165, 0.494724963965631, 0.0, 0.6317423070706343, 0.0, 0.4351905463586081, 0.019451367524410516, 0.11392026534019284, 0.0, 0.7674183972620773, 0.0, 0.0, 0.009728790198518009, 0.0, 0.0, 0.03627546054320975, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0140288190005624, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4229718175600567, 0.0752980218883111, 0.013070459322559402, 0.0, 0.4793455258509991, 0.0, 0.0, 0.0, 0.0, 0.08811633730455204, 0.08467555208376226, 0.0, 0.0, 1.4824413586764027, 0.27442978968992093, 0.0, 0.0028104905637611507, 0.1480443868451215, 0.6744061467564826, 0.043998508948338316, 0.8697701045107844, 0.8469834954000914, 0.0, 0.02425613445444166, 0.0, 0.0, 0.0, 0.9679232213808633, 0.6774894982888419, 0.35310753741950685, 0.0, 0.2841919624705342, 0.0, 0.0, 0.7349602036514661, 0.0, 0.2186035505612502, 0.04416466424315345, 0.0, 0.0, 0.25133653271826356, 0.11160837067772013, 0.0, 0.14048007630402903, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.035048636482553235, 0.038653696504083, 0.10014641420794519, 0.0, 0.0837599459565703, 0.10942537757376403, 0.0, 0.0, 0.0, 0.08204362658289192, 0.0, 0.0, 0.011591138384550418, 0.0, 0.038004261053570694, 0.0066189468466051875, 0.06306237088337384, 0.0, 0.0, 0.03121664575636558, 0.0, 0.0, 0.02910081181829685, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08506408857544866, 0.0, 0.0, 0.0, 0.7712870388819855, 0.2285263236705729, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6966121466049721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6540220401149287, 0.0, 0.0, 0.2336670223432043, 0.0, 0.0, 0.005339227289024509, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4930665741229495, 0.0, 0.029559863815954657, 0.0, 0.0, 0.10760363562562023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15061269543615943, 0.6107211710602042, 0.0, 0.0, 0.733057860494869, 0.057362855000467054, 0.17650444882735677, 0.0, 0.0, 0.014677698725170102, 0.0, 0.17821555119599314, 0.0025555492556658746, 0.14808219427948163, 0.0, 0.11979690859781277, 0.0, 0.0, 0.13110894332781436, 0.04449679654819995, 0.0, 0.1133902344080122, 0.0, 0.0, 0.0017495044850376442, 0.013900993801975209, 0.03729472110086185, 0.19173933242455474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1709634942172136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13051636625786148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4357713541613571, 0.0, 0.0, 0.0, 0.0, 0.07073267416682687, 0.0, 0.0, 0.0, 0.0, 0.06443354393704691, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28238918313491895, 0.0, 0.0, 0.0, 0.28219752132848275, 0.0, 0.0, 0.0, 0.3045252869011713, 0.0, 0.0, 0.019083741024338748, 0.0, 0.06257051306836219, 0.7837995208867943, 0.05061887468267648, 0.0, 0.0, 0.05139533007879884, 0.0, 0.0, 0.04791180451081585, 0.0, 0.5281077501137272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7571506519409951, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.026654196669850944, 0.0, 0.0, 0.0, 0.0, 0.23843473688023176, 0.0, 0.0, 0.6051386294410329, 0.0, 0.0, 0.0, 0.0, 0.04194764240896927, 0.13628497764809444, 0.0, 0.0, 0.0, 0.0, 0.06043639304510901, 0.04344269031549197, 0.0, 0.7296328391677986, 0.6312346675367022, 0.0, 0.0, 0.22817557354519388, 0.0, 0.05073205279897938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1594527848545218, 0.1381227206057285, 0.0, 0.0, 0.0, 0.0, 0.4217049712314433, 0.0, 0.0, 0.31027130374464373, 0.0, 0.0, 0.08118863628632204, 0.8304208532182848, 0.983137215831116, 0.0, 0.24897028906526258, 0.14034260442091134, 0.0, 0.691731307052974, 0.7500207108166317, 0.833859795824118, 0.0, 0.05053828036951065, 0.8139843305766208, 0.0, 0.0, 1.3856822465181273, 1.0343729450047436, 0.0, 0.0, 1.09811525132565, 0.0, 0.0, 0.005748580563865145, 0.0, 0.0, 0.8634025361175751, 0.0, 0.14616020357939458, 0.7514377111323272, 0.0, 0.0, 0.1732108049991939, 0.0, 0.0, 0.0, 0.1879649588465491, 0.0, 0.0, 0.0, 0.00817965091901661, 0.0, 0.0, 0.0, 0.5779500058438248, 0.0, 0.0, 0.0, 0.005249336837927155, 0.0, 0.0, 0.0, 0.3064350538312218, 0.0, 0.001890320177282059, 0.45976772975798436, 0.40475012946470096, 0.3442380471075082, 0.017185108496187197, 0.288035784498987, 0.0, 0.0, 0.2827566402954316, 0.0, 0.0, 0.2635916697723135, 0.0, 0.0, 0.0, 0.3588181656747819, 0.0, 0.0, 0.0, 0.17570163169455902, 0.0, 0.2076868163288207, 0.0, 0.0, 0.0, 0.5097341557191223, 0.14587016095283192, 0.0, 0.4172405362928458, 0.0, 0.13951021052730417, 0.0, 0.6033798708626249, 0.0, 0.0, 0.0, 0.0, 0.4512081190066747, 1.1915177253624374, 0.0, 0.53626862172996, 0.26267190873847124, 0.024946529615964698, 0.630282563668714, 0.0, 0.7072072219187049, 0.2557442098535857, 0.0, 0.0, 0.0, 0.027439980552710016, 0.0, 0.2634523244708942, 0.750190928083991, 0.31925762502388877, 0.0, 0.0, 0.0, 0.17667054707398389, 1.689395274403252, 0.0, 0.5386556936239477, 0.21507940395745773, 0.30698899212442304, 0.11255638742238075, 0.12042909174810443, 0.1983506487023661, 0.0, 0.2156156778918424, 0.0, 0.6179686549930762, 0.0, 0.0, 0.0, 0.011162873807723371, 0.003307472295366656, 0.5417377310206691, 0.1038404263557644, 0.0, 0.0, 0.0, 0.010082100558503525, 0.5759566406752274, 0.5373731999014957, 0.24497595431253502, 0.3628321361534925, 0.4271139218413104, 0.0888079037091352, 0.013607634508588288, 0.5254304747208627, 0.0, 0.0, 0.0033818738704913948, 0.0, 0.3894178985059293, 7.727488918331624e-5, 0.4719056234120719, 0.0, 0.0, 0.7548965169480935, 0.0, 0.0, 0.0, 0.0, 0.04295271297559638, 0.3048341885265102, 0.0, 0.16015654700904233, 0.0, 0.03255288417829475, 0.0, 0.0, 0.0, 0.0, 0.16019347860889097, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3426629556463706, 0.0, 0.33591507158653605, 0.0, 0.0, 0.04025140372585128, 0.0, 0.5756230062735465, 0.0, 0.0, 0.0, 0.0, 0.0017523850398583757, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32421546356710995, 0.0, 0.011282618188498251, 0.6937716630434633, 0.2529924781123921, 0.2596235239889639, 0.0, 0.043120476743740394, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2793513283874615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7775441119273626, 0.14515455310675088, 0.0, 0.13510821830370037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44716225485327055, 0.0, 0.0, 0.0, 0.8291790553392753, 0.0, 0.0, 0.26334889102262177, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15479667093058366, 0.0, 0.0, 0.0, 0.36993623982653, 0.0, 0.0, 0.0, 0.0, 0.3623559014178924, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029233403628440607, 0.1427335193249464, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051251957334763225, 0.691166524717783, 0.29938181148714105, 0.5349464228115411, 0.06157899241792075, 0.0, 0.0, 0.2102727260204982, 0.0, 0.60145477594229, 0.0, 0.0, 0.03062034252430008, 0.1635016794080344, 0.7915679370358879, 0.0, 0.07317711878131836, 0.9809574403621903, 0.0, 0.0, 0.44926990137304107, 0.21608537407152922, 0.0, 0.05777727499408966, 0.039929965496851884, 0.0, 0.29915326656993935, 0.31099294960838475, 0.13414679708282617, 0.10949528327933863, 0.0, 0.10753728810972799, 0.0, 0.3797681313373244, 0.5019126906002389, 0.8077076191871431, 0.02575673641821252, 0.0, 0.0, 0.0, 0.0, 0.08748647908145589, 0.0, 0.0, 0.2775216739406583, 0.0, 0.0, 0.0, 0.0, 0.19491911889837882, 0.0, 0.626470449065492, 0.0, 0.1610321350235478, 0.0, 0.6250193931691284, 0.18518836301307043, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5645059221744597, 0.0, 0.30319370972913895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5299926460898494, 0.0, 0.0, 0.18935417444624433, 0.0, 0.3520383786660897, 0.004326690884129795, 0.4266082560323472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04753484047689839, 0.0, 0.0, 0.0, 0.2507515224933452, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011126595947621928, 0.0, 0.0, 0.0, 0.0, 0.07883374294223121, 0.0, 0.0, 0.06158319631443835, 0.0, 0.0, 0.0, 0.0, 0.18599923723367912, 0.0, 0.0, 0.0, 0.0, 0.07225540391146344, 0.0, 0.17222213577613948, 0.0, 0.07109258970520838, 0.30622054435699597, 0.0, 0.0, 0.021600783197372013, 0.0, 0.0, 0.0, 0.0, 0.08747962563792125, 0.0, 0.0, 0.6661127453751955, 0.20906059268184987, 0.0, 0.14050781836020196, 0.0, 0.0, 0.38775347026983786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016520556870922822, 0.08066242485362363, 0.0, 0.0, 0.008507877750707929, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19963971549104562, 0.0, 0.0, 0.0, 0.0, 0.1517009343251733, 0.5312750255062002, 0.0, 0.0, 0.1147421120346578, 0.0, 0.0, 0.0, 0.0, 0.35500190337758014, 0.0, 0.045597713545061126, 0.0, 0.5209586802357347, 0.021862434298379425, 0.0, 0.0, 0.0, 1.0378291015579089, 0.0, 0.0, 0.0, 0.08585205631703301, 0.0483968201096062, 0.0, 0.025471357792783925, 0.0, 0.22080976260076224, 1.166729214929465, 0.0, 0.677323308302816, 0.18535083381370177, 0.0, 0.0, 0.8704056171514745, 0.0, 0.8935663748182991, 0.015577429284835683, 0.13855228421849528, 0.0, 0.0, 0.1744139155099896, 0.15132491229634354, 0.0, 0.02513332482807071, 0.0, 0.17525145412541498, 0.0, 0.0, 0.0, 0.0, 0.02694350993020697, 0.162823516985682, 0.0, 0.0, 0.6457319525792288, 0.07238426196350153, 0.05745965468198706, 0.0, 0.0, 0.46532195926191927, 0.08460519940877058, 0.22064747420300995, 0.07874956387307873, 0.02873453791303352, 0.04102153493571428, 0.0, 0.0, 0.0, 0.0, 0.49780157894039, 0.0, 0.0, 0.301057479311315, 0.0, 0.0, 0.0016911663416444849, 0.0, 0.2606342751929628, 0.0, 0.0, 0.0, 0.4832976838898257, 0.2663912296088851, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16074512474220395, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0418820657562929, 0.0, 0.04820286684177523, 0.0, 0.1580443847885599, 0.0, 0.1278562139832909, 0.0, 0.0, 0.12981743196566292, 0.0, 0.0, 0.12101853247948517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13204408757174144, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024933507022188058, 0.0, 0.0, 0.0, 0.22607531711845172, 0.06698435016959209, 0.0, 0.0, 0.38137808720681626, 0.04452538198512276, 0.13682477248336336, 0.20418703285947667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19170326048233127, 0.0, 0.0, 0.0684911628398921, 0.0, 0.0, 0.0015650042612973304, 0.0, 0.0, 0.0, 0.0, 0.3663518735291969, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27889642218469224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09171777647431381, 0.0, 0.031206336760942092, 0.04657190125204274, 0.0, 0.21168995340334643, 0.0, 0.0, 0.2715554536912592, 0.0, 0.0, 0.0476752915188213, 0.0, 0.0, 0.0, 0.0, 0.36410628949886475, 0.021118814521733104, 0.0, 0.25524059151149175, 0.22081888489064375, 0.0, 0.0008303397905156581, 0.0, 0.0, 0.0, 0.0, 0.07229346662213895, 0.0, 0.0, 0.820696989198259, 0.541933064129384, 0.0, 0.6976372073851678, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6559268620540256, 0.0, 0.0, 0.00593765430305966, 0.0, 0.03489392321475427, 0.14772313877299487, 0.20501704987333305, 0.04206924698360076, 0.15444861072003166, 0.0, 0.8226903093195739, 0.0, 0.15641473972332146, 0.790869654262485, 0.6842130952365706, 0.03633117370294634, 0.0, 0.18980663716791113, 0.0, 0.0, 0.038558748063994376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.40339029653363123, 0.0, 0.0, 0.0, 0.0, 0.43960663038458647, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08136147927839638, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16190486570077617, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2992216626110986, 0.0, 0.0, 0.0, 0.17882109545054858, 0.0, 0.0796930212837386, 0.0, 0.620119218817229, 0.06539128749913568, 0.09575080072237417, 0.0, 0.0, 0.0, 0.31047674992819635, 0.0, 0.0, 0.3258789812741783, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03198183278340688, 0.0, 0.0, 0.05159425685765124, 0.22600144774467618, 0.0, 0.17025710615993656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27035127710875223, 0.6595462305837041, 0.0, 0.0, 0.0, 0.5146584726996519, 0.0, 0.4337121439657544, 0.0, 0.0, 0.006091807521893999, 0.0, 0.0, 0.0, 0.0, 0.012888743397998564, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.336652909816104, 0.0, 0.0, 0.33692102610329455, 0.41892538326879675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4637663755690806, 0.004049689433712162, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2537974154183676, 0.0, 0.0, 0.0, 0.01941920682276314, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2625647477298457, 0.0, 0.0, 0.0, 0.28333914754514417, 0.0, 0.009497623447163565, 0.0, 0.0, 0.015321911880061054, 0.7292697770823527, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4913667474115824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8188140725936274, 0.23538609489199455, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004251660745273512, 0.2915534718195699, 0.8234990979680312, 0.15784851698528854, 0.31856705993041073, 0.0, 0.0, 0.0, 0.0, 0.8168645454274115, 0.2887084121601819, 0.0, 0.0, 0.0, 0.020685054965967595, 0.784321469374656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5719438351419199, 0.0, 0.0, 0.0, 0.0, 0.23810583942791758, 0.0, 0.0, 0.13190421809771516, 0.2355353163505223, 0.21414939137204708, 0.0, 0.13824109301205073, 0.0, 0.0, 0.4059138329547745, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3839879400284894, 0.7642606292584734, 0.17337082964447778, 0.43085204172578645, 0.5071846921436626, 0.0, 0.0, 0.0, 0.0, 0.0064224562854441995, 0.45947466920879537, 0.0, 0.048013668076727356, 0.008826989297459209, 0.0, 0.10254269883954115, 0.0, 0.449628221574638, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3130138916400048, 0.0, 0.0, 0.0, 0.0, 0.2198473045188841, 0.0, 0.7854658854954149, 0.0, 0.0, 0.0, 0.27482511606730614, 0.0, 0.0, 0.0, 0.10051459327881847, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03619595572786272, 0.2919384825550201, 0.0, 0.27020767480793234, 0.32906141180848986, 0.4014761358710767, 0.0, 0.0, 0.22194819820968317, 0.0, 0.397060529897469, 0.20690476484626705, 0.4811671410392035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10602724763101144, 0.0, 0.9180290974697203, 0.32581440887276836, 0.11505868376784396, 0.5289668394733107, 0.0, 0.5268986674479775, 0.2483018891728648, 0.0, 0.0, 0.014289568621041568, 0.0, 0.0, 0.0, 0.0, 1.010481071564644, 0.06469540823739603, 0.19880648111582097, 0.580377081863938, 0.3701240890884847, 0.00033976241032705203, 0.5120222312956857, 0.54263963710411, 0.24775903677393285, 0.13070835967347635, 0.9915175816548458, 0.2899273698579516, 0.0, 0.0, 1.1703372022756595, 0.02329003991510475, 0.11859776061629382, 0.69977713771051, 0.6054053013517017, 0.707408250006164, 0.0, 0.2196427786747048, 0.5323095043727097, 0.027197936547729912, 0.0, 0.0, 0.6744130710240275, 0.0, 0.0, 0.009472629890788111, 0.0, 0.0, 0.0, 0.0, 0.11454144616825819, 0.0, 0.0, 0.0, 1.3982199856641506, 0.2538753491205552, 0.20096391172061562, 0.0, 0.0, 0.04239072289345119, 0.0, 0.773883065549572, 0.0, 0.0, 0.41373255777296347, 0.0, 0.0, 0.0, 0.0, 0.7328658795072667, 0.0, 0.0, 0.8143261048934798, 0.0, 0.0, 0.005931475071506779, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5479468542821576, 0.0, 0.32433512795769415, 0.7968405650512658, 0.0, 0.0, 0.2823749754870304, 0.0, 0.08888138290002096, 0.0, 0.0, 0.0, 0.17925820791427197, 0.2000876096746871, 0.0, 0.0, 0.40815111826084144, 0.16385977895936904, 0.06986036185280928, 0.0, 0.0, 0.0, 0.0, 0.05375441115395932, 0.0, 0.1762464226118615, 0.0, 0.3587964347388992, 0.0, 0.8015475518237312, 0.175514484083294, 0.0, 0.0, 0.13495628742382593, 0.0, 0.3105505922199702, 0.7248528773530782, 0.0, 0.18705292898123585, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1748071953278316, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7043431257504623, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13971141189132677, 0.0, 0.0, 0.0, 0.0, 0.0, 0.031464801155366616, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16845401890236414, 0.14573633601571054, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13998624971109908, 0.0, 0.07426203609200491, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2049851695927226, 0.0, 0.0, 0.17338949919280155, 0.0, 0.0, 0.18945472552918335, 0.0, 0.0, 0.0, 0.0, 0.0, 0.036320374032721404, 0.15648907735824633, 0.0, 0.05859334953381536, 0.0, 0.0, 0.0, 0.0, 0.5224897007715829, 0.0, 0.0, 0.0, 0.0, 0.08480845151311803, 0.0, 0.0, 0.0, 0.0, 0.07725579657733653, 0.0, 0.14447765770967222, 0.0, 0.0, 0.0, 0.15678431253622274, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.255601945951803, 0.0, 0.0, 0.2867970808516741, 0.3376079838842544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.29929546323980694, 0.0, 0.0, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = parallel(_)
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = ((ex.bodies[1]).bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    result = nothing
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = (Finch).moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = (Finch).moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            B_lvl_q = B_lvl_ptr[1]
            B_lvl_q_stop = B_lvl_ptr[1 + 1]
            if B_lvl_q < B_lvl_q_stop
                B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
            else
                B_lvl_i_stop = 0
            end
            phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                if B_lvl_tbl2[B_lvl_q] < phase_start_2
                    B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                end
                while true
                    B_lvl_i = B_lvl_tbl2[B_lvl_q]
                    B_lvl_q_step = B_lvl_q
                    if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                        B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    if B_lvl_i < phase_stop_2
                        Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                        val_4 = Ct_lvl_2_val
                        Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                        A_lvl_ptr_3 = A_lvl_ptr
                        A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                        A_lvl_tbl1_3 = A_lvl_tbl1
                        A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                        A_lvl_tbl2_3 = A_lvl_tbl2
                        A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                        val_5 = A_lvl_val
                        A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
                        B_lvl_ptr_3 = B_lvl_ptr
                        B_lvl_tbl1_3 = B_lvl_tbl1
                        B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                        B_lvl_tbl2_3 = B_lvl_tbl2
                        val_6 = B_lvl_val
                        B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
                        Threads.@threads for i_10 = 1:Threads.nthreads()
                                phase_start_7 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_10), Threads.nthreads()))
                                phase_stop_8 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_10, Threads.nthreads()))
                                if phase_stop_8 >= phase_start_7
                                    for i_13 = phase_start_7:phase_stop_8
                                        Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_13
                                        A_lvl_q = A_lvl_ptr[1]
                                        A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                        if A_lvl_q < A_lvl_q_stop
                                            A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                        else
                                            A_lvl_i_stop = 0
                                        end
                                        B_lvl_q_3 = B_lvl_q
                                        if B_lvl_q < B_lvl_q_step
                                            B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                        else
                                            B_lvl_i_stop_3 = 0
                                        end
                                        phase_stop_9 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                        if phase_stop_9 >= 1
                                            k = 1
                                            if A_lvl_tbl2[A_lvl_q] < 1
                                                A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            if B_lvl_tbl1[B_lvl_q] < 1
                                                B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                            end
                                            while k <= phase_stop_9
                                                A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                                A_lvl_q_step = A_lvl_q
                                                if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                    A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                                end
                                                B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                                phase_stop_10 = min(B_lvl_i_3, phase_stop_9, A_lvl_i)
                                                if A_lvl_i == phase_stop_10 && B_lvl_i_3 == phase_stop_10
                                                    B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                                    A_lvl_q_2 = A_lvl_q
                                                    if A_lvl_q < A_lvl_q_step
                                                        A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                    else
                                                        A_lvl_i_stop_2 = 0
                                                    end
                                                    phase_stop_11 = min(i_13, A_lvl_i_stop_2)
                                                    if phase_stop_11 >= i_13
                                                        if A_lvl_tbl1[A_lvl_q] < i_13
                                                            A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_13, A_lvl_q, A_lvl_q_step - 1)
                                                        end
                                                        while true
                                                            A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                            if A_lvl_i_2 < phase_stop_11
                                                                A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                                Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                                A_lvl_q_2 += 1
                                                            else
                                                                phase_stop_13 = min(A_lvl_i_2, phase_stop_11)
                                                                if A_lvl_i_2 == phase_stop_13
                                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                                    Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                                    A_lvl_q_2 += 1
                                                                end
                                                                break
                                                            end
                                                        end
                                                    end
                                                    A_lvl_q = A_lvl_q_step
                                                    B_lvl_q_3 += 1
                                                elseif B_lvl_i_3 == phase_stop_10
                                                    B_lvl_q_3 += 1
                                                elseif A_lvl_i == phase_stop_10
                                                    A_lvl_q = A_lvl_q_step
                                                end
                                                k = phase_stop_10 + 1
                                            end
                                        end
                                    end
                                end
                            end
                        Ct_lvl_2_val = val_4
                        A_lvl_ptr = A_lvl_ptr_3
                        A_lvl_tbl1 = A_lvl_tbl1_3
                        A_lvl_tbl2 = A_lvl_tbl2_3
                        A_lvl_val = val_5
                        B_lvl_ptr = B_lvl_ptr_3
                        B_lvl_tbl1 = B_lvl_tbl1_3
                        B_lvl_tbl2 = B_lvl_tbl2_3
                        B_lvl_val = val_6
                        B_lvl_q = B_lvl_q_step
                    else
                        phase_stop_19 = min(B_lvl_i, phase_stop_2)
                        if B_lvl_i == phase_stop_19
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_19
                            val_7 = Ct_lvl_2_val
                            Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                            A_lvl_ptr_4 = A_lvl_ptr
                            A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                            A_lvl_tbl1_4 = A_lvl_tbl1
                            A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                            A_lvl_tbl2_4 = A_lvl_tbl2
                            A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                            val_8 = A_lvl_val
                            A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
                            B_lvl_ptr_4 = B_lvl_ptr
                            B_lvl_tbl1_4 = B_lvl_tbl1
                            B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                            B_lvl_tbl2_4 = B_lvl_tbl2
                            val_9 = B_lvl_val
                            B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
                            Threads.@threads for i_20 = 1:Threads.nthreads()
                                    phase_start_22 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_20), Threads.nthreads()))
                                    phase_stop_24 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_20, Threads.nthreads()))
                                    if phase_stop_24 >= phase_start_22
                                        for i_23 = phase_start_22:phase_stop_24
                                            Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_23
                                            A_lvl_q = A_lvl_ptr[1]
                                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                            if A_lvl_q < A_lvl_q_stop
                                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                            else
                                                A_lvl_i_stop = 0
                                            end
                                            B_lvl_q_3 = B_lvl_q
                                            if B_lvl_q < B_lvl_q_step
                                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                            else
                                                B_lvl_i_stop_3 = 0
                                            end
                                            phase_stop_25 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                            if phase_stop_25 >= 1
                                                k = 1
                                                if A_lvl_tbl2[A_lvl_q] < 1
                                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                                end
                                                if B_lvl_tbl1[B_lvl_q] < 1
                                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                                end
                                                while k <= phase_stop_25
                                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                                    A_lvl_q_step = A_lvl_q
                                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                                    end
                                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                                    phase_stop_26 = min(B_lvl_i_3, A_lvl_i, phase_stop_25)
                                                    if A_lvl_i == phase_stop_26 && B_lvl_i_3 == phase_stop_26
                                                        B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                                        A_lvl_q_4 = A_lvl_q
                                                        if A_lvl_q < A_lvl_q_step
                                                            A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                        else
                                                            A_lvl_i_stop_4 = 0
                                                        end
                                                        phase_stop_27 = min(i_23, A_lvl_i_stop_4)
                                                        if phase_stop_27 >= i_23
                                                            if A_lvl_tbl1[A_lvl_q] < i_23
                                                                A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_23, A_lvl_q, A_lvl_q_step - 1)
                                                            end
                                                            while true
                                                                A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                                if A_lvl_i_4 < phase_stop_27
                                                                    A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                    Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                                    A_lvl_q_4 += 1
                                                                else
                                                                    phase_stop_29 = min(A_lvl_i_4, phase_stop_27)
                                                                    if A_lvl_i_4 == phase_stop_29
                                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                                        A_lvl_q_4 += 1
                                                                    end
                                                                    break
                                                                end
                                                            end
                                                        end
                                                        A_lvl_q = A_lvl_q_step
                                                        B_lvl_q_3 += 1
                                                    elseif B_lvl_i_3 == phase_stop_26
                                                        B_lvl_q_3 += 1
                                                    elseif A_lvl_i == phase_stop_26
                                                        A_lvl_q = A_lvl_q_step
                                                    end
                                                    k = phase_stop_26 + 1
                                                end
                                            end
                                        end
                                    end
                                end
                            Ct_lvl_2_val = val_7
                            A_lvl_ptr = A_lvl_ptr_4
                            A_lvl_tbl1 = A_lvl_tbl1_4
                            A_lvl_tbl2 = A_lvl_tbl2_4
                            A_lvl_val = val_8
                            B_lvl_ptr = B_lvl_ptr_4
                            B_lvl_tbl1 = B_lvl_tbl1_4
                            B_lvl_tbl2 = B_lvl_tbl2_4
                            B_lvl_val = val_9
                            B_lvl_q = B_lvl_q_step
                        end
                        break
                    end
                end
            end
        end
    resize!(val, A_lvl.shape[1] * B_lvl.shape[2])
    result = (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
    result
end
julia> @finch begin
        Ct .= 0
        for j = parallel(_)
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0, 0.0, 0.1619393094464903, 0.0, 0.0, 0.0, 0.0, 0.11373910752308043, 0.0, 0.32533429668145175, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5405512925681031, 0.0, 0.0, 0.5409817971066755, 0.6726532010052559, 0.0, 0.0, 0.0, 0.04498001780375231, 0.0, 0.7446527460595233, 0.0, 0.0, 0.0, 0.0648053115514859, 0.0, 0.20542126000594346, 0.0, 0.2489342378383824, 0.0, 0.0, 0.2446702787341799, 0.0, 0.0, 0.4075132486743765, 0.372693911014397, 0.0, 0.03990868671702989, 0.32865292999993995, 0.0, 0.0, 0.0, 0.037570880212435515, 0.0, 0.0, 0.2585440952996821, 0.0, 0.0, 0.5429032184680616, 0.0, 0.0, 0.0, 0.7310331654112876, 1.6974310666890076, 0.13434284643730737, 0.0, 0.15388431962676646, 0.0, 0.018799628425153446, 0.026448056346556916, 0.020943024178172204, 0.08671614507629592, 0.19272805440861934, 0.07015243227726202, 0.0, 0.1267665206913561, 0.1014023431949165, 0.494724963965631, 0.0, 0.6317423070706343, 0.0, 0.4351905463586081, 0.019451367524410516, 0.11392026534019284, 0.0, 0.7674183972620773, 0.0, 0.0, 0.009728790198518009, 0.0, 0.0, 0.03627546054320975, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0140288190005624, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4229718175600567, 0.0752980218883111, 0.013070459322559402, 0.0, 0.4793455258509991, 0.0, 0.0, 0.0, 0.0, 0.08811633730455204, 0.08467555208376226, 0.0, 0.0, 1.4824413586764027, 0.27442978968992093, 0.0, 0.0028104905637611507, 0.1480443868451215, 0.6744061467564826, 0.043998508948338316, 0.8697701045107844, 0.8469834954000914, 0.0, 0.02425613445444166, 0.0, 0.0, 0.0, 0.9679232213808633, 0.6774894982888419, 0.35310753741950685, 0.0, 0.2841919624705342, 0.0, 0.0, 0.7349602036514661, 0.0, 0.2186035505612502, 0.04416466424315345, 0.0, 0.0, 0.25133653271826356, 0.11160837067772013, 0.0, 0.14048007630402903, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.035048636482553235, 0.038653696504083, 0.10014641420794519, 0.0, 0.0837599459565703, 0.10942537757376403, 0.0, 0.0, 0.0, 0.08204362658289192, 0.0, 0.0, 0.011591138384550418, 0.0, 0.038004261053570694, 0.0066189468466051875, 0.06306237088337384, 0.0, 0.0, 0.03121664575636558, 0.0, 0.0, 0.02910081181829685, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08506408857544866, 0.0, 0.0, 0.0, 0.7712870388819855, 0.2285263236705729, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6966121466049721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6540220401149287, 0.0, 0.0, 0.2336670223432043, 0.0, 0.0, 0.005339227289024509, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4930665741229495, 0.0, 0.029559863815954657, 0.0, 0.0, 0.10760363562562023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15061269543615943, 0.6107211710602042, 0.0, 0.0, 0.733057860494869, 0.057362855000467054, 0.17650444882735677, 0.0, 0.0, 0.014677698725170102, 0.0, 0.17821555119599314, 0.0025555492556658746, 0.14808219427948163, 0.0, 0.11979690859781277, 0.0, 0.0, 0.13110894332781436, 0.04449679654819995, 0.0, 0.1133902344080122, 0.0, 0.0, 0.0017495044850376442, 0.013900993801975209, 0.03729472110086185, 0.19173933242455474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1709634942172136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13051636625786148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4357713541613571, 0.0, 0.0, 0.0, 0.0, 0.07073267416682687, 0.0, 0.0, 0.0, 0.0, 0.06443354393704691, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28238918313491895, 0.0, 0.0, 0.0, 0.28219752132848275, 0.0, 0.0, 0.0, 0.3045252869011713, 0.0, 0.0, 0.019083741024338748, 0.0, 0.06257051306836219, 0.7837995208867943, 0.05061887468267648, 0.0, 0.0, 0.05139533007879884, 0.0, 0.0, 0.04791180451081585, 0.0, 0.5281077501137272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7571506519409951, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.026654196669850944, 0.0, 0.0, 0.0, 0.0, 0.23843473688023176, 0.0, 0.0, 0.6051386294410329, 0.0, 0.0, 0.0, 0.0, 0.04194764240896927, 0.13628497764809444, 0.0, 0.0, 0.0, 0.0, 0.06043639304510901, 0.04344269031549197, 0.0, 0.7296328391677986, 0.6312346675367022, 0.0, 0.0, 0.22817557354519388, 0.0, 0.05073205279897938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1594527848545218, 0.1381227206057285, 0.0, 0.0, 0.0, 0.0, 0.4217049712314433, 0.0, 0.0, 0.31027130374464373, 0.0, 0.0, 0.08118863628632204, 0.8304208532182848, 0.983137215831116, 0.0, 0.24897028906526258, 0.14034260442091134, 0.0, 0.691731307052974, 0.7500207108166317, 0.833859795824118, 0.0, 0.05053828036951065, 0.8139843305766208, 0.0, 0.0, 1.3856822465181273, 1.0343729450047436, 0.0, 0.0, 1.09811525132565, 0.0, 0.0, 0.005748580563865145, 0.0, 0.0, 0.8634025361175751, 0.0, 0.14616020357939458, 0.7514377111323272, 0.0, 0.0, 0.1732108049991939, 0.0, 0.0, 0.0, 0.1879649588465491, 0.0, 0.0, 0.0, 0.00817965091901661, 0.0, 0.0, 0.0, 0.5779500058438248, 0.0, 0.0, 0.0, 0.005249336837927155, 0.0, 0.0, 0.0, 0.3064350538312218, 0.0, 0.001890320177282059, 0.45976772975798436, 0.40475012946470096, 0.3442380471075082, 0.017185108496187197, 0.288035784498987, 0.0, 0.0, 0.2827566402954316, 0.0, 0.0, 0.2635916697723135, 0.0, 0.0, 0.0, 0.3588181656747819, 0.0, 0.0, 0.0, 0.17570163169455902, 0.0, 0.2076868163288207, 0.0, 0.0, 0.0, 0.5097341557191223, 0.14587016095283192, 0.0, 0.4172405362928458, 0.0, 0.13951021052730417, 0.0, 0.6033798708626249, 0.0, 0.0, 0.0, 0.0, 0.4512081190066747, 1.1915177253624374, 0.0, 0.53626862172996, 0.26267190873847124, 0.024946529615964698, 0.630282563668714, 0.0, 0.7072072219187049, 0.2557442098535857, 0.0, 0.0, 0.0, 0.027439980552710016, 0.0, 0.2634523244708942, 0.750190928083991, 0.31925762502388877, 0.0, 0.0, 0.0, 0.17667054707398389, 1.689395274403252, 0.0, 0.5386556936239477, 0.21507940395745773, 0.30698899212442304, 0.11255638742238075, 0.12042909174810443, 0.1983506487023661, 0.0, 0.2156156778918424, 0.0, 0.6179686549930762, 0.0, 0.0, 0.0, 0.011162873807723371, 0.003307472295366656, 0.5417377310206691, 0.1038404263557644, 0.0, 0.0, 0.0, 0.010082100558503525, 0.5759566406752274, 0.5373731999014957, 0.24497595431253502, 0.3628321361534925, 0.4271139218413104, 0.0888079037091352, 0.013607634508588288, 0.5254304747208627, 0.0, 0.0, 0.0033818738704913948, 0.0, 0.3894178985059293, 7.727488918331624e-5, 0.4719056234120719, 0.0, 0.0, 0.7548965169480935, 0.0, 0.0, 0.0, 0.0, 0.04295271297559638, 0.3048341885265102, 0.0, 0.16015654700904233, 0.0, 0.03255288417829475, 0.0, 0.0, 0.0, 0.0, 0.16019347860889097, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3426629556463706, 0.0, 0.33591507158653605, 0.0, 0.0, 0.04025140372585128, 0.0, 0.5756230062735465, 0.0, 0.0, 0.0, 0.0, 0.0017523850398583757, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32421546356710995, 0.0, 0.011282618188498251, 0.6937716630434633, 0.2529924781123921, 0.2596235239889639, 0.0, 0.043120476743740394, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2793513283874615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7775441119273626, 0.14515455310675088, 0.0, 0.13510821830370037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44716225485327055, 0.0, 0.0, 0.0, 0.8291790553392753, 0.0, 0.0, 0.26334889102262177, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15479667093058366, 0.0, 0.0, 0.0, 0.36993623982653, 0.0, 0.0, 0.0, 0.0, 0.3623559014178924, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029233403628440607, 0.1427335193249464, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051251957334763225, 0.691166524717783, 0.29938181148714105, 0.5349464228115411, 0.06157899241792075, 0.0, 0.0, 0.2102727260204982, 0.0, 0.60145477594229, 0.0, 0.0, 0.03062034252430008, 0.1635016794080344, 0.7915679370358879, 0.0, 0.07317711878131836, 0.9809574403621903, 0.0, 0.0, 0.44926990137304107, 0.21608537407152922, 0.0, 0.05777727499408966, 0.039929965496851884, 0.0, 0.29915326656993935, 0.31099294960838475, 0.13414679708282617, 0.10949528327933863, 0.0, 0.10753728810972799, 0.0, 0.3797681313373244, 0.5019126906002389, 0.8077076191871431, 0.02575673641821252, 0.0, 0.0, 0.0, 0.0, 0.08748647908145589, 0.0, 0.0, 0.2775216739406583, 0.0, 0.0, 0.0, 0.0, 0.19491911889837882, 0.0, 0.626470449065492, 0.0, 0.1610321350235478, 0.0, 0.6250193931691284, 0.18518836301307043, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5645059221744597, 0.0, 0.30319370972913895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5299926460898494, 0.0, 0.0, 0.18935417444624433, 0.0, 0.3520383786660897, 0.004326690884129795, 0.4266082560323472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04753484047689839, 0.0, 0.0, 0.0, 0.2507515224933452, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011126595947621928, 0.0, 0.0, 0.0, 0.0, 0.07883374294223121, 0.0, 0.0, 0.06158319631443835, 0.0, 0.0, 0.0, 0.0, 0.18599923723367912, 0.0, 0.0, 0.0, 0.0, 0.07225540391146344, 0.0, 0.17222213577613948, 0.0, 0.07109258970520838, 0.30622054435699597, 0.0, 0.0, 0.021600783197372013, 0.0, 0.0, 0.0, 0.0, 0.08747962563792125, 0.0, 0.0, 0.6661127453751955, 0.20906059268184987, 0.0, 0.14050781836020196, 0.0, 0.0, 0.38775347026983786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016520556870922822, 0.08066242485362363, 0.0, 0.0, 0.008507877750707929, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19963971549104562, 0.0, 0.0, 0.0, 0.0, 0.1517009343251733, 0.5312750255062002, 0.0, 0.0, 0.1147421120346578, 0.0, 0.0, 0.0, 0.0, 0.35500190337758014, 0.0, 0.045597713545061126, 0.0, 0.5209586802357347, 0.021862434298379425, 0.0, 0.0, 0.0, 1.0378291015579089, 0.0, 0.0, 0.0, 0.08585205631703301, 0.0483968201096062, 0.0, 0.025471357792783925, 0.0, 0.22080976260076224, 1.166729214929465, 0.0, 0.677323308302816, 0.18535083381370177, 0.0, 0.0, 0.8704056171514745, 0.0, 0.8935663748182991, 0.015577429284835683, 0.13855228421849528, 0.0, 0.0, 0.1744139155099896, 0.15132491229634354, 0.0, 0.02513332482807071, 0.0, 0.17525145412541498, 0.0, 0.0, 0.0, 0.0, 0.02694350993020697, 0.162823516985682, 0.0, 0.0, 0.6457319525792288, 0.07238426196350153, 0.05745965468198706, 0.0, 0.0, 0.46532195926191927, 0.08460519940877058, 0.22064747420300995, 0.07874956387307873, 0.02873453791303352, 0.04102153493571428, 0.0, 0.0, 0.0, 0.0, 0.49780157894039, 0.0, 0.0, 0.301057479311315, 0.0, 0.0, 0.0016911663416444849, 0.0, 0.2606342751929628, 0.0, 0.0, 0.0, 0.4832976838898257, 0.2663912296088851, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16074512474220395, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0418820657562929, 0.0, 0.04820286684177523, 0.0, 0.1580443847885599, 0.0, 0.1278562139832909, 0.0, 0.0, 0.12981743196566292, 0.0, 0.0, 0.12101853247948517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13204408757174144, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024933507022188058, 0.0, 0.0, 0.0, 0.22607531711845172, 0.06698435016959209, 0.0, 0.0, 0.38137808720681626, 0.04452538198512276, 0.13682477248336336, 0.20418703285947667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19170326048233127, 0.0, 0.0, 0.0684911628398921, 0.0, 0.0, 0.0015650042612973304, 0.0, 0.0, 0.0, 0.0, 0.3663518735291969, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27889642218469224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09171777647431381, 0.0, 0.031206336760942092, 0.04657190125204274, 0.0, 0.21168995340334643, 0.0, 0.0, 0.2715554536912592, 0.0, 0.0, 0.0476752915188213, 0.0, 0.0, 0.0, 0.0, 0.36410628949886475, 0.021118814521733104, 0.0, 0.25524059151149175, 0.22081888489064375, 0.0, 0.0008303397905156581, 0.0, 0.0, 0.0, 0.0, 0.07229346662213895, 0.0, 0.0, 0.820696989198259, 0.541933064129384, 0.0, 0.6976372073851678, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6559268620540256, 0.0, 0.0, 0.00593765430305966, 0.0, 0.03489392321475427, 0.14772313877299487, 0.20501704987333305, 0.04206924698360076, 0.15444861072003166, 0.0, 0.8226903093195739, 0.0, 0.15641473972332146, 0.790869654262485, 0.6842130952365706, 0.03633117370294634, 0.0, 0.18980663716791113, 0.0, 0.0, 0.038558748063994376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.40339029653363123, 0.0, 0.0, 0.0, 0.0, 0.43960663038458647, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08136147927839638, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16190486570077617, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2992216626110986, 0.0, 0.0, 0.0, 0.17882109545054858, 0.0, 0.0796930212837386, 0.0, 0.620119218817229, 0.06539128749913568, 0.09575080072237417, 0.0, 0.0, 0.0, 0.31047674992819635, 0.0, 0.0, 0.3258789812741783, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03198183278340688, 0.0, 0.0, 0.05159425685765124, 0.22600144774467618, 0.0, 0.17025710615993656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27035127710875223, 0.6595462305837041, 0.0, 0.0, 0.0, 0.5146584726996519, 0.0, 0.4337121439657544, 0.0, 0.0, 0.006091807521893999, 0.0, 0.0, 0.0, 0.0, 0.012888743397998564, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.336652909816104, 0.0, 0.0, 0.33692102610329455, 0.41892538326879675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4637663755690806, 0.004049689433712162, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2537974154183676, 0.0, 0.0, 0.0, 0.01941920682276314, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2625647477298457, 0.0, 0.0, 0.0, 0.28333914754514417, 0.0, 0.009497623447163565, 0.0, 0.0, 0.015321911880061054, 0.7292697770823527, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4913667474115824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8188140725936274, 0.23538609489199455, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004251660745273512, 0.2915534718195699, 0.8234990979680312, 0.15784851698528854, 0.31856705993041073, 0.0, 0.0, 0.0, 0.0, 0.8168645454274115, 0.2887084121601819, 0.0, 0.0, 0.0, 0.020685054965967595, 0.784321469374656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5719438351419199, 0.0, 0.0, 0.0, 0.0, 0.23810583942791758, 0.0, 0.0, 0.13190421809771516, 0.2355353163505223, 0.21414939137204708, 0.0, 0.13824109301205073, 0.0, 0.0, 0.4059138329547745, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3839879400284894, 0.7642606292584734, 0.17337082964447778, 0.43085204172578645, 0.5071846921436626, 0.0, 0.0, 0.0, 0.0, 0.0064224562854441995, 0.45947466920879537, 0.0, 0.048013668076727356, 0.008826989297459209, 0.0, 0.10254269883954115, 0.0, 0.449628221574638, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3130138916400048, 0.0, 0.0, 0.0, 0.0, 0.2198473045188841, 0.0, 0.7854658854954149, 0.0, 0.0, 0.0, 0.27482511606730614, 0.0, 0.0, 0.0, 0.10051459327881847, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03619595572786272, 0.2919384825550201, 0.0, 0.27020767480793234, 0.32906141180848986, 0.4014761358710767, 0.0, 0.0, 0.22194819820968317, 0.0, 0.397060529897469, 0.20690476484626705, 0.4811671410392035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10602724763101144, 0.0, 0.9180290974697203, 0.32581440887276836, 0.11505868376784396, 0.5289668394733107, 0.0, 0.5268986674479775, 0.2483018891728648, 0.0, 0.0, 0.014289568621041568, 0.0, 0.0, 0.0, 0.0, 1.010481071564644, 0.06469540823739603, 0.19880648111582097, 0.580377081863938, 0.3701240890884847, 0.00033976241032705203, 0.5120222312956857, 0.54263963710411, 0.24775903677393285, 0.13070835967347635, 0.9915175816548458, 0.2899273698579516, 0.0, 0.0, 1.1703372022756595, 0.02329003991510475, 0.11859776061629382, 0.69977713771051, 0.6054053013517017, 0.707408250006164, 0.0, 0.2196427786747048, 0.5323095043727097, 0.027197936547729912, 0.0, 0.0, 0.6744130710240275, 0.0, 0.0, 0.009472629890788111, 0.0, 0.0, 0.0, 0.0, 0.11454144616825819, 0.0, 0.0, 0.0, 1.3982199856641506, 0.2538753491205552, 0.20096391172061562, 0.0, 0.0, 0.04239072289345119, 0.0, 0.773883065549572, 0.0, 0.0, 0.41373255777296347, 0.0, 0.0, 0.0, 0.0, 0.7328658795072667, 0.0, 0.0, 0.8143261048934798, 0.0, 0.0, 0.005931475071506779, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5479468542821576, 0.0, 0.32433512795769415, 0.7968405650512658, 0.0, 0.0, 0.2823749754870304, 0.0, 0.08888138290002096, 0.0, 0.0, 0.0, 0.17925820791427197, 0.2000876096746871, 0.0, 0.0, 0.40815111826084144, 0.16385977895936904, 0.06986036185280928, 0.0, 0.0, 0.0, 0.0, 0.05375441115395932, 0.0, 0.1762464226118615, 0.0, 0.3587964347388992, 0.0, 0.8015475518237312, 0.175514484083294, 0.0, 0.0, 0.13495628742382593, 0.0, 0.3105505922199702, 0.7248528773530782, 0.0, 0.18705292898123585, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1748071953278316, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7043431257504623, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13971141189132677, 0.0, 0.0, 0.0, 0.0, 0.0, 0.031464801155366616, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16845401890236414, 0.14573633601571054, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13998624971109908, 0.0, 0.07426203609200491, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2049851695927226, 0.0, 0.0, 0.17338949919280155, 0.0, 0.0, 0.18945472552918335, 0.0, 0.0, 0.0, 0.0, 0.0, 0.036320374032721404, 0.15648907735824633, 0.0, 0.05859334953381536, 0.0, 0.0, 0.0, 0.0, 0.5224897007715829, 0.0, 0.0, 0.0, 0.0, 0.08480845151311803, 0.0, 0.0, 0.0, 0.0, 0.07725579657733653, 0.0, 0.14447765770967222, 0.0, 0.0, 0.0, 0.15678431253622274, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.255601945951803, 0.0, 0.0, 0.2867970808516741, 0.3376079838842544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.29929546323980694, 0.0, 0.0, 0.0]), 42), 42)),)

