julia> @finch begin
        CR .= 0
        for i = _
            for j = _
                for k = _
                    CR[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(CR = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            phase_start_2 = max(1, 1 + fld(A_lvl.shape[1] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                for i_7 = phase_start_2:phase_stop_2
                    B_lvl_q = B_lvl_ptr[1]
                    B_lvl_q_stop = B_lvl_ptr[1 + 1]
                    if B_lvl_q < B_lvl_q_stop
                        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                    else
                        B_lvl_i_stop = 0
                    end
                    phase_stop_3 = min(B_lvl.shape[2], B_lvl_i_stop)
                    if phase_stop_3 >= 1
                        if B_lvl_tbl2[B_lvl_q] < 1
                            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        while true
                            B_lvl_i = B_lvl_tbl2[B_lvl_q]
                            B_lvl_q_step = B_lvl_q
                            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                            end
                            if B_lvl_i < phase_stop_3
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_5 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_5 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_5
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_6 = min(B_lvl_i_2, phase_stop_5, A_lvl_i)
                                        if A_lvl_i == phase_stop_6 && B_lvl_i_2 == phase_stop_6
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_7 = min(i_7, A_lvl_i_stop_2)
                                            if phase_stop_7 >= i_7
                                                if A_lvl_tbl1[A_lvl_q] < i_7
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_7
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_9 = min(A_lvl_i_2, phase_stop_7)
                                                        if A_lvl_i_2 == phase_stop_9
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_6
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_6
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_6 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            else
                                phase_stop_14 = min(B_lvl_i, phase_stop_3)
                                if B_lvl_i == phase_stop_14
                                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_14
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_15 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_15 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_15
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_16 = min(B_lvl_i_2, A_lvl_i, phase_stop_15)
                                            if A_lvl_i == phase_stop_16 && B_lvl_i_2 == phase_stop_16
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_17 = min(i_7, A_lvl_i_stop_4)
                                                if phase_stop_17 >= i_7
                                                    if A_lvl_tbl1[A_lvl_q] < i_7
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_17
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_19 = min(A_lvl_i_4, phase_stop_17)
                                                            if A_lvl_i_4 == phase_stop_19
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_16
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_16
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_16 + 1
                                        end
                                    end
                                    B_lvl_q = B_lvl_q_step
                                end
                                break
                            end
                        end
                    end
                end
            end
        end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    for i_4 = 1:A_lvl.shape[1]
        val = Ct_lvl_2_val
        Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
        B_lvl_ptr_2 = B_lvl_ptr
        B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
        B_lvl_tbl1_2 = B_lvl_tbl1
        B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
        B_lvl_tbl2_2 = B_lvl_tbl2
        B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
        val_2 = B_lvl_val
        B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
        A_lvl_ptr_2 = A_lvl_ptr
        A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
        A_lvl_tbl1_2 = A_lvl_tbl1
        A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
        A_lvl_tbl2_2 = A_lvl_tbl2
        A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
        val_3 = A_lvl_val
        A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
        Threads.@threads for i_5 = 1:Threads.nthreads()
                B_lvl_q = B_lvl_ptr[1]
                B_lvl_q_stop = B_lvl_ptr[1 + 1]
                if B_lvl_q < B_lvl_q_stop
                    B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                else
                    B_lvl_i_stop = 0
                end
                phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_5 + -1), Threads.nthreads()))
                phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_5, Threads.nthreads()))
                if phase_stop_2 >= phase_start_2
                    if B_lvl_tbl2[B_lvl_q] < phase_start_2
                        B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    while true
                        B_lvl_i = B_lvl_tbl2[B_lvl_q]
                        B_lvl_q_step = B_lvl_q
                        if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                            B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        if B_lvl_i < phase_stop_2
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_4, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_4
                                            if A_lvl_tbl1[A_lvl_q] < i_4
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        else
                            phase_stop_13 = min(B_lvl_i, phase_stop_2)
                            if B_lvl_i == phase_stop_13
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_4, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_4
                                                if A_lvl_tbl1[A_lvl_q] < i_4
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            end
                            break
                        end
                    end
                end
            end
        Ct_lvl_2_val = val
        B_lvl_ptr = B_lvl_ptr_2
        B_lvl_tbl1 = B_lvl_tbl1_2
        B_lvl_tbl2 = B_lvl_tbl2_2
        B_lvl_val = val_2
        A_lvl_ptr = A_lvl_ptr_2
        A_lvl_tbl1 = A_lvl_tbl1_2
        A_lvl_tbl2 = A_lvl_tbl2_2
        A_lvl_val = val_3
    end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(Ct_lvl_2_val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            B_lvl_q = B_lvl_ptr[1]
            B_lvl_q_stop = B_lvl_ptr[1 + 1]
            if B_lvl_q < B_lvl_q_stop
                B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
            else
                B_lvl_i_stop = 0
            end
            phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                if B_lvl_tbl2[B_lvl_q] < phase_start_2
                    B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                end
                while true
                    B_lvl_i = B_lvl_tbl2[B_lvl_q]
                    B_lvl_q_step = B_lvl_q
                    if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                        B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    if B_lvl_i < phase_stop_2
                        Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                        for i_6 = 1:A_lvl.shape[1]
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_6
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_6, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_6
                                            if A_lvl_tbl1[A_lvl_q] < i_6
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_6, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                        end
                        B_lvl_q = B_lvl_q_step
                    else
                        phase_stop_13 = min(B_lvl_i, phase_stop_2)
                        if B_lvl_i == phase_stop_13
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                            for i_8 = 1:A_lvl.shape[1]
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_8
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_8, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_8
                                                if A_lvl_tbl1[A_lvl_q] < i_8
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_8, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        end
                        break
                    end
                end
            end
        end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    B_lvl_q = B_lvl_ptr[1]
    B_lvl_q_stop = B_lvl_ptr[1 + 1]
    if B_lvl_q < B_lvl_q_stop
        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
    else
        B_lvl_i_stop = 0
    end
    phase_stop = min(B_lvl.shape[2], B_lvl_i_stop)
    if phase_stop >= 1
        if B_lvl_tbl2[B_lvl_q] < 1
            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
        end
        while true
            B_lvl_i = B_lvl_tbl2[B_lvl_q]
            B_lvl_q_step = B_lvl_q
            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
            end
            if B_lvl_i < phase_stop
                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                val = Ct_lvl_2_val
                Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                B_lvl_tbl1_2 = B_lvl_tbl1
                B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                B_lvl_tbl2_2 = B_lvl_tbl2
                val_2 = B_lvl_val
                B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                A_lvl_ptr_2 = A_lvl_ptr
                A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                A_lvl_tbl1_2 = A_lvl_tbl1
                A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                A_lvl_tbl2_2 = A_lvl_tbl2
                A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                val_3 = A_lvl_val
                A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                Threads.@threads for i_9 = 1:Threads.nthreads()
                        phase_start_6 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_9), Threads.nthreads()))
                        phase_stop_7 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_9, Threads.nthreads()))
                        if phase_stop_7 >= phase_start_6
                            for i_12 = phase_start_6:phase_stop_7
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_12
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_8 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_8 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_8
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_9 = min(B_lvl_i_2, phase_stop_8, A_lvl_i)
                                        if A_lvl_i == phase_stop_9 && B_lvl_i_2 == phase_stop_9
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_10 = min(i_12, A_lvl_i_stop_2)
                                            if phase_stop_10 >= i_12
                                                if A_lvl_tbl1[A_lvl_q] < i_12
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_12, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_10
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_12 = min(A_lvl_i_2, phase_stop_10)
                                                        if A_lvl_i_2 == phase_stop_12
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_9
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_9
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_9 + 1
                                    end
                                end
                            end
                        end
                    end
                Ct_lvl_2_val = val
                B_lvl_tbl1 = B_lvl_tbl1_2
                B_lvl_tbl2 = B_lvl_tbl2_2
                B_lvl_val = val_2
                A_lvl_ptr = A_lvl_ptr_2
                A_lvl_tbl1 = A_lvl_tbl1_2
                A_lvl_tbl2 = A_lvl_tbl2_2
                A_lvl_val = val_3
                B_lvl_q = B_lvl_q_step
            else
                phase_stop_18 = min(B_lvl_i, phase_stop)
                if B_lvl_i == phase_stop_18
                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_18
                    val_4 = Ct_lvl_2_val
                    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                    B_lvl_tbl1_3 = B_lvl_tbl1
                    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                    B_lvl_tbl2_3 = B_lvl_tbl2
                    val_5 = B_lvl_val
                    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                    A_lvl_ptr_3 = A_lvl_ptr
                    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                    A_lvl_tbl1_3 = A_lvl_tbl1
                    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                    A_lvl_tbl2_3 = A_lvl_tbl2
                    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                    val_6 = A_lvl_val
                    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                    Threads.@threads for i_19 = 1:Threads.nthreads()
                            phase_start_21 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_19), Threads.nthreads()))
                            phase_stop_23 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_19, Threads.nthreads()))
                            if phase_stop_23 >= phase_start_21
                                for i_22 = phase_start_21:phase_stop_23
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_22
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_24 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_24 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_24
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_25 = min(B_lvl_i_2, A_lvl_i, phase_stop_24)
                                            if A_lvl_i == phase_stop_25 && B_lvl_i_2 == phase_stop_25
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_26 = min(i_22, A_lvl_i_stop_4)
                                                if phase_stop_26 >= i_22
                                                    if A_lvl_tbl1[A_lvl_q] < i_22
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_22, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_26
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_28 = min(A_lvl_i_4, phase_stop_26)
                                                            if A_lvl_i_4 == phase_stop_28
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_25
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_25
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_25 + 1
                                        end
                                    end
                                end
                            end
                        end
                    Ct_lvl_2_val = val_4
                    B_lvl_tbl1 = B_lvl_tbl1_3
                    B_lvl_tbl2 = B_lvl_tbl2_3
                    B_lvl_val = val_5
                    A_lvl_ptr = A_lvl_ptr_3
                    A_lvl_tbl1 = A_lvl_tbl1_3
                    A_lvl_tbl2 = A_lvl_tbl2_3
                    A_lvl_val = val_6
                    B_lvl_q = B_lvl_q_step
                end
                break
            end
        end
    end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(Ct_lvl_2_val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            phase_start_2 = max(1, 1 + fld(A_lvl.shape[1] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                for i_7 = phase_start_2:phase_stop_2
                    B_lvl_q = B_lvl_ptr[1]
                    B_lvl_q_stop = B_lvl_ptr[1 + 1]
                    if B_lvl_q < B_lvl_q_stop
                        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                    else
                        B_lvl_i_stop = 0
                    end
                    phase_stop_3 = min(B_lvl.shape[2], B_lvl_i_stop)
                    if phase_stop_3 >= 1
                        if B_lvl_tbl2[B_lvl_q] < 1
                            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        while true
                            B_lvl_i = B_lvl_tbl2[B_lvl_q]
                            B_lvl_q_step = B_lvl_q
                            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                            end
                            if B_lvl_i < phase_stop_3
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_5 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_5 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_5
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_6 = min(B_lvl_i_2, phase_stop_5, A_lvl_i)
                                        if A_lvl_i == phase_stop_6 && B_lvl_i_2 == phase_stop_6
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_7 = min(i_7, A_lvl_i_stop_2)
                                            if phase_stop_7 >= i_7
                                                if A_lvl_tbl1[A_lvl_q] < i_7
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_7
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_9 = min(A_lvl_i_2, phase_stop_7)
                                                        if A_lvl_i_2 == phase_stop_9
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_6
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_6
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_6 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            else
                                phase_stop_14 = min(B_lvl_i, phase_stop_3)
                                if B_lvl_i == phase_stop_14
                                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_14
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_15 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_15 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_15
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_16 = min(B_lvl_i_2, A_lvl_i, phase_stop_15)
                                            if A_lvl_i == phase_stop_16 && B_lvl_i_2 == phase_stop_16
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_17 = min(i_7, A_lvl_i_stop_4)
                                                if phase_stop_17 >= i_7
                                                    if A_lvl_tbl1[A_lvl_q] < i_7
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_17
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_19 = min(A_lvl_i_4, phase_stop_17)
                                                            if A_lvl_i_4 == phase_stop_19
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_16
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_16
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_16 + 1
                                        end
                                    end
                                    B_lvl_q = B_lvl_q_step
                                end
                                break
                            end
                        end
                    end
                end
            end
        end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    for i_4 = 1:A_lvl.shape[1]
        val = Ct_lvl_2_val
        Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
        B_lvl_ptr_2 = B_lvl_ptr
        B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
        B_lvl_tbl1_2 = B_lvl_tbl1
        B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
        B_lvl_tbl2_2 = B_lvl_tbl2
        B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
        val_2 = B_lvl_val
        B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
        A_lvl_ptr_2 = A_lvl_ptr
        A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
        A_lvl_tbl1_2 = A_lvl_tbl1
        A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
        A_lvl_tbl2_2 = A_lvl_tbl2
        A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
        val_3 = A_lvl_val
        A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
        Threads.@threads for i_5 = 1:Threads.nthreads()
                B_lvl_q = B_lvl_ptr[1]
                B_lvl_q_stop = B_lvl_ptr[1 + 1]
                if B_lvl_q < B_lvl_q_stop
                    B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                else
                    B_lvl_i_stop = 0
                end
                phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_5 + -1), Threads.nthreads()))
                phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_5, Threads.nthreads()))
                if phase_stop_2 >= phase_start_2
                    if B_lvl_tbl2[B_lvl_q] < phase_start_2
                        B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    while true
                        B_lvl_i = B_lvl_tbl2[B_lvl_q]
                        B_lvl_q_step = B_lvl_q
                        if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                            B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        if B_lvl_i < phase_stop_2
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_4, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_4
                                            if A_lvl_tbl1[A_lvl_q] < i_4
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        else
                            phase_stop_13 = min(B_lvl_i, phase_stop_2)
                            if B_lvl_i == phase_stop_13
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_4, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_4
                                                if A_lvl_tbl1[A_lvl_q] < i_4
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            end
                            break
                        end
                    end
                end
            end
        Ct_lvl_2_val = val
        B_lvl_ptr = B_lvl_ptr_2
        B_lvl_tbl1 = B_lvl_tbl1_2
        B_lvl_tbl2 = B_lvl_tbl2_2
        B_lvl_val = val_2
        A_lvl_ptr = A_lvl_ptr_2
        A_lvl_tbl1 = A_lvl_tbl1_2
        A_lvl_tbl2 = A_lvl_tbl2_2
        A_lvl_val = val_3
    end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(Ct_lvl_2_val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            B_lvl_q = B_lvl_ptr[1]
            B_lvl_q_stop = B_lvl_ptr[1 + 1]
            if B_lvl_q < B_lvl_q_stop
                B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
            else
                B_lvl_i_stop = 0
            end
            phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                if B_lvl_tbl2[B_lvl_q] < phase_start_2
                    B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                end
                while true
                    B_lvl_i = B_lvl_tbl2[B_lvl_q]
                    B_lvl_q_step = B_lvl_q
                    if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                        B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    if B_lvl_i < phase_stop_2
                        Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                        for i_6 = 1:A_lvl.shape[1]
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_6
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_6, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_6
                                            if A_lvl_tbl1[A_lvl_q] < i_6
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_6, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                        end
                        B_lvl_q = B_lvl_q_step
                    else
                        phase_stop_13 = min(B_lvl_i, phase_stop_2)
                        if B_lvl_i == phase_stop_13
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                            for i_8 = 1:A_lvl.shape[1]
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_8
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_8, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_8
                                                if A_lvl_tbl1[A_lvl_q] < i_8
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_8, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        end
                        break
                    end
                end
            end
        end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    B_lvl_q = B_lvl_ptr[1]
    B_lvl_q_stop = B_lvl_ptr[1 + 1]
    if B_lvl_q < B_lvl_q_stop
        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
    else
        B_lvl_i_stop = 0
    end
    phase_stop = min(B_lvl.shape[2], B_lvl_i_stop)
    if phase_stop >= 1
        if B_lvl_tbl2[B_lvl_q] < 1
            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
        end
        while true
            B_lvl_i = B_lvl_tbl2[B_lvl_q]
            B_lvl_q_step = B_lvl_q
            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
            end
            if B_lvl_i < phase_stop
                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                val = Ct_lvl_2_val
                Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                B_lvl_tbl1_2 = B_lvl_tbl1
                B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                B_lvl_tbl2_2 = B_lvl_tbl2
                val_2 = B_lvl_val
                B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                A_lvl_ptr_2 = A_lvl_ptr
                A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                A_lvl_tbl1_2 = A_lvl_tbl1
                A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                A_lvl_tbl2_2 = A_lvl_tbl2
                A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                val_3 = A_lvl_val
                A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                Threads.@threads for i_9 = 1:Threads.nthreads()
                        phase_start_6 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_9), Threads.nthreads()))
                        phase_stop_7 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_9, Threads.nthreads()))
                        if phase_stop_7 >= phase_start_6
                            for i_12 = phase_start_6:phase_stop_7
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_12
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_8 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_8 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_8
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_9 = min(B_lvl_i_2, phase_stop_8, A_lvl_i)
                                        if A_lvl_i == phase_stop_9 && B_lvl_i_2 == phase_stop_9
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_10 = min(i_12, A_lvl_i_stop_2)
                                            if phase_stop_10 >= i_12
                                                if A_lvl_tbl1[A_lvl_q] < i_12
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_12, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_10
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_12 = min(A_lvl_i_2, phase_stop_10)
                                                        if A_lvl_i_2 == phase_stop_12
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_9
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_9
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_9 + 1
                                    end
                                end
                            end
                        end
                    end
                Ct_lvl_2_val = val
                B_lvl_tbl1 = B_lvl_tbl1_2
                B_lvl_tbl2 = B_lvl_tbl2_2
                B_lvl_val = val_2
                A_lvl_ptr = A_lvl_ptr_2
                A_lvl_tbl1 = A_lvl_tbl1_2
                A_lvl_tbl2 = A_lvl_tbl2_2
                A_lvl_val = val_3
                B_lvl_q = B_lvl_q_step
            else
                phase_stop_18 = min(B_lvl_i, phase_stop)
                if B_lvl_i == phase_stop_18
                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_18
                    val_4 = Ct_lvl_2_val
                    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                    B_lvl_tbl1_3 = B_lvl_tbl1
                    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                    B_lvl_tbl2_3 = B_lvl_tbl2
                    val_5 = B_lvl_val
                    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                    A_lvl_ptr_3 = A_lvl_ptr
                    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                    A_lvl_tbl1_3 = A_lvl_tbl1
                    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                    A_lvl_tbl2_3 = A_lvl_tbl2
                    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                    val_6 = A_lvl_val
                    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                    Threads.@threads for i_19 = 1:Threads.nthreads()
                            phase_start_21 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_19), Threads.nthreads()))
                            phase_stop_23 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_19, Threads.nthreads()))
                            if phase_stop_23 >= phase_start_21
                                for i_22 = phase_start_21:phase_stop_23
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_22
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_24 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_24 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_24
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_25 = min(B_lvl_i_2, A_lvl_i, phase_stop_24)
                                            if A_lvl_i == phase_stop_25 && B_lvl_i_2 == phase_stop_25
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_26 = min(i_22, A_lvl_i_stop_4)
                                                if phase_stop_26 >= i_22
                                                    if A_lvl_tbl1[A_lvl_q] < i_22
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_22, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_26
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_28 = min(A_lvl_i_4, phase_stop_26)
                                                            if A_lvl_i_4 == phase_stop_28
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_25
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_25
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_25 + 1
                                        end
                                    end
                                end
                            end
                        end
                    Ct_lvl_2_val = val_4
                    B_lvl_tbl1 = B_lvl_tbl1_3
                    B_lvl_tbl2 = B_lvl_tbl2_3
                    B_lvl_val = val_5
                    A_lvl_ptr = A_lvl_ptr_3
                    A_lvl_tbl1 = A_lvl_tbl1_3
                    A_lvl_tbl2 = A_lvl_tbl2_3
                    A_lvl_val = val_6
                    B_lvl_q = B_lvl_q_step
                end
                break
            end
        end
    end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(Ct_lvl_2_val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            phase_start_2 = max(1, 1 + fld(A_lvl.shape[1] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                for i_7 = phase_start_2:phase_stop_2
                    B_lvl_q = B_lvl_ptr[1]
                    B_lvl_q_stop = B_lvl_ptr[1 + 1]
                    if B_lvl_q < B_lvl_q_stop
                        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                    else
                        B_lvl_i_stop = 0
                    end
                    phase_stop_3 = min(B_lvl.shape[2], B_lvl_i_stop)
                    if phase_stop_3 >= 1
                        if B_lvl_tbl2[B_lvl_q] < 1
                            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        while true
                            B_lvl_i = B_lvl_tbl2[B_lvl_q]
                            B_lvl_q_step = B_lvl_q
                            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                            end
                            if B_lvl_i < phase_stop_3
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_5 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_5 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_5
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_6 = min(B_lvl_i_2, phase_stop_5, A_lvl_i)
                                        if A_lvl_i == phase_stop_6 && B_lvl_i_2 == phase_stop_6
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_7 = min(i_7, A_lvl_i_stop_2)
                                            if phase_stop_7 >= i_7
                                                if A_lvl_tbl1[A_lvl_q] < i_7
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_7
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_9 = min(A_lvl_i_2, phase_stop_7)
                                                        if A_lvl_i_2 == phase_stop_9
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_6
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_6
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_6 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            else
                                phase_stop_14 = min(B_lvl_i, phase_stop_3)
                                if B_lvl_i == phase_stop_14
                                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_14
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_15 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_15 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_15
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_16 = min(B_lvl_i_2, A_lvl_i, phase_stop_15)
                                            if A_lvl_i == phase_stop_16 && B_lvl_i_2 == phase_stop_16
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_17 = min(i_7, A_lvl_i_stop_4)
                                                if phase_stop_17 >= i_7
                                                    if A_lvl_tbl1[A_lvl_q] < i_7
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_17
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_19 = min(A_lvl_i_4, phase_stop_17)
                                                            if A_lvl_i_4 == phase_stop_19
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_16
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_16
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_16 + 1
                                        end
                                    end
                                    B_lvl_q = B_lvl_q_step
                                end
                                break
                            end
                        end
                    end
                end
            end
        end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    for i_4 = 1:A_lvl.shape[1]
        val = Ct_lvl_2_val
        Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
        B_lvl_ptr_2 = B_lvl_ptr
        B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
        B_lvl_tbl1_2 = B_lvl_tbl1
        B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
        B_lvl_tbl2_2 = B_lvl_tbl2
        B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
        val_2 = B_lvl_val
        B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
        A_lvl_ptr_2 = A_lvl_ptr
        A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
        A_lvl_tbl1_2 = A_lvl_tbl1
        A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
        A_lvl_tbl2_2 = A_lvl_tbl2
        A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
        val_3 = A_lvl_val
        A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
        Threads.@threads for i_5 = 1:Threads.nthreads()
                B_lvl_q = B_lvl_ptr[1]
                B_lvl_q_stop = B_lvl_ptr[1 + 1]
                if B_lvl_q < B_lvl_q_stop
                    B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                else
                    B_lvl_i_stop = 0
                end
                phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_5 + -1), Threads.nthreads()))
                phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_5, Threads.nthreads()))
                if phase_stop_2 >= phase_start_2
                    if B_lvl_tbl2[B_lvl_q] < phase_start_2
                        B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    while true
                        B_lvl_i = B_lvl_tbl2[B_lvl_q]
                        B_lvl_q_step = B_lvl_q
                        if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                            B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        if B_lvl_i < phase_stop_2
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_4, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_4
                                            if A_lvl_tbl1[A_lvl_q] < i_4
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        else
                            phase_stop_13 = min(B_lvl_i, phase_stop_2)
                            if B_lvl_i == phase_stop_13
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_4, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_4
                                                if A_lvl_tbl1[A_lvl_q] < i_4
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            end
                            break
                        end
                    end
                end
            end
        Ct_lvl_2_val = val
        B_lvl_ptr = B_lvl_ptr_2
        B_lvl_tbl1 = B_lvl_tbl1_2
        B_lvl_tbl2 = B_lvl_tbl2_2
        B_lvl_val = val_2
        A_lvl_ptr = A_lvl_ptr_2
        A_lvl_tbl1 = A_lvl_tbl1_2
        A_lvl_tbl2 = A_lvl_tbl2_2
        A_lvl_val = val_3
    end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(Ct_lvl_2_val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            B_lvl_q = B_lvl_ptr[1]
            B_lvl_q_stop = B_lvl_ptr[1 + 1]
            if B_lvl_q < B_lvl_q_stop
                B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
            else
                B_lvl_i_stop = 0
            end
            phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                if B_lvl_tbl2[B_lvl_q] < phase_start_2
                    B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                end
                while true
                    B_lvl_i = B_lvl_tbl2[B_lvl_q]
                    B_lvl_q_step = B_lvl_q
                    if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                        B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    if B_lvl_i < phase_stop_2
                        Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                        for i_6 = 1:A_lvl.shape[1]
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_6
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_6, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_6
                                            if A_lvl_tbl1[A_lvl_q] < i_6
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_6, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                        end
                        B_lvl_q = B_lvl_q_step
                    else
                        phase_stop_13 = min(B_lvl_i, phase_stop_2)
                        if B_lvl_i == phase_stop_13
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                            for i_8 = 1:A_lvl.shape[1]
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_8
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_8, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_8
                                                if A_lvl_tbl1[A_lvl_q] < i_8
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_8, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        end
                        break
                    end
                end
            end
        end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    B_lvl_q = B_lvl_ptr[1]
    B_lvl_q_stop = B_lvl_ptr[1 + 1]
    if B_lvl_q < B_lvl_q_stop
        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
    else
        B_lvl_i_stop = 0
    end
    phase_stop = min(B_lvl.shape[2], B_lvl_i_stop)
    if phase_stop >= 1
        if B_lvl_tbl2[B_lvl_q] < 1
            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
        end
        while true
            B_lvl_i = B_lvl_tbl2[B_lvl_q]
            B_lvl_q_step = B_lvl_q
            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
            end
            if B_lvl_i < phase_stop
                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                val = Ct_lvl_2_val
                Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                B_lvl_tbl1_2 = B_lvl_tbl1
                B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                B_lvl_tbl2_2 = B_lvl_tbl2
                val_2 = B_lvl_val
                B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                A_lvl_ptr_2 = A_lvl_ptr
                A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                A_lvl_tbl1_2 = A_lvl_tbl1
                A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                A_lvl_tbl2_2 = A_lvl_tbl2
                A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                val_3 = A_lvl_val
                A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                Threads.@threads for i_9 = 1:Threads.nthreads()
                        phase_start_6 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_9), Threads.nthreads()))
                        phase_stop_7 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_9, Threads.nthreads()))
                        if phase_stop_7 >= phase_start_6
                            for i_12 = phase_start_6:phase_stop_7
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_12
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_8 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_8 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_8
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_9 = min(B_lvl_i_2, phase_stop_8, A_lvl_i)
                                        if A_lvl_i == phase_stop_9 && B_lvl_i_2 == phase_stop_9
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_10 = min(i_12, A_lvl_i_stop_2)
                                            if phase_stop_10 >= i_12
                                                if A_lvl_tbl1[A_lvl_q] < i_12
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_12, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_10
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_12 = min(A_lvl_i_2, phase_stop_10)
                                                        if A_lvl_i_2 == phase_stop_12
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_9
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_9
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_9 + 1
                                    end
                                end
                            end
                        end
                    end
                Ct_lvl_2_val = val
                B_lvl_tbl1 = B_lvl_tbl1_2
                B_lvl_tbl2 = B_lvl_tbl2_2
                B_lvl_val = val_2
                A_lvl_ptr = A_lvl_ptr_2
                A_lvl_tbl1 = A_lvl_tbl1_2
                A_lvl_tbl2 = A_lvl_tbl2_2
                A_lvl_val = val_3
                B_lvl_q = B_lvl_q_step
            else
                phase_stop_18 = min(B_lvl_i, phase_stop)
                if B_lvl_i == phase_stop_18
                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_18
                    val_4 = Ct_lvl_2_val
                    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                    B_lvl_tbl1_3 = B_lvl_tbl1
                    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                    B_lvl_tbl2_3 = B_lvl_tbl2
                    val_5 = B_lvl_val
                    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                    A_lvl_ptr_3 = A_lvl_ptr
                    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                    A_lvl_tbl1_3 = A_lvl_tbl1
                    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                    A_lvl_tbl2_3 = A_lvl_tbl2
                    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                    val_6 = A_lvl_val
                    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                    Threads.@threads for i_19 = 1:Threads.nthreads()
                            phase_start_21 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_19), Threads.nthreads()))
                            phase_stop_23 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_19, Threads.nthreads()))
                            if phase_stop_23 >= phase_start_21
                                for i_22 = phase_start_21:phase_stop_23
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_22
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_24 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_24 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_24
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_25 = min(B_lvl_i_2, A_lvl_i, phase_stop_24)
                                            if A_lvl_i == phase_stop_25 && B_lvl_i_2 == phase_stop_25
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_26 = min(i_22, A_lvl_i_stop_4)
                                                if phase_stop_26 >= i_22
                                                    if A_lvl_tbl1[A_lvl_q] < i_22
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_22, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_26
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_28 = min(A_lvl_i_4, phase_stop_26)
                                                            if A_lvl_i_4 == phase_stop_28
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_25
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_25
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_25 + 1
                                        end
                                    end
                                end
                            end
                        end
                    Ct_lvl_2_val = val_4
                    B_lvl_tbl1 = B_lvl_tbl1_3
                    B_lvl_tbl2 = B_lvl_tbl2_3
                    B_lvl_val = val_5
                    A_lvl_ptr = A_lvl_ptr_3
                    A_lvl_tbl1 = A_lvl_tbl1_3
                    A_lvl_tbl2 = A_lvl_tbl2_3
                    A_lvl_val = val_6
                    B_lvl_q = B_lvl_q_step
                end
                break
            end
        end
    end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(Ct_lvl_2_val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            phase_start_2 = max(1, 1 + fld(A_lvl.shape[1] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                for i_7 = phase_start_2:phase_stop_2
                    B_lvl_q = B_lvl_ptr[1]
                    B_lvl_q_stop = B_lvl_ptr[1 + 1]
                    if B_lvl_q < B_lvl_q_stop
                        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                    else
                        B_lvl_i_stop = 0
                    end
                    phase_stop_3 = min(B_lvl.shape[2], B_lvl_i_stop)
                    if phase_stop_3 >= 1
                        if B_lvl_tbl2[B_lvl_q] < 1
                            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        while true
                            B_lvl_i = B_lvl_tbl2[B_lvl_q]
                            B_lvl_q_step = B_lvl_q
                            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                            end
                            if B_lvl_i < phase_stop_3
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_5 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_5 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_5
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_6 = min(B_lvl_i_2, phase_stop_5, A_lvl_i)
                                        if A_lvl_i == phase_stop_6 && B_lvl_i_2 == phase_stop_6
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_7 = min(i_7, A_lvl_i_stop_2)
                                            if phase_stop_7 >= i_7
                                                if A_lvl_tbl1[A_lvl_q] < i_7
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_7
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_9 = min(A_lvl_i_2, phase_stop_7)
                                                        if A_lvl_i_2 == phase_stop_9
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_6
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_6
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_6 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            else
                                phase_stop_14 = min(B_lvl_i, phase_stop_3)
                                if B_lvl_i == phase_stop_14
                                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_14
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_15 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_15 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_15
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_16 = min(B_lvl_i_2, A_lvl_i, phase_stop_15)
                                            if A_lvl_i == phase_stop_16 && B_lvl_i_2 == phase_stop_16
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_17 = min(i_7, A_lvl_i_stop_4)
                                                if phase_stop_17 >= i_7
                                                    if A_lvl_tbl1[A_lvl_q] < i_7
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_17
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_19 = min(A_lvl_i_4, phase_stop_17)
                                                            if A_lvl_i_4 == phase_stop_19
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_16
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_16
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_16 + 1
                                        end
                                    end
                                    B_lvl_q = B_lvl_q_step
                                end
                                break
                            end
                        end
                    end
                end
            end
        end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    for i_4 = 1:A_lvl.shape[1]
        val = Ct_lvl_2_val
        Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
        B_lvl_ptr_2 = B_lvl_ptr
        B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
        B_lvl_tbl1_2 = B_lvl_tbl1
        B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
        B_lvl_tbl2_2 = B_lvl_tbl2
        B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
        val_2 = B_lvl_val
        B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
        A_lvl_ptr_2 = A_lvl_ptr
        A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
        A_lvl_tbl1_2 = A_lvl_tbl1
        A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
        A_lvl_tbl2_2 = A_lvl_tbl2
        A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
        val_3 = A_lvl_val
        A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
        Threads.@threads for i_5 = 1:Threads.nthreads()
                B_lvl_q = B_lvl_ptr[1]
                B_lvl_q_stop = B_lvl_ptr[1 + 1]
                if B_lvl_q < B_lvl_q_stop
                    B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                else
                    B_lvl_i_stop = 0
                end
                phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_5 + -1), Threads.nthreads()))
                phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_5, Threads.nthreads()))
                if phase_stop_2 >= phase_start_2
                    if B_lvl_tbl2[B_lvl_q] < phase_start_2
                        B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    while true
                        B_lvl_i = B_lvl_tbl2[B_lvl_q]
                        B_lvl_q_step = B_lvl_q
                        if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                            B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        if B_lvl_i < phase_stop_2
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_4, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_4
                                            if A_lvl_tbl1[A_lvl_q] < i_4
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        else
                            phase_stop_13 = min(B_lvl_i, phase_stop_2)
                            if B_lvl_i == phase_stop_13
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_4, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_4
                                                if A_lvl_tbl1[A_lvl_q] < i_4
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            end
                            break
                        end
                    end
                end
            end
        Ct_lvl_2_val = val
        B_lvl_ptr = B_lvl_ptr_2
        B_lvl_tbl1 = B_lvl_tbl1_2
        B_lvl_tbl2 = B_lvl_tbl2_2
        B_lvl_val = val_2
        A_lvl_ptr = A_lvl_ptr_2
        A_lvl_tbl1 = A_lvl_tbl1_2
        A_lvl_tbl2 = A_lvl_tbl2_2
        A_lvl_val = val_3
    end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(Ct_lvl_2_val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            B_lvl_q = B_lvl_ptr[1]
            B_lvl_q_stop = B_lvl_ptr[1 + 1]
            if B_lvl_q < B_lvl_q_stop
                B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
            else
                B_lvl_i_stop = 0
            end
            phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                if B_lvl_tbl2[B_lvl_q] < phase_start_2
                    B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                end
                while true
                    B_lvl_i = B_lvl_tbl2[B_lvl_q]
                    B_lvl_q_step = B_lvl_q
                    if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                        B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    if B_lvl_i < phase_stop_2
                        Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                        for i_6 = 1:A_lvl.shape[1]
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_6
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_6, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_6
                                            if A_lvl_tbl1[A_lvl_q] < i_6
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_6, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                        end
                        B_lvl_q = B_lvl_q_step
                    else
                        phase_stop_13 = min(B_lvl_i, phase_stop_2)
                        if B_lvl_i == phase_stop_13
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                            for i_8 = 1:A_lvl.shape[1]
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_8
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_8, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_8
                                                if A_lvl_tbl1[A_lvl_q] < i_8
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_8, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        end
                        break
                    end
                end
            end
        end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    B_lvl_q = B_lvl_ptr[1]
    B_lvl_q_stop = B_lvl_ptr[1 + 1]
    if B_lvl_q < B_lvl_q_stop
        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
    else
        B_lvl_i_stop = 0
    end
    phase_stop = min(B_lvl.shape[2], B_lvl_i_stop)
    if phase_stop >= 1
        if B_lvl_tbl2[B_lvl_q] < 1
            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
        end
        while true
            B_lvl_i = B_lvl_tbl2[B_lvl_q]
            B_lvl_q_step = B_lvl_q
            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
            end
            if B_lvl_i < phase_stop
                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                val = Ct_lvl_2_val
                Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                B_lvl_tbl1_2 = B_lvl_tbl1
                B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                B_lvl_tbl2_2 = B_lvl_tbl2
                val_2 = B_lvl_val
                B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                A_lvl_ptr_2 = A_lvl_ptr
                A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                A_lvl_tbl1_2 = A_lvl_tbl1
                A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                A_lvl_tbl2_2 = A_lvl_tbl2
                A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                val_3 = A_lvl_val
                A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                Threads.@threads for i_9 = 1:Threads.nthreads()
                        phase_start_6 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_9), Threads.nthreads()))
                        phase_stop_7 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_9, Threads.nthreads()))
                        if phase_stop_7 >= phase_start_6
                            for i_12 = phase_start_6:phase_stop_7
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_12
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_8 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_8 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_8
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_9 = min(B_lvl_i_2, phase_stop_8, A_lvl_i)
                                        if A_lvl_i == phase_stop_9 && B_lvl_i_2 == phase_stop_9
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_10 = min(i_12, A_lvl_i_stop_2)
                                            if phase_stop_10 >= i_12
                                                if A_lvl_tbl1[A_lvl_q] < i_12
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_12, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_10
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_12 = min(A_lvl_i_2, phase_stop_10)
                                                        if A_lvl_i_2 == phase_stop_12
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_9
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_9
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_9 + 1
                                    end
                                end
                            end
                        end
                    end
                Ct_lvl_2_val = val
                B_lvl_tbl1 = B_lvl_tbl1_2
                B_lvl_tbl2 = B_lvl_tbl2_2
                B_lvl_val = val_2
                A_lvl_ptr = A_lvl_ptr_2
                A_lvl_tbl1 = A_lvl_tbl1_2
                A_lvl_tbl2 = A_lvl_tbl2_2
                A_lvl_val = val_3
                B_lvl_q = B_lvl_q_step
            else
                phase_stop_18 = min(B_lvl_i, phase_stop)
                if B_lvl_i == phase_stop_18
                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_18
                    val_4 = Ct_lvl_2_val
                    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                    B_lvl_tbl1_3 = B_lvl_tbl1
                    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                    B_lvl_tbl2_3 = B_lvl_tbl2
                    val_5 = B_lvl_val
                    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                    A_lvl_ptr_3 = A_lvl_ptr
                    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                    A_lvl_tbl1_3 = A_lvl_tbl1
                    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                    A_lvl_tbl2_3 = A_lvl_tbl2
                    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                    val_6 = A_lvl_val
                    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                    Threads.@threads for i_19 = 1:Threads.nthreads()
                            phase_start_21 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_19), Threads.nthreads()))
                            phase_stop_23 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_19, Threads.nthreads()))
                            if phase_stop_23 >= phase_start_21
                                for i_22 = phase_start_21:phase_stop_23
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_22
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_24 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_24 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_24
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_25 = min(B_lvl_i_2, A_lvl_i, phase_stop_24)
                                            if A_lvl_i == phase_stop_25 && B_lvl_i_2 == phase_stop_25
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_26 = min(i_22, A_lvl_i_stop_4)
                                                if phase_stop_26 >= i_22
                                                    if A_lvl_tbl1[A_lvl_q] < i_22
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_22, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_26
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_28 = min(A_lvl_i_4, phase_stop_26)
                                                            if A_lvl_i_4 == phase_stop_28
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_25
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_25
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_25 + 1
                                        end
                                    end
                                end
                            end
                        end
                    Ct_lvl_2_val = val_4
                    B_lvl_tbl1 = B_lvl_tbl1_3
                    B_lvl_tbl2 = B_lvl_tbl2_3
                    B_lvl_val = val_5
                    A_lvl_ptr = A_lvl_ptr_3
                    A_lvl_tbl1 = A_lvl_tbl1_3
                    A_lvl_tbl2 = A_lvl_tbl2_3
                    A_lvl_val = val_6
                    B_lvl_q = B_lvl_q_step
                end
                break
            end
        end
    end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(Ct_lvl_2_val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            phase_start_2 = max(1, 1 + fld(A_lvl.shape[1] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                for i_7 = phase_start_2:phase_stop_2
                    B_lvl_q = B_lvl_ptr[1]
                    B_lvl_q_stop = B_lvl_ptr[1 + 1]
                    if B_lvl_q < B_lvl_q_stop
                        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                    else
                        B_lvl_i_stop = 0
                    end
                    phase_stop_3 = min(B_lvl.shape[2], B_lvl_i_stop)
                    if phase_stop_3 >= 1
                        if B_lvl_tbl2[B_lvl_q] < 1
                            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        while true
                            B_lvl_i = B_lvl_tbl2[B_lvl_q]
                            B_lvl_q_step = B_lvl_q
                            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                            end
                            if B_lvl_i < phase_stop_3
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_5 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_5 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_5
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_6 = min(B_lvl_i_2, phase_stop_5, A_lvl_i)
                                        if A_lvl_i == phase_stop_6 && B_lvl_i_2 == phase_stop_6
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_7 = min(i_7, A_lvl_i_stop_2)
                                            if phase_stop_7 >= i_7
                                                if A_lvl_tbl1[A_lvl_q] < i_7
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_7
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_9 = min(A_lvl_i_2, phase_stop_7)
                                                        if A_lvl_i_2 == phase_stop_9
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_6
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_6
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_6 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            else
                                phase_stop_14 = min(B_lvl_i, phase_stop_3)
                                if B_lvl_i == phase_stop_14
                                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_14
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_15 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_15 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_15
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_16 = min(B_lvl_i_2, A_lvl_i, phase_stop_15)
                                            if A_lvl_i == phase_stop_16 && B_lvl_i_2 == phase_stop_16
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_17 = min(i_7, A_lvl_i_stop_4)
                                                if phase_stop_17 >= i_7
                                                    if A_lvl_tbl1[A_lvl_q] < i_7
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_17
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_19 = min(A_lvl_i_4, phase_stop_17)
                                                            if A_lvl_i_4 == phase_stop_19
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_16
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_16
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_16 + 1
                                        end
                                    end
                                    B_lvl_q = B_lvl_q_step
                                end
                                break
                            end
                        end
                    end
                end
            end
        end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    for i_4 = 1:A_lvl.shape[1]
        val = Ct_lvl_2_val
        Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
        B_lvl_ptr_2 = B_lvl_ptr
        B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
        B_lvl_tbl1_2 = B_lvl_tbl1
        B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
        B_lvl_tbl2_2 = B_lvl_tbl2
        B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
        val_2 = B_lvl_val
        B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
        A_lvl_ptr_2 = A_lvl_ptr
        A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
        A_lvl_tbl1_2 = A_lvl_tbl1
        A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
        A_lvl_tbl2_2 = A_lvl_tbl2
        A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
        val_3 = A_lvl_val
        A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
        Threads.@threads for i_5 = 1:Threads.nthreads()
                B_lvl_q = B_lvl_ptr[1]
                B_lvl_q_stop = B_lvl_ptr[1 + 1]
                if B_lvl_q < B_lvl_q_stop
                    B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                else
                    B_lvl_i_stop = 0
                end
                phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_5 + -1), Threads.nthreads()))
                phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_5, Threads.nthreads()))
                if phase_stop_2 >= phase_start_2
                    if B_lvl_tbl2[B_lvl_q] < phase_start_2
                        B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    while true
                        B_lvl_i = B_lvl_tbl2[B_lvl_q]
                        B_lvl_q_step = B_lvl_q
                        if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                            B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        if B_lvl_i < phase_stop_2
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_4, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_4
                                            if A_lvl_tbl1[A_lvl_q] < i_4
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        else
                            phase_stop_13 = min(B_lvl_i, phase_stop_2)
                            if B_lvl_i == phase_stop_13
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_4, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_4
                                                if A_lvl_tbl1[A_lvl_q] < i_4
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            end
                            break
                        end
                    end
                end
            end
        Ct_lvl_2_val = val
        B_lvl_ptr = B_lvl_ptr_2
        B_lvl_tbl1 = B_lvl_tbl1_2
        B_lvl_tbl2 = B_lvl_tbl2_2
        B_lvl_val = val_2
        A_lvl_ptr = A_lvl_ptr_2
        A_lvl_tbl1 = A_lvl_tbl1_2
        A_lvl_tbl2 = A_lvl_tbl2_2
        A_lvl_val = val_3
    end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(Ct_lvl_2_val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            B_lvl_q = B_lvl_ptr[1]
            B_lvl_q_stop = B_lvl_ptr[1 + 1]
            if B_lvl_q < B_lvl_q_stop
                B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
            else
                B_lvl_i_stop = 0
            end
            phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                if B_lvl_tbl2[B_lvl_q] < phase_start_2
                    B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                end
                while true
                    B_lvl_i = B_lvl_tbl2[B_lvl_q]
                    B_lvl_q_step = B_lvl_q
                    if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                        B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    if B_lvl_i < phase_stop_2
                        Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                        for i_6 = 1:A_lvl.shape[1]
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_6
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_6, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_6
                                            if A_lvl_tbl1[A_lvl_q] < i_6
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_6, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                        end
                        B_lvl_q = B_lvl_q_step
                    else
                        phase_stop_13 = min(B_lvl_i, phase_stop_2)
                        if B_lvl_i == phase_stop_13
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                            for i_8 = 1:A_lvl.shape[1]
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_8
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_8, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_8
                                                if A_lvl_tbl1[A_lvl_q] < i_8
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_8, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        end
                        break
                    end
                end
            end
        end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    B_lvl_q = B_lvl_ptr[1]
    B_lvl_q_stop = B_lvl_ptr[1 + 1]
    if B_lvl_q < B_lvl_q_stop
        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
    else
        B_lvl_i_stop = 0
    end
    phase_stop = min(B_lvl.shape[2], B_lvl_i_stop)
    if phase_stop >= 1
        if B_lvl_tbl2[B_lvl_q] < 1
            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
        end
        while true
            B_lvl_i = B_lvl_tbl2[B_lvl_q]
            B_lvl_q_step = B_lvl_q
            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
            end
            if B_lvl_i < phase_stop
                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                val = Ct_lvl_2_val
                Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                B_lvl_tbl1_2 = B_lvl_tbl1
                B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                B_lvl_tbl2_2 = B_lvl_tbl2
                val_2 = B_lvl_val
                B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                A_lvl_ptr_2 = A_lvl_ptr
                A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                A_lvl_tbl1_2 = A_lvl_tbl1
                A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                A_lvl_tbl2_2 = A_lvl_tbl2
                A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                val_3 = A_lvl_val
                A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                Threads.@threads for i_9 = 1:Threads.nthreads()
                        phase_start_6 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_9), Threads.nthreads()))
                        phase_stop_7 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_9, Threads.nthreads()))
                        if phase_stop_7 >= phase_start_6
                            for i_12 = phase_start_6:phase_stop_7
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_12
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_8 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_8 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_8
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_9 = min(B_lvl_i_2, phase_stop_8, A_lvl_i)
                                        if A_lvl_i == phase_stop_9 && B_lvl_i_2 == phase_stop_9
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_10 = min(i_12, A_lvl_i_stop_2)
                                            if phase_stop_10 >= i_12
                                                if A_lvl_tbl1[A_lvl_q] < i_12
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_12, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_10
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_12 = min(A_lvl_i_2, phase_stop_10)
                                                        if A_lvl_i_2 == phase_stop_12
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_9
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_9
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_9 + 1
                                    end
                                end
                            end
                        end
                    end
                Ct_lvl_2_val = val
                B_lvl_tbl1 = B_lvl_tbl1_2
                B_lvl_tbl2 = B_lvl_tbl2_2
                B_lvl_val = val_2
                A_lvl_ptr = A_lvl_ptr_2
                A_lvl_tbl1 = A_lvl_tbl1_2
                A_lvl_tbl2 = A_lvl_tbl2_2
                A_lvl_val = val_3
                B_lvl_q = B_lvl_q_step
            else
                phase_stop_18 = min(B_lvl_i, phase_stop)
                if B_lvl_i == phase_stop_18
                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_18
                    val_4 = Ct_lvl_2_val
                    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                    B_lvl_tbl1_3 = B_lvl_tbl1
                    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                    B_lvl_tbl2_3 = B_lvl_tbl2
                    val_5 = B_lvl_val
                    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                    A_lvl_ptr_3 = A_lvl_ptr
                    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                    A_lvl_tbl1_3 = A_lvl_tbl1
                    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                    A_lvl_tbl2_3 = A_lvl_tbl2
                    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                    val_6 = A_lvl_val
                    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                    Threads.@threads for i_19 = 1:Threads.nthreads()
                            phase_start_21 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_19), Threads.nthreads()))
                            phase_stop_23 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_19, Threads.nthreads()))
                            if phase_stop_23 >= phase_start_21
                                for i_22 = phase_start_21:phase_stop_23
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_22
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_24 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_24 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_24
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_25 = min(B_lvl_i_2, A_lvl_i, phase_stop_24)
                                            if A_lvl_i == phase_stop_25 && B_lvl_i_2 == phase_stop_25
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_26 = min(i_22, A_lvl_i_stop_4)
                                                if phase_stop_26 >= i_22
                                                    if A_lvl_tbl1[A_lvl_q] < i_22
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_22, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_26
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_28 = min(A_lvl_i_4, phase_stop_26)
                                                            if A_lvl_i_4 == phase_stop_28
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_25
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_25
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_25 + 1
                                        end
                                    end
                                end
                            end
                        end
                    Ct_lvl_2_val = val_4
                    B_lvl_tbl1 = B_lvl_tbl1_3
                    B_lvl_tbl2 = B_lvl_tbl2_3
                    B_lvl_val = val_5
                    A_lvl_ptr = A_lvl_ptr_3
                    A_lvl_tbl1 = A_lvl_tbl1_3
                    A_lvl_tbl2 = A_lvl_tbl2_3
                    A_lvl_val = val_6
                    B_lvl_q = B_lvl_q_step
                end
                break
            end
        end
    end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(Ct_lvl_2_val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            phase_start_2 = max(1, 1 + fld(A_lvl.shape[1] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                for i_7 = phase_start_2:phase_stop_2
                    B_lvl_q = B_lvl_ptr[1]
                    B_lvl_q_stop = B_lvl_ptr[1 + 1]
                    if B_lvl_q < B_lvl_q_stop
                        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                    else
                        B_lvl_i_stop = 0
                    end
                    phase_stop_3 = min(B_lvl.shape[2], B_lvl_i_stop)
                    if phase_stop_3 >= 1
                        if B_lvl_tbl2[B_lvl_q] < 1
                            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        while true
                            B_lvl_i = B_lvl_tbl2[B_lvl_q]
                            B_lvl_q_step = B_lvl_q
                            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                            end
                            if B_lvl_i < phase_stop_3
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_5 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_5 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_5
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_6 = min(B_lvl_i_2, phase_stop_5, A_lvl_i)
                                        if A_lvl_i == phase_stop_6 && B_lvl_i_2 == phase_stop_6
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_7 = min(i_7, A_lvl_i_stop_2)
                                            if phase_stop_7 >= i_7
                                                if A_lvl_tbl1[A_lvl_q] < i_7
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_7
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_9 = min(A_lvl_i_2, phase_stop_7)
                                                        if A_lvl_i_2 == phase_stop_9
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_6
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_6
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_6 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            else
                                phase_stop_14 = min(B_lvl_i, phase_stop_3)
                                if B_lvl_i == phase_stop_14
                                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_14
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_15 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_15 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_15
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_16 = min(B_lvl_i_2, A_lvl_i, phase_stop_15)
                                            if A_lvl_i == phase_stop_16 && B_lvl_i_2 == phase_stop_16
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_17 = min(i_7, A_lvl_i_stop_4)
                                                if phase_stop_17 >= i_7
                                                    if A_lvl_tbl1[A_lvl_q] < i_7
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_17
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_19 = min(A_lvl_i_4, phase_stop_17)
                                                            if A_lvl_i_4 == phase_stop_19
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_16
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_16
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_16 + 1
                                        end
                                    end
                                    B_lvl_q = B_lvl_q_step
                                end
                                break
                            end
                        end
                    end
                end
            end
        end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    for i_4 = 1:A_lvl.shape[1]
        val = Ct_lvl_2_val
        Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
        B_lvl_ptr_2 = B_lvl_ptr
        B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
        B_lvl_tbl1_2 = B_lvl_tbl1
        B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
        B_lvl_tbl2_2 = B_lvl_tbl2
        B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
        val_2 = B_lvl_val
        B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
        A_lvl_ptr_2 = A_lvl_ptr
        A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
        A_lvl_tbl1_2 = A_lvl_tbl1
        A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
        A_lvl_tbl2_2 = A_lvl_tbl2
        A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
        val_3 = A_lvl_val
        A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
        Threads.@threads for i_5 = 1:Threads.nthreads()
                B_lvl_q = B_lvl_ptr[1]
                B_lvl_q_stop = B_lvl_ptr[1 + 1]
                if B_lvl_q < B_lvl_q_stop
                    B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                else
                    B_lvl_i_stop = 0
                end
                phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_5 + -1), Threads.nthreads()))
                phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_5, Threads.nthreads()))
                if phase_stop_2 >= phase_start_2
                    if B_lvl_tbl2[B_lvl_q] < phase_start_2
                        B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    while true
                        B_lvl_i = B_lvl_tbl2[B_lvl_q]
                        B_lvl_q_step = B_lvl_q
                        if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                            B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        if B_lvl_i < phase_stop_2
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_4, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_4
                                            if A_lvl_tbl1[A_lvl_q] < i_4
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        else
                            phase_stop_13 = min(B_lvl_i, phase_stop_2)
                            if B_lvl_i == phase_stop_13
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_4, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_4
                                                if A_lvl_tbl1[A_lvl_q] < i_4
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            end
                            break
                        end
                    end
                end
            end
        Ct_lvl_2_val = val
        B_lvl_ptr = B_lvl_ptr_2
        B_lvl_tbl1 = B_lvl_tbl1_2
        B_lvl_tbl2 = B_lvl_tbl2_2
        B_lvl_val = val_2
        A_lvl_ptr = A_lvl_ptr_2
        A_lvl_tbl1 = A_lvl_tbl1_2
        A_lvl_tbl2 = A_lvl_tbl2_2
        A_lvl_val = val_3
    end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(Ct_lvl_2_val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            B_lvl_q = B_lvl_ptr[1]
            B_lvl_q_stop = B_lvl_ptr[1 + 1]
            if B_lvl_q < B_lvl_q_stop
                B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
            else
                B_lvl_i_stop = 0
            end
            phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                if B_lvl_tbl2[B_lvl_q] < phase_start_2
                    B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                end
                while true
                    B_lvl_i = B_lvl_tbl2[B_lvl_q]
                    B_lvl_q_step = B_lvl_q
                    if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                        B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    if B_lvl_i < phase_stop_2
                        Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                        for i_6 = 1:A_lvl.shape[1]
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_6
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_6, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_6
                                            if A_lvl_tbl1[A_lvl_q] < i_6
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_6, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                        end
                        B_lvl_q = B_lvl_q_step
                    else
                        phase_stop_13 = min(B_lvl_i, phase_stop_2)
                        if B_lvl_i == phase_stop_13
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                            for i_8 = 1:A_lvl.shape[1]
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_8
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_8, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_8
                                                if A_lvl_tbl1[A_lvl_q] < i_8
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_8, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        end
                        break
                    end
                end
            end
        end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    B_lvl_q = B_lvl_ptr[1]
    B_lvl_q_stop = B_lvl_ptr[1 + 1]
    if B_lvl_q < B_lvl_q_stop
        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
    else
        B_lvl_i_stop = 0
    end
    phase_stop = min(B_lvl.shape[2], B_lvl_i_stop)
    if phase_stop >= 1
        if B_lvl_tbl2[B_lvl_q] < 1
            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
        end
        while true
            B_lvl_i = B_lvl_tbl2[B_lvl_q]
            B_lvl_q_step = B_lvl_q
            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
            end
            if B_lvl_i < phase_stop
                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                val = Ct_lvl_2_val
                Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                B_lvl_tbl1_2 = B_lvl_tbl1
                B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                B_lvl_tbl2_2 = B_lvl_tbl2
                val_2 = B_lvl_val
                B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                A_lvl_ptr_2 = A_lvl_ptr
                A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                A_lvl_tbl1_2 = A_lvl_tbl1
                A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                A_lvl_tbl2_2 = A_lvl_tbl2
                A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                val_3 = A_lvl_val
                A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                Threads.@threads for i_9 = 1:Threads.nthreads()
                        phase_start_6 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_9), Threads.nthreads()))
                        phase_stop_7 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_9, Threads.nthreads()))
                        if phase_stop_7 >= phase_start_6
                            for i_12 = phase_start_6:phase_stop_7
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_12
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_8 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_8 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_8
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_9 = min(B_lvl_i_2, phase_stop_8, A_lvl_i)
                                        if A_lvl_i == phase_stop_9 && B_lvl_i_2 == phase_stop_9
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_10 = min(i_12, A_lvl_i_stop_2)
                                            if phase_stop_10 >= i_12
                                                if A_lvl_tbl1[A_lvl_q] < i_12
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_12, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_10
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_12 = min(A_lvl_i_2, phase_stop_10)
                                                        if A_lvl_i_2 == phase_stop_12
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_9
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_9
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_9 + 1
                                    end
                                end
                            end
                        end
                    end
                Ct_lvl_2_val = val
                B_lvl_tbl1 = B_lvl_tbl1_2
                B_lvl_tbl2 = B_lvl_tbl2_2
                B_lvl_val = val_2
                A_lvl_ptr = A_lvl_ptr_2
                A_lvl_tbl1 = A_lvl_tbl1_2
                A_lvl_tbl2 = A_lvl_tbl2_2
                A_lvl_val = val_3
                B_lvl_q = B_lvl_q_step
            else
                phase_stop_18 = min(B_lvl_i, phase_stop)
                if B_lvl_i == phase_stop_18
                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_18
                    val_4 = Ct_lvl_2_val
                    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                    B_lvl_tbl1_3 = B_lvl_tbl1
                    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                    B_lvl_tbl2_3 = B_lvl_tbl2
                    val_5 = B_lvl_val
                    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                    A_lvl_ptr_3 = A_lvl_ptr
                    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                    A_lvl_tbl1_3 = A_lvl_tbl1
                    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                    A_lvl_tbl2_3 = A_lvl_tbl2
                    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                    val_6 = A_lvl_val
                    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                    Threads.@threads for i_19 = 1:Threads.nthreads()
                            phase_start_21 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_19), Threads.nthreads()))
                            phase_stop_23 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_19, Threads.nthreads()))
                            if phase_stop_23 >= phase_start_21
                                for i_22 = phase_start_21:phase_stop_23
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_22
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_24 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_24 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_24
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_25 = min(B_lvl_i_2, A_lvl_i, phase_stop_24)
                                            if A_lvl_i == phase_stop_25 && B_lvl_i_2 == phase_stop_25
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_26 = min(i_22, A_lvl_i_stop_4)
                                                if phase_stop_26 >= i_22
                                                    if A_lvl_tbl1[A_lvl_q] < i_22
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_22, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_26
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_28 = min(A_lvl_i_4, phase_stop_26)
                                                            if A_lvl_i_4 == phase_stop_28
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_25
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_25
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_25 + 1
                                        end
                                    end
                                end
                            end
                        end
                    Ct_lvl_2_val = val_4
                    B_lvl_tbl1 = B_lvl_tbl1_3
                    B_lvl_tbl2 = B_lvl_tbl2_3
                    B_lvl_val = val_5
                    A_lvl_ptr = A_lvl_ptr_3
                    A_lvl_tbl1 = A_lvl_tbl1_3
                    A_lvl_tbl2 = A_lvl_tbl2_3
                    A_lvl_val = val_6
                    B_lvl_q = B_lvl_q_step
                end
                break
            end
        end
    end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(Ct_lvl_2_val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            phase_start_2 = max(1, 1 + fld(A_lvl.shape[1] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                for i_7 = phase_start_2:phase_stop_2
                    B_lvl_q = B_lvl_ptr[1]
                    B_lvl_q_stop = B_lvl_ptr[1 + 1]
                    if B_lvl_q < B_lvl_q_stop
                        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                    else
                        B_lvl_i_stop = 0
                    end
                    phase_stop_3 = min(B_lvl.shape[2], B_lvl_i_stop)
                    if phase_stop_3 >= 1
                        if B_lvl_tbl2[B_lvl_q] < 1
                            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        while true
                            B_lvl_i = B_lvl_tbl2[B_lvl_q]
                            B_lvl_q_step = B_lvl_q
                            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                            end
                            if B_lvl_i < phase_stop_3
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_5 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_5 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_5
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_6 = min(B_lvl_i_2, phase_stop_5, A_lvl_i)
                                        if A_lvl_i == phase_stop_6 && B_lvl_i_2 == phase_stop_6
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_7 = min(i_7, A_lvl_i_stop_2)
                                            if phase_stop_7 >= i_7
                                                if A_lvl_tbl1[A_lvl_q] < i_7
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_7
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_9 = min(A_lvl_i_2, phase_stop_7)
                                                        if A_lvl_i_2 == phase_stop_9
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_6
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_6
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_6 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            else
                                phase_stop_14 = min(B_lvl_i, phase_stop_3)
                                if B_lvl_i == phase_stop_14
                                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_14
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_15 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_15 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_15
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_16 = min(B_lvl_i_2, A_lvl_i, phase_stop_15)
                                            if A_lvl_i == phase_stop_16 && B_lvl_i_2 == phase_stop_16
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_17 = min(i_7, A_lvl_i_stop_4)
                                                if phase_stop_17 >= i_7
                                                    if A_lvl_tbl1[A_lvl_q] < i_7
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_17
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_19 = min(A_lvl_i_4, phase_stop_17)
                                                            if A_lvl_i_4 == phase_stop_19
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_16
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_16
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_16 + 1
                                        end
                                    end
                                    B_lvl_q = B_lvl_q_step
                                end
                                break
                            end
                        end
                    end
                end
            end
        end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    for i_4 = 1:A_lvl.shape[1]
        val = Ct_lvl_2_val
        Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
        B_lvl_ptr_2 = B_lvl_ptr
        B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
        B_lvl_tbl1_2 = B_lvl_tbl1
        B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
        B_lvl_tbl2_2 = B_lvl_tbl2
        B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
        val_2 = B_lvl_val
        B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
        A_lvl_ptr_2 = A_lvl_ptr
        A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
        A_lvl_tbl1_2 = A_lvl_tbl1
        A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
        A_lvl_tbl2_2 = A_lvl_tbl2
        A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
        val_3 = A_lvl_val
        A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
        Threads.@threads for i_5 = 1:Threads.nthreads()
                B_lvl_q = B_lvl_ptr[1]
                B_lvl_q_stop = B_lvl_ptr[1 + 1]
                if B_lvl_q < B_lvl_q_stop
                    B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                else
                    B_lvl_i_stop = 0
                end
                phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_5 + -1), Threads.nthreads()))
                phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_5, Threads.nthreads()))
                if phase_stop_2 >= phase_start_2
                    if B_lvl_tbl2[B_lvl_q] < phase_start_2
                        B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    while true
                        B_lvl_i = B_lvl_tbl2[B_lvl_q]
                        B_lvl_q_step = B_lvl_q
                        if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                            B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        if B_lvl_i < phase_stop_2
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_4, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_4
                                            if A_lvl_tbl1[A_lvl_q] < i_4
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        else
                            phase_stop_13 = min(B_lvl_i, phase_stop_2)
                            if B_lvl_i == phase_stop_13
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_4, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_4
                                                if A_lvl_tbl1[A_lvl_q] < i_4
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            end
                            break
                        end
                    end
                end
            end
        Ct_lvl_2_val = val
        B_lvl_ptr = B_lvl_ptr_2
        B_lvl_tbl1 = B_lvl_tbl1_2
        B_lvl_tbl2 = B_lvl_tbl2_2
        B_lvl_val = val_2
        A_lvl_ptr = A_lvl_ptr_2
        A_lvl_tbl1 = A_lvl_tbl1_2
        A_lvl_tbl2 = A_lvl_tbl2_2
        A_lvl_val = val_3
    end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(Ct_lvl_2_val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            B_lvl_q = B_lvl_ptr[1]
            B_lvl_q_stop = B_lvl_ptr[1 + 1]
            if B_lvl_q < B_lvl_q_stop
                B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
            else
                B_lvl_i_stop = 0
            end
            phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                if B_lvl_tbl2[B_lvl_q] < phase_start_2
                    B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                end
                while true
                    B_lvl_i = B_lvl_tbl2[B_lvl_q]
                    B_lvl_q_step = B_lvl_q
                    if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                        B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    if B_lvl_i < phase_stop_2
                        Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                        for i_6 = 1:A_lvl.shape[1]
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_6
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_6, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_6
                                            if A_lvl_tbl1[A_lvl_q] < i_6
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_6, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                        end
                        B_lvl_q = B_lvl_q_step
                    else
                        phase_stop_13 = min(B_lvl_i, phase_stop_2)
                        if B_lvl_i == phase_stop_13
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                            for i_8 = 1:A_lvl.shape[1]
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_8
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_8, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_8
                                                if A_lvl_tbl1[A_lvl_q] < i_8
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_8, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        end
                        break
                    end
                end
            end
        end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    B_lvl_q = B_lvl_ptr[1]
    B_lvl_q_stop = B_lvl_ptr[1 + 1]
    if B_lvl_q < B_lvl_q_stop
        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
    else
        B_lvl_i_stop = 0
    end
    phase_stop = min(B_lvl.shape[2], B_lvl_i_stop)
    if phase_stop >= 1
        if B_lvl_tbl2[B_lvl_q] < 1
            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
        end
        while true
            B_lvl_i = B_lvl_tbl2[B_lvl_q]
            B_lvl_q_step = B_lvl_q
            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
            end
            if B_lvl_i < phase_stop
                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                val = Ct_lvl_2_val
                Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                B_lvl_tbl1_2 = B_lvl_tbl1
                B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                B_lvl_tbl2_2 = B_lvl_tbl2
                val_2 = B_lvl_val
                B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                A_lvl_ptr_2 = A_lvl_ptr
                A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                A_lvl_tbl1_2 = A_lvl_tbl1
                A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                A_lvl_tbl2_2 = A_lvl_tbl2
                A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                val_3 = A_lvl_val
                A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                Threads.@threads for i_9 = 1:Threads.nthreads()
                        phase_start_6 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_9), Threads.nthreads()))
                        phase_stop_7 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_9, Threads.nthreads()))
                        if phase_stop_7 >= phase_start_6
                            for i_12 = phase_start_6:phase_stop_7
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_12
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_8 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_8 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_8
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_9 = min(B_lvl_i_2, phase_stop_8, A_lvl_i)
                                        if A_lvl_i == phase_stop_9 && B_lvl_i_2 == phase_stop_9
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_10 = min(i_12, A_lvl_i_stop_2)
                                            if phase_stop_10 >= i_12
                                                if A_lvl_tbl1[A_lvl_q] < i_12
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_12, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_10
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_12 = min(A_lvl_i_2, phase_stop_10)
                                                        if A_lvl_i_2 == phase_stop_12
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_9
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_9
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_9 + 1
                                    end
                                end
                            end
                        end
                    end
                Ct_lvl_2_val = val
                B_lvl_tbl1 = B_lvl_tbl1_2
                B_lvl_tbl2 = B_lvl_tbl2_2
                B_lvl_val = val_2
                A_lvl_ptr = A_lvl_ptr_2
                A_lvl_tbl1 = A_lvl_tbl1_2
                A_lvl_tbl2 = A_lvl_tbl2_2
                A_lvl_val = val_3
                B_lvl_q = B_lvl_q_step
            else
                phase_stop_18 = min(B_lvl_i, phase_stop)
                if B_lvl_i == phase_stop_18
                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_18
                    val_4 = Ct_lvl_2_val
                    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                    B_lvl_tbl1_3 = B_lvl_tbl1
                    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                    B_lvl_tbl2_3 = B_lvl_tbl2
                    val_5 = B_lvl_val
                    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                    A_lvl_ptr_3 = A_lvl_ptr
                    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                    A_lvl_tbl1_3 = A_lvl_tbl1
                    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                    A_lvl_tbl2_3 = A_lvl_tbl2
                    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                    val_6 = A_lvl_val
                    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                    Threads.@threads for i_19 = 1:Threads.nthreads()
                            phase_start_21 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_19), Threads.nthreads()))
                            phase_stop_23 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_19, Threads.nthreads()))
                            if phase_stop_23 >= phase_start_21
                                for i_22 = phase_start_21:phase_stop_23
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_22
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_24 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_24 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_24
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_25 = min(B_lvl_i_2, A_lvl_i, phase_stop_24)
                                            if A_lvl_i == phase_stop_25 && B_lvl_i_2 == phase_stop_25
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_26 = min(i_22, A_lvl_i_stop_4)
                                                if phase_stop_26 >= i_22
                                                    if A_lvl_tbl1[A_lvl_q] < i_22
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_22, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_26
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_28 = min(A_lvl_i_4, phase_stop_26)
                                                            if A_lvl_i_4 == phase_stop_28
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_25
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_25
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_25 + 1
                                        end
                                    end
                                end
                            end
                        end
                    Ct_lvl_2_val = val_4
                    B_lvl_tbl1 = B_lvl_tbl1_3
                    B_lvl_tbl2 = B_lvl_tbl2_3
                    B_lvl_val = val_5
                    A_lvl_ptr = A_lvl_ptr_3
                    A_lvl_tbl1 = A_lvl_tbl1_3
                    A_lvl_tbl2 = A_lvl_tbl2_3
                    A_lvl_val = val_6
                    B_lvl_q = B_lvl_q_step
                end
                break
            end
        end
    end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(Ct_lvl_2_val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            phase_start_2 = max(1, 1 + fld(A_lvl.shape[1] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                for i_7 = phase_start_2:phase_stop_2
                    B_lvl_q = B_lvl_ptr[1]
                    B_lvl_q_stop = B_lvl_ptr[1 + 1]
                    if B_lvl_q < B_lvl_q_stop
                        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                    else
                        B_lvl_i_stop = 0
                    end
                    phase_stop_3 = min(B_lvl.shape[2], B_lvl_i_stop)
                    if phase_stop_3 >= 1
                        if B_lvl_tbl2[B_lvl_q] < 1
                            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        while true
                            B_lvl_i = B_lvl_tbl2[B_lvl_q]
                            B_lvl_q_step = B_lvl_q
                            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                            end
                            if B_lvl_i < phase_stop_3
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_5 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_5 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_5
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_6 = min(B_lvl_i_2, phase_stop_5, A_lvl_i)
                                        if A_lvl_i == phase_stop_6 && B_lvl_i_2 == phase_stop_6
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_7 = min(i_7, A_lvl_i_stop_2)
                                            if phase_stop_7 >= i_7
                                                if A_lvl_tbl1[A_lvl_q] < i_7
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_7
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_9 = min(A_lvl_i_2, phase_stop_7)
                                                        if A_lvl_i_2 == phase_stop_9
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_6
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_6
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_6 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            else
                                phase_stop_14 = min(B_lvl_i, phase_stop_3)
                                if B_lvl_i == phase_stop_14
                                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_14
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_15 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_15 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_15
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_16 = min(B_lvl_i_2, A_lvl_i, phase_stop_15)
                                            if A_lvl_i == phase_stop_16 && B_lvl_i_2 == phase_stop_16
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_17 = min(i_7, A_lvl_i_stop_4)
                                                if phase_stop_17 >= i_7
                                                    if A_lvl_tbl1[A_lvl_q] < i_7
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_17
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_19 = min(A_lvl_i_4, phase_stop_17)
                                                            if A_lvl_i_4 == phase_stop_19
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_16
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_16
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_16 + 1
                                        end
                                    end
                                    B_lvl_q = B_lvl_q_step
                                end
                                break
                            end
                        end
                    end
                end
            end
        end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    for i_4 = 1:A_lvl.shape[1]
        val = Ct_lvl_2_val
        Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
        B_lvl_ptr_2 = B_lvl_ptr
        B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
        B_lvl_tbl1_2 = B_lvl_tbl1
        B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
        B_lvl_tbl2_2 = B_lvl_tbl2
        B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
        val_2 = B_lvl_val
        B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
        A_lvl_ptr_2 = A_lvl_ptr
        A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
        A_lvl_tbl1_2 = A_lvl_tbl1
        A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
        A_lvl_tbl2_2 = A_lvl_tbl2
        A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
        val_3 = A_lvl_val
        A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
        Threads.@threads for i_5 = 1:Threads.nthreads()
                B_lvl_q = B_lvl_ptr[1]
                B_lvl_q_stop = B_lvl_ptr[1 + 1]
                if B_lvl_q < B_lvl_q_stop
                    B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                else
                    B_lvl_i_stop = 0
                end
                phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_5 + -1), Threads.nthreads()))
                phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_5, Threads.nthreads()))
                if phase_stop_2 >= phase_start_2
                    if B_lvl_tbl2[B_lvl_q] < phase_start_2
                        B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    while true
                        B_lvl_i = B_lvl_tbl2[B_lvl_q]
                        B_lvl_q_step = B_lvl_q
                        if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                            B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        if B_lvl_i < phase_stop_2
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_4, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_4
                                            if A_lvl_tbl1[A_lvl_q] < i_4
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        else
                            phase_stop_13 = min(B_lvl_i, phase_stop_2)
                            if B_lvl_i == phase_stop_13
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_4, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_4
                                                if A_lvl_tbl1[A_lvl_q] < i_4
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            end
                            break
                        end
                    end
                end
            end
        Ct_lvl_2_val = val
        B_lvl_ptr = B_lvl_ptr_2
        B_lvl_tbl1 = B_lvl_tbl1_2
        B_lvl_tbl2 = B_lvl_tbl2_2
        B_lvl_val = val_2
        A_lvl_ptr = A_lvl_ptr_2
        A_lvl_tbl1 = A_lvl_tbl1_2
        A_lvl_tbl2 = A_lvl_tbl2_2
        A_lvl_val = val_3
    end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(Ct_lvl_2_val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            B_lvl_q = B_lvl_ptr[1]
            B_lvl_q_stop = B_lvl_ptr[1 + 1]
            if B_lvl_q < B_lvl_q_stop
                B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
            else
                B_lvl_i_stop = 0
            end
            phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                if B_lvl_tbl2[B_lvl_q] < phase_start_2
                    B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                end
                while true
                    B_lvl_i = B_lvl_tbl2[B_lvl_q]
                    B_lvl_q_step = B_lvl_q
                    if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                        B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    if B_lvl_i < phase_stop_2
                        Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                        for i_6 = 1:A_lvl.shape[1]
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_6
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_6, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_6
                                            if A_lvl_tbl1[A_lvl_q] < i_6
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_6, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                        end
                        B_lvl_q = B_lvl_q_step
                    else
                        phase_stop_13 = min(B_lvl_i, phase_stop_2)
                        if B_lvl_i == phase_stop_13
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                            for i_8 = 1:A_lvl.shape[1]
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_8
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_8, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_8
                                                if A_lvl_tbl1[A_lvl_q] < i_8
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_8, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        end
                        break
                    end
                end
            end
        end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    B_lvl_q = B_lvl_ptr[1]
    B_lvl_q_stop = B_lvl_ptr[1 + 1]
    if B_lvl_q < B_lvl_q_stop
        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
    else
        B_lvl_i_stop = 0
    end
    phase_stop = min(B_lvl.shape[2], B_lvl_i_stop)
    if phase_stop >= 1
        if B_lvl_tbl2[B_lvl_q] < 1
            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
        end
        while true
            B_lvl_i = B_lvl_tbl2[B_lvl_q]
            B_lvl_q_step = B_lvl_q
            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
            end
            if B_lvl_i < phase_stop
                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                val = Ct_lvl_2_val
                Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                B_lvl_tbl1_2 = B_lvl_tbl1
                B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                B_lvl_tbl2_2 = B_lvl_tbl2
                val_2 = B_lvl_val
                B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                A_lvl_ptr_2 = A_lvl_ptr
                A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                A_lvl_tbl1_2 = A_lvl_tbl1
                A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                A_lvl_tbl2_2 = A_lvl_tbl2
                A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                val_3 = A_lvl_val
                A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                Threads.@threads for i_9 = 1:Threads.nthreads()
                        phase_start_6 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_9), Threads.nthreads()))
                        phase_stop_7 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_9, Threads.nthreads()))
                        if phase_stop_7 >= phase_start_6
                            for i_12 = phase_start_6:phase_stop_7
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_12
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_8 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_8 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_8
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_9 = min(B_lvl_i_2, phase_stop_8, A_lvl_i)
                                        if A_lvl_i == phase_stop_9 && B_lvl_i_2 == phase_stop_9
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_10 = min(i_12, A_lvl_i_stop_2)
                                            if phase_stop_10 >= i_12
                                                if A_lvl_tbl1[A_lvl_q] < i_12
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_12, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_10
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_12 = min(A_lvl_i_2, phase_stop_10)
                                                        if A_lvl_i_2 == phase_stop_12
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_9
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_9
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_9 + 1
                                    end
                                end
                            end
                        end
                    end
                Ct_lvl_2_val = val
                B_lvl_tbl1 = B_lvl_tbl1_2
                B_lvl_tbl2 = B_lvl_tbl2_2
                B_lvl_val = val_2
                A_lvl_ptr = A_lvl_ptr_2
                A_lvl_tbl1 = A_lvl_tbl1_2
                A_lvl_tbl2 = A_lvl_tbl2_2
                A_lvl_val = val_3
                B_lvl_q = B_lvl_q_step
            else
                phase_stop_18 = min(B_lvl_i, phase_stop)
                if B_lvl_i == phase_stop_18
                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_18
                    val_4 = Ct_lvl_2_val
                    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                    B_lvl_tbl1_3 = B_lvl_tbl1
                    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                    B_lvl_tbl2_3 = B_lvl_tbl2
                    val_5 = B_lvl_val
                    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                    A_lvl_ptr_3 = A_lvl_ptr
                    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                    A_lvl_tbl1_3 = A_lvl_tbl1
                    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                    A_lvl_tbl2_3 = A_lvl_tbl2
                    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                    val_6 = A_lvl_val
                    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                    Threads.@threads for i_19 = 1:Threads.nthreads()
                            phase_start_21 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_19), Threads.nthreads()))
                            phase_stop_23 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_19, Threads.nthreads()))
                            if phase_stop_23 >= phase_start_21
                                for i_22 = phase_start_21:phase_stop_23
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_22
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_24 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_24 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_24
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_25 = min(B_lvl_i_2, A_lvl_i, phase_stop_24)
                                            if A_lvl_i == phase_stop_25 && B_lvl_i_2 == phase_stop_25
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_26 = min(i_22, A_lvl_i_stop_4)
                                                if phase_stop_26 >= i_22
                                                    if A_lvl_tbl1[A_lvl_q] < i_22
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_22, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_26
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_28 = min(A_lvl_i_4, phase_stop_26)
                                                            if A_lvl_i_4 == phase_stop_28
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_25
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_25
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_25 + 1
                                        end
                                    end
                                end
                            end
                        end
                    Ct_lvl_2_val = val_4
                    B_lvl_tbl1 = B_lvl_tbl1_3
                    B_lvl_tbl2 = B_lvl_tbl2_3
                    B_lvl_val = val_5
                    A_lvl_ptr = A_lvl_ptr_3
                    A_lvl_tbl1 = A_lvl_tbl1_3
                    A_lvl_tbl2 = A_lvl_tbl2_3
                    A_lvl_val = val_6
                    B_lvl_q = B_lvl_q_step
                end
                break
            end
        end
    end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(Ct_lvl_2_val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            phase_start_2 = max(1, 1 + fld(A_lvl.shape[1] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                for i_7 = phase_start_2:phase_stop_2
                    B_lvl_q = B_lvl_ptr[1]
                    B_lvl_q_stop = B_lvl_ptr[1 + 1]
                    if B_lvl_q < B_lvl_q_stop
                        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                    else
                        B_lvl_i_stop = 0
                    end
                    phase_stop_3 = min(B_lvl.shape[2], B_lvl_i_stop)
                    if phase_stop_3 >= 1
                        if B_lvl_tbl2[B_lvl_q] < 1
                            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        while true
                            B_lvl_i = B_lvl_tbl2[B_lvl_q]
                            B_lvl_q_step = B_lvl_q
                            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                            end
                            if B_lvl_i < phase_stop_3
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_5 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_5 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_5
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_6 = min(B_lvl_i_2, phase_stop_5, A_lvl_i)
                                        if A_lvl_i == phase_stop_6 && B_lvl_i_2 == phase_stop_6
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_7 = min(i_7, A_lvl_i_stop_2)
                                            if phase_stop_7 >= i_7
                                                if A_lvl_tbl1[A_lvl_q] < i_7
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_7
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_9 = min(A_lvl_i_2, phase_stop_7)
                                                        if A_lvl_i_2 == phase_stop_9
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_6
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_6
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_6 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            else
                                phase_stop_14 = min(B_lvl_i, phase_stop_3)
                                if B_lvl_i == phase_stop_14
                                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_14
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_15 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_15 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_15
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_16 = min(B_lvl_i_2, A_lvl_i, phase_stop_15)
                                            if A_lvl_i == phase_stop_16 && B_lvl_i_2 == phase_stop_16
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_17 = min(i_7, A_lvl_i_stop_4)
                                                if phase_stop_17 >= i_7
                                                    if A_lvl_tbl1[A_lvl_q] < i_7
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_17
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_19 = min(A_lvl_i_4, phase_stop_17)
                                                            if A_lvl_i_4 == phase_stop_19
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_16
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_16
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_16 + 1
                                        end
                                    end
                                    B_lvl_q = B_lvl_q_step
                                end
                                break
                            end
                        end
                    end
                end
            end
        end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    for i_4 = 1:A_lvl.shape[1]
        val = Ct_lvl_2_val
        Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
        B_lvl_ptr_2 = B_lvl_ptr
        B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
        B_lvl_tbl1_2 = B_lvl_tbl1
        B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
        B_lvl_tbl2_2 = B_lvl_tbl2
        B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
        val_2 = B_lvl_val
        B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
        A_lvl_ptr_2 = A_lvl_ptr
        A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
        A_lvl_tbl1_2 = A_lvl_tbl1
        A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
        A_lvl_tbl2_2 = A_lvl_tbl2
        A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
        val_3 = A_lvl_val
        A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
        Threads.@threads for i_5 = 1:Threads.nthreads()
                B_lvl_q = B_lvl_ptr[1]
                B_lvl_q_stop = B_lvl_ptr[1 + 1]
                if B_lvl_q < B_lvl_q_stop
                    B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                else
                    B_lvl_i_stop = 0
                end
                phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_5 + -1), Threads.nthreads()))
                phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_5, Threads.nthreads()))
                if phase_stop_2 >= phase_start_2
                    if B_lvl_tbl2[B_lvl_q] < phase_start_2
                        B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    while true
                        B_lvl_i = B_lvl_tbl2[B_lvl_q]
                        B_lvl_q_step = B_lvl_q
                        if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                            B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        if B_lvl_i < phase_stop_2
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_4, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_4
                                            if A_lvl_tbl1[A_lvl_q] < i_4
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        else
                            phase_stop_13 = min(B_lvl_i, phase_stop_2)
                            if B_lvl_i == phase_stop_13
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_4, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_4
                                                if A_lvl_tbl1[A_lvl_q] < i_4
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            end
                            break
                        end
                    end
                end
            end
        Ct_lvl_2_val = val
        B_lvl_ptr = B_lvl_ptr_2
        B_lvl_tbl1 = B_lvl_tbl1_2
        B_lvl_tbl2 = B_lvl_tbl2_2
        B_lvl_val = val_2
        A_lvl_ptr = A_lvl_ptr_2
        A_lvl_tbl1 = A_lvl_tbl1_2
        A_lvl_tbl2 = A_lvl_tbl2_2
        A_lvl_val = val_3
    end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(Ct_lvl_2_val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            B_lvl_q = B_lvl_ptr[1]
            B_lvl_q_stop = B_lvl_ptr[1 + 1]
            if B_lvl_q < B_lvl_q_stop
                B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
            else
                B_lvl_i_stop = 0
            end
            phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                if B_lvl_tbl2[B_lvl_q] < phase_start_2
                    B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                end
                while true
                    B_lvl_i = B_lvl_tbl2[B_lvl_q]
                    B_lvl_q_step = B_lvl_q
                    if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                        B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    if B_lvl_i < phase_stop_2
                        Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                        for i_6 = 1:A_lvl.shape[1]
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_6
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_6, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_6
                                            if A_lvl_tbl1[A_lvl_q] < i_6
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_6, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                        end
                        B_lvl_q = B_lvl_q_step
                    else
                        phase_stop_13 = min(B_lvl_i, phase_stop_2)
                        if B_lvl_i == phase_stop_13
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                            for i_8 = 1:A_lvl.shape[1]
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_8
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_8, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_8
                                                if A_lvl_tbl1[A_lvl_q] < i_8
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_8, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        end
                        break
                    end
                end
            end
        end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    B_lvl_q = B_lvl_ptr[1]
    B_lvl_q_stop = B_lvl_ptr[1 + 1]
    if B_lvl_q < B_lvl_q_stop
        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
    else
        B_lvl_i_stop = 0
    end
    phase_stop = min(B_lvl.shape[2], B_lvl_i_stop)
    if phase_stop >= 1
        if B_lvl_tbl2[B_lvl_q] < 1
            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
        end
        while true
            B_lvl_i = B_lvl_tbl2[B_lvl_q]
            B_lvl_q_step = B_lvl_q
            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
            end
            if B_lvl_i < phase_stop
                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                val = Ct_lvl_2_val
                Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                B_lvl_tbl1_2 = B_lvl_tbl1
                B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                B_lvl_tbl2_2 = B_lvl_tbl2
                val_2 = B_lvl_val
                B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                A_lvl_ptr_2 = A_lvl_ptr
                A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                A_lvl_tbl1_2 = A_lvl_tbl1
                A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                A_lvl_tbl2_2 = A_lvl_tbl2
                A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                val_3 = A_lvl_val
                A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                Threads.@threads for i_9 = 1:Threads.nthreads()
                        phase_start_6 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_9), Threads.nthreads()))
                        phase_stop_7 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_9, Threads.nthreads()))
                        if phase_stop_7 >= phase_start_6
                            for i_12 = phase_start_6:phase_stop_7
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_12
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_8 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_8 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_8
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_9 = min(B_lvl_i_2, phase_stop_8, A_lvl_i)
                                        if A_lvl_i == phase_stop_9 && B_lvl_i_2 == phase_stop_9
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_10 = min(i_12, A_lvl_i_stop_2)
                                            if phase_stop_10 >= i_12
                                                if A_lvl_tbl1[A_lvl_q] < i_12
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_12, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_10
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_12 = min(A_lvl_i_2, phase_stop_10)
                                                        if A_lvl_i_2 == phase_stop_12
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_9
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_9
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_9 + 1
                                    end
                                end
                            end
                        end
                    end
                Ct_lvl_2_val = val
                B_lvl_tbl1 = B_lvl_tbl1_2
                B_lvl_tbl2 = B_lvl_tbl2_2
                B_lvl_val = val_2
                A_lvl_ptr = A_lvl_ptr_2
                A_lvl_tbl1 = A_lvl_tbl1_2
                A_lvl_tbl2 = A_lvl_tbl2_2
                A_lvl_val = val_3
                B_lvl_q = B_lvl_q_step
            else
                phase_stop_18 = min(B_lvl_i, phase_stop)
                if B_lvl_i == phase_stop_18
                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_18
                    val_4 = Ct_lvl_2_val
                    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                    B_lvl_tbl1_3 = B_lvl_tbl1
                    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                    B_lvl_tbl2_3 = B_lvl_tbl2
                    val_5 = B_lvl_val
                    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                    A_lvl_ptr_3 = A_lvl_ptr
                    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                    A_lvl_tbl1_3 = A_lvl_tbl1
                    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                    A_lvl_tbl2_3 = A_lvl_tbl2
                    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                    val_6 = A_lvl_val
                    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                    Threads.@threads for i_19 = 1:Threads.nthreads()
                            phase_start_21 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_19), Threads.nthreads()))
                            phase_stop_23 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_19, Threads.nthreads()))
                            if phase_stop_23 >= phase_start_21
                                for i_22 = phase_start_21:phase_stop_23
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_22
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_24 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_24 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_24
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_25 = min(B_lvl_i_2, A_lvl_i, phase_stop_24)
                                            if A_lvl_i == phase_stop_25 && B_lvl_i_2 == phase_stop_25
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_26 = min(i_22, A_lvl_i_stop_4)
                                                if phase_stop_26 >= i_22
                                                    if A_lvl_tbl1[A_lvl_q] < i_22
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_22, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_26
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_28 = min(A_lvl_i_4, phase_stop_26)
                                                            if A_lvl_i_4 == phase_stop_28
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_25
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_25
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_25 + 1
                                        end
                                    end
                                end
                            end
                        end
                    Ct_lvl_2_val = val_4
                    B_lvl_tbl1 = B_lvl_tbl1_3
                    B_lvl_tbl2 = B_lvl_tbl2_3
                    B_lvl_val = val_5
                    A_lvl_ptr = A_lvl_ptr_3
                    A_lvl_tbl1 = A_lvl_tbl1_3
                    A_lvl_tbl2 = A_lvl_tbl2_3
                    A_lvl_val = val_6
                    B_lvl_q = B_lvl_q_step
                end
                break
            end
        end
    end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(Ct_lvl_2_val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            phase_start_2 = max(1, 1 + fld(A_lvl.shape[1] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                for i_7 = phase_start_2:phase_stop_2
                    B_lvl_q = B_lvl_ptr[1]
                    B_lvl_q_stop = B_lvl_ptr[1 + 1]
                    if B_lvl_q < B_lvl_q_stop
                        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                    else
                        B_lvl_i_stop = 0
                    end
                    phase_stop_3 = min(B_lvl.shape[2], B_lvl_i_stop)
                    if phase_stop_3 >= 1
                        if B_lvl_tbl2[B_lvl_q] < 1
                            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        while true
                            B_lvl_i = B_lvl_tbl2[B_lvl_q]
                            B_lvl_q_step = B_lvl_q
                            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                            end
                            if B_lvl_i < phase_stop_3
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_5 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_5 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_5
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_6 = min(B_lvl_i_2, phase_stop_5, A_lvl_i)
                                        if A_lvl_i == phase_stop_6 && B_lvl_i_2 == phase_stop_6
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_7 = min(i_7, A_lvl_i_stop_2)
                                            if phase_stop_7 >= i_7
                                                if A_lvl_tbl1[A_lvl_q] < i_7
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_7
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_9 = min(A_lvl_i_2, phase_stop_7)
                                                        if A_lvl_i_2 == phase_stop_9
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_6
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_6
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_6 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            else
                                phase_stop_14 = min(B_lvl_i, phase_stop_3)
                                if B_lvl_i == phase_stop_14
                                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_14
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_15 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_15 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_15
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_16 = min(B_lvl_i_2, A_lvl_i, phase_stop_15)
                                            if A_lvl_i == phase_stop_16 && B_lvl_i_2 == phase_stop_16
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_17 = min(i_7, A_lvl_i_stop_4)
                                                if phase_stop_17 >= i_7
                                                    if A_lvl_tbl1[A_lvl_q] < i_7
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_17
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_19 = min(A_lvl_i_4, phase_stop_17)
                                                            if A_lvl_i_4 == phase_stop_19
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_16
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_16
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_16 + 1
                                        end
                                    end
                                    B_lvl_q = B_lvl_q_step
                                end
                                break
                            end
                        end
                    end
                end
            end
        end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    for i_4 = 1:A_lvl.shape[1]
        val = Ct_lvl_2_val
        Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
        B_lvl_ptr_2 = B_lvl_ptr
        B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
        B_lvl_tbl1_2 = B_lvl_tbl1
        B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
        B_lvl_tbl2_2 = B_lvl_tbl2
        B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
        val_2 = B_lvl_val
        B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
        A_lvl_ptr_2 = A_lvl_ptr
        A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
        A_lvl_tbl1_2 = A_lvl_tbl1
        A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
        A_lvl_tbl2_2 = A_lvl_tbl2
        A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
        val_3 = A_lvl_val
        A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
        Threads.@threads for i_5 = 1:Threads.nthreads()
                B_lvl_q = B_lvl_ptr[1]
                B_lvl_q_stop = B_lvl_ptr[1 + 1]
                if B_lvl_q < B_lvl_q_stop
                    B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                else
                    B_lvl_i_stop = 0
                end
                phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_5 + -1), Threads.nthreads()))
                phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_5, Threads.nthreads()))
                if phase_stop_2 >= phase_start_2
                    if B_lvl_tbl2[B_lvl_q] < phase_start_2
                        B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    while true
                        B_lvl_i = B_lvl_tbl2[B_lvl_q]
                        B_lvl_q_step = B_lvl_q
                        if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                            B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        if B_lvl_i < phase_stop_2
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_4, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_4
                                            if A_lvl_tbl1[A_lvl_q] < i_4
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        else
                            phase_stop_13 = min(B_lvl_i, phase_stop_2)
                            if B_lvl_i == phase_stop_13
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_4, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_4
                                                if A_lvl_tbl1[A_lvl_q] < i_4
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            end
                            break
                        end
                    end
                end
            end
        Ct_lvl_2_val = val
        B_lvl_ptr = B_lvl_ptr_2
        B_lvl_tbl1 = B_lvl_tbl1_2
        B_lvl_tbl2 = B_lvl_tbl2_2
        B_lvl_val = val_2
        A_lvl_ptr = A_lvl_ptr_2
        A_lvl_tbl1 = A_lvl_tbl1_2
        A_lvl_tbl2 = A_lvl_tbl2_2
        A_lvl_val = val_3
    end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(Ct_lvl_2_val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            B_lvl_q = B_lvl_ptr[1]
            B_lvl_q_stop = B_lvl_ptr[1 + 1]
            if B_lvl_q < B_lvl_q_stop
                B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
            else
                B_lvl_i_stop = 0
            end
            phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                if B_lvl_tbl2[B_lvl_q] < phase_start_2
                    B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                end
                while true
                    B_lvl_i = B_lvl_tbl2[B_lvl_q]
                    B_lvl_q_step = B_lvl_q
                    if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                        B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    if B_lvl_i < phase_stop_2
                        Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                        for i_6 = 1:A_lvl.shape[1]
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_6
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_6, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_6
                                            if A_lvl_tbl1[A_lvl_q] < i_6
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_6, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                        end
                        B_lvl_q = B_lvl_q_step
                    else
                        phase_stop_13 = min(B_lvl_i, phase_stop_2)
                        if B_lvl_i == phase_stop_13
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                            for i_8 = 1:A_lvl.shape[1]
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_8
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_8, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_8
                                                if A_lvl_tbl1[A_lvl_q] < i_8
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_8, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        end
                        break
                    end
                end
            end
        end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    B_lvl_q = B_lvl_ptr[1]
    B_lvl_q_stop = B_lvl_ptr[1 + 1]
    if B_lvl_q < B_lvl_q_stop
        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
    else
        B_lvl_i_stop = 0
    end
    phase_stop = min(B_lvl.shape[2], B_lvl_i_stop)
    if phase_stop >= 1
        if B_lvl_tbl2[B_lvl_q] < 1
            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
        end
        while true
            B_lvl_i = B_lvl_tbl2[B_lvl_q]
            B_lvl_q_step = B_lvl_q
            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
            end
            if B_lvl_i < phase_stop
                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                val = Ct_lvl_2_val
                Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                B_lvl_tbl1_2 = B_lvl_tbl1
                B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                B_lvl_tbl2_2 = B_lvl_tbl2
                val_2 = B_lvl_val
                B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                A_lvl_ptr_2 = A_lvl_ptr
                A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                A_lvl_tbl1_2 = A_lvl_tbl1
                A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                A_lvl_tbl2_2 = A_lvl_tbl2
                A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                val_3 = A_lvl_val
                A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                Threads.@threads for i_9 = 1:Threads.nthreads()
                        phase_start_6 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_9), Threads.nthreads()))
                        phase_stop_7 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_9, Threads.nthreads()))
                        if phase_stop_7 >= phase_start_6
                            for i_12 = phase_start_6:phase_stop_7
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_12
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_8 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_8 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_8
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_9 = min(B_lvl_i_2, phase_stop_8, A_lvl_i)
                                        if A_lvl_i == phase_stop_9 && B_lvl_i_2 == phase_stop_9
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_10 = min(i_12, A_lvl_i_stop_2)
                                            if phase_stop_10 >= i_12
                                                if A_lvl_tbl1[A_lvl_q] < i_12
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_12, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_10
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_12 = min(A_lvl_i_2, phase_stop_10)
                                                        if A_lvl_i_2 == phase_stop_12
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_9
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_9
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_9 + 1
                                    end
                                end
                            end
                        end
                    end
                Ct_lvl_2_val = val
                B_lvl_tbl1 = B_lvl_tbl1_2
                B_lvl_tbl2 = B_lvl_tbl2_2
                B_lvl_val = val_2
                A_lvl_ptr = A_lvl_ptr_2
                A_lvl_tbl1 = A_lvl_tbl1_2
                A_lvl_tbl2 = A_lvl_tbl2_2
                A_lvl_val = val_3
                B_lvl_q = B_lvl_q_step
            else
                phase_stop_18 = min(B_lvl_i, phase_stop)
                if B_lvl_i == phase_stop_18
                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_18
                    val_4 = Ct_lvl_2_val
                    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                    B_lvl_tbl1_3 = B_lvl_tbl1
                    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                    B_lvl_tbl2_3 = B_lvl_tbl2
                    val_5 = B_lvl_val
                    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                    A_lvl_ptr_3 = A_lvl_ptr
                    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                    A_lvl_tbl1_3 = A_lvl_tbl1
                    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                    A_lvl_tbl2_3 = A_lvl_tbl2
                    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                    val_6 = A_lvl_val
                    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                    Threads.@threads for i_19 = 1:Threads.nthreads()
                            phase_start_21 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_19), Threads.nthreads()))
                            phase_stop_23 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_19, Threads.nthreads()))
                            if phase_stop_23 >= phase_start_21
                                for i_22 = phase_start_21:phase_stop_23
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_22
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_24 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_24 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_24
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_25 = min(B_lvl_i_2, A_lvl_i, phase_stop_24)
                                            if A_lvl_i == phase_stop_25 && B_lvl_i_2 == phase_stop_25
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_26 = min(i_22, A_lvl_i_stop_4)
                                                if phase_stop_26 >= i_22
                                                    if A_lvl_tbl1[A_lvl_q] < i_22
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_22, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_26
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_28 = min(A_lvl_i_4, phase_stop_26)
                                                            if A_lvl_i_4 == phase_stop_28
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_25
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_25
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_25 + 1
                                        end
                                    end
                                end
                            end
                        end
                    Ct_lvl_2_val = val_4
                    B_lvl_tbl1 = B_lvl_tbl1_3
                    B_lvl_tbl2 = B_lvl_tbl2_3
                    B_lvl_val = val_5
                    A_lvl_ptr = A_lvl_ptr_3
                    A_lvl_tbl1 = A_lvl_tbl1_3
                    A_lvl_tbl2 = A_lvl_tbl2_3
                    A_lvl_val = val_6
                    B_lvl_q = B_lvl_q_step
                end
                break
            end
        end
    end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(Ct_lvl_2_val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            phase_start_2 = max(1, 1 + fld(A_lvl.shape[1] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                for i_7 = phase_start_2:phase_stop_2
                    B_lvl_q = B_lvl_ptr[1]
                    B_lvl_q_stop = B_lvl_ptr[1 + 1]
                    if B_lvl_q < B_lvl_q_stop
                        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                    else
                        B_lvl_i_stop = 0
                    end
                    phase_stop_3 = min(B_lvl.shape[2], B_lvl_i_stop)
                    if phase_stop_3 >= 1
                        if B_lvl_tbl2[B_lvl_q] < 1
                            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        while true
                            B_lvl_i = B_lvl_tbl2[B_lvl_q]
                            B_lvl_q_step = B_lvl_q
                            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                            end
                            if B_lvl_i < phase_stop_3
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_5 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_5 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_5
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_6 = min(B_lvl_i_2, phase_stop_5, A_lvl_i)
                                        if A_lvl_i == phase_stop_6 && B_lvl_i_2 == phase_stop_6
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_7 = min(i_7, A_lvl_i_stop_2)
                                            if phase_stop_7 >= i_7
                                                if A_lvl_tbl1[A_lvl_q] < i_7
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_7
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_9 = min(A_lvl_i_2, phase_stop_7)
                                                        if A_lvl_i_2 == phase_stop_9
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_6
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_6
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_6 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            else
                                phase_stop_14 = min(B_lvl_i, phase_stop_3)
                                if B_lvl_i == phase_stop_14
                                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_14
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_15 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_15 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_15
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_16 = min(B_lvl_i_2, A_lvl_i, phase_stop_15)
                                            if A_lvl_i == phase_stop_16 && B_lvl_i_2 == phase_stop_16
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_17 = min(i_7, A_lvl_i_stop_4)
                                                if phase_stop_17 >= i_7
                                                    if A_lvl_tbl1[A_lvl_q] < i_7
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_17
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_19 = min(A_lvl_i_4, phase_stop_17)
                                                            if A_lvl_i_4 == phase_stop_19
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_16
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_16
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_16 + 1
                                        end
                                    end
                                    B_lvl_q = B_lvl_q_step
                                end
                                break
                            end
                        end
                    end
                end
            end
        end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    for i_4 = 1:A_lvl.shape[1]
        val = Ct_lvl_2_val
        Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
        B_lvl_ptr_2 = B_lvl_ptr
        B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
        B_lvl_tbl1_2 = B_lvl_tbl1
        B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
        B_lvl_tbl2_2 = B_lvl_tbl2
        B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
        val_2 = B_lvl_val
        B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
        A_lvl_ptr_2 = A_lvl_ptr
        A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
        A_lvl_tbl1_2 = A_lvl_tbl1
        A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
        A_lvl_tbl2_2 = A_lvl_tbl2
        A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
        val_3 = A_lvl_val
        A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
        Threads.@threads for i_5 = 1:Threads.nthreads()
                B_lvl_q = B_lvl_ptr[1]
                B_lvl_q_stop = B_lvl_ptr[1 + 1]
                if B_lvl_q < B_lvl_q_stop
                    B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                else
                    B_lvl_i_stop = 0
                end
                phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_5 + -1), Threads.nthreads()))
                phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_5, Threads.nthreads()))
                if phase_stop_2 >= phase_start_2
                    if B_lvl_tbl2[B_lvl_q] < phase_start_2
                        B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    while true
                        B_lvl_i = B_lvl_tbl2[B_lvl_q]
                        B_lvl_q_step = B_lvl_q
                        if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                            B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        if B_lvl_i < phase_stop_2
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_4, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_4
                                            if A_lvl_tbl1[A_lvl_q] < i_4
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        else
                            phase_stop_13 = min(B_lvl_i, phase_stop_2)
                            if B_lvl_i == phase_stop_13
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_4, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_4
                                                if A_lvl_tbl1[A_lvl_q] < i_4
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            end
                            break
                        end
                    end
                end
            end
        Ct_lvl_2_val = val
        B_lvl_ptr = B_lvl_ptr_2
        B_lvl_tbl1 = B_lvl_tbl1_2
        B_lvl_tbl2 = B_lvl_tbl2_2
        B_lvl_val = val_2
        A_lvl_ptr = A_lvl_ptr_2
        A_lvl_tbl1 = A_lvl_tbl1_2
        A_lvl_tbl2 = A_lvl_tbl2_2
        A_lvl_val = val_3
    end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(Ct_lvl_2_val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            B_lvl_q = B_lvl_ptr[1]
            B_lvl_q_stop = B_lvl_ptr[1 + 1]
            if B_lvl_q < B_lvl_q_stop
                B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
            else
                B_lvl_i_stop = 0
            end
            phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                if B_lvl_tbl2[B_lvl_q] < phase_start_2
                    B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                end
                while true
                    B_lvl_i = B_lvl_tbl2[B_lvl_q]
                    B_lvl_q_step = B_lvl_q
                    if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                        B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    if B_lvl_i < phase_stop_2
                        Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                        for i_6 = 1:A_lvl.shape[1]
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_6
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_6, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_6
                                            if A_lvl_tbl1[A_lvl_q] < i_6
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_6, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                        end
                        B_lvl_q = B_lvl_q_step
                    else
                        phase_stop_13 = min(B_lvl_i, phase_stop_2)
                        if B_lvl_i == phase_stop_13
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                            for i_8 = 1:A_lvl.shape[1]
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_8
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_8, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_8
                                                if A_lvl_tbl1[A_lvl_q] < i_8
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_8, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        end
                        break
                    end
                end
            end
        end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    B_lvl_q = B_lvl_ptr[1]
    B_lvl_q_stop = B_lvl_ptr[1 + 1]
    if B_lvl_q < B_lvl_q_stop
        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
    else
        B_lvl_i_stop = 0
    end
    phase_stop = min(B_lvl.shape[2], B_lvl_i_stop)
    if phase_stop >= 1
        if B_lvl_tbl2[B_lvl_q] < 1
            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
        end
        while true
            B_lvl_i = B_lvl_tbl2[B_lvl_q]
            B_lvl_q_step = B_lvl_q
            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
            end
            if B_lvl_i < phase_stop
                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                val = Ct_lvl_2_val
                Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                B_lvl_tbl1_2 = B_lvl_tbl1
                B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                B_lvl_tbl2_2 = B_lvl_tbl2
                val_2 = B_lvl_val
                B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                A_lvl_ptr_2 = A_lvl_ptr
                A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                A_lvl_tbl1_2 = A_lvl_tbl1
                A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                A_lvl_tbl2_2 = A_lvl_tbl2
                A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                val_3 = A_lvl_val
                A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                Threads.@threads for i_9 = 1:Threads.nthreads()
                        phase_start_6 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_9), Threads.nthreads()))
                        phase_stop_7 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_9, Threads.nthreads()))
                        if phase_stop_7 >= phase_start_6
                            for i_12 = phase_start_6:phase_stop_7
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_12
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_8 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_8 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_8
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_9 = min(B_lvl_i_2, phase_stop_8, A_lvl_i)
                                        if A_lvl_i == phase_stop_9 && B_lvl_i_2 == phase_stop_9
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_10 = min(i_12, A_lvl_i_stop_2)
                                            if phase_stop_10 >= i_12
                                                if A_lvl_tbl1[A_lvl_q] < i_12
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_12, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_10
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_12 = min(A_lvl_i_2, phase_stop_10)
                                                        if A_lvl_i_2 == phase_stop_12
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_9
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_9
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_9 + 1
                                    end
                                end
                            end
                        end
                    end
                Ct_lvl_2_val = val
                B_lvl_tbl1 = B_lvl_tbl1_2
                B_lvl_tbl2 = B_lvl_tbl2_2
                B_lvl_val = val_2
                A_lvl_ptr = A_lvl_ptr_2
                A_lvl_tbl1 = A_lvl_tbl1_2
                A_lvl_tbl2 = A_lvl_tbl2_2
                A_lvl_val = val_3
                B_lvl_q = B_lvl_q_step
            else
                phase_stop_18 = min(B_lvl_i, phase_stop)
                if B_lvl_i == phase_stop_18
                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_18
                    val_4 = Ct_lvl_2_val
                    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                    B_lvl_tbl1_3 = B_lvl_tbl1
                    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                    B_lvl_tbl2_3 = B_lvl_tbl2
                    val_5 = B_lvl_val
                    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                    A_lvl_ptr_3 = A_lvl_ptr
                    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                    A_lvl_tbl1_3 = A_lvl_tbl1
                    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                    A_lvl_tbl2_3 = A_lvl_tbl2
                    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                    val_6 = A_lvl_val
                    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                    Threads.@threads for i_19 = 1:Threads.nthreads()
                            phase_start_21 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_19), Threads.nthreads()))
                            phase_stop_23 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_19, Threads.nthreads()))
                            if phase_stop_23 >= phase_start_21
                                for i_22 = phase_start_21:phase_stop_23
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_22
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_24 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_24 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_24
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_25 = min(B_lvl_i_2, A_lvl_i, phase_stop_24)
                                            if A_lvl_i == phase_stop_25 && B_lvl_i_2 == phase_stop_25
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_26 = min(i_22, A_lvl_i_stop_4)
                                                if phase_stop_26 >= i_22
                                                    if A_lvl_tbl1[A_lvl_q] < i_22
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_22, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_26
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_28 = min(A_lvl_i_4, phase_stop_26)
                                                            if A_lvl_i_4 == phase_stop_28
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_25
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_25
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_25 + 1
                                        end
                                    end
                                end
                            end
                        end
                    Ct_lvl_2_val = val_4
                    B_lvl_tbl1 = B_lvl_tbl1_3
                    B_lvl_tbl2 = B_lvl_tbl2_3
                    B_lvl_val = val_5
                    A_lvl_ptr = A_lvl_ptr_3
                    A_lvl_tbl1 = A_lvl_tbl1_3
                    A_lvl_tbl2 = A_lvl_tbl2_3
                    A_lvl_val = val_6
                    B_lvl_q = B_lvl_q_step
                end
                break
            end
        end
    end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(Ct_lvl_2_val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            phase_start_2 = max(1, 1 + fld(A_lvl.shape[1] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                for i_7 = phase_start_2:phase_stop_2
                    B_lvl_q = B_lvl_ptr[1]
                    B_lvl_q_stop = B_lvl_ptr[1 + 1]
                    if B_lvl_q < B_lvl_q_stop
                        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                    else
                        B_lvl_i_stop = 0
                    end
                    phase_stop_3 = min(B_lvl.shape[2], B_lvl_i_stop)
                    if phase_stop_3 >= 1
                        if B_lvl_tbl2[B_lvl_q] < 1
                            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        while true
                            B_lvl_i = B_lvl_tbl2[B_lvl_q]
                            B_lvl_q_step = B_lvl_q
                            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                            end
                            if B_lvl_i < phase_stop_3
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_5 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_5 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_5
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_6 = min(B_lvl_i_2, phase_stop_5, A_lvl_i)
                                        if A_lvl_i == phase_stop_6 && B_lvl_i_2 == phase_stop_6
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_7 = min(i_7, A_lvl_i_stop_2)
                                            if phase_stop_7 >= i_7
                                                if A_lvl_tbl1[A_lvl_q] < i_7
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_7
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_9 = min(A_lvl_i_2, phase_stop_7)
                                                        if A_lvl_i_2 == phase_stop_9
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_6
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_6
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_6 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            else
                                phase_stop_14 = min(B_lvl_i, phase_stop_3)
                                if B_lvl_i == phase_stop_14
                                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_14
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_15 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_15 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_15
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_16 = min(B_lvl_i_2, A_lvl_i, phase_stop_15)
                                            if A_lvl_i == phase_stop_16 && B_lvl_i_2 == phase_stop_16
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_17 = min(i_7, A_lvl_i_stop_4)
                                                if phase_stop_17 >= i_7
                                                    if A_lvl_tbl1[A_lvl_q] < i_7
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_17
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_19 = min(A_lvl_i_4, phase_stop_17)
                                                            if A_lvl_i_4 == phase_stop_19
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_16
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_16
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_16 + 1
                                        end
                                    end
                                    B_lvl_q = B_lvl_q_step
                                end
                                break
                            end
                        end
                    end
                end
            end
        end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    for i_4 = 1:A_lvl.shape[1]
        val = Ct_lvl_2_val
        Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
        B_lvl_ptr_2 = B_lvl_ptr
        B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
        B_lvl_tbl1_2 = B_lvl_tbl1
        B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
        B_lvl_tbl2_2 = B_lvl_tbl2
        B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
        val_2 = B_lvl_val
        B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
        A_lvl_ptr_2 = A_lvl_ptr
        A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
        A_lvl_tbl1_2 = A_lvl_tbl1
        A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
        A_lvl_tbl2_2 = A_lvl_tbl2
        A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
        val_3 = A_lvl_val
        A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
        Threads.@threads for i_5 = 1:Threads.nthreads()
                B_lvl_q = B_lvl_ptr[1]
                B_lvl_q_stop = B_lvl_ptr[1 + 1]
                if B_lvl_q < B_lvl_q_stop
                    B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                else
                    B_lvl_i_stop = 0
                end
                phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_5 + -1), Threads.nthreads()))
                phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_5, Threads.nthreads()))
                if phase_stop_2 >= phase_start_2
                    if B_lvl_tbl2[B_lvl_q] < phase_start_2
                        B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    while true
                        B_lvl_i = B_lvl_tbl2[B_lvl_q]
                        B_lvl_q_step = B_lvl_q
                        if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                            B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        if B_lvl_i < phase_stop_2
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_4, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_4
                                            if A_lvl_tbl1[A_lvl_q] < i_4
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        else
                            phase_stop_13 = min(B_lvl_i, phase_stop_2)
                            if B_lvl_i == phase_stop_13
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_4, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_4
                                                if A_lvl_tbl1[A_lvl_q] < i_4
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            end
                            break
                        end
                    end
                end
            end
        Ct_lvl_2_val = val
        B_lvl_ptr = B_lvl_ptr_2
        B_lvl_tbl1 = B_lvl_tbl1_2
        B_lvl_tbl2 = B_lvl_tbl2_2
        B_lvl_val = val_2
        A_lvl_ptr = A_lvl_ptr_2
        A_lvl_tbl1 = A_lvl_tbl1_2
        A_lvl_tbl2 = A_lvl_tbl2_2
        A_lvl_val = val_3
    end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(Ct_lvl_2_val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            B_lvl_q = B_lvl_ptr[1]
            B_lvl_q_stop = B_lvl_ptr[1 + 1]
            if B_lvl_q < B_lvl_q_stop
                B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
            else
                B_lvl_i_stop = 0
            end
            phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                if B_lvl_tbl2[B_lvl_q] < phase_start_2
                    B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                end
                while true
                    B_lvl_i = B_lvl_tbl2[B_lvl_q]
                    B_lvl_q_step = B_lvl_q
                    if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                        B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    if B_lvl_i < phase_stop_2
                        Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                        for i_6 = 1:A_lvl.shape[1]
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_6
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_6, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_6
                                            if A_lvl_tbl1[A_lvl_q] < i_6
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_6, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                        end
                        B_lvl_q = B_lvl_q_step
                    else
                        phase_stop_13 = min(B_lvl_i, phase_stop_2)
                        if B_lvl_i == phase_stop_13
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                            for i_8 = 1:A_lvl.shape[1]
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_8
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_8, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_8
                                                if A_lvl_tbl1[A_lvl_q] < i_8
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_8, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        end
                        break
                    end
                end
            end
        end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    B_lvl_q = B_lvl_ptr[1]
    B_lvl_q_stop = B_lvl_ptr[1 + 1]
    if B_lvl_q < B_lvl_q_stop
        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
    else
        B_lvl_i_stop = 0
    end
    phase_stop = min(B_lvl.shape[2], B_lvl_i_stop)
    if phase_stop >= 1
        if B_lvl_tbl2[B_lvl_q] < 1
            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
        end
        while true
            B_lvl_i = B_lvl_tbl2[B_lvl_q]
            B_lvl_q_step = B_lvl_q
            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
            end
            if B_lvl_i < phase_stop
                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                val = Ct_lvl_2_val
                Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                B_lvl_tbl1_2 = B_lvl_tbl1
                B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                B_lvl_tbl2_2 = B_lvl_tbl2
                val_2 = B_lvl_val
                B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                A_lvl_ptr_2 = A_lvl_ptr
                A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                A_lvl_tbl1_2 = A_lvl_tbl1
                A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                A_lvl_tbl2_2 = A_lvl_tbl2
                A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                val_3 = A_lvl_val
                A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                Threads.@threads for i_9 = 1:Threads.nthreads()
                        phase_start_6 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_9), Threads.nthreads()))
                        phase_stop_7 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_9, Threads.nthreads()))
                        if phase_stop_7 >= phase_start_6
                            for i_12 = phase_start_6:phase_stop_7
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_12
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_8 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_8 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_8
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_9 = min(B_lvl_i_2, phase_stop_8, A_lvl_i)
                                        if A_lvl_i == phase_stop_9 && B_lvl_i_2 == phase_stop_9
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_10 = min(i_12, A_lvl_i_stop_2)
                                            if phase_stop_10 >= i_12
                                                if A_lvl_tbl1[A_lvl_q] < i_12
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_12, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_10
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_12 = min(A_lvl_i_2, phase_stop_10)
                                                        if A_lvl_i_2 == phase_stop_12
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_9
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_9
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_9 + 1
                                    end
                                end
                            end
                        end
                    end
                Ct_lvl_2_val = val
                B_lvl_tbl1 = B_lvl_tbl1_2
                B_lvl_tbl2 = B_lvl_tbl2_2
                B_lvl_val = val_2
                A_lvl_ptr = A_lvl_ptr_2
                A_lvl_tbl1 = A_lvl_tbl1_2
                A_lvl_tbl2 = A_lvl_tbl2_2
                A_lvl_val = val_3
                B_lvl_q = B_lvl_q_step
            else
                phase_stop_18 = min(B_lvl_i, phase_stop)
                if B_lvl_i == phase_stop_18
                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_18
                    val_4 = Ct_lvl_2_val
                    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                    B_lvl_tbl1_3 = B_lvl_tbl1
                    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                    B_lvl_tbl2_3 = B_lvl_tbl2
                    val_5 = B_lvl_val
                    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                    A_lvl_ptr_3 = A_lvl_ptr
                    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                    A_lvl_tbl1_3 = A_lvl_tbl1
                    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                    A_lvl_tbl2_3 = A_lvl_tbl2
                    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                    val_6 = A_lvl_val
                    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                    Threads.@threads for i_19 = 1:Threads.nthreads()
                            phase_start_21 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_19), Threads.nthreads()))
                            phase_stop_23 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_19, Threads.nthreads()))
                            if phase_stop_23 >= phase_start_21
                                for i_22 = phase_start_21:phase_stop_23
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_22
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_24 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_24 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_24
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_25 = min(B_lvl_i_2, A_lvl_i, phase_stop_24)
                                            if A_lvl_i == phase_stop_25 && B_lvl_i_2 == phase_stop_25
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_26 = min(i_22, A_lvl_i_stop_4)
                                                if phase_stop_26 >= i_22
                                                    if A_lvl_tbl1[A_lvl_q] < i_22
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_22, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_26
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_28 = min(A_lvl_i_4, phase_stop_26)
                                                            if A_lvl_i_4 == phase_stop_28
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_25
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_25
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_25 + 1
                                        end
                                    end
                                end
                            end
                        end
                    Ct_lvl_2_val = val_4
                    B_lvl_tbl1 = B_lvl_tbl1_3
                    B_lvl_tbl2 = B_lvl_tbl2_3
                    B_lvl_val = val_5
                    A_lvl_ptr = A_lvl_ptr_3
                    A_lvl_tbl1 = A_lvl_tbl1_3
                    A_lvl_tbl2 = A_lvl_tbl2_3
                    A_lvl_val = val_6
                    B_lvl_q = B_lvl_q_step
                end
                break
            end
        end
    end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(Ct_lvl_2_val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            phase_start_2 = max(1, 1 + fld(A_lvl.shape[1] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                for i_7 = phase_start_2:phase_stop_2
                    B_lvl_q = B_lvl_ptr[1]
                    B_lvl_q_stop = B_lvl_ptr[1 + 1]
                    if B_lvl_q < B_lvl_q_stop
                        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                    else
                        B_lvl_i_stop = 0
                    end
                    phase_stop_3 = min(B_lvl.shape[2], B_lvl_i_stop)
                    if phase_stop_3 >= 1
                        if B_lvl_tbl2[B_lvl_q] < 1
                            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        while true
                            B_lvl_i = B_lvl_tbl2[B_lvl_q]
                            B_lvl_q_step = B_lvl_q
                            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                            end
                            if B_lvl_i < phase_stop_3
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_5 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_5 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_5
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_6 = min(B_lvl_i_2, phase_stop_5, A_lvl_i)
                                        if A_lvl_i == phase_stop_6 && B_lvl_i_2 == phase_stop_6
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_7 = min(i_7, A_lvl_i_stop_2)
                                            if phase_stop_7 >= i_7
                                                if A_lvl_tbl1[A_lvl_q] < i_7
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_7
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_9 = min(A_lvl_i_2, phase_stop_7)
                                                        if A_lvl_i_2 == phase_stop_9
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_6
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_6
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_6 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            else
                                phase_stop_14 = min(B_lvl_i, phase_stop_3)
                                if B_lvl_i == phase_stop_14
                                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_14
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_15 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_15 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_15
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_16 = min(B_lvl_i_2, A_lvl_i, phase_stop_15)
                                            if A_lvl_i == phase_stop_16 && B_lvl_i_2 == phase_stop_16
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_17 = min(i_7, A_lvl_i_stop_4)
                                                if phase_stop_17 >= i_7
                                                    if A_lvl_tbl1[A_lvl_q] < i_7
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_17
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_19 = min(A_lvl_i_4, phase_stop_17)
                                                            if A_lvl_i_4 == phase_stop_19
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_16
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_16
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_16 + 1
                                        end
                                    end
                                    B_lvl_q = B_lvl_q_step
                                end
                                break
                            end
                        end
                    end
                end
            end
        end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    for i_4 = 1:A_lvl.shape[1]
        val = Ct_lvl_2_val
        Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
        B_lvl_ptr_2 = B_lvl_ptr
        B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
        B_lvl_tbl1_2 = B_lvl_tbl1
        B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
        B_lvl_tbl2_2 = B_lvl_tbl2
        B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
        val_2 = B_lvl_val
        B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
        A_lvl_ptr_2 = A_lvl_ptr
        A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
        A_lvl_tbl1_2 = A_lvl_tbl1
        A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
        A_lvl_tbl2_2 = A_lvl_tbl2
        A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
        val_3 = A_lvl_val
        A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
        Threads.@threads for i_5 = 1:Threads.nthreads()
                B_lvl_q = B_lvl_ptr[1]
                B_lvl_q_stop = B_lvl_ptr[1 + 1]
                if B_lvl_q < B_lvl_q_stop
                    B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                else
                    B_lvl_i_stop = 0
                end
                phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_5 + -1), Threads.nthreads()))
                phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_5, Threads.nthreads()))
                if phase_stop_2 >= phase_start_2
                    if B_lvl_tbl2[B_lvl_q] < phase_start_2
                        B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    while true
                        B_lvl_i = B_lvl_tbl2[B_lvl_q]
                        B_lvl_q_step = B_lvl_q
                        if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                            B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        if B_lvl_i < phase_stop_2
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_4, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_4
                                            if A_lvl_tbl1[A_lvl_q] < i_4
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        else
                            phase_stop_13 = min(B_lvl_i, phase_stop_2)
                            if B_lvl_i == phase_stop_13
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_4, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_4
                                                if A_lvl_tbl1[A_lvl_q] < i_4
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            end
                            break
                        end
                    end
                end
            end
        Ct_lvl_2_val = val
        B_lvl_ptr = B_lvl_ptr_2
        B_lvl_tbl1 = B_lvl_tbl1_2
        B_lvl_tbl2 = B_lvl_tbl2_2
        B_lvl_val = val_2
        A_lvl_ptr = A_lvl_ptr_2
        A_lvl_tbl1 = A_lvl_tbl1_2
        A_lvl_tbl2 = A_lvl_tbl2_2
        A_lvl_val = val_3
    end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(Ct_lvl_2_val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            B_lvl_q = B_lvl_ptr[1]
            B_lvl_q_stop = B_lvl_ptr[1 + 1]
            if B_lvl_q < B_lvl_q_stop
                B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
            else
                B_lvl_i_stop = 0
            end
            phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                if B_lvl_tbl2[B_lvl_q] < phase_start_2
                    B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                end
                while true
                    B_lvl_i = B_lvl_tbl2[B_lvl_q]
                    B_lvl_q_step = B_lvl_q
                    if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                        B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    if B_lvl_i < phase_stop_2
                        Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                        for i_6 = 1:A_lvl.shape[1]
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_6
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_6, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_6
                                            if A_lvl_tbl1[A_lvl_q] < i_6
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_6, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                        end
                        B_lvl_q = B_lvl_q_step
                    else
                        phase_stop_13 = min(B_lvl_i, phase_stop_2)
                        if B_lvl_i == phase_stop_13
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                            for i_8 = 1:A_lvl.shape[1]
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_8
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_8, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_8
                                                if A_lvl_tbl1[A_lvl_q] < i_8
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_8, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        end
                        break
                    end
                end
            end
        end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    B_lvl_q = B_lvl_ptr[1]
    B_lvl_q_stop = B_lvl_ptr[1 + 1]
    if B_lvl_q < B_lvl_q_stop
        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
    else
        B_lvl_i_stop = 0
    end
    phase_stop = min(B_lvl.shape[2], B_lvl_i_stop)
    if phase_stop >= 1
        if B_lvl_tbl2[B_lvl_q] < 1
            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
        end
        while true
            B_lvl_i = B_lvl_tbl2[B_lvl_q]
            B_lvl_q_step = B_lvl_q
            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
            end
            if B_lvl_i < phase_stop
                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                val = Ct_lvl_2_val
                Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                B_lvl_tbl1_2 = B_lvl_tbl1
                B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                B_lvl_tbl2_2 = B_lvl_tbl2
                val_2 = B_lvl_val
                B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                A_lvl_ptr_2 = A_lvl_ptr
                A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                A_lvl_tbl1_2 = A_lvl_tbl1
                A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                A_lvl_tbl2_2 = A_lvl_tbl2
                A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                val_3 = A_lvl_val
                A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                Threads.@threads for i_9 = 1:Threads.nthreads()
                        phase_start_6 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_9), Threads.nthreads()))
                        phase_stop_7 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_9, Threads.nthreads()))
                        if phase_stop_7 >= phase_start_6
                            for i_12 = phase_start_6:phase_stop_7
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_12
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_8 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_8 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_8
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_9 = min(B_lvl_i_2, phase_stop_8, A_lvl_i)
                                        if A_lvl_i == phase_stop_9 && B_lvl_i_2 == phase_stop_9
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_10 = min(i_12, A_lvl_i_stop_2)
                                            if phase_stop_10 >= i_12
                                                if A_lvl_tbl1[A_lvl_q] < i_12
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_12, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_10
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_12 = min(A_lvl_i_2, phase_stop_10)
                                                        if A_lvl_i_2 == phase_stop_12
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_9
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_9
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_9 + 1
                                    end
                                end
                            end
                        end
                    end
                Ct_lvl_2_val = val
                B_lvl_tbl1 = B_lvl_tbl1_2
                B_lvl_tbl2 = B_lvl_tbl2_2
                B_lvl_val = val_2
                A_lvl_ptr = A_lvl_ptr_2
                A_lvl_tbl1 = A_lvl_tbl1_2
                A_lvl_tbl2 = A_lvl_tbl2_2
                A_lvl_val = val_3
                B_lvl_q = B_lvl_q_step
            else
                phase_stop_18 = min(B_lvl_i, phase_stop)
                if B_lvl_i == phase_stop_18
                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_18
                    val_4 = Ct_lvl_2_val
                    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                    B_lvl_tbl1_3 = B_lvl_tbl1
                    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                    B_lvl_tbl2_3 = B_lvl_tbl2
                    val_5 = B_lvl_val
                    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                    A_lvl_ptr_3 = A_lvl_ptr
                    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                    A_lvl_tbl1_3 = A_lvl_tbl1
                    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                    A_lvl_tbl2_3 = A_lvl_tbl2
                    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                    val_6 = A_lvl_val
                    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                    Threads.@threads for i_19 = 1:Threads.nthreads()
                            phase_start_21 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_19), Threads.nthreads()))
                            phase_stop_23 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_19, Threads.nthreads()))
                            if phase_stop_23 >= phase_start_21
                                for i_22 = phase_start_21:phase_stop_23
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_22
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_24 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_24 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_24
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_25 = min(B_lvl_i_2, A_lvl_i, phase_stop_24)
                                            if A_lvl_i == phase_stop_25 && B_lvl_i_2 == phase_stop_25
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_26 = min(i_22, A_lvl_i_stop_4)
                                                if phase_stop_26 >= i_22
                                                    if A_lvl_tbl1[A_lvl_q] < i_22
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_22, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_26
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_28 = min(A_lvl_i_4, phase_stop_26)
                                                            if A_lvl_i_4 == phase_stop_28
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_25
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_25
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_25 + 1
                                        end
                                    end
                                end
                            end
                        end
                    Ct_lvl_2_val = val_4
                    B_lvl_tbl1 = B_lvl_tbl1_3
                    B_lvl_tbl2 = B_lvl_tbl2_3
                    B_lvl_val = val_5
                    A_lvl_ptr = A_lvl_ptr_3
                    A_lvl_tbl1 = A_lvl_tbl1_3
                    A_lvl_tbl2 = A_lvl_tbl2_3
                    A_lvl_val = val_6
                    B_lvl_q = B_lvl_q_step
                end
                break
            end
        end
    end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(Ct_lvl_2_val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            phase_start_2 = max(1, 1 + fld(A_lvl.shape[1] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                for i_7 = phase_start_2:phase_stop_2
                    B_lvl_q = B_lvl_ptr[1]
                    B_lvl_q_stop = B_lvl_ptr[1 + 1]
                    if B_lvl_q < B_lvl_q_stop
                        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                    else
                        B_lvl_i_stop = 0
                    end
                    phase_stop_3 = min(B_lvl.shape[2], B_lvl_i_stop)
                    if phase_stop_3 >= 1
                        if B_lvl_tbl2[B_lvl_q] < 1
                            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        while true
                            B_lvl_i = B_lvl_tbl2[B_lvl_q]
                            B_lvl_q_step = B_lvl_q
                            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                            end
                            if B_lvl_i < phase_stop_3
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_5 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_5 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_5
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_6 = min(B_lvl_i_2, phase_stop_5, A_lvl_i)
                                        if A_lvl_i == phase_stop_6 && B_lvl_i_2 == phase_stop_6
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_7 = min(i_7, A_lvl_i_stop_2)
                                            if phase_stop_7 >= i_7
                                                if A_lvl_tbl1[A_lvl_q] < i_7
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_7
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_9 = min(A_lvl_i_2, phase_stop_7)
                                                        if A_lvl_i_2 == phase_stop_9
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_6
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_6
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_6 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            else
                                phase_stop_14 = min(B_lvl_i, phase_stop_3)
                                if B_lvl_i == phase_stop_14
                                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_14
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_15 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_15 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_15
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_16 = min(B_lvl_i_2, A_lvl_i, phase_stop_15)
                                            if A_lvl_i == phase_stop_16 && B_lvl_i_2 == phase_stop_16
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_17 = min(i_7, A_lvl_i_stop_4)
                                                if phase_stop_17 >= i_7
                                                    if A_lvl_tbl1[A_lvl_q] < i_7
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_17
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_19 = min(A_lvl_i_4, phase_stop_17)
                                                            if A_lvl_i_4 == phase_stop_19
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_16
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_16
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_16 + 1
                                        end
                                    end
                                    B_lvl_q = B_lvl_q_step
                                end
                                break
                            end
                        end
                    end
                end
            end
        end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    for i_4 = 1:A_lvl.shape[1]
        val = Ct_lvl_2_val
        Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
        B_lvl_ptr_2 = B_lvl_ptr
        B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
        B_lvl_tbl1_2 = B_lvl_tbl1
        B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
        B_lvl_tbl2_2 = B_lvl_tbl2
        B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
        val_2 = B_lvl_val
        B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
        A_lvl_ptr_2 = A_lvl_ptr
        A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
        A_lvl_tbl1_2 = A_lvl_tbl1
        A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
        A_lvl_tbl2_2 = A_lvl_tbl2
        A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
        val_3 = A_lvl_val
        A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
        Threads.@threads for i_5 = 1:Threads.nthreads()
                B_lvl_q = B_lvl_ptr[1]
                B_lvl_q_stop = B_lvl_ptr[1 + 1]
                if B_lvl_q < B_lvl_q_stop
                    B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                else
                    B_lvl_i_stop = 0
                end
                phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_5 + -1), Threads.nthreads()))
                phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_5, Threads.nthreads()))
                if phase_stop_2 >= phase_start_2
                    if B_lvl_tbl2[B_lvl_q] < phase_start_2
                        B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    while true
                        B_lvl_i = B_lvl_tbl2[B_lvl_q]
                        B_lvl_q_step = B_lvl_q
                        if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                            B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        if B_lvl_i < phase_stop_2
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_4, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_4
                                            if A_lvl_tbl1[A_lvl_q] < i_4
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        else
                            phase_stop_13 = min(B_lvl_i, phase_stop_2)
                            if B_lvl_i == phase_stop_13
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_4, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_4
                                                if A_lvl_tbl1[A_lvl_q] < i_4
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            end
                            break
                        end
                    end
                end
            end
        Ct_lvl_2_val = val
        B_lvl_ptr = B_lvl_ptr_2
        B_lvl_tbl1 = B_lvl_tbl1_2
        B_lvl_tbl2 = B_lvl_tbl2_2
        B_lvl_val = val_2
        A_lvl_ptr = A_lvl_ptr_2
        A_lvl_tbl1 = A_lvl_tbl1_2
        A_lvl_tbl2 = A_lvl_tbl2_2
        A_lvl_val = val_3
    end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(Ct_lvl_2_val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            B_lvl_q = B_lvl_ptr[1]
            B_lvl_q_stop = B_lvl_ptr[1 + 1]
            if B_lvl_q < B_lvl_q_stop
                B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
            else
                B_lvl_i_stop = 0
            end
            phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                if B_lvl_tbl2[B_lvl_q] < phase_start_2
                    B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                end
                while true
                    B_lvl_i = B_lvl_tbl2[B_lvl_q]
                    B_lvl_q_step = B_lvl_q
                    if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                        B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    if B_lvl_i < phase_stop_2
                        Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                        for i_6 = 1:A_lvl.shape[1]
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_6
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_6, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_6
                                            if A_lvl_tbl1[A_lvl_q] < i_6
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_6, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                        end
                        B_lvl_q = B_lvl_q_step
                    else
                        phase_stop_13 = min(B_lvl_i, phase_stop_2)
                        if B_lvl_i == phase_stop_13
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                            for i_8 = 1:A_lvl.shape[1]
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_8
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_8, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_8
                                                if A_lvl_tbl1[A_lvl_q] < i_8
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_8, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        end
                        break
                    end
                end
            end
        end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    B_lvl_q = B_lvl_ptr[1]
    B_lvl_q_stop = B_lvl_ptr[1 + 1]
    if B_lvl_q < B_lvl_q_stop
        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
    else
        B_lvl_i_stop = 0
    end
    phase_stop = min(B_lvl.shape[2], B_lvl_i_stop)
    if phase_stop >= 1
        if B_lvl_tbl2[B_lvl_q] < 1
            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
        end
        while true
            B_lvl_i = B_lvl_tbl2[B_lvl_q]
            B_lvl_q_step = B_lvl_q
            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
            end
            if B_lvl_i < phase_stop
                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                val = Ct_lvl_2_val
                Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                B_lvl_tbl1_2 = B_lvl_tbl1
                B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                B_lvl_tbl2_2 = B_lvl_tbl2
                val_2 = B_lvl_val
                B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                A_lvl_ptr_2 = A_lvl_ptr
                A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                A_lvl_tbl1_2 = A_lvl_tbl1
                A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                A_lvl_tbl2_2 = A_lvl_tbl2
                A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                val_3 = A_lvl_val
                A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                Threads.@threads for i_9 = 1:Threads.nthreads()
                        phase_start_6 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_9), Threads.nthreads()))
                        phase_stop_7 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_9, Threads.nthreads()))
                        if phase_stop_7 >= phase_start_6
                            for i_12 = phase_start_6:phase_stop_7
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_12
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_8 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_8 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_8
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_9 = min(B_lvl_i_2, phase_stop_8, A_lvl_i)
                                        if A_lvl_i == phase_stop_9 && B_lvl_i_2 == phase_stop_9
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_10 = min(i_12, A_lvl_i_stop_2)
                                            if phase_stop_10 >= i_12
                                                if A_lvl_tbl1[A_lvl_q] < i_12
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_12, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_10
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_12 = min(A_lvl_i_2, phase_stop_10)
                                                        if A_lvl_i_2 == phase_stop_12
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_9
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_9
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_9 + 1
                                    end
                                end
                            end
                        end
                    end
                Ct_lvl_2_val = val
                B_lvl_tbl1 = B_lvl_tbl1_2
                B_lvl_tbl2 = B_lvl_tbl2_2
                B_lvl_val = val_2
                A_lvl_ptr = A_lvl_ptr_2
                A_lvl_tbl1 = A_lvl_tbl1_2
                A_lvl_tbl2 = A_lvl_tbl2_2
                A_lvl_val = val_3
                B_lvl_q = B_lvl_q_step
            else
                phase_stop_18 = min(B_lvl_i, phase_stop)
                if B_lvl_i == phase_stop_18
                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_18
                    val_4 = Ct_lvl_2_val
                    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                    B_lvl_tbl1_3 = B_lvl_tbl1
                    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                    B_lvl_tbl2_3 = B_lvl_tbl2
                    val_5 = B_lvl_val
                    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                    A_lvl_ptr_3 = A_lvl_ptr
                    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                    A_lvl_tbl1_3 = A_lvl_tbl1
                    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                    A_lvl_tbl2_3 = A_lvl_tbl2
                    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                    val_6 = A_lvl_val
                    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                    Threads.@threads for i_19 = 1:Threads.nthreads()
                            phase_start_21 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_19), Threads.nthreads()))
                            phase_stop_23 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_19, Threads.nthreads()))
                            if phase_stop_23 >= phase_start_21
                                for i_22 = phase_start_21:phase_stop_23
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_22
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_24 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_24 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_24
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_25 = min(B_lvl_i_2, A_lvl_i, phase_stop_24)
                                            if A_lvl_i == phase_stop_25 && B_lvl_i_2 == phase_stop_25
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_26 = min(i_22, A_lvl_i_stop_4)
                                                if phase_stop_26 >= i_22
                                                    if A_lvl_tbl1[A_lvl_q] < i_22
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_22, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_26
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_28 = min(A_lvl_i_4, phase_stop_26)
                                                            if A_lvl_i_4 == phase_stop_28
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_25
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_25
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_25 + 1
                                        end
                                    end
                                end
                            end
                        end
                    Ct_lvl_2_val = val_4
                    B_lvl_tbl1 = B_lvl_tbl1_3
                    B_lvl_tbl2 = B_lvl_tbl2_3
                    B_lvl_val = val_5
                    A_lvl_ptr = A_lvl_ptr_3
                    A_lvl_tbl1 = A_lvl_tbl1_3
                    A_lvl_tbl2 = A_lvl_tbl2_3
                    A_lvl_val = val_6
                    B_lvl_q = B_lvl_q_step
                end
                break
            end
        end
    end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(Ct_lvl_2_val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            phase_start_2 = max(1, 1 + fld(A_lvl.shape[1] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                for i_7 = phase_start_2:phase_stop_2
                    B_lvl_q = B_lvl_ptr[1]
                    B_lvl_q_stop = B_lvl_ptr[1 + 1]
                    if B_lvl_q < B_lvl_q_stop
                        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                    else
                        B_lvl_i_stop = 0
                    end
                    phase_stop_3 = min(B_lvl.shape[2], B_lvl_i_stop)
                    if phase_stop_3 >= 1
                        if B_lvl_tbl2[B_lvl_q] < 1
                            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        while true
                            B_lvl_i = B_lvl_tbl2[B_lvl_q]
                            B_lvl_q_step = B_lvl_q
                            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                            end
                            if B_lvl_i < phase_stop_3
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_5 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_5 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_5
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_6 = min(B_lvl_i_2, phase_stop_5, A_lvl_i)
                                        if A_lvl_i == phase_stop_6 && B_lvl_i_2 == phase_stop_6
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_7 = min(i_7, A_lvl_i_stop_2)
                                            if phase_stop_7 >= i_7
                                                if A_lvl_tbl1[A_lvl_q] < i_7
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_7
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_9 = min(A_lvl_i_2, phase_stop_7)
                                                        if A_lvl_i_2 == phase_stop_9
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_6
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_6
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_6 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            else
                                phase_stop_14 = min(B_lvl_i, phase_stop_3)
                                if B_lvl_i == phase_stop_14
                                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_14
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_15 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_15 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_15
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_16 = min(B_lvl_i_2, A_lvl_i, phase_stop_15)
                                            if A_lvl_i == phase_stop_16 && B_lvl_i_2 == phase_stop_16
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_17 = min(i_7, A_lvl_i_stop_4)
                                                if phase_stop_17 >= i_7
                                                    if A_lvl_tbl1[A_lvl_q] < i_7
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_17
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_19 = min(A_lvl_i_4, phase_stop_17)
                                                            if A_lvl_i_4 == phase_stop_19
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_16
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_16
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_16 + 1
                                        end
                                    end
                                    B_lvl_q = B_lvl_q_step
                                end
                                break
                            end
                        end
                    end
                end
            end
        end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    for i_4 = 1:A_lvl.shape[1]
        val = Ct_lvl_2_val
        Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
        B_lvl_ptr_2 = B_lvl_ptr
        B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
        B_lvl_tbl1_2 = B_lvl_tbl1
        B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
        B_lvl_tbl2_2 = B_lvl_tbl2
        B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
        val_2 = B_lvl_val
        B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
        A_lvl_ptr_2 = A_lvl_ptr
        A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
        A_lvl_tbl1_2 = A_lvl_tbl1
        A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
        A_lvl_tbl2_2 = A_lvl_tbl2
        A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
        val_3 = A_lvl_val
        A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
        Threads.@threads for i_5 = 1:Threads.nthreads()
                B_lvl_q = B_lvl_ptr[1]
                B_lvl_q_stop = B_lvl_ptr[1 + 1]
                if B_lvl_q < B_lvl_q_stop
                    B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                else
                    B_lvl_i_stop = 0
                end
                phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_5 + -1), Threads.nthreads()))
                phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_5, Threads.nthreads()))
                if phase_stop_2 >= phase_start_2
                    if B_lvl_tbl2[B_lvl_q] < phase_start_2
                        B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    while true
                        B_lvl_i = B_lvl_tbl2[B_lvl_q]
                        B_lvl_q_step = B_lvl_q
                        if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                            B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        if B_lvl_i < phase_stop_2
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_4, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_4
                                            if A_lvl_tbl1[A_lvl_q] < i_4
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        else
                            phase_stop_13 = min(B_lvl_i, phase_stop_2)
                            if B_lvl_i == phase_stop_13
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_4, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_4
                                                if A_lvl_tbl1[A_lvl_q] < i_4
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            end
                            break
                        end
                    end
                end
            end
        Ct_lvl_2_val = val
        B_lvl_ptr = B_lvl_ptr_2
        B_lvl_tbl1 = B_lvl_tbl1_2
        B_lvl_tbl2 = B_lvl_tbl2_2
        B_lvl_val = val_2
        A_lvl_ptr = A_lvl_ptr_2
        A_lvl_tbl1 = A_lvl_tbl1_2
        A_lvl_tbl2 = A_lvl_tbl2_2
        A_lvl_val = val_3
    end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(Ct_lvl_2_val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            B_lvl_q = B_lvl_ptr[1]
            B_lvl_q_stop = B_lvl_ptr[1 + 1]
            if B_lvl_q < B_lvl_q_stop
                B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
            else
                B_lvl_i_stop = 0
            end
            phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                if B_lvl_tbl2[B_lvl_q] < phase_start_2
                    B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                end
                while true
                    B_lvl_i = B_lvl_tbl2[B_lvl_q]
                    B_lvl_q_step = B_lvl_q
                    if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                        B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    if B_lvl_i < phase_stop_2
                        Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                        for i_6 = 1:A_lvl.shape[1]
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_6
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_6, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_6
                                            if A_lvl_tbl1[A_lvl_q] < i_6
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_6, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                        end
                        B_lvl_q = B_lvl_q_step
                    else
                        phase_stop_13 = min(B_lvl_i, phase_stop_2)
                        if B_lvl_i == phase_stop_13
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                            for i_8 = 1:A_lvl.shape[1]
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_8
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_8, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_8
                                                if A_lvl_tbl1[A_lvl_q] < i_8
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_8, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        end
                        break
                    end
                end
            end
        end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    B_lvl_q = B_lvl_ptr[1]
    B_lvl_q_stop = B_lvl_ptr[1 + 1]
    if B_lvl_q < B_lvl_q_stop
        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
    else
        B_lvl_i_stop = 0
    end
    phase_stop = min(B_lvl.shape[2], B_lvl_i_stop)
    if phase_stop >= 1
        if B_lvl_tbl2[B_lvl_q] < 1
            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
        end
        while true
            B_lvl_i = B_lvl_tbl2[B_lvl_q]
            B_lvl_q_step = B_lvl_q
            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
            end
            if B_lvl_i < phase_stop
                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                val = Ct_lvl_2_val
                Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                B_lvl_tbl1_2 = B_lvl_tbl1
                B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                B_lvl_tbl2_2 = B_lvl_tbl2
                val_2 = B_lvl_val
                B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                A_lvl_ptr_2 = A_lvl_ptr
                A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                A_lvl_tbl1_2 = A_lvl_tbl1
                A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                A_lvl_tbl2_2 = A_lvl_tbl2
                A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                val_3 = A_lvl_val
                A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                Threads.@threads for i_9 = 1:Threads.nthreads()
                        phase_start_6 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_9), Threads.nthreads()))
                        phase_stop_7 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_9, Threads.nthreads()))
                        if phase_stop_7 >= phase_start_6
                            for i_12 = phase_start_6:phase_stop_7
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_12
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_8 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_8 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_8
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_9 = min(B_lvl_i_2, phase_stop_8, A_lvl_i)
                                        if A_lvl_i == phase_stop_9 && B_lvl_i_2 == phase_stop_9
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_10 = min(i_12, A_lvl_i_stop_2)
                                            if phase_stop_10 >= i_12
                                                if A_lvl_tbl1[A_lvl_q] < i_12
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_12, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_10
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_12 = min(A_lvl_i_2, phase_stop_10)
                                                        if A_lvl_i_2 == phase_stop_12
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_9
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_9
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_9 + 1
                                    end
                                end
                            end
                        end
                    end
                Ct_lvl_2_val = val
                B_lvl_tbl1 = B_lvl_tbl1_2
                B_lvl_tbl2 = B_lvl_tbl2_2
                B_lvl_val = val_2
                A_lvl_ptr = A_lvl_ptr_2
                A_lvl_tbl1 = A_lvl_tbl1_2
                A_lvl_tbl2 = A_lvl_tbl2_2
                A_lvl_val = val_3
                B_lvl_q = B_lvl_q_step
            else
                phase_stop_18 = min(B_lvl_i, phase_stop)
                if B_lvl_i == phase_stop_18
                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_18
                    val_4 = Ct_lvl_2_val
                    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                    B_lvl_tbl1_3 = B_lvl_tbl1
                    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                    B_lvl_tbl2_3 = B_lvl_tbl2
                    val_5 = B_lvl_val
                    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                    A_lvl_ptr_3 = A_lvl_ptr
                    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                    A_lvl_tbl1_3 = A_lvl_tbl1
                    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                    A_lvl_tbl2_3 = A_lvl_tbl2
                    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                    val_6 = A_lvl_val
                    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                    Threads.@threads for i_19 = 1:Threads.nthreads()
                            phase_start_21 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_19), Threads.nthreads()))
                            phase_stop_23 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_19, Threads.nthreads()))
                            if phase_stop_23 >= phase_start_21
                                for i_22 = phase_start_21:phase_stop_23
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_22
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_24 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_24 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_24
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_25 = min(B_lvl_i_2, A_lvl_i, phase_stop_24)
                                            if A_lvl_i == phase_stop_25 && B_lvl_i_2 == phase_stop_25
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_26 = min(i_22, A_lvl_i_stop_4)
                                                if phase_stop_26 >= i_22
                                                    if A_lvl_tbl1[A_lvl_q] < i_22
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_22, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_26
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_28 = min(A_lvl_i_4, phase_stop_26)
                                                            if A_lvl_i_4 == phase_stop_28
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_25
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_25
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_25 + 1
                                        end
                                    end
                                end
                            end
                        end
                    Ct_lvl_2_val = val_4
                    B_lvl_tbl1 = B_lvl_tbl1_3
                    B_lvl_tbl2 = B_lvl_tbl2_3
                    B_lvl_val = val_5
                    A_lvl_ptr = A_lvl_ptr_3
                    A_lvl_tbl1 = A_lvl_tbl1_3
                    A_lvl_tbl2 = A_lvl_tbl2_3
                    A_lvl_val = val_6
                    B_lvl_q = B_lvl_q_step
                end
                break
            end
        end
    end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(Ct_lvl_2_val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            phase_start_2 = max(1, 1 + fld(A_lvl.shape[1] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                for i_7 = phase_start_2:phase_stop_2
                    B_lvl_q = B_lvl_ptr[1]
                    B_lvl_q_stop = B_lvl_ptr[1 + 1]
                    if B_lvl_q < B_lvl_q_stop
                        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                    else
                        B_lvl_i_stop = 0
                    end
                    phase_stop_3 = min(B_lvl.shape[2], B_lvl_i_stop)
                    if phase_stop_3 >= 1
                        if B_lvl_tbl2[B_lvl_q] < 1
                            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        while true
                            B_lvl_i = B_lvl_tbl2[B_lvl_q]
                            B_lvl_q_step = B_lvl_q
                            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                            end
                            if B_lvl_i < phase_stop_3
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_5 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_5 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_5
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_6 = min(B_lvl_i_2, phase_stop_5, A_lvl_i)
                                        if A_lvl_i == phase_stop_6 && B_lvl_i_2 == phase_stop_6
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_7 = min(i_7, A_lvl_i_stop_2)
                                            if phase_stop_7 >= i_7
                                                if A_lvl_tbl1[A_lvl_q] < i_7
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_7
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_9 = min(A_lvl_i_2, phase_stop_7)
                                                        if A_lvl_i_2 == phase_stop_9
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_6
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_6
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_6 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            else
                                phase_stop_14 = min(B_lvl_i, phase_stop_3)
                                if B_lvl_i == phase_stop_14
                                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_14
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_15 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_15 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_15
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_16 = min(B_lvl_i_2, A_lvl_i, phase_stop_15)
                                            if A_lvl_i == phase_stop_16 && B_lvl_i_2 == phase_stop_16
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_17 = min(i_7, A_lvl_i_stop_4)
                                                if phase_stop_17 >= i_7
                                                    if A_lvl_tbl1[A_lvl_q] < i_7
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_17
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_19 = min(A_lvl_i_4, phase_stop_17)
                                                            if A_lvl_i_4 == phase_stop_19
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_16
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_16
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_16 + 1
                                        end
                                    end
                                    B_lvl_q = B_lvl_q_step
                                end
                                break
                            end
                        end
                    end
                end
            end
        end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    for i_4 = 1:A_lvl.shape[1]
        val = Ct_lvl_2_val
        Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
        B_lvl_ptr_2 = B_lvl_ptr
        B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
        B_lvl_tbl1_2 = B_lvl_tbl1
        B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
        B_lvl_tbl2_2 = B_lvl_tbl2
        B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
        val_2 = B_lvl_val
        B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
        A_lvl_ptr_2 = A_lvl_ptr
        A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
        A_lvl_tbl1_2 = A_lvl_tbl1
        A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
        A_lvl_tbl2_2 = A_lvl_tbl2
        A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
        val_3 = A_lvl_val
        A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
        Threads.@threads for i_5 = 1:Threads.nthreads()
                B_lvl_q = B_lvl_ptr[1]
                B_lvl_q_stop = B_lvl_ptr[1 + 1]
                if B_lvl_q < B_lvl_q_stop
                    B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                else
                    B_lvl_i_stop = 0
                end
                phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_5 + -1), Threads.nthreads()))
                phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_5, Threads.nthreads()))
                if phase_stop_2 >= phase_start_2
                    if B_lvl_tbl2[B_lvl_q] < phase_start_2
                        B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    while true
                        B_lvl_i = B_lvl_tbl2[B_lvl_q]
                        B_lvl_q_step = B_lvl_q
                        if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                            B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        if B_lvl_i < phase_stop_2
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_4, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_4
                                            if A_lvl_tbl1[A_lvl_q] < i_4
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        else
                            phase_stop_13 = min(B_lvl_i, phase_stop_2)
                            if B_lvl_i == phase_stop_13
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_4, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_4
                                                if A_lvl_tbl1[A_lvl_q] < i_4
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            end
                            break
                        end
                    end
                end
            end
        Ct_lvl_2_val = val
        B_lvl_ptr = B_lvl_ptr_2
        B_lvl_tbl1 = B_lvl_tbl1_2
        B_lvl_tbl2 = B_lvl_tbl2_2
        B_lvl_val = val_2
        A_lvl_ptr = A_lvl_ptr_2
        A_lvl_tbl1 = A_lvl_tbl1_2
        A_lvl_tbl2 = A_lvl_tbl2_2
        A_lvl_val = val_3
    end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(Ct_lvl_2_val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            B_lvl_q = B_lvl_ptr[1]
            B_lvl_q_stop = B_lvl_ptr[1 + 1]
            if B_lvl_q < B_lvl_q_stop
                B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
            else
                B_lvl_i_stop = 0
            end
            phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                if B_lvl_tbl2[B_lvl_q] < phase_start_2
                    B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                end
                while true
                    B_lvl_i = B_lvl_tbl2[B_lvl_q]
                    B_lvl_q_step = B_lvl_q
                    if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                        B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    if B_lvl_i < phase_stop_2
                        Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                        for i_6 = 1:A_lvl.shape[1]
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_6
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_6, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_6
                                            if A_lvl_tbl1[A_lvl_q] < i_6
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_6, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                        end
                        B_lvl_q = B_lvl_q_step
                    else
                        phase_stop_13 = min(B_lvl_i, phase_stop_2)
                        if B_lvl_i == phase_stop_13
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                            for i_8 = 1:A_lvl.shape[1]
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_8
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_8, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_8
                                                if A_lvl_tbl1[A_lvl_q] < i_8
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_8, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        end
                        break
                    end
                end
            end
        end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = (ex.bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = ((ex.bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    B_lvl_q = B_lvl_ptr[1]
    B_lvl_q_stop = B_lvl_ptr[1 + 1]
    if B_lvl_q < B_lvl_q_stop
        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
    else
        B_lvl_i_stop = 0
    end
    phase_stop = min(B_lvl.shape[2], B_lvl_i_stop)
    if phase_stop >= 1
        if B_lvl_tbl2[B_lvl_q] < 1
            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
        end
        while true
            B_lvl_i = B_lvl_tbl2[B_lvl_q]
            B_lvl_q_step = B_lvl_q
            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
            end
            if B_lvl_i < phase_stop
                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                val = Ct_lvl_2_val
                Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                B_lvl_tbl1_2 = B_lvl_tbl1
                B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                B_lvl_tbl2_2 = B_lvl_tbl2
                val_2 = B_lvl_val
                B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                A_lvl_ptr_2 = A_lvl_ptr
                A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                A_lvl_tbl1_2 = A_lvl_tbl1
                A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                A_lvl_tbl2_2 = A_lvl_tbl2
                A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                val_3 = A_lvl_val
                A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                Threads.@threads for i_9 = 1:Threads.nthreads()
                        phase_start_6 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_9), Threads.nthreads()))
                        phase_stop_7 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_9, Threads.nthreads()))
                        if phase_stop_7 >= phase_start_6
                            for i_12 = phase_start_6:phase_stop_7
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_12
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_8 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_8 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_8
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_9 = min(B_lvl_i_2, phase_stop_8, A_lvl_i)
                                        if A_lvl_i == phase_stop_9 && B_lvl_i_2 == phase_stop_9
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_10 = min(i_12, A_lvl_i_stop_2)
                                            if phase_stop_10 >= i_12
                                                if A_lvl_tbl1[A_lvl_q] < i_12
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_12, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_10
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_12 = min(A_lvl_i_2, phase_stop_10)
                                                        if A_lvl_i_2 == phase_stop_12
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] = Ct_lvl_2_val[Ct_lvl_2_q] + B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_9
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_9
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_9 + 1
                                    end
                                end
                            end
                        end
                    end
                Ct_lvl_2_val = val
                B_lvl_tbl1 = B_lvl_tbl1_2
                B_lvl_tbl2 = B_lvl_tbl2_2
                B_lvl_val = val_2
                A_lvl_ptr = A_lvl_ptr_2
                A_lvl_tbl1 = A_lvl_tbl1_2
                A_lvl_tbl2 = A_lvl_tbl2_2
                A_lvl_val = val_3
                B_lvl_q = B_lvl_q_step
            else
                phase_stop_18 = min(B_lvl_i, phase_stop)
                if B_lvl_i == phase_stop_18
                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_18
                    val_4 = Ct_lvl_2_val
                    Ct_lvl_2_val = moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                    B_lvl_tbl1_3 = B_lvl_tbl1
                    B_lvl_tbl1 = moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                    B_lvl_tbl2_3 = B_lvl_tbl2
                    val_5 = B_lvl_val
                    B_lvl_val = moveto(B_lvl_val, CPU(Threads.nthreads()))
                    A_lvl_ptr_3 = A_lvl_ptr
                    A_lvl_ptr = moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                    A_lvl_tbl1_3 = A_lvl_tbl1
                    A_lvl_tbl1 = moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                    A_lvl_tbl2_3 = A_lvl_tbl2
                    A_lvl_tbl2 = moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                    val_6 = A_lvl_val
                    A_lvl_val = moveto(A_lvl_val, CPU(Threads.nthreads()))
                    Threads.@threads for i_19 = 1:Threads.nthreads()
                            phase_start_21 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_19), Threads.nthreads()))
                            phase_stop_23 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_19, Threads.nthreads()))
                            if phase_stop_23 >= phase_start_21
                                for i_22 = phase_start_21:phase_stop_23
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_22
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_24 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_24 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_24
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_25 = min(B_lvl_i_2, A_lvl_i, phase_stop_24)
                                            if A_lvl_i == phase_stop_25 && B_lvl_i_2 == phase_stop_25
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_26 = min(i_22, A_lvl_i_stop_4)
                                                if phase_stop_26 >= i_22
                                                    if A_lvl_tbl1[A_lvl_q] < i_22
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_22, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_26
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_28 = min(A_lvl_i_4, phase_stop_26)
                                                            if A_lvl_i_4 == phase_stop_28
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] = Ct_lvl_2_val[Ct_lvl_2_q_2] + B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_25
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_25
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_25 + 1
                                        end
                                    end
                                end
                            end
                        end
                    Ct_lvl_2_val = val_4
                    B_lvl_tbl1 = B_lvl_tbl1_3
                    B_lvl_tbl2 = B_lvl_tbl2_3
                    B_lvl_val = val_5
                    A_lvl_ptr = A_lvl_ptr_3
                    A_lvl_tbl1 = A_lvl_tbl1_3
                    A_lvl_tbl2 = A_lvl_tbl2_3
                    A_lvl_val = val_6
                    B_lvl_q = B_lvl_q_step
                end
                break
            end
        end
    end
    qos = 1 * B_lvl.shape[2]
    qos_2 = qos * A_lvl.shape[1]
    resize!(Ct_lvl_2_val, qos_2)
    (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
end
julia> @finch begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0428789624797608, 0.08743611590208564, 0.0, 0.0, 0.7771943736883342, 0.8566307285039446, 0.0, 0.0, 0.3074726668114594, 0.1926824617371268, 0.0, 0.09744961392848928, 0.0, 0.0, 0.0, 0.40472958722306107, 0.1334234132871559, 0.13502037966414665, 0.384753712683889, 0.0, 0.0, 0.0, 0.04245930803079244, 0.0, 0.0, 0.0, 0.0, 0.232543498088826, 0.0, 0.0, 0.006208250652474222, 0.853241683312649, 0.13838159598021277, 0.6718696550712213, 0.0, 0.0, 0.3009328018875558, 0.7548914207055313, 0.08315770511918749, 0.0, 0.7921267306488304, 0.022757453132970167, 0.11800917617191224, 0.0, 0.1318085604836392, 0.092937183769726, 0.0, 0.0, 0.5602136568766394, 0.0, 0.057182493142543506, 0.5661083208979463, 0.4700341919673409, 0.0, 0.0396628382904592, 0.0, 0.0, 0.0, 0.026047939672719983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1324912296481703, 0.0, 0.0, 0.0, 0.0, 0.15406810900869447, 0.0, 0.42604244498161886, 0.0, 0.0, 0.6228079510297908, 0.5517488269014854, 0.0, 0.0, 1.3454800388738404, 0.0, 0.0, 0.27878996604355294, 0.14981346752804697, 0.48493565222165463, 0.0, 0.0, 0.175566348560725, 0.0, 0.0, 0.488211234512452, 0.4049191271055379, 0.0, 0.2572596255697597, 0.0, 0.0, 0.0, 0.12860715077271778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19926601116667575, 0.0, 0.0, 0.07981372691290523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11282549654596616, 0.0, 0.0, 0.0, 0.0, 0.1960709877223577, 0.08463772824948687, 0.6000921975147593, 0.06933209047757498, 0.5903397548150102, 0.4533479677609482, 0.37600376723029033, 0.0, 0.22536704278038036, 0.0, 0.8331675068397575, 0.10858320912285253, 0.0, 0.0, 0.0, 0.0, 0.07235014265388351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3648532188260543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056011038702209984, 0.0, 0.0, 0.8259337343417852, 0.661246530580228, 0.0, 0.0, 0.7308796928316751, 0.7486200426294148, 0.9189908393837516, 0.08049624774744675, 0.0, 0.6880790783907694, 0.0, 0.29554706458768826, 0.0, 0.13722052816256544, 0.11084092260151136, 0.23626535452605812, 0.0, 0.0, 0.0, 0.011381056978069693, 0.0, 0.0, 0.0, 0.10535317070728263, 0.14543482544301695, 0.0, 0.051239181888286314, 0.0, 0.0, 0.0747577813296711, 0.38519578325327114, 0.0, 0.0, 0.0, 0.31107126997725243, 0.0, 0.062381440699744946, 0.0, 0.0034568921571664145, 0.30403434807575835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08818303936466654, 0.13025353097218012, 0.0, 0.007961594298685966, 0.7271286455077467, 0.0, 0.22219791560962007, 0.3133242399846383, 0.3268548923736554, 0.0, 0.0, 0.0, 0.062043385392843244, 0.0, 0.0, 0.0, 0.5118979232699669, 0.0, 0.0, 0.304968704724498, 0.07757320111041376, 0.0, 0.0, 0.0, 0.36244482394528155, 0.48729955975902517, 0.0, 0.2578809850807504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26405495493704934, 0.0, 0.029266633556388796, 0.0, 1.2110472367818281, 0.0, 0.1187033409885821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2425740129164943, 0.3654529227835142, 0.5153300334395746, 0.0, 0.7303532509990378, 0.0, 0.30280176045543894, 0.0, 0.03763541722633813, 0.0, 0.16154011545850713, 0.3300236906507485, 0.0, 0.3564243509437679, 0.5015875337682285, 0.1275860441701041, 0.4852730978010842, 0.24949859939723054, 0.48822755084987346, 0.5961195445742726, 0.0, 0.4258373206096391, 0.04034804507220536, 0.10905685359790995, 0.23322362309902195, 0.0, 0.0, 0.0, 0.0, 0.04617600763364615, 0.0, 0.0, 0.0, 0.054740847021331536, 0.2289534857639407, 0.0, 0.9566746815161189, 0.5173366928677203, 0.7507086774699239, 0.0, 0.3397283595719449, 0.0, 0.046992692111316466, 0.0, 0.0, 0.0, 0.545517123849588, 0.7385352770089988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.044184743700219443, 0.0, 0.34893340603779743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03408711743655356, 0.0, 0.0, 0.0, 0.0, 0.10288421033324832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4034440902637878, 0.0, 0.039572494858250686, 0.0, 0.0, 0.02965744996763479, 0.0, 0.0, 0.0, 0.07891932240827818, 0.38097929040006584, 0.5157796766942258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12359808033862156, 0.6117393031610942, 0.0, 0.0, 0.9673911366769413, 0.0, 0.6373984763052702, 0.0, 0.0, 0.0, 0.0, 0.8392753225054719, 0.0, 0.0, 0.0, 0.0, 0.07111900058169553, 0.8489937307498225, 0.0, 0.0, 0.0, 0.2136974454996388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.698777895294578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6214075302206429, 1.036249718606956, 0.288175662886351, 0.30891233093854337, 0.0, 0.0, 0.030879508082115467, 0.0, 0.2558976234079212, 0.0, 0.0, 0.042278765434284306, 0.0, 0.4724462226601247, 0.0, 0.0, 0.0, 0.25363078745957207, 0.0, 0.16744470661332633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7263137977886603, 0.0, 0.0, 0.0, 0.0, 0.4815382645923394, 0.0, 0.5389771095257322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1308584725861496, 0.0, 0.0, 0.08251830255971462, 0.0, 0.0034293294111591966, 0.0, 0.0, 0.28462730006232495, 0.0, 0.0, 0.0, 0.0, 0.7672574634035603, 0.06799234219219645, 0.7686836665518981, 0.0, 0.0, 0.07569662526896655, 0.12433421108969092, 0.029719729594264294, 0.0, 0.0, 0.033375050906541616, 0.0, 0.0, 0.027755923820063425, 0.0, 0.0, 0.06780373210090862, 0.0, 0.062393471497503045, 0.06376572488764005, 0.8812259136810331, 0.11171978836893101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3032071316064303, 0.2778418947590191, 0.3836048827669035, 0.0, 0.0, 0.0, 0.0, 0.09600748829358721, 0.12392534705601368, 0.0, 0.9502560961906908, 0.0, 0.17771819515786774, 0.12012525452436125, 0.0, 0.0, 0.0, 0.0, 0.08800052638012112, 0.20460534683496254, 0.07836656003273164, 0.0, 0.0, 0.07733204623742608, 0.07445288630270257, 0.0, 0.09175578431673431, 0.0, 0.0, 0.15710541867277852, 0.0, 0.0, 0.0, 0.9471006301345436, 0.0, 0.8510630894604628, 0.043156570388064336, 0.20190939205426583, 0.0, 0.24328040556319352, 0.0, 0.0, 0.44542258264750934, 0.2732554892966366, 0.3559126085975685, 0.0, 0.0, 0.0, 0.0, 0.15244632551386036, 0.044750397692748425, 0.00015064272565775368, 0.0, 0.0, 0.16975321325404802, 0.0, 0.0, 0.10785372583674148, 0.016893034441964917, 0.0, 0.0, 0.0, 0.44878532871196397, 0.0, 0.0, 0.07579107513921182, 0.04907550677474498, 0.013333642103260275, 0.0630306546556411, 0.0, 0.0, 0.15397482894540573, 0.0, 0.0, 0.0, 0.7932520980106772, 0.0, 0.05504999836643944, 0.0, 0.00024803754949565015, 0.1772448247224359, 0.0, 0.3468482175761097, 0.489095290853196, 0.0, 0.0, 0.0, 0.49187643261565206, 0.08968952021475389, 0.0, 0.308198226139378, 0.0, 0.0, 0.0015257507226665138, 0.0, 0.4760524029218542, 0.13725876569793635, 0.022870502148480364, 0.0, 0.0, 0.5657719191928595, 0.7044371199107592, 0.0, 0.10928958999181013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37407199094140375, 0.04230765786478687, 0.0, 0.47125018764213217, 0.26841666363242983, 0.17159678882355658, 0.0, 0.7745962913367755, 0.0, 0.0, 0.0, 0.002512192133666035, 0.36038961387137014, 0.0, 0.0, 0.0, 0.0, 0.0030060643809689458, 0.0, 0.0, 0.0, 0.018410227422041164, 0.0, 0.1461608376469839, 0.22122149305176325, 0.0, 0.22042404000828647, 0.0, 0.0, 0.3028860300781355, 0.0, 0.8734338428431226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010843395115652311, 0.005239337502791826, 0.0, 0.0, 0.009327907773831686, 0.0, 0.2044511921886214, 0.040228854408754025, 0.0, 0.0, 0.017509071146779225, 0.0, 0.1974149550135206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009353957912913354, 0.0, 0.009791741151994864, 0.267707205354198, 0.0, 0.0, 0.0, 0.23026690546616582, 0.10199610442505093, 0.018990981850212127, 0.781584440023405, 0.0, 0.20124209479398683, 0.08466814366498222, 0.027298809970356448, 0.18532942423497814, 0.0, 0.0, 0.0, 0.07261615073050222, 0.05523527246334513, 0.0, 0.0, 0.0, 0.018577758887185037, 0.007556744576286696, 0.0, 0.0, 0.0, 0.041841469476118526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6999983171558223, 0.059630996036698, 0.03126919391610552, 0.0, 0.0, 0.0, 0.0, 0.13882649349368803, 0.0, 0.7707381738420093, 0.6105281431452728, 0.0, 0.3659349563659441, 0.0, 0.0, 0.792957039057469, 0.0, 0.0, 0.0, 0.0, 0.5578879570307803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7396112185709535, 0.36395144592450657, 0.0, 0.0, 0.0, 0.0, 0.11215320418590438, 0.0, 0.0, 0.0, 0.37591385888738293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20042855252697583, 0.0, 0.0, 0.0, 0.5044932689068156, 0.45830830153566743, 0.0, 0.019010357405419995, 0.3432031229937067, 0.9630609973086364, 0.0, 0.4086466251047053, 1.141575986881623, 0.6190496931553665, 0.6248450596019536, 0.0, 0.0, 0.0, 0.4859500703314839, 0.7752364001615992, 0.24066890708585614, 0.739514646287994, 0.0, 0.0, 0.0, 0.7972651300568858, 0.8044998248681345, 0.0, 0.0, 0.8800727106905054, 0.6774702345165845, 0.0, 0.0, 0.007691494652914049, 0.0, 0.2158362063320794, 0.0, 0.0, 0.3461932102404406, 0.0, 0.8862967368459654, 0.0, 0.5112247277468743, 0.10072082431619588, 0.028194549299716588, 0.29345191211085103, 0.0, 0.5957331179078807, 0.8033379685501598, 0.2763250870762641, 0.0, 0.0, 0.0, 0.2584468323192244, 1.4962170109225457, 0.737036973347589, 0.2163561342681589, 0.35157824666928306, 0.0, 0.0, 0.41592555026126565, 0.0, 0.7335626645924566, 0.0, 0.3824651051446043, 0.0, 0.0, 0.4465623503712833, 0.0, 0.0, 0.0, 0.0, 1.0075622514657745, 0.7531699947855945, 0.0, 0.01688562190231179, 0.0, 0.37637966615105906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26615193276732196, 0.40160098586018766, 0.22111875959610844, 0.7838712501005154, 0.11455850998047643, 0.0, 0.3244597290050513, 0.277935077217096, 0.678296539647923, 0.7943120870076059, 0.0, 0.0, 0.07580591228505294, 0.22739219631322996, 0.0, 0.36799633486687344, 0.13560057928971747, 0.34415022333694595, 0.0, 0.375286097502392, 0.0, 0.0, 0.0, 0.14930770854320144, 0.0, 0.0, 0.18538707235881216, 0.0, 0.0, 0.08415385567314168, 0.0, 0.2805839758523421, 0.30767376323246487, 0.0, 0.0, 0.0, 0.6362548390624749, 0.0, 0.0, 0.0, 0.0, 0.11896074672644522, 0.3082259124522087, 0.0, 0.0, 0.2818460549073561, 0.0, 0.0, 0.41257445550835625, 0.0, 0.053048065787345625, 0.0, 0.0, 0.11304579411509989, 0.034218884366002976, 0.6661360806603157, 0.0, 0.21411392040876626, 0.4841785777481824, 0.39302693151947515, 0.0, 0.21525894026398354, 0.6082774198662322, 0.6563770490501982, 1.3837370014200125, 0.0, 0.0, 0.0, 0.23111477100248784, 0.0, 0.2571742494634567, 0.1875593198419512, 0.10199527697624192, 0.39558938718223496, 0.23356202823127883, 0.0, 0.02146297966152243, 0.6833662064837734, 0.4784087412560625, 0.0, 0.0, 0.4952171342595967, 0.7395829004642561, 0.460637982190491, 0.8327926559189114, 0.20784537658466082, 0.28105967712936836, 0.07867639067477498, 0.0, 0.0, 0.03632767891315117, 0.03736223492624939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09874650817518764, 0.058514193648989486, 0.019680035455543327, 0.0, 0.0, 0.20854380210200846, 0.0, 0.01957821103544756, 0.0, 0.0, 0.0, 0.0, 0.029477943186437056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03018367933290697, 0.001246426225276636, 0.0, 0.0, 0.24436387761375777, 0.04319325220757273, 0.0, 0.0005544873380769801, 0.0, 0.0, 0.0, 0.0, 0.03240374035365331, 0.0, 0.16622987482814208, 0.14572703657816696, 0.0, 0.10691320164555042, 0.0, 0.0, 0.23771054420223764, 0.0, 0.028353365685102484, 0.0, 0.0, 0.42652095476268087, 0.0, 0.0, 0.0, 0.14037068091673455, 0.0, 0.1958655978459535, 0.11013721353955239, 0.0, 0.0, 0.21805933264244692, 0.16486976689470612, 0.5439006978172312, 0.0, 0.0, 0.0, 0.0, 0.08637154070931971, 0.0, 0.0, 0.0, 0.2894991668028485, 0.0, 0.1797369262849939, 0.0, 0.5725198911522605, 0.17418484945225488, 0.0, 0.06213015716122782, 0.0, 0.10490895444261257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638490918443707, 0.5408789006672917, 0.09595630463039345, 0.37725368293279926, 0.016866261296368933, 0.0, 0.19466688554493405, 0.17773611080285265, 0.0, 0.0, 0.0, 0.24617777466330076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3776666637032188, 0.16143600164517602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862919495673197, 0.0, 0.2413234196977018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06779206472141591, 0.0, 0.0, 0.0, 0.0, 0.5076468400331536, 0.0, 0.0, 0.01391829091764819, 0.8212518440711771, 0.0, 0.0, 0.0, 1.108905221486613, 0.1188499796570694, 0.0, 0.0, 0.0, 0.0, 0.22392824827003446, 0.08374521709059522, 1.0377868678426458, 0.0, 0.0, 0.0, 0.06314656483715254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05065689017238068, 0.0, 0.0, 0.6182862700881498, 0.0, 0.0, 0.0, 0.08961698508270353, 0.0, 0.0, 0.0, 0.028552165079219483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046090351907509264, 0.0329968133815159, 0.0, 0.0, 0.0, 0.03171690167459951, 0.0, 0.02138595257853391, 0.0, 0.0, 0.0, 0.0, 0.5659059869109166, 0.0, 0.45486425104211037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009588419023083293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007952625215232818, 0.0, 0.0, 0.0, 0.007023779107344989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004358964367081868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006161876385986237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053700530589800725, 0.0, 0.0022317086420093543, 0.0, 0.0, 0.05852821749944063, 0.516635908828728, 0.2985986620129579, 0.0, 0.6111927478449756, 0.0, 0.5825534608215744, 0.4090183318160424, 0.0, 0.0, 0.04926117981958254, 0.6562128001900569, 0.019340742583046598, 0.19538581363137364, 0.0, 0.0, 0.0, 0.6631613890315422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0406038711512622, 0.0, 0.5618914298633669, 0.0393496168530609, 0.628914960665529, 0.5901431396338964, 0.0, 0.0, 0.0, 0.09249307403952937, 0.35753332158552337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177254384032435, 0.5834401941969394, 0.37342231634653106, 0.0, 0.0, 0.0, 0.0, 0.08909339054628015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051104660404469975, 0.0, 0.0, 0.0, 0.0, 0.4179127862572812, 0.0, 0.0, 0.0, 0.0, 0.4947946164068441, 0.7667446714679307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039434063584478374, 0.0, 0.0, 0.04843104241813036, 0.0, 0.5144335832828751, 0.0, 0.14581846115367056, 0.4166578844514915, 0.7614285159679925, 0.1630656521589184, 0.22441005680825418, 0.02866208643995639, 0.0, 0.4532407170428936, 0.3020453149417785, 0.07305001099695578, 0.0, 6.132735412841357e-6, 0.0976804037908706, 0.6199019333919525, 0.10006338754398636, 0.0, 0.0, 0.38147706213707766, 0.17006771536151036, 0.21491907339709776, 0.047799731379243794, 0.0, 0.0, 0.0, 6.654191086662508e-6, 0.01765054989887291, 0.0, 0.13362041195455804, 0.17492677428990264, 0.824706676344676, 0.31443512995233835, 0.00785205432182622, 0.13746268939688996, 0.36371132005929235, 0.15385951324448066, 5.921522701045136e-6, 0.0, 0.0, 0.0, 0.08994971181314479, 0.12683928655841373, 0.0, 0.5520092622508177, 0.7084594380639649, 0.0, 0.19562848681502015, 0.0, 0.03299702042160346, 0.0, 0.0, 0.34772152374282883, 0.13492246968382748, 0.12345681563544324, 0.3661265971029467, 0.13447656157275498, 0.0, 0.0, 0.14672418223455458, 0.0, 0.0, 0.0, 0.07499766265833707, 0.0, 0.0, 0.11068125753487118, 0.10089393862599626, 0.0, 0.0, 0.08908047116677639, 0.2015873379074186, 0.10298741816605479, 0.24527252520911447, 0.0, 0.03460879613727743, 0.16720966215203137, 0.013662888409749127, 0.12118730303780559, 0.0, 0.0, 0.0, 0.0, 0.12212178365153345, 0.0, 0.0, 0.005577609659891184, 0.0, 0.0, 0.26991070073046763, 0.0, 0.1174815203046113, 0.041692377559084594, 0.0, 0.5300753779838534, 0.0, 0.0, 0.31928275766072467, 0.1616832599984604, 0.04907933392612172, 0.0, 0.0, 0.0, 0.03821526045987873, 0.0, 0.3049648769572113, 0.1411321941361396, 0.0, 0.0, 0.041593282517463706, 0.13565782467478937, 0.011917705635957943, 0.09372751798208423, 0.22117213861445317, 0.050301121765738824, 0.3526465092680141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16684332398446952, 0.008272237453333834, 0.0, 0.0, 0.0, 0.2099321573597463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3287814803677072, 0.0, 0.4067318447798367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16563163900318595, 0.0, 0.0, 0.0, 0.0, 0.009667754061011632, 0.16959705252964216, 0.0, 0.0, 0.0, 0.56845350906485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3642749168228618, 0.0, 0.0, 0.0, 0.11984491012649627, 0.0, 0.41956953134409886, 0.0, 0.12317090215225207, 0.10102093683672053, 0.5963158912439469, 0.0, 0.4359200535321227, 0.6096786976430637, 0.0, 0.6499247921346847, 0.3073461538590094, 0.1405982316884511, 0.0, 0.04772903972078857, 0.7936024804714342, 0.9185709768343875, 0.0, 0.0, 0.0, 0.5470192570764263, 0.0, 0.21476868150199754, 0.0, 0.19073933171248297, 0.0, 0.3793331332535416, 0.0, 0.0, 0.01964391641210212, 0.0, 0.4378619125703662, 0.8223521000494463, 0.45088443908444353, 0.0, 0.4127633192993181, 0.7065876710503416, 0.12592180772438738, 0.0, 0.257238877891668, 0.1476433349129333, 0.0, 0.0, 0.6125568935640959, 0.11983099560604582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1358449450175441, 0.4856923886029592, 0.1982340562753604, 0.05114032089334242, 0.0, 0.0, 0.0, 0.34526566558380767, 0.0, 0.9079343502122469, 0.0, 0.0, 0.0, 0.019895722205129522, 0.0, 0.0, 0.0, 0.7714661769327793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31723887855236743, 0.0, 1.0495317958683972, 0.0, 0.7769221189304751, 0.0, 0.3594651552897292, 0.1931659239165346, 0.0, 0.0, 0.0, 0.0, 0.18587496888225893, 0.0, 0.014477652583978751, 0.8542571033156305, 0.0, 0.0, 0.0, 0.7347846293218806, 0.3803337673859788, 0.0, 0.0, 0.0, 0.0, 0.12216816529465464, 0.08711085044717441, 0.9436854562373743, 0.0, 0.0, 0.0, 0.19518451942944104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15016206786962047, 0.0, 0.0, 0.0, 0.5560042822620145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09321859870063849, 0.0, 0.0, 0.10827134941090288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1798518802325591, 0.0, 0.006417885147557111, 0.0159284523840847, 0.0, 0.16831380706777663, 0.0, 0.0, 0.0, 0.0, 0.0011820045463448933, 0.12724558967212649, 0.0, 0.0, 0.0, 0.14166391990604377, 0.0416750433027245, 0.055619565309699526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019332972712293686, 0.0, 0.0, 0.0, 0.01387244209455808, 0.11676747434216192, 0.0, 0.668276571400472, 0.11316052499395393, 0.0, 0.0, 0.0, 0.08695671597540733, 0.0, 0.0, 0.0, 0.0, 0.1964135023100263, 0.10797072815481346, 1.0763277030656333, 0.2696605992966385, 0.0, 0.008523313764823862, 0.506568380299954, 0.0, 0.032308115066342104, 0.0, 0.0, 0.38865688229464423, 0.0, 0.06005515175011382, 0.41556879130475816, 0.0, 0.0, 0.0, 0.03345947819837139, 0.0, 0.0, 0.0, 0.4958150659228213, 0.013511443741365857, 0.0, 0.0, 0.3848802389406841, 0.6796486976984905, 0.48129339927534, 0.0, 0.0, 0.019040654623360107, 0.7224444805840577, 0.04874639236388979, 0.0, 0.06426589870448678, 0.05319623403578879, 0.0, 0.15672070660980222, 0.0, 0.017033728136402918, 0.0, 0.08893706026336692, 0.0, 0.0, 0.0, 0.5487476129882687, 0.0, 0.15373184613084703, 0.04326471846211246, 0.22217865785086524, 0.2718297910539172, 0.0, 0.514430285095647, 0.002053672195206153, 0.0, 0.0, 0.0, 0.0, 0.21853919696408158, 0.0005532519945171588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37120039072692296, 0.43803271578487757, 0.31766672717556577, 0.0, 0.0, 0.0, 0.08026542821449031, 0.0, 0.235827620380954, 0.33254366820014003, 0.2686363331540573, 0.0, 0.0, 0.0, 0.06397748514988684, 0.21835360234859336, 0.0, 0.0, 0.4207199716601354, 0.0, 0.2634641669240437, 0.32367560122479555, 0.08233157081159413, 0.6620846217534156, 0.0, 0.3608913493520587, 0.38467732748933553, 0.0, 0.0, 0.21194788219064958, 0.0, 0.17239581808619228, 0.0, 0.05772352142055052, 0.0, 0.2170221602935571, 0.0, 0.0, 0.3867463771605307, 0.7274124563110836, 0.47713236111314844, 0.0, 0.2802647113151722, 0.49577669395011376, 0.11064312843188615, 0.5577384537186072, 0.0, 0.0, 0.0, 0.0, 0.42232119497840825, 0.0, 0.6660939990696797, 0.0, 0.0, 0.0, 0.03313428015688339, 0.05353962018206741, 0.0, 0.0, 0.0, 0.013909404170161314, 0.06142458692982868, 0.0, 0.4232534811180027, 0.0, 0.014492828854647392, 0.08738978198228943, 0.0, 0.2602424099063423, 0.0, 0.2510099113339794, 0.01352683366266603, 0.040192721606149735, 0.0, 0.16988522552757826, 0.0, 0.017854314097131885, 0.14128282204278342, 0.01562985045651764, 0.0, 0.5192290571749145, 0.0, 0.06339353874663697, 0.0, 0.11558641537807815, 0.0, 0.12937379361463164, 0.19594344599337632, 0.0, 0.0, 0.0, 0.0, 0.5398764845949043, 0.19451932421561768, 0.2519126729176587, 0.3410459315418626, 0.0, 0.0, 0.23421455529817845, 0.8809966008694553, 0.0, 0.0, 0.0830150881250035, 0.0, 0.0, 0.1611328831713825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.678075267927486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5149677006256428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5835127942681666, 0.3135624868315288, 0.0]), 42), 42)),)

