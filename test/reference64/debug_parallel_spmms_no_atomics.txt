julia> @finch begin
        CR .= 0
        for i = _
            for j = _
                for k = _
                    CR[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(CR = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.70856859734722, 0.0, 0.0, 0.0, 0.0, 0.40683416426697433, 0.0, 0.0, 0.5116135014596547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11665081191597346, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6815260724911955, 0.0, 0.012792585308225086, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23414421666297028, 0.0, 0.049360715516608075, 0.0, 0.17114666211685992, 0.0, 0.0, 0.016462013473269064, 0.10889740119368015, 0.0, 0.0, 0.0, 0.00577445736315422, 0.03699291283586617, 0.0, 0.054643694670994745, 0.0, 0.0, 0.0, 0.08011353665096754, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1586425298490503, 0.5528778968703049, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22133004329744424, 0.198518145547811, 0.038821202773921835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18278114382714625, 0.1962641247777959, 0.33892974018746314, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013014677888550407, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6923776912647246, 0.0, 0.0, 0.0, 0.0, 0.03372992996786747, 0.0, 0.0, 0.0, 0.16613962476105457, 0.0, 0.21040813690603985, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12848588404681333, 0.44877210624067243, 0.0, 0.0, 0.00922143015221846, 0.0, 0.334367843457963, 0.0, 0.3344728737871945, 0.0, 0.007609184274463018, 0.0, 0.0783652603407444, 0.0, 0.008880000989208168, 0.6489339658812214, 0.0, 0.0, 0.4325454998052285, 0.24409841671258664, 0.0, 0.5857475361200408, 0.0, 0.006543890265716072, 0.0, 0.0, 0.0, 0.008425346810589148, 0.0, 0.0, 0.16622566484891482, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27228959440031625, 0.16863240701914226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16867493346874554, 0.42330165278515836, 0.0, 0.0, 0.6457463733709866, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12409061480277174, 0.0, 0.14869580809977814, 0.0, 0.0, 0.0, 0.0, 0.14872661472502066, 0.5062551989716859, 0.06509123699478471, 0.0, 0.026886171078575067, 0.0, 0.3503937118835625, 0.0, 0.13397275360612415, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07224002362266076, 0.0, 0.0, 0.014356019852976252, 0.2658719803791635, 0.13166323290951196, 0.0, 0.0, 0.0, 0.0, 0.020308821426184575, 0.025635708510132263, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007980130035150811, 0.4090538905999397, 0.0, 0.0, 0.008918289569362796, 0.31893009912431536, 0.0, 0.0, 0.0, 0.009525001505142072, 0.0, 0.0, 0.013278407346240287, 0.021967736559129595, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12136922915980554, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.279484838006678, 0.2506790805406071, 0.0, 0.0, 0.10182245842840729, 0.0, 0.0, 0.5745756063332743, 0.0, 0.5180568530499269, 0.0, 0.19018853437509187, 0.2984452378932974, 0.0, 0.0, 0.0, 0.2447378349547936, 0.0, 0.48522550177156104, 0.5906428332019245, 0.0, 0.0, 0.756905839792281, 0.0, 0.4128008966782496, 0.7173389246941403, 0.0, 0.4558370427008323, 0.6596592042653375, 0.33014948101925834, 0.0, 0.0, 0.0, 0.0, 0.17629318543732997, 0.5676926328184286, 0.6751757284159318, 0.18305092512120225, 0.0, 0.5526511646802545, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6816784313112039, 0.0, 0.0, 0.0, 0.0, 0.4384965381282249, 0.0, 0.5249476708016299, 0.5514304573066787, 0.0, 0.0, 0.0278361825874964, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.38619300372291254, 0.0, 0.5997636810868653, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20257599957103595, 0.0, 0.10844521538627999, 0.0, 1.2226089817686585, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04229463432907405, 0.0, 0.0, 0.0, 0.0, 0.19946965003853376, 0.048044013732540566, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.036066754029278635, 0.0, 0.16231330637401103, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6062852141104628, 0.40481276616279827, 0.0, 0.0, 0.2539845497787725, 0.0, 0.0, 0.5691049256163461, 0.572636917198234, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.026986118300396385, 0.0, 0.0, 0.2578776387682179, 0.2201878278780288, 0.3521973225117564, 0.49204059164875263, 0.20695213801377368, 0.3889153033164169, 0.12024801465448598, 0.5015222202004627, 0.0, 0.0, 0.0, 0.25511838171251733, 0.3395573400007252, 0.0, 0.14076018335068016, 0.006079069440287111, 0.3120090695218348, 0.02735798901473654, 0.23564550369089346, 0.0, 0.5201790908279286, 0.42693711336544043, 0.0, 0.0, 0.32942094250755716, 0.0, 0.0, 0.0, 0.0, 0.42208957099917227, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27253622974111436, 0.20091824181940837, 0.0, 0.0, 0.014451665708310772, 0.0648234167882922, 0.0, 0.0, 0.0, 0.0, 0.43998592755822313, 0.200499194569837, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10517105271620596, 0.4525079677294799, 0.0, 0.0, 0.0, 0.3992933674102322, 0.0, 0.6336315500314595, 0.5683248347075297, 0.0, 0.03911423543998137, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5618719435725092, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13717215169360888, 0.6023639264726295, 0.0, 0.16451246906039718, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2428824655720991, 0.0, 0.0, 0.0, 0.1307489669067023, 0.016490708613327746, 0.0, 0.5332965109179645, 0.0, 0.72835164827528, 0.5456771996801212, 0.20306669911566266, 0.8042852233793947, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08986996958215537, 0.0, 0.09372773893140769, 0.2910950907401526, 0.0, 0.0, 0.11400417325056827, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15073719147466486, 0.0, 0.0062702773417321524, 0.0, 0.11018067217476075, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03517847753788876, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012565073058016527, 0.0, 0.013511533493035587, 0.0, 0.0, 0.0, 0.1160032767395455, 0.3288776375406247, 0.009384967607632352, 0.013704851364696426, 0.0, 0.0, 0.0, 0.10679904950368906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5396299966121684, 0.0, 0.0, 0.0, 0.4120885163537877, 0.0, 0.0, 0.0, 0.0, 0.1374333138690627, 0.0, 0.0, 0.0, 0.0, 0.0, 0.308835769631579, 0.0, 0.0, 0.2935183046180839, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21641442424094412, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16667427577804808, 0.0, 0.0, 0.0, 0.0, 0.1072149118509185, 0.0, 0.0, 0.13482790109225204, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32702264908263307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17999984836884408, 0.0, 0.0, 0.0, 0.0, 0.17960575635997003, 0.0, 0.0, 0.0, 0.0, 0.08691531027429766, 0.0, 0.2893303365574358, 0.0, 0.21730094701941294, 0.07940443022930238, 0.0, 0.0, 0.01060094130869171, 0.0, 0.0, 0.21897146914670435, 0.3193413479843402, 0.11502306512986679, 0.16078084781731777, 0.0, 0.0, 0.029138065953053547, 0.12899773650646262, 0.0, 0.0, 0.0, 0.0, 0.35209003333845906, 0.0, 0.0, 0.42881832168904727, 0.0, 0.07551665376676242, 0.0, 0.10828996414360535, 0.0, 0.0, 0.0, 0.06175478190339224, 0.07317612880730584, 0.0, 0.4573326969055594, 0.13616590004764864, 0.1733951270114995, 0.0, 0.0, 0.0, 0.0, 0.004682414366264115, 0.0, 0.0, 0.014457116882680743, 0.477538324169661, 0.6690522150262886, 0.0, 0.0, 0.35074798851029604, 0.0, 0.6171788496488323, 0.0, 0.0, 0.0, 0.4598571719367441, 0.020424541646230095, 0.5014147387061915, 0.0, 0.0, 0.18767693158032267, 0.0, 0.08703683411330847, 0.0, 0.3388800152965881, 0.004390191394809013, 0.2131890539109787, 0.0, 0.43631258167790793, 0.0, 0.0, 0.0, 0.0, 0.011243656021000366, 0.0, 0.0, 0.004302223899340062, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3252052607129126, 0.10943362605720088, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4600533876598886, 0.0, 0.0, 0.0, 0.6238654988124779, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17327928789102656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3007942289792172, 0.49763260068933673, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1811653464990684, 0.0, 0.7596663692737102, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24115083978451807, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4132016125575898, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.30212649550849624, 0.45064666528381636, 0.0, 0.0, 1.0295831107021076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4924133218118872, 0.0, 0.0, 0.6123145833841932, 1.0074416855456372, 0.8974627446983252, 0.0, 0.0, 0.39387754214687604, 0.6443411524082342, 0.0, 0.6700619016074552, 0.0, 0.0, 0.4377096051189058, 0.3977735630344448, 0.0, 0.0, 0.0, 0.054395363429349475, 0.46912699721752377, 0.09054692381782561, 0.0, 0.6320539215802606, 0.33393006705956935, 0.614294006786966, 0.0, 0.0, 0.3986581318369477, 0.630705395059908, 0.0, 0.4266820903955834, 0.6230830913144384, 0.0, 1.0831760350408295, 0.21088758697054613, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7135848094649876, 0.06891111903866448, 0.0, 0.0, 0.0, 0.32920907856114856, 0.5438621275845595, 0.0, 0.4139962279186794, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13809169802867077, 0.0, 0.14849341459532064, 0.0, 0.0, 0.0, 0.5533499952652239, 0.0, 0.10314194807289108, 0.15061800177708015, 0.0, 0.5514890096422472, 0.23190068111070972, 0.0, 0.701822721136001, 0.6294877235260824, 0.0, 0.0, 0.0, 1.1059687974440209, 0.0, 0.0, 0.0, 0.96169924491107, 0.0, 0.0, 0.0, 0.0, 0.06628312350990566, 0.0, 0.37743722057381907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08765206247905213, 0.0, 0.5218343219200703, 0.0, 0.0, 0.0, 0.41461579287108996, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6671900885778136, 0.0, 0.0, 0.0, 0.028262864260027967, 0.7909490968152504, 0.04511006432931489, 0.0, 0.0, 0.0, 0.4564818142302977, 0.35113439772465294, 0.0, 0.0, 0.0, 0.03776705986522304, 0.6129906961879193, 0.0, 0.0, 0.0, 0.0, 0.0, 0.42034814190539166, 0.0, 0.0, 0.21638060933821013, 0.0, 0.0, 0.0, 0.2526638526129318, 0.03715346614099295, 0.6358424325058142, 0.0, 0.0, 0.04152129953915335, 0.04614236094065058, 0.0, 0.0, 0.07015688762319212, 0.05740172594394938, 0.18351207696535793, 0.0, 0.0, 0.008864400691370657, 0.0, 0.8651249823989057, 0.407678395491645, 0.845679782901505, 0.0, 0.0, 0.0, 0.0, 0.4753420456230014, 1.6107190368773618, 0.0, 0.0, 0.0638102435424578, 0.24041718723542044, 0.5177820553003522, 0.44619181405158337, 0.4489609795607919, 0.0, 0.07301898938869086, 0.0902695074635508, 0.4268753686592877, 0.0, 0.0, 0.0, 0.16408594940133228, 0.0, 0.0, 0.6736439849461895, 0.031382854877976686, 0.034000045192845295, 0.0, 0.0, 0.8630676095443404, 0.3932053636261741, 0.0, 0.0, 0.0, 0.23747714078691665, 0.2179752640521007, 0.0, 0.059020425946533776, 0.23507519209688904, 0.0, 0.6184935143504988, 0.18475168643298573, 0.3255150538360457, 0.0, 0.0, 0.10559996237493433, 0.258855010743052, 0.0, 0.0, 0.0, 0.0, 0.5931453366617909, 0.0, 0.012879882724963071, 0.34456425817306313, 0.0, 0.10363371305008902, 0.0, 0.0, 0.13032437159832005, 0.0, 0.0, 0.9136517678658531, 0.11645026631438779, 0.0, 0.0, 1.006195673838974, 0.0, 0.49183662858528426, 0.5242175629103283, 0.0, 0.0, 0.5842253265103485, 0.09175087532086124, 0.0, 0.0, 0.6346229344595797, 0.8483440750859271, 0.0, 0.0, 0.43168828228976236, 1.1746392457903614, 0.9786548179849323, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22466432888463708, 0.0, 0.0, 0.23383689086971213, 0.07868759200794351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5047067971651431, 0.0, 0.04466167239654772, 0.0, 0.18031282035336027, 0.06880056169195574, 0.0, 0.039760598424459205, 0.2052193360923158, 0.0, 0.1245954319549744, 0.0, 0.0, 0.3371869924989234, 0.10076818165067654, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21628428501390357, 0.41878760793751535, 0.0, 0.2743772579349106, 0.0, 0.0, 0.0, 0.23737336558608516, 0.0, 0.5609891075534331, 0.0, 0.0, 0.0, 0.0, 0.11206304892649907, 0.03770992438488979, 0.0, 0.0, 0.17383517173220803, 0.252655932334268, 0.0, 0.1585306005724301, 0.0, 0.1865298137691172, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05971061253533409, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15913149160221682, 0.0, 0.4030910369398146, 0.0, 0.32022434202130456, 0.23687245172165972, 0.0, 0.3599325265793679, 0.0, 0.4765964362382492, 0.0, 0.0, 0.0, 0.0, 0.6107120779202261, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.573260313114729, 0.5768180939544553, 0.0, 0.17499066339826644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7201822598869417, 0.0, 0.0, 0.0, 0.0, 0.08725982492406978, 0.5051841445138487, 0.0, 0.0, 0.0, 0.2569811590076317, 0.3964028116744446, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31198143094865527, 0.0, 0.0, 0.0, 0.0, 0.0, 0.42812611727229755, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.40187139064927635, 0.40436549376609193, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5533450612442088, 0.3541481069684905, 0.0, 0.0, 0.0, 0.1801509251180122, 0.0, 0.0, 0.0, 0.08899148634361333, 0.0, 0.4004935508153472, 0.1664002225353232, 0.6660561411338942, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5274372110974553, 0.0, 0.0, 0.11745018036413006, 0.03952272816910389, 0.654574176319042, 0.0, 0.0, 0.0, 0.0, 0.29642163335908794, 0.1274499525156547, 0.03345490004078419, 0.0, 0.0, 0.07653915037041417, 0.0, 0.02978363268713748, 0.15372448023095572, 0.03967384765850083, 0.06258104057589436, 0.0, 0.0, 0.044337981986606795, 0.07548283043416787, 0.0, 0.0, 0.0, 0.04735429836324166, 0.0, 0.0, 0.10863396357319674, 0.1797235339241024, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2053113311622715, 0.0, 0.0, 0.6339058613302375, 0.26881844559242324, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.047384735026192754, 0.0, 0.09394657688969027, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.41199723535819494, 0.0, 0.0, 0.19249813639294353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4930042077083581, 0.01569004651529754, 0.0, 0.1886409972802957, 0.0, 0.6504613027483894, 0.0, 0.0, 0.0, 0.4491308783325726, 0.0, 0.710600063326777, 0.0, 0.0, 0.0, 0.0, 0.6392481732081897, 0.0, 0.0, 0.21075790790635565, 0.143953089400151, 0.04517962123405222, 0.12446587761719602, 0.011602660433066613, 0.0, 0.021141064793252704, 0.017873704948355638, 0.0, 0.010329409925325224, 0.21730544865000528, 0.03874493610862431, 0.0, 0.0, 0.0, 0.04329986579681618, 0.02617857620219232, 0.0, 0.0, 0.14198488087669728, 0.04624555904799935, 0.33624630483183143, 0.0, 0.18065873234459212, 0.001167765010724762, 0.0, 0.41156050949339695, 0.06138103351330629, 0.0, 0.0, 0.0, 0.0, 0.0, 0.45223330011890606, 0.0, 0.42719481956226096, 0.0, 0.09390028378745956, 0.03159803909732349, 0.0, 0.0, 0.0, 0.14464053921535813, 0.0, 0.355579453624945, 0.27582314245244144, 0.0, 0.0, 0.4779189904559178, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6540512683692249, 0.16386729047093346, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08595593354654336, 0.0868518036911852, 0.1436872279500763, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08976911428083846, 0.0, 0.0, 0.0, 0.06561633035108058, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0209499775034313, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06082234101182034, 0.19585779687130894, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01528743323594608, 0.0, 0.0, 0.0, 0.011174280569280867, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3922416239543676, 0.0, 0.5308327922857218, 0.38811106643298066, 0.0, 0.5861742359361938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010357877371534024, 0.03335404406511716, 0.2121541431315042, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022181366862053645, 0.0, 0.0, 0.3232792127186002, 0.21780290259329063, 0.0, 0.0, 0.6724740217065875, 0.33454771410797746, 0.0, 0.0, 0.0, 0.0, 0.1517011050211833, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20421012621580428, 0.0, 0.0, 0.0, 0.09554663093280256, 0.0, 0.9066194809744413, 0.0, 0.6530356828494477, 0.0, 0.0, 0.4162310356447254, 0.2004622291406719, 0.0, 0.06531630990369706, 0.0, 0.0, 0.22516333873266767, 0.08751639468488193, 0.31273858097846424, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10565952264223451, 0.25213753900728575, 0.0, 0.13605306731422323, 0.0, 0.0, 0.20467409186458246, 0.0, 0.0, 0.0, 0.14062561725182204, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06654804138060808, 0.04780210989364991, 0.3312858803709398, 0.0, 0.09056986573031543, 0.0, 0.0, 0.22425218794100035, 0.6978010919630194, 0.5644822411311934, 0.0, 0.0, 0.0, 0.0, 0.25245963204466193, 0.5714387205668741, 0.0, 0.7794778768245345, 0.5229243836029062, 0.6068223768633771, 0.0, 0.0, 0.3439063861094934, 0.0, 0.5825998753106167, 0.07738278703840655, 0.0, 0.0, 0.45088731313609587, 0.020026145653267647, 0.49371220470576765, 0.0, 0.1967497337446331, 0.9333687888834381, 0.2687116640308751, 0.3109501951809905, 0.0, 0.628995837333548, 0.6431918870734405, 0.0, 0.0, 0.4278019777568812, 0.0, 0.0, 0.3426634565816645, 0.0, 0.7642079896603969, 0.15720215492969136, 0.0, 0.5627301734522071, 0.0, 0.0, 0.03992028432745955, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8902694418730629, 0.24785210085431258, 0.0, 0.0, 0.0, 0.19750384789873962, 0.0, 0.0, 0.24837057467209334, 0.0, 0.0, 0.06196917660673224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9196979517908253, 0.47523150409994275, 0.0, 0.05084636736867684, 0.30281629383729847, 0.0, 0.0, 0.0, 0.0, 0.7797375130571516, 0.0, 0.0, 0.009193696508787878, 0.7164774465476489, 0.37223214885126943, 0.36077619715591613]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = ((ex.bodies[1]).bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    result = nothing
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = (Finch).moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = (Finch).moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            phase_start_2 = max(1, 1 + fld(A_lvl.shape[1] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                for i_7 = phase_start_2:phase_stop_2
                    B_lvl_q = B_lvl_ptr[1]
                    B_lvl_q_stop = B_lvl_ptr[1 + 1]
                    if B_lvl_q < B_lvl_q_stop
                        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                    else
                        B_lvl_i_stop = 0
                    end
                    phase_stop_3 = min(B_lvl.shape[2], B_lvl_i_stop)
                    if phase_stop_3 >= 1
                        if B_lvl_tbl2[B_lvl_q] < 1
                            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        while true
                            B_lvl_i = B_lvl_tbl2[B_lvl_q]
                            B_lvl_q_step = B_lvl_q
                            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                            end
                            if B_lvl_i < phase_stop_3
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_5 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_5 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_5
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_6 = min(B_lvl_i_2, phase_stop_5, A_lvl_i)
                                        if A_lvl_i == phase_stop_6 && B_lvl_i_2 == phase_stop_6
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_7 = min(i_7, A_lvl_i_stop_2)
                                            if phase_stop_7 >= i_7
                                                if A_lvl_tbl1[A_lvl_q] < i_7
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_7
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_9 = min(A_lvl_i_2, phase_stop_7)
                                                        if A_lvl_i_2 == phase_stop_9
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_6
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_6
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_6 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            else
                                phase_stop_14 = min(B_lvl_i, phase_stop_3)
                                if B_lvl_i == phase_stop_14
                                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_14
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_15 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_15 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_15
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_16 = min(B_lvl_i_2, A_lvl_i, phase_stop_15)
                                            if A_lvl_i == phase_stop_16 && B_lvl_i_2 == phase_stop_16
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_17 = min(i_7, A_lvl_i_stop_4)
                                                if phase_stop_17 >= i_7
                                                    if A_lvl_tbl1[A_lvl_q] < i_7
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_17
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_19 = min(A_lvl_i_4, phase_stop_17)
                                                            if A_lvl_i_4 == phase_stop_19
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_16
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_16
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_16 + 1
                                        end
                                    end
                                    B_lvl_q = B_lvl_q_step
                                end
                                break
                            end
                        end
                    end
                end
            end
        end
    resize!(val, A_lvl.shape[1] * B_lvl.shape[2])
    result = (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
    result
end
julia> @finch begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.70856859734722, 0.0, 0.0, 0.0, 0.0, 0.40683416426697433, 0.0, 0.0, 0.5116135014596547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11665081191597346, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6815260724911955, 0.0, 0.012792585308225086, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23414421666297028, 0.0, 0.049360715516608075, 0.0, 0.17114666211685992, 0.0, 0.0, 0.016462013473269064, 0.10889740119368015, 0.0, 0.0, 0.0, 0.00577445736315422, 0.03699291283586617, 0.0, 0.054643694670994745, 0.0, 0.0, 0.0, 0.08011353665096754, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1586425298490503, 0.5528778968703049, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22133004329744424, 0.198518145547811, 0.038821202773921835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18278114382714625, 0.1962641247777959, 0.33892974018746314, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013014677888550407, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6923776912647246, 0.0, 0.0, 0.0, 0.0, 0.03372992996786747, 0.0, 0.0, 0.0, 0.16613962476105457, 0.0, 0.21040813690603985, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12848588404681333, 0.44877210624067243, 0.0, 0.0, 0.00922143015221846, 0.0, 0.334367843457963, 0.0, 0.3344728737871945, 0.0, 0.007609184274463018, 0.0, 0.0783652603407444, 0.0, 0.008880000989208168, 0.6489339658812214, 0.0, 0.0, 0.4325454998052285, 0.24409841671258664, 0.0, 0.5857475361200408, 0.0, 0.006543890265716072, 0.0, 0.0, 0.0, 0.008425346810589148, 0.0, 0.0, 0.16622566484891482, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27228959440031625, 0.16863240701914226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16867493346874554, 0.42330165278515836, 0.0, 0.0, 0.6457463733709866, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12409061480277174, 0.0, 0.14869580809977814, 0.0, 0.0, 0.0, 0.0, 0.14872661472502066, 0.5062551989716859, 0.06509123699478471, 0.0, 0.026886171078575067, 0.0, 0.3503937118835625, 0.0, 0.13397275360612415, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07224002362266076, 0.0, 0.0, 0.014356019852976252, 0.2658719803791635, 0.13166323290951196, 0.0, 0.0, 0.0, 0.0, 0.020308821426184575, 0.025635708510132263, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007980130035150811, 0.4090538905999397, 0.0, 0.0, 0.008918289569362796, 0.31893009912431536, 0.0, 0.0, 0.0, 0.009525001505142072, 0.0, 0.0, 0.013278407346240287, 0.021967736559129595, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12136922915980554, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.279484838006678, 0.2506790805406071, 0.0, 0.0, 0.10182245842840729, 0.0, 0.0, 0.5745756063332743, 0.0, 0.5180568530499269, 0.0, 0.19018853437509187, 0.2984452378932974, 0.0, 0.0, 0.0, 0.2447378349547936, 0.0, 0.48522550177156104, 0.5906428332019245, 0.0, 0.0, 0.756905839792281, 0.0, 0.4128008966782496, 0.7173389246941403, 0.0, 0.4558370427008323, 0.6596592042653375, 0.33014948101925834, 0.0, 0.0, 0.0, 0.0, 0.17629318543732997, 0.5676926328184286, 0.6751757284159318, 0.18305092512120225, 0.0, 0.5526511646802545, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6816784313112039, 0.0, 0.0, 0.0, 0.0, 0.4384965381282249, 0.0, 0.5249476708016299, 0.5514304573066787, 0.0, 0.0, 0.0278361825874964, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.38619300372291254, 0.0, 0.5997636810868653, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20257599957103595, 0.0, 0.10844521538627999, 0.0, 1.2226089817686585, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04229463432907405, 0.0, 0.0, 0.0, 0.0, 0.19946965003853376, 0.048044013732540566, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.036066754029278635, 0.0, 0.16231330637401103, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6062852141104628, 0.40481276616279827, 0.0, 0.0, 0.2539845497787725, 0.0, 0.0, 0.5691049256163461, 0.572636917198234, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.026986118300396385, 0.0, 0.0, 0.2578776387682179, 0.2201878278780288, 0.3521973225117564, 0.49204059164875263, 0.20695213801377368, 0.3889153033164169, 0.12024801465448598, 0.5015222202004627, 0.0, 0.0, 0.0, 0.25511838171251733, 0.3395573400007252, 0.0, 0.14076018335068016, 0.006079069440287111, 0.3120090695218348, 0.02735798901473654, 0.23564550369089346, 0.0, 0.5201790908279286, 0.42693711336544043, 0.0, 0.0, 0.32942094250755716, 0.0, 0.0, 0.0, 0.0, 0.42208957099917227, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27253622974111436, 0.20091824181940837, 0.0, 0.0, 0.014451665708310772, 0.0648234167882922, 0.0, 0.0, 0.0, 0.0, 0.43998592755822313, 0.200499194569837, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10517105271620596, 0.4525079677294799, 0.0, 0.0, 0.0, 0.3992933674102322, 0.0, 0.6336315500314595, 0.5683248347075297, 0.0, 0.03911423543998137, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5618719435725092, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13717215169360888, 0.6023639264726295, 0.0, 0.16451246906039718, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2428824655720991, 0.0, 0.0, 0.0, 0.1307489669067023, 0.016490708613327746, 0.0, 0.5332965109179645, 0.0, 0.72835164827528, 0.5456771996801212, 0.20306669911566266, 0.8042852233793947, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08986996958215537, 0.0, 0.09372773893140769, 0.2910950907401526, 0.0, 0.0, 0.11400417325056827, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15073719147466486, 0.0, 0.0062702773417321524, 0.0, 0.11018067217476075, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03517847753788876, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012565073058016527, 0.0, 0.013511533493035587, 0.0, 0.0, 0.0, 0.1160032767395455, 0.3288776375406247, 0.009384967607632352, 0.013704851364696426, 0.0, 0.0, 0.0, 0.10679904950368906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5396299966121684, 0.0, 0.0, 0.0, 0.4120885163537877, 0.0, 0.0, 0.0, 0.0, 0.1374333138690627, 0.0, 0.0, 0.0, 0.0, 0.0, 0.308835769631579, 0.0, 0.0, 0.2935183046180839, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21641442424094412, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16667427577804808, 0.0, 0.0, 0.0, 0.0, 0.1072149118509185, 0.0, 0.0, 0.13482790109225204, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32702264908263307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17999984836884408, 0.0, 0.0, 0.0, 0.0, 0.17960575635997003, 0.0, 0.0, 0.0, 0.0, 0.08691531027429766, 0.0, 0.2893303365574358, 0.0, 0.21730094701941294, 0.07940443022930238, 0.0, 0.0, 0.01060094130869171, 0.0, 0.0, 0.21897146914670435, 0.3193413479843402, 0.11502306512986679, 0.16078084781731777, 0.0, 0.0, 0.029138065953053547, 0.12899773650646262, 0.0, 0.0, 0.0, 0.0, 0.35209003333845906, 0.0, 0.0, 0.42881832168904727, 0.0, 0.07551665376676242, 0.0, 0.10828996414360535, 0.0, 0.0, 0.0, 0.06175478190339224, 0.07317612880730584, 0.0, 0.4573326969055594, 0.13616590004764864, 0.1733951270114995, 0.0, 0.0, 0.0, 0.0, 0.004682414366264115, 0.0, 0.0, 0.014457116882680743, 0.477538324169661, 0.6690522150262886, 0.0, 0.0, 0.35074798851029604, 0.0, 0.6171788496488323, 0.0, 0.0, 0.0, 0.4598571719367441, 0.020424541646230095, 0.5014147387061915, 0.0, 0.0, 0.18767693158032267, 0.0, 0.08703683411330847, 0.0, 0.3388800152965881, 0.004390191394809013, 0.2131890539109787, 0.0, 0.43631258167790793, 0.0, 0.0, 0.0, 0.0, 0.011243656021000366, 0.0, 0.0, 0.004302223899340062, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3252052607129126, 0.10943362605720088, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4600533876598886, 0.0, 0.0, 0.0, 0.6238654988124779, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17327928789102656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3007942289792172, 0.49763260068933673, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1811653464990684, 0.0, 0.7596663692737102, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24115083978451807, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4132016125575898, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.30212649550849624, 0.45064666528381636, 0.0, 0.0, 1.0295831107021076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4924133218118872, 0.0, 0.0, 0.6123145833841932, 1.0074416855456372, 0.8974627446983252, 0.0, 0.0, 0.39387754214687604, 0.6443411524082342, 0.0, 0.6700619016074552, 0.0, 0.0, 0.4377096051189058, 0.3977735630344448, 0.0, 0.0, 0.0, 0.054395363429349475, 0.46912699721752377, 0.09054692381782561, 0.0, 0.6320539215802606, 0.33393006705956935, 0.614294006786966, 0.0, 0.0, 0.3986581318369477, 0.630705395059908, 0.0, 0.4266820903955834, 0.6230830913144384, 0.0, 1.0831760350408295, 0.21088758697054613, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7135848094649876, 0.06891111903866448, 0.0, 0.0, 0.0, 0.32920907856114856, 0.5438621275845595, 0.0, 0.4139962279186794, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13809169802867077, 0.0, 0.14849341459532064, 0.0, 0.0, 0.0, 0.5533499952652239, 0.0, 0.10314194807289108, 0.15061800177708015, 0.0, 0.5514890096422472, 0.23190068111070972, 0.0, 0.701822721136001, 0.6294877235260824, 0.0, 0.0, 0.0, 1.1059687974440209, 0.0, 0.0, 0.0, 0.96169924491107, 0.0, 0.0, 0.0, 0.0, 0.06628312350990566, 0.0, 0.37743722057381907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08765206247905213, 0.0, 0.5218343219200703, 0.0, 0.0, 0.0, 0.41461579287108996, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6671900885778136, 0.0, 0.0, 0.0, 0.028262864260027967, 0.7909490968152504, 0.04511006432931489, 0.0, 0.0, 0.0, 0.4564818142302977, 0.35113439772465294, 0.0, 0.0, 0.0, 0.03776705986522304, 0.6129906961879193, 0.0, 0.0, 0.0, 0.0, 0.0, 0.42034814190539166, 0.0, 0.0, 0.21638060933821013, 0.0, 0.0, 0.0, 0.2526638526129318, 0.03715346614099295, 0.6358424325058142, 0.0, 0.0, 0.04152129953915335, 0.04614236094065058, 0.0, 0.0, 0.07015688762319212, 0.05740172594394938, 0.18351207696535793, 0.0, 0.0, 0.008864400691370657, 0.0, 0.8651249823989057, 0.407678395491645, 0.845679782901505, 0.0, 0.0, 0.0, 0.0, 0.4753420456230014, 1.6107190368773618, 0.0, 0.0, 0.0638102435424578, 0.24041718723542044, 0.5177820553003522, 0.44619181405158337, 0.4489609795607919, 0.0, 0.07301898938869086, 0.0902695074635508, 0.4268753686592877, 0.0, 0.0, 0.0, 0.16408594940133228, 0.0, 0.0, 0.6736439849461895, 0.031382854877976686, 0.034000045192845295, 0.0, 0.0, 0.8630676095443404, 0.3932053636261741, 0.0, 0.0, 0.0, 0.23747714078691665, 0.2179752640521007, 0.0, 0.059020425946533776, 0.23507519209688904, 0.0, 0.6184935143504988, 0.18475168643298573, 0.3255150538360457, 0.0, 0.0, 0.10559996237493433, 0.258855010743052, 0.0, 0.0, 0.0, 0.0, 0.5931453366617909, 0.0, 0.012879882724963071, 0.34456425817306313, 0.0, 0.10363371305008902, 0.0, 0.0, 0.13032437159832005, 0.0, 0.0, 0.9136517678658531, 0.11645026631438779, 0.0, 0.0, 1.006195673838974, 0.0, 0.49183662858528426, 0.5242175629103283, 0.0, 0.0, 0.5842253265103485, 0.09175087532086124, 0.0, 0.0, 0.6346229344595797, 0.8483440750859271, 0.0, 0.0, 0.43168828228976236, 1.1746392457903614, 0.9786548179849323, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22466432888463708, 0.0, 0.0, 0.23383689086971213, 0.07868759200794351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5047067971651431, 0.0, 0.04466167239654772, 0.0, 0.18031282035336027, 0.06880056169195574, 0.0, 0.039760598424459205, 0.2052193360923158, 0.0, 0.1245954319549744, 0.0, 0.0, 0.3371869924989234, 0.10076818165067654, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21628428501390357, 0.41878760793751535, 0.0, 0.2743772579349106, 0.0, 0.0, 0.0, 0.23737336558608516, 0.0, 0.5609891075534331, 0.0, 0.0, 0.0, 0.0, 0.11206304892649907, 0.03770992438488979, 0.0, 0.0, 0.17383517173220803, 0.252655932334268, 0.0, 0.1585306005724301, 0.0, 0.1865298137691172, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05971061253533409, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15913149160221682, 0.0, 0.4030910369398146, 0.0, 0.32022434202130456, 0.23687245172165972, 0.0, 0.3599325265793679, 0.0, 0.4765964362382492, 0.0, 0.0, 0.0, 0.0, 0.6107120779202261, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.573260313114729, 0.5768180939544553, 0.0, 0.17499066339826644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7201822598869417, 0.0, 0.0, 0.0, 0.0, 0.08725982492406978, 0.5051841445138487, 0.0, 0.0, 0.0, 0.2569811590076317, 0.3964028116744446, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31198143094865527, 0.0, 0.0, 0.0, 0.0, 0.0, 0.42812611727229755, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.40187139064927635, 0.40436549376609193, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5533450612442088, 0.3541481069684905, 0.0, 0.0, 0.0, 0.1801509251180122, 0.0, 0.0, 0.0, 0.08899148634361333, 0.0, 0.4004935508153472, 0.1664002225353232, 0.6660561411338942, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5274372110974553, 0.0, 0.0, 0.11745018036413006, 0.03952272816910389, 0.654574176319042, 0.0, 0.0, 0.0, 0.0, 0.29642163335908794, 0.1274499525156547, 0.03345490004078419, 0.0, 0.0, 0.07653915037041417, 0.0, 0.02978363268713748, 0.15372448023095572, 0.03967384765850083, 0.06258104057589436, 0.0, 0.0, 0.044337981986606795, 0.07548283043416787, 0.0, 0.0, 0.0, 0.04735429836324166, 0.0, 0.0, 0.10863396357319674, 0.1797235339241024, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2053113311622715, 0.0, 0.0, 0.6339058613302375, 0.26881844559242324, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.047384735026192754, 0.0, 0.09394657688969027, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.41199723535819494, 0.0, 0.0, 0.19249813639294353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4930042077083581, 0.01569004651529754, 0.0, 0.1886409972802957, 0.0, 0.6504613027483894, 0.0, 0.0, 0.0, 0.4491308783325726, 0.0, 0.710600063326777, 0.0, 0.0, 0.0, 0.0, 0.6392481732081897, 0.0, 0.0, 0.21075790790635565, 0.143953089400151, 0.04517962123405222, 0.12446587761719602, 0.011602660433066613, 0.0, 0.021141064793252704, 0.017873704948355638, 0.0, 0.010329409925325224, 0.21730544865000528, 0.03874493610862431, 0.0, 0.0, 0.0, 0.04329986579681618, 0.02617857620219232, 0.0, 0.0, 0.14198488087669728, 0.04624555904799935, 0.33624630483183143, 0.0, 0.18065873234459212, 0.001167765010724762, 0.0, 0.41156050949339695, 0.06138103351330629, 0.0, 0.0, 0.0, 0.0, 0.0, 0.45223330011890606, 0.0, 0.42719481956226096, 0.0, 0.09390028378745956, 0.03159803909732349, 0.0, 0.0, 0.0, 0.14464053921535813, 0.0, 0.355579453624945, 0.27582314245244144, 0.0, 0.0, 0.4779189904559178, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6540512683692249, 0.16386729047093346, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08595593354654336, 0.0868518036911852, 0.1436872279500763, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08976911428083846, 0.0, 0.0, 0.0, 0.06561633035108058, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0209499775034313, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06082234101182034, 0.19585779687130894, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01528743323594608, 0.0, 0.0, 0.0, 0.011174280569280867, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3922416239543676, 0.0, 0.5308327922857218, 0.38811106643298066, 0.0, 0.5861742359361938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010357877371534024, 0.03335404406511716, 0.2121541431315042, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022181366862053645, 0.0, 0.0, 0.3232792127186002, 0.21780290259329063, 0.0, 0.0, 0.6724740217065875, 0.33454771410797746, 0.0, 0.0, 0.0, 0.0, 0.1517011050211833, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20421012621580428, 0.0, 0.0, 0.0, 0.09554663093280256, 0.0, 0.9066194809744413, 0.0, 0.6530356828494477, 0.0, 0.0, 0.4162310356447254, 0.2004622291406719, 0.0, 0.06531630990369706, 0.0, 0.0, 0.22516333873266767, 0.08751639468488193, 0.31273858097846424, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10565952264223451, 0.25213753900728575, 0.0, 0.13605306731422323, 0.0, 0.0, 0.20467409186458246, 0.0, 0.0, 0.0, 0.14062561725182204, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06654804138060808, 0.04780210989364991, 0.3312858803709398, 0.0, 0.09056986573031543, 0.0, 0.0, 0.22425218794100035, 0.6978010919630194, 0.5644822411311934, 0.0, 0.0, 0.0, 0.0, 0.25245963204466193, 0.5714387205668741, 0.0, 0.7794778768245345, 0.5229243836029062, 0.6068223768633771, 0.0, 0.0, 0.3439063861094934, 0.0, 0.5825998753106167, 0.07738278703840655, 0.0, 0.0, 0.45088731313609587, 0.020026145653267647, 0.49371220470576765, 0.0, 0.1967497337446331, 0.9333687888834381, 0.2687116640308751, 0.3109501951809905, 0.0, 0.628995837333548, 0.6431918870734405, 0.0, 0.0, 0.4278019777568812, 0.0, 0.0, 0.3426634565816645, 0.0, 0.7642079896603969, 0.15720215492969136, 0.0, 0.5627301734522071, 0.0, 0.0, 0.03992028432745955, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8902694418730629, 0.24785210085431258, 0.0, 0.0, 0.0, 0.19750384789873962, 0.0, 0.0, 0.24837057467209334, 0.0, 0.0, 0.06196917660673224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9196979517908253, 0.47523150409994275, 0.0, 0.05084636736867684, 0.30281629383729847, 0.0, 0.0, 0.0, 0.0, 0.7797375130571516, 0.0, 0.0, 0.009193696508787878, 0.7164774465476489, 0.37223214885126943, 0.36077619715591613]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = ((ex.bodies[1]).bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    result = nothing
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    for i_4 = 1:A_lvl.shape[1]
        val = Ct_lvl_2_val
        Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
        B_lvl_ptr_2 = B_lvl_ptr
        B_lvl_ptr = (Finch).moveto(B_lvl_ptr, CPU(Threads.nthreads()))
        B_lvl_tbl1_2 = B_lvl_tbl1
        B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
        B_lvl_tbl2_2 = B_lvl_tbl2
        B_lvl_tbl2 = (Finch).moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
        val_2 = B_lvl_val
        B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
        A_lvl_ptr_2 = A_lvl_ptr
        A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
        A_lvl_tbl1_2 = A_lvl_tbl1
        A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
        A_lvl_tbl2_2 = A_lvl_tbl2
        A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
        val_3 = A_lvl_val
        A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
        Threads.@threads for i_5 = 1:Threads.nthreads()
                B_lvl_q = B_lvl_ptr[1]
                B_lvl_q_stop = B_lvl_ptr[1 + 1]
                if B_lvl_q < B_lvl_q_stop
                    B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                else
                    B_lvl_i_stop = 0
                end
                phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_5 + -1), Threads.nthreads()))
                phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_5, Threads.nthreads()))
                if phase_stop_2 >= phase_start_2
                    if B_lvl_tbl2[B_lvl_q] < phase_start_2
                        B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    while true
                        B_lvl_i = B_lvl_tbl2[B_lvl_q]
                        B_lvl_q_step = B_lvl_q
                        if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                            B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        if B_lvl_i < phase_stop_2
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_4, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_4
                                            if A_lvl_tbl1[A_lvl_q] < i_4
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        else
                            phase_stop_13 = min(B_lvl_i, phase_stop_2)
                            if B_lvl_i == phase_stop_13
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_4, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_4
                                                if A_lvl_tbl1[A_lvl_q] < i_4
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            end
                            break
                        end
                    end
                end
            end
        Ct_lvl_2_val = val
        B_lvl_ptr = B_lvl_ptr_2
        B_lvl_tbl1 = B_lvl_tbl1_2
        B_lvl_tbl2 = B_lvl_tbl2_2
        B_lvl_val = val_2
        A_lvl_ptr = A_lvl_ptr_2
        A_lvl_tbl1 = A_lvl_tbl1_2
        A_lvl_tbl2 = A_lvl_tbl2_2
        A_lvl_val = val_3
    end
    resize!(Ct_lvl_2_val, A_lvl.shape[1] * B_lvl.shape[2])
    result = (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
    result
end
julia> @finch begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.70856859734722, 0.0, 0.0, 0.0, 0.0, 0.40683416426697433, 0.0, 0.0, 0.5116135014596547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11665081191597346, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6815260724911955, 0.0, 0.012792585308225086, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23414421666297028, 0.0, 0.049360715516608075, 0.0, 0.17114666211685992, 0.0, 0.0, 0.016462013473269064, 0.10889740119368015, 0.0, 0.0, 0.0, 0.00577445736315422, 0.03699291283586617, 0.0, 0.054643694670994745, 0.0, 0.0, 0.0, 0.08011353665096754, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1586425298490503, 0.5528778968703049, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22133004329744424, 0.198518145547811, 0.038821202773921835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18278114382714625, 0.1962641247777959, 0.33892974018746314, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013014677888550407, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6923776912647246, 0.0, 0.0, 0.0, 0.0, 0.03372992996786747, 0.0, 0.0, 0.0, 0.16613962476105457, 0.0, 0.21040813690603985, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12848588404681333, 0.44877210624067243, 0.0, 0.0, 0.00922143015221846, 0.0, 0.334367843457963, 0.0, 0.3344728737871945, 0.0, 0.007609184274463018, 0.0, 0.0783652603407444, 0.0, 0.008880000989208168, 0.6489339658812214, 0.0, 0.0, 0.4325454998052285, 0.24409841671258664, 0.0, 0.5857475361200408, 0.0, 0.006543890265716072, 0.0, 0.0, 0.0, 0.008425346810589148, 0.0, 0.0, 0.16622566484891482, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27228959440031625, 0.16863240701914226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16867493346874554, 0.42330165278515836, 0.0, 0.0, 0.6457463733709866, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12409061480277174, 0.0, 0.14869580809977814, 0.0, 0.0, 0.0, 0.0, 0.14872661472502066, 0.5062551989716859, 0.06509123699478471, 0.0, 0.026886171078575067, 0.0, 0.3503937118835625, 0.0, 0.13397275360612415, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07224002362266076, 0.0, 0.0, 0.014356019852976252, 0.2658719803791635, 0.13166323290951196, 0.0, 0.0, 0.0, 0.0, 0.020308821426184575, 0.025635708510132263, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007980130035150811, 0.4090538905999397, 0.0, 0.0, 0.008918289569362796, 0.31893009912431536, 0.0, 0.0, 0.0, 0.009525001505142072, 0.0, 0.0, 0.013278407346240287, 0.021967736559129595, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12136922915980554, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.279484838006678, 0.2506790805406071, 0.0, 0.0, 0.10182245842840729, 0.0, 0.0, 0.5745756063332743, 0.0, 0.5180568530499269, 0.0, 0.19018853437509187, 0.2984452378932974, 0.0, 0.0, 0.0, 0.2447378349547936, 0.0, 0.48522550177156104, 0.5906428332019245, 0.0, 0.0, 0.756905839792281, 0.0, 0.4128008966782496, 0.7173389246941403, 0.0, 0.4558370427008323, 0.6596592042653375, 0.33014948101925834, 0.0, 0.0, 0.0, 0.0, 0.17629318543732997, 0.5676926328184286, 0.6751757284159318, 0.18305092512120225, 0.0, 0.5526511646802545, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6816784313112039, 0.0, 0.0, 0.0, 0.0, 0.4384965381282249, 0.0, 0.5249476708016299, 0.5514304573066787, 0.0, 0.0, 0.0278361825874964, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.38619300372291254, 0.0, 0.5997636810868653, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20257599957103595, 0.0, 0.10844521538627999, 0.0, 1.2226089817686585, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04229463432907405, 0.0, 0.0, 0.0, 0.0, 0.19946965003853376, 0.048044013732540566, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.036066754029278635, 0.0, 0.16231330637401103, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6062852141104628, 0.40481276616279827, 0.0, 0.0, 0.2539845497787725, 0.0, 0.0, 0.5691049256163461, 0.572636917198234, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.026986118300396385, 0.0, 0.0, 0.2578776387682179, 0.2201878278780288, 0.3521973225117564, 0.49204059164875263, 0.20695213801377368, 0.3889153033164169, 0.12024801465448598, 0.5015222202004627, 0.0, 0.0, 0.0, 0.25511838171251733, 0.3395573400007252, 0.0, 0.14076018335068016, 0.006079069440287111, 0.3120090695218348, 0.02735798901473654, 0.23564550369089346, 0.0, 0.5201790908279286, 0.42693711336544043, 0.0, 0.0, 0.32942094250755716, 0.0, 0.0, 0.0, 0.0, 0.42208957099917227, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27253622974111436, 0.20091824181940837, 0.0, 0.0, 0.014451665708310772, 0.0648234167882922, 0.0, 0.0, 0.0, 0.0, 0.43998592755822313, 0.200499194569837, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10517105271620596, 0.4525079677294799, 0.0, 0.0, 0.0, 0.3992933674102322, 0.0, 0.6336315500314595, 0.5683248347075297, 0.0, 0.03911423543998137, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5618719435725092, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13717215169360888, 0.6023639264726295, 0.0, 0.16451246906039718, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2428824655720991, 0.0, 0.0, 0.0, 0.1307489669067023, 0.016490708613327746, 0.0, 0.5332965109179645, 0.0, 0.72835164827528, 0.5456771996801212, 0.20306669911566266, 0.8042852233793947, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08986996958215537, 0.0, 0.09372773893140769, 0.2910950907401526, 0.0, 0.0, 0.11400417325056827, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15073719147466486, 0.0, 0.0062702773417321524, 0.0, 0.11018067217476075, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03517847753788876, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012565073058016527, 0.0, 0.013511533493035587, 0.0, 0.0, 0.0, 0.1160032767395455, 0.3288776375406247, 0.009384967607632352, 0.013704851364696426, 0.0, 0.0, 0.0, 0.10679904950368906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5396299966121684, 0.0, 0.0, 0.0, 0.4120885163537877, 0.0, 0.0, 0.0, 0.0, 0.1374333138690627, 0.0, 0.0, 0.0, 0.0, 0.0, 0.308835769631579, 0.0, 0.0, 0.2935183046180839, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21641442424094412, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16667427577804808, 0.0, 0.0, 0.0, 0.0, 0.1072149118509185, 0.0, 0.0, 0.13482790109225204, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32702264908263307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17999984836884408, 0.0, 0.0, 0.0, 0.0, 0.17960575635997003, 0.0, 0.0, 0.0, 0.0, 0.08691531027429766, 0.0, 0.2893303365574358, 0.0, 0.21730094701941294, 0.07940443022930238, 0.0, 0.0, 0.01060094130869171, 0.0, 0.0, 0.21897146914670435, 0.3193413479843402, 0.11502306512986679, 0.16078084781731777, 0.0, 0.0, 0.029138065953053547, 0.12899773650646262, 0.0, 0.0, 0.0, 0.0, 0.35209003333845906, 0.0, 0.0, 0.42881832168904727, 0.0, 0.07551665376676242, 0.0, 0.10828996414360535, 0.0, 0.0, 0.0, 0.06175478190339224, 0.07317612880730584, 0.0, 0.4573326969055594, 0.13616590004764864, 0.1733951270114995, 0.0, 0.0, 0.0, 0.0, 0.004682414366264115, 0.0, 0.0, 0.014457116882680743, 0.477538324169661, 0.6690522150262886, 0.0, 0.0, 0.35074798851029604, 0.0, 0.6171788496488323, 0.0, 0.0, 0.0, 0.4598571719367441, 0.020424541646230095, 0.5014147387061915, 0.0, 0.0, 0.18767693158032267, 0.0, 0.08703683411330847, 0.0, 0.3388800152965881, 0.004390191394809013, 0.2131890539109787, 0.0, 0.43631258167790793, 0.0, 0.0, 0.0, 0.0, 0.011243656021000366, 0.0, 0.0, 0.004302223899340062, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3252052607129126, 0.10943362605720088, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4600533876598886, 0.0, 0.0, 0.0, 0.6238654988124779, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17327928789102656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3007942289792172, 0.49763260068933673, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1811653464990684, 0.0, 0.7596663692737102, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24115083978451807, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4132016125575898, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.30212649550849624, 0.45064666528381636, 0.0, 0.0, 1.0295831107021076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4924133218118872, 0.0, 0.0, 0.6123145833841932, 1.0074416855456372, 0.8974627446983252, 0.0, 0.0, 0.39387754214687604, 0.6443411524082342, 0.0, 0.6700619016074552, 0.0, 0.0, 0.4377096051189058, 0.3977735630344448, 0.0, 0.0, 0.0, 0.054395363429349475, 0.46912699721752377, 0.09054692381782561, 0.0, 0.6320539215802606, 0.33393006705956935, 0.614294006786966, 0.0, 0.0, 0.3986581318369477, 0.630705395059908, 0.0, 0.4266820903955834, 0.6230830913144384, 0.0, 1.0831760350408295, 0.21088758697054613, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7135848094649876, 0.06891111903866448, 0.0, 0.0, 0.0, 0.32920907856114856, 0.5438621275845595, 0.0, 0.4139962279186794, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13809169802867077, 0.0, 0.14849341459532064, 0.0, 0.0, 0.0, 0.5533499952652239, 0.0, 0.10314194807289108, 0.15061800177708015, 0.0, 0.5514890096422472, 0.23190068111070972, 0.0, 0.701822721136001, 0.6294877235260824, 0.0, 0.0, 0.0, 1.1059687974440209, 0.0, 0.0, 0.0, 0.96169924491107, 0.0, 0.0, 0.0, 0.0, 0.06628312350990566, 0.0, 0.37743722057381907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08765206247905213, 0.0, 0.5218343219200703, 0.0, 0.0, 0.0, 0.41461579287108996, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6671900885778136, 0.0, 0.0, 0.0, 0.028262864260027967, 0.7909490968152504, 0.04511006432931489, 0.0, 0.0, 0.0, 0.4564818142302977, 0.35113439772465294, 0.0, 0.0, 0.0, 0.03776705986522304, 0.6129906961879193, 0.0, 0.0, 0.0, 0.0, 0.0, 0.42034814190539166, 0.0, 0.0, 0.21638060933821013, 0.0, 0.0, 0.0, 0.2526638526129318, 0.03715346614099295, 0.6358424325058142, 0.0, 0.0, 0.04152129953915335, 0.04614236094065058, 0.0, 0.0, 0.07015688762319212, 0.05740172594394938, 0.18351207696535793, 0.0, 0.0, 0.008864400691370657, 0.0, 0.8651249823989057, 0.407678395491645, 0.845679782901505, 0.0, 0.0, 0.0, 0.0, 0.4753420456230014, 1.6107190368773618, 0.0, 0.0, 0.0638102435424578, 0.24041718723542044, 0.5177820553003522, 0.44619181405158337, 0.4489609795607919, 0.0, 0.07301898938869086, 0.0902695074635508, 0.4268753686592877, 0.0, 0.0, 0.0, 0.16408594940133228, 0.0, 0.0, 0.6736439849461895, 0.031382854877976686, 0.034000045192845295, 0.0, 0.0, 0.8630676095443404, 0.3932053636261741, 0.0, 0.0, 0.0, 0.23747714078691665, 0.2179752640521007, 0.0, 0.059020425946533776, 0.23507519209688904, 0.0, 0.6184935143504988, 0.18475168643298573, 0.3255150538360457, 0.0, 0.0, 0.10559996237493433, 0.258855010743052, 0.0, 0.0, 0.0, 0.0, 0.5931453366617909, 0.0, 0.012879882724963071, 0.34456425817306313, 0.0, 0.10363371305008902, 0.0, 0.0, 0.13032437159832005, 0.0, 0.0, 0.9136517678658531, 0.11645026631438779, 0.0, 0.0, 1.006195673838974, 0.0, 0.49183662858528426, 0.5242175629103283, 0.0, 0.0, 0.5842253265103485, 0.09175087532086124, 0.0, 0.0, 0.6346229344595797, 0.8483440750859271, 0.0, 0.0, 0.43168828228976236, 1.1746392457903614, 0.9786548179849323, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22466432888463708, 0.0, 0.0, 0.23383689086971213, 0.07868759200794351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5047067971651431, 0.0, 0.04466167239654772, 0.0, 0.18031282035336027, 0.06880056169195574, 0.0, 0.039760598424459205, 0.2052193360923158, 0.0, 0.1245954319549744, 0.0, 0.0, 0.3371869924989234, 0.10076818165067654, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21628428501390357, 0.41878760793751535, 0.0, 0.2743772579349106, 0.0, 0.0, 0.0, 0.23737336558608516, 0.0, 0.5609891075534331, 0.0, 0.0, 0.0, 0.0, 0.11206304892649907, 0.03770992438488979, 0.0, 0.0, 0.17383517173220803, 0.252655932334268, 0.0, 0.1585306005724301, 0.0, 0.1865298137691172, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05971061253533409, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15913149160221682, 0.0, 0.4030910369398146, 0.0, 0.32022434202130456, 0.23687245172165972, 0.0, 0.3599325265793679, 0.0, 0.4765964362382492, 0.0, 0.0, 0.0, 0.0, 0.6107120779202261, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.573260313114729, 0.5768180939544553, 0.0, 0.17499066339826644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7201822598869417, 0.0, 0.0, 0.0, 0.0, 0.08725982492406978, 0.5051841445138487, 0.0, 0.0, 0.0, 0.2569811590076317, 0.3964028116744446, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31198143094865527, 0.0, 0.0, 0.0, 0.0, 0.0, 0.42812611727229755, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.40187139064927635, 0.40436549376609193, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5533450612442088, 0.3541481069684905, 0.0, 0.0, 0.0, 0.1801509251180122, 0.0, 0.0, 0.0, 0.08899148634361333, 0.0, 0.4004935508153472, 0.1664002225353232, 0.6660561411338942, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5274372110974553, 0.0, 0.0, 0.11745018036413006, 0.03952272816910389, 0.654574176319042, 0.0, 0.0, 0.0, 0.0, 0.29642163335908794, 0.1274499525156547, 0.03345490004078419, 0.0, 0.0, 0.07653915037041417, 0.0, 0.02978363268713748, 0.15372448023095572, 0.03967384765850083, 0.06258104057589436, 0.0, 0.0, 0.044337981986606795, 0.07548283043416787, 0.0, 0.0, 0.0, 0.04735429836324166, 0.0, 0.0, 0.10863396357319674, 0.1797235339241024, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2053113311622715, 0.0, 0.0, 0.6339058613302375, 0.26881844559242324, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.047384735026192754, 0.0, 0.09394657688969027, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.41199723535819494, 0.0, 0.0, 0.19249813639294353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4930042077083581, 0.01569004651529754, 0.0, 0.1886409972802957, 0.0, 0.6504613027483894, 0.0, 0.0, 0.0, 0.4491308783325726, 0.0, 0.710600063326777, 0.0, 0.0, 0.0, 0.0, 0.6392481732081897, 0.0, 0.0, 0.21075790790635565, 0.143953089400151, 0.04517962123405222, 0.12446587761719602, 0.011602660433066613, 0.0, 0.021141064793252704, 0.017873704948355638, 0.0, 0.010329409925325224, 0.21730544865000528, 0.03874493610862431, 0.0, 0.0, 0.0, 0.04329986579681618, 0.02617857620219232, 0.0, 0.0, 0.14198488087669728, 0.04624555904799935, 0.33624630483183143, 0.0, 0.18065873234459212, 0.001167765010724762, 0.0, 0.41156050949339695, 0.06138103351330629, 0.0, 0.0, 0.0, 0.0, 0.0, 0.45223330011890606, 0.0, 0.42719481956226096, 0.0, 0.09390028378745956, 0.03159803909732349, 0.0, 0.0, 0.0, 0.14464053921535813, 0.0, 0.355579453624945, 0.27582314245244144, 0.0, 0.0, 0.4779189904559178, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6540512683692249, 0.16386729047093346, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08595593354654336, 0.0868518036911852, 0.1436872279500763, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08976911428083846, 0.0, 0.0, 0.0, 0.06561633035108058, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0209499775034313, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06082234101182034, 0.19585779687130894, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01528743323594608, 0.0, 0.0, 0.0, 0.011174280569280867, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3922416239543676, 0.0, 0.5308327922857218, 0.38811106643298066, 0.0, 0.5861742359361938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010357877371534024, 0.03335404406511716, 0.2121541431315042, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022181366862053645, 0.0, 0.0, 0.3232792127186002, 0.21780290259329063, 0.0, 0.0, 0.6724740217065875, 0.33454771410797746, 0.0, 0.0, 0.0, 0.0, 0.1517011050211833, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20421012621580428, 0.0, 0.0, 0.0, 0.09554663093280256, 0.0, 0.9066194809744413, 0.0, 0.6530356828494477, 0.0, 0.0, 0.4162310356447254, 0.2004622291406719, 0.0, 0.06531630990369706, 0.0, 0.0, 0.22516333873266767, 0.08751639468488193, 0.31273858097846424, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10565952264223451, 0.25213753900728575, 0.0, 0.13605306731422323, 0.0, 0.0, 0.20467409186458246, 0.0, 0.0, 0.0, 0.14062561725182204, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06654804138060808, 0.04780210989364991, 0.3312858803709398, 0.0, 0.09056986573031543, 0.0, 0.0, 0.22425218794100035, 0.6978010919630194, 0.5644822411311934, 0.0, 0.0, 0.0, 0.0, 0.25245963204466193, 0.5714387205668741, 0.0, 0.7794778768245345, 0.5229243836029062, 0.6068223768633771, 0.0, 0.0, 0.3439063861094934, 0.0, 0.5825998753106167, 0.07738278703840655, 0.0, 0.0, 0.45088731313609587, 0.020026145653267647, 0.49371220470576765, 0.0, 0.1967497337446331, 0.9333687888834381, 0.2687116640308751, 0.3109501951809905, 0.0, 0.628995837333548, 0.6431918870734405, 0.0, 0.0, 0.4278019777568812, 0.0, 0.0, 0.3426634565816645, 0.0, 0.7642079896603969, 0.15720215492969136, 0.0, 0.5627301734522071, 0.0, 0.0, 0.03992028432745955, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8902694418730629, 0.24785210085431258, 0.0, 0.0, 0.0, 0.19750384789873962, 0.0, 0.0, 0.24837057467209334, 0.0, 0.0, 0.06196917660673224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9196979517908253, 0.47523150409994275, 0.0, 0.05084636736867684, 0.30281629383729847, 0.0, 0.0, 0.0, 0.0, 0.7797375130571516, 0.0, 0.0, 0.009193696508787878, 0.7164774465476489, 0.37223214885126943, 0.36077619715591613]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = ((ex.bodies[1]).bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    result = nothing
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = (Finch).moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = (Finch).moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            B_lvl_q = B_lvl_ptr[1]
            B_lvl_q_stop = B_lvl_ptr[1 + 1]
            if B_lvl_q < B_lvl_q_stop
                B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
            else
                B_lvl_i_stop = 0
            end
            phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                if B_lvl_tbl2[B_lvl_q] < phase_start_2
                    B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                end
                while true
                    B_lvl_i = B_lvl_tbl2[B_lvl_q]
                    B_lvl_q_step = B_lvl_q
                    if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                        B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    if B_lvl_i < phase_stop_2
                        Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                        for i_6 = 1:A_lvl.shape[1]
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_6
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_6, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_6
                                            if A_lvl_tbl1[A_lvl_q] < i_6
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_6, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                        end
                        B_lvl_q = B_lvl_q_step
                    else
                        phase_stop_13 = min(B_lvl_i, phase_stop_2)
                        if B_lvl_i == phase_stop_13
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                            for i_8 = 1:A_lvl.shape[1]
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_8
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_8, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_8
                                                if A_lvl_tbl1[A_lvl_q] < i_8
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_8, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        end
                        break
                    end
                end
            end
        end
    resize!(val, A_lvl.shape[1] * B_lvl.shape[2])
    result = (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
    result
end
julia> @finch begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.70856859734722, 0.0, 0.0, 0.0, 0.0, 0.40683416426697433, 0.0, 0.0, 0.5116135014596547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11665081191597346, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6815260724911955, 0.0, 0.012792585308225086, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23414421666297028, 0.0, 0.049360715516608075, 0.0, 0.17114666211685992, 0.0, 0.0, 0.016462013473269064, 0.10889740119368015, 0.0, 0.0, 0.0, 0.00577445736315422, 0.03699291283586617, 0.0, 0.054643694670994745, 0.0, 0.0, 0.0, 0.08011353665096754, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1586425298490503, 0.5528778968703049, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22133004329744424, 0.198518145547811, 0.038821202773921835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18278114382714625, 0.1962641247777959, 0.33892974018746314, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013014677888550407, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6923776912647246, 0.0, 0.0, 0.0, 0.0, 0.03372992996786747, 0.0, 0.0, 0.0, 0.16613962476105457, 0.0, 0.21040813690603985, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12848588404681333, 0.44877210624067243, 0.0, 0.0, 0.00922143015221846, 0.0, 0.334367843457963, 0.0, 0.3344728737871945, 0.0, 0.007609184274463018, 0.0, 0.0783652603407444, 0.0, 0.008880000989208168, 0.6489339658812214, 0.0, 0.0, 0.4325454998052285, 0.24409841671258664, 0.0, 0.5857475361200408, 0.0, 0.006543890265716072, 0.0, 0.0, 0.0, 0.008425346810589148, 0.0, 0.0, 0.16622566484891482, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27228959440031625, 0.16863240701914226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16867493346874554, 0.42330165278515836, 0.0, 0.0, 0.6457463733709866, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12409061480277174, 0.0, 0.14869580809977814, 0.0, 0.0, 0.0, 0.0, 0.14872661472502066, 0.5062551989716859, 0.06509123699478471, 0.0, 0.026886171078575067, 0.0, 0.3503937118835625, 0.0, 0.13397275360612415, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07224002362266076, 0.0, 0.0, 0.014356019852976252, 0.2658719803791635, 0.13166323290951196, 0.0, 0.0, 0.0, 0.0, 0.020308821426184575, 0.025635708510132263, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007980130035150811, 0.4090538905999397, 0.0, 0.0, 0.008918289569362796, 0.31893009912431536, 0.0, 0.0, 0.0, 0.009525001505142072, 0.0, 0.0, 0.013278407346240287, 0.021967736559129595, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12136922915980554, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.279484838006678, 0.2506790805406071, 0.0, 0.0, 0.10182245842840729, 0.0, 0.0, 0.5745756063332743, 0.0, 0.5180568530499269, 0.0, 0.19018853437509187, 0.2984452378932974, 0.0, 0.0, 0.0, 0.2447378349547936, 0.0, 0.48522550177156104, 0.5906428332019245, 0.0, 0.0, 0.756905839792281, 0.0, 0.4128008966782496, 0.7173389246941403, 0.0, 0.4558370427008323, 0.6596592042653375, 0.33014948101925834, 0.0, 0.0, 0.0, 0.0, 0.17629318543732997, 0.5676926328184286, 0.6751757284159318, 0.18305092512120225, 0.0, 0.5526511646802545, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6816784313112039, 0.0, 0.0, 0.0, 0.0, 0.4384965381282249, 0.0, 0.5249476708016299, 0.5514304573066787, 0.0, 0.0, 0.0278361825874964, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.38619300372291254, 0.0, 0.5997636810868653, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20257599957103595, 0.0, 0.10844521538627999, 0.0, 1.2226089817686585, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04229463432907405, 0.0, 0.0, 0.0, 0.0, 0.19946965003853376, 0.048044013732540566, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.036066754029278635, 0.0, 0.16231330637401103, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6062852141104628, 0.40481276616279827, 0.0, 0.0, 0.2539845497787725, 0.0, 0.0, 0.5691049256163461, 0.572636917198234, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.026986118300396385, 0.0, 0.0, 0.2578776387682179, 0.2201878278780288, 0.3521973225117564, 0.49204059164875263, 0.20695213801377368, 0.3889153033164169, 0.12024801465448598, 0.5015222202004627, 0.0, 0.0, 0.0, 0.25511838171251733, 0.3395573400007252, 0.0, 0.14076018335068016, 0.006079069440287111, 0.3120090695218348, 0.02735798901473654, 0.23564550369089346, 0.0, 0.5201790908279286, 0.42693711336544043, 0.0, 0.0, 0.32942094250755716, 0.0, 0.0, 0.0, 0.0, 0.42208957099917227, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27253622974111436, 0.20091824181940837, 0.0, 0.0, 0.014451665708310772, 0.0648234167882922, 0.0, 0.0, 0.0, 0.0, 0.43998592755822313, 0.200499194569837, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10517105271620596, 0.4525079677294799, 0.0, 0.0, 0.0, 0.3992933674102322, 0.0, 0.6336315500314595, 0.5683248347075297, 0.0, 0.03911423543998137, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5618719435725092, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13717215169360888, 0.6023639264726295, 0.0, 0.16451246906039718, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2428824655720991, 0.0, 0.0, 0.0, 0.1307489669067023, 0.016490708613327746, 0.0, 0.5332965109179645, 0.0, 0.72835164827528, 0.5456771996801212, 0.20306669911566266, 0.8042852233793947, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08986996958215537, 0.0, 0.09372773893140769, 0.2910950907401526, 0.0, 0.0, 0.11400417325056827, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15073719147466486, 0.0, 0.0062702773417321524, 0.0, 0.11018067217476075, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03517847753788876, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012565073058016527, 0.0, 0.013511533493035587, 0.0, 0.0, 0.0, 0.1160032767395455, 0.3288776375406247, 0.009384967607632352, 0.013704851364696426, 0.0, 0.0, 0.0, 0.10679904950368906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5396299966121684, 0.0, 0.0, 0.0, 0.4120885163537877, 0.0, 0.0, 0.0, 0.0, 0.1374333138690627, 0.0, 0.0, 0.0, 0.0, 0.0, 0.308835769631579, 0.0, 0.0, 0.2935183046180839, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21641442424094412, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16667427577804808, 0.0, 0.0, 0.0, 0.0, 0.1072149118509185, 0.0, 0.0, 0.13482790109225204, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32702264908263307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17999984836884408, 0.0, 0.0, 0.0, 0.0, 0.17960575635997003, 0.0, 0.0, 0.0, 0.0, 0.08691531027429766, 0.0, 0.2893303365574358, 0.0, 0.21730094701941294, 0.07940443022930238, 0.0, 0.0, 0.01060094130869171, 0.0, 0.0, 0.21897146914670435, 0.3193413479843402, 0.11502306512986679, 0.16078084781731777, 0.0, 0.0, 0.029138065953053547, 0.12899773650646262, 0.0, 0.0, 0.0, 0.0, 0.35209003333845906, 0.0, 0.0, 0.42881832168904727, 0.0, 0.07551665376676242, 0.0, 0.10828996414360535, 0.0, 0.0, 0.0, 0.06175478190339224, 0.07317612880730584, 0.0, 0.4573326969055594, 0.13616590004764864, 0.1733951270114995, 0.0, 0.0, 0.0, 0.0, 0.004682414366264115, 0.0, 0.0, 0.014457116882680743, 0.477538324169661, 0.6690522150262886, 0.0, 0.0, 0.35074798851029604, 0.0, 0.6171788496488323, 0.0, 0.0, 0.0, 0.4598571719367441, 0.020424541646230095, 0.5014147387061915, 0.0, 0.0, 0.18767693158032267, 0.0, 0.08703683411330847, 0.0, 0.3388800152965881, 0.004390191394809013, 0.2131890539109787, 0.0, 0.43631258167790793, 0.0, 0.0, 0.0, 0.0, 0.011243656021000366, 0.0, 0.0, 0.004302223899340062, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3252052607129126, 0.10943362605720088, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4600533876598886, 0.0, 0.0, 0.0, 0.6238654988124779, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17327928789102656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3007942289792172, 0.49763260068933673, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1811653464990684, 0.0, 0.7596663692737102, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24115083978451807, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4132016125575898, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.30212649550849624, 0.45064666528381636, 0.0, 0.0, 1.0295831107021076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4924133218118872, 0.0, 0.0, 0.6123145833841932, 1.0074416855456372, 0.8974627446983252, 0.0, 0.0, 0.39387754214687604, 0.6443411524082342, 0.0, 0.6700619016074552, 0.0, 0.0, 0.4377096051189058, 0.3977735630344448, 0.0, 0.0, 0.0, 0.054395363429349475, 0.46912699721752377, 0.09054692381782561, 0.0, 0.6320539215802606, 0.33393006705956935, 0.614294006786966, 0.0, 0.0, 0.3986581318369477, 0.630705395059908, 0.0, 0.4266820903955834, 0.6230830913144384, 0.0, 1.0831760350408295, 0.21088758697054613, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7135848094649876, 0.06891111903866448, 0.0, 0.0, 0.0, 0.32920907856114856, 0.5438621275845595, 0.0, 0.4139962279186794, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13809169802867077, 0.0, 0.14849341459532064, 0.0, 0.0, 0.0, 0.5533499952652239, 0.0, 0.10314194807289108, 0.15061800177708015, 0.0, 0.5514890096422472, 0.23190068111070972, 0.0, 0.701822721136001, 0.6294877235260824, 0.0, 0.0, 0.0, 1.1059687974440209, 0.0, 0.0, 0.0, 0.96169924491107, 0.0, 0.0, 0.0, 0.0, 0.06628312350990566, 0.0, 0.37743722057381907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08765206247905213, 0.0, 0.5218343219200703, 0.0, 0.0, 0.0, 0.41461579287108996, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6671900885778136, 0.0, 0.0, 0.0, 0.028262864260027967, 0.7909490968152504, 0.04511006432931489, 0.0, 0.0, 0.0, 0.4564818142302977, 0.35113439772465294, 0.0, 0.0, 0.0, 0.03776705986522304, 0.6129906961879193, 0.0, 0.0, 0.0, 0.0, 0.0, 0.42034814190539166, 0.0, 0.0, 0.21638060933821013, 0.0, 0.0, 0.0, 0.2526638526129318, 0.03715346614099295, 0.6358424325058142, 0.0, 0.0, 0.04152129953915335, 0.04614236094065058, 0.0, 0.0, 0.07015688762319212, 0.05740172594394938, 0.18351207696535793, 0.0, 0.0, 0.008864400691370657, 0.0, 0.8651249823989057, 0.407678395491645, 0.845679782901505, 0.0, 0.0, 0.0, 0.0, 0.4753420456230014, 1.6107190368773618, 0.0, 0.0, 0.0638102435424578, 0.24041718723542044, 0.5177820553003522, 0.44619181405158337, 0.4489609795607919, 0.0, 0.07301898938869086, 0.0902695074635508, 0.4268753686592877, 0.0, 0.0, 0.0, 0.16408594940133228, 0.0, 0.0, 0.6736439849461895, 0.031382854877976686, 0.034000045192845295, 0.0, 0.0, 0.8630676095443404, 0.3932053636261741, 0.0, 0.0, 0.0, 0.23747714078691665, 0.2179752640521007, 0.0, 0.059020425946533776, 0.23507519209688904, 0.0, 0.6184935143504988, 0.18475168643298573, 0.3255150538360457, 0.0, 0.0, 0.10559996237493433, 0.258855010743052, 0.0, 0.0, 0.0, 0.0, 0.5931453366617909, 0.0, 0.012879882724963071, 0.34456425817306313, 0.0, 0.10363371305008902, 0.0, 0.0, 0.13032437159832005, 0.0, 0.0, 0.9136517678658531, 0.11645026631438779, 0.0, 0.0, 1.006195673838974, 0.0, 0.49183662858528426, 0.5242175629103283, 0.0, 0.0, 0.5842253265103485, 0.09175087532086124, 0.0, 0.0, 0.6346229344595797, 0.8483440750859271, 0.0, 0.0, 0.43168828228976236, 1.1746392457903614, 0.9786548179849323, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22466432888463708, 0.0, 0.0, 0.23383689086971213, 0.07868759200794351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5047067971651431, 0.0, 0.04466167239654772, 0.0, 0.18031282035336027, 0.06880056169195574, 0.0, 0.039760598424459205, 0.2052193360923158, 0.0, 0.1245954319549744, 0.0, 0.0, 0.3371869924989234, 0.10076818165067654, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21628428501390357, 0.41878760793751535, 0.0, 0.2743772579349106, 0.0, 0.0, 0.0, 0.23737336558608516, 0.0, 0.5609891075534331, 0.0, 0.0, 0.0, 0.0, 0.11206304892649907, 0.03770992438488979, 0.0, 0.0, 0.17383517173220803, 0.252655932334268, 0.0, 0.1585306005724301, 0.0, 0.1865298137691172, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05971061253533409, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15913149160221682, 0.0, 0.4030910369398146, 0.0, 0.32022434202130456, 0.23687245172165972, 0.0, 0.3599325265793679, 0.0, 0.4765964362382492, 0.0, 0.0, 0.0, 0.0, 0.6107120779202261, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.573260313114729, 0.5768180939544553, 0.0, 0.17499066339826644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7201822598869417, 0.0, 0.0, 0.0, 0.0, 0.08725982492406978, 0.5051841445138487, 0.0, 0.0, 0.0, 0.2569811590076317, 0.3964028116744446, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31198143094865527, 0.0, 0.0, 0.0, 0.0, 0.0, 0.42812611727229755, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.40187139064927635, 0.40436549376609193, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5533450612442088, 0.3541481069684905, 0.0, 0.0, 0.0, 0.1801509251180122, 0.0, 0.0, 0.0, 0.08899148634361333, 0.0, 0.4004935508153472, 0.1664002225353232, 0.6660561411338942, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5274372110974553, 0.0, 0.0, 0.11745018036413006, 0.03952272816910389, 0.654574176319042, 0.0, 0.0, 0.0, 0.0, 0.29642163335908794, 0.1274499525156547, 0.03345490004078419, 0.0, 0.0, 0.07653915037041417, 0.0, 0.02978363268713748, 0.15372448023095572, 0.03967384765850083, 0.06258104057589436, 0.0, 0.0, 0.044337981986606795, 0.07548283043416787, 0.0, 0.0, 0.0, 0.04735429836324166, 0.0, 0.0, 0.10863396357319674, 0.1797235339241024, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2053113311622715, 0.0, 0.0, 0.6339058613302375, 0.26881844559242324, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.047384735026192754, 0.0, 0.09394657688969027, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.41199723535819494, 0.0, 0.0, 0.19249813639294353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4930042077083581, 0.01569004651529754, 0.0, 0.1886409972802957, 0.0, 0.6504613027483894, 0.0, 0.0, 0.0, 0.4491308783325726, 0.0, 0.710600063326777, 0.0, 0.0, 0.0, 0.0, 0.6392481732081897, 0.0, 0.0, 0.21075790790635565, 0.143953089400151, 0.04517962123405222, 0.12446587761719602, 0.011602660433066613, 0.0, 0.021141064793252704, 0.017873704948355638, 0.0, 0.010329409925325224, 0.21730544865000528, 0.03874493610862431, 0.0, 0.0, 0.0, 0.04329986579681618, 0.02617857620219232, 0.0, 0.0, 0.14198488087669728, 0.04624555904799935, 0.33624630483183143, 0.0, 0.18065873234459212, 0.001167765010724762, 0.0, 0.41156050949339695, 0.06138103351330629, 0.0, 0.0, 0.0, 0.0, 0.0, 0.45223330011890606, 0.0, 0.42719481956226096, 0.0, 0.09390028378745956, 0.03159803909732349, 0.0, 0.0, 0.0, 0.14464053921535813, 0.0, 0.355579453624945, 0.27582314245244144, 0.0, 0.0, 0.4779189904559178, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6540512683692249, 0.16386729047093346, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08595593354654336, 0.0868518036911852, 0.1436872279500763, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08976911428083846, 0.0, 0.0, 0.0, 0.06561633035108058, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0209499775034313, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06082234101182034, 0.19585779687130894, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01528743323594608, 0.0, 0.0, 0.0, 0.011174280569280867, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3922416239543676, 0.0, 0.5308327922857218, 0.38811106643298066, 0.0, 0.5861742359361938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010357877371534024, 0.03335404406511716, 0.2121541431315042, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022181366862053645, 0.0, 0.0, 0.3232792127186002, 0.21780290259329063, 0.0, 0.0, 0.6724740217065875, 0.33454771410797746, 0.0, 0.0, 0.0, 0.0, 0.1517011050211833, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20421012621580428, 0.0, 0.0, 0.0, 0.09554663093280256, 0.0, 0.9066194809744413, 0.0, 0.6530356828494477, 0.0, 0.0, 0.4162310356447254, 0.2004622291406719, 0.0, 0.06531630990369706, 0.0, 0.0, 0.22516333873266767, 0.08751639468488193, 0.31273858097846424, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10565952264223451, 0.25213753900728575, 0.0, 0.13605306731422323, 0.0, 0.0, 0.20467409186458246, 0.0, 0.0, 0.0, 0.14062561725182204, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06654804138060808, 0.04780210989364991, 0.3312858803709398, 0.0, 0.09056986573031543, 0.0, 0.0, 0.22425218794100035, 0.6978010919630194, 0.5644822411311934, 0.0, 0.0, 0.0, 0.0, 0.25245963204466193, 0.5714387205668741, 0.0, 0.7794778768245345, 0.5229243836029062, 0.6068223768633771, 0.0, 0.0, 0.3439063861094934, 0.0, 0.5825998753106167, 0.07738278703840655, 0.0, 0.0, 0.45088731313609587, 0.020026145653267647, 0.49371220470576765, 0.0, 0.1967497337446331, 0.9333687888834381, 0.2687116640308751, 0.3109501951809905, 0.0, 0.628995837333548, 0.6431918870734405, 0.0, 0.0, 0.4278019777568812, 0.0, 0.0, 0.3426634565816645, 0.0, 0.7642079896603969, 0.15720215492969136, 0.0, 0.5627301734522071, 0.0, 0.0, 0.03992028432745955, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8902694418730629, 0.24785210085431258, 0.0, 0.0, 0.0, 0.19750384789873962, 0.0, 0.0, 0.24837057467209334, 0.0, 0.0, 0.06196917660673224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9196979517908253, 0.47523150409994275, 0.0, 0.05084636736867684, 0.30281629383729847, 0.0, 0.0, 0.0, 0.0, 0.7797375130571516, 0.0, 0.0, 0.009193696508787878, 0.7164774465476489, 0.37223214885126943, 0.36077619715591613]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = ((ex.bodies[1]).bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    result = nothing
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    B_lvl_q = B_lvl_ptr[1]
    B_lvl_q_stop = B_lvl_ptr[1 + 1]
    if B_lvl_q < B_lvl_q_stop
        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
    else
        B_lvl_i_stop = 0
    end
    phase_stop = min(B_lvl.shape[2], B_lvl_i_stop)
    if phase_stop >= 1
        if B_lvl_tbl2[B_lvl_q] < 1
            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
        end
        while true
            B_lvl_i = B_lvl_tbl2[B_lvl_q]
            B_lvl_q_step = B_lvl_q
            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
            end
            if B_lvl_i < phase_stop
                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                val = Ct_lvl_2_val
                Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                B_lvl_tbl1_2 = B_lvl_tbl1
                B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                B_lvl_tbl2_2 = B_lvl_tbl2
                val_2 = B_lvl_val
                B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
                A_lvl_ptr_2 = A_lvl_ptr
                A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                A_lvl_tbl1_2 = A_lvl_tbl1
                A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                A_lvl_tbl2_2 = A_lvl_tbl2
                A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                val_3 = A_lvl_val
                A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
                Threads.@threads for i_9 = 1:Threads.nthreads()
                        phase_start_6 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_9), Threads.nthreads()))
                        phase_stop_7 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_9, Threads.nthreads()))
                        if phase_stop_7 >= phase_start_6
                            for i_12 = phase_start_6:phase_stop_7
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_12
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_8 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_8 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_8
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_9 = min(B_lvl_i_2, phase_stop_8, A_lvl_i)
                                        if A_lvl_i == phase_stop_9 && B_lvl_i_2 == phase_stop_9
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_10 = min(i_12, A_lvl_i_stop_2)
                                            if phase_stop_10 >= i_12
                                                if A_lvl_tbl1[A_lvl_q] < i_12
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_12, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_10
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_12 = min(A_lvl_i_2, phase_stop_10)
                                                        if A_lvl_i_2 == phase_stop_12
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_9
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_9
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_9 + 1
                                    end
                                end
                            end
                        end
                    end
                Ct_lvl_2_val = val
                B_lvl_tbl1 = B_lvl_tbl1_2
                B_lvl_tbl2 = B_lvl_tbl2_2
                B_lvl_val = val_2
                A_lvl_ptr = A_lvl_ptr_2
                A_lvl_tbl1 = A_lvl_tbl1_2
                A_lvl_tbl2 = A_lvl_tbl2_2
                A_lvl_val = val_3
                B_lvl_q = B_lvl_q_step
            else
                phase_stop_18 = min(B_lvl_i, phase_stop)
                if B_lvl_i == phase_stop_18
                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_18
                    val_4 = Ct_lvl_2_val
                    Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                    B_lvl_tbl1_3 = B_lvl_tbl1
                    B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                    B_lvl_tbl2_3 = B_lvl_tbl2
                    val_5 = B_lvl_val
                    B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
                    A_lvl_ptr_3 = A_lvl_ptr
                    A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                    A_lvl_tbl1_3 = A_lvl_tbl1
                    A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                    A_lvl_tbl2_3 = A_lvl_tbl2
                    A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                    val_6 = A_lvl_val
                    A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
                    Threads.@threads for i_19 = 1:Threads.nthreads()
                            phase_start_21 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_19), Threads.nthreads()))
                            phase_stop_23 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_19, Threads.nthreads()))
                            if phase_stop_23 >= phase_start_21
                                for i_22 = phase_start_21:phase_stop_23
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_22
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_24 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_24 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_24
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_25 = min(B_lvl_i_2, A_lvl_i, phase_stop_24)
                                            if A_lvl_i == phase_stop_25 && B_lvl_i_2 == phase_stop_25
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_26 = min(i_22, A_lvl_i_stop_4)
                                                if phase_stop_26 >= i_22
                                                    if A_lvl_tbl1[A_lvl_q] < i_22
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_22, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_26
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_28 = min(A_lvl_i_4, phase_stop_26)
                                                            if A_lvl_i_4 == phase_stop_28
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_25
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_25
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_25 + 1
                                        end
                                    end
                                end
                            end
                        end
                    Ct_lvl_2_val = val_4
                    B_lvl_tbl1 = B_lvl_tbl1_3
                    B_lvl_tbl2 = B_lvl_tbl2_3
                    B_lvl_val = val_5
                    A_lvl_ptr = A_lvl_ptr_3
                    A_lvl_tbl1 = A_lvl_tbl1_3
                    A_lvl_tbl2 = A_lvl_tbl2_3
                    A_lvl_val = val_6
                    B_lvl_q = B_lvl_q_step
                end
                break
            end
        end
    end
    resize!(Ct_lvl_2_val, A_lvl.shape[1] * B_lvl.shape[2])
    result = (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
    result
end
julia> @finch begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.70856859734722, 0.0, 0.0, 0.0, 0.0, 0.40683416426697433, 0.0, 0.0, 0.5116135014596547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11665081191597346, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6815260724911955, 0.0, 0.012792585308225086, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23414421666297028, 0.0, 0.049360715516608075, 0.0, 0.17114666211685992, 0.0, 0.0, 0.016462013473269064, 0.10889740119368015, 0.0, 0.0, 0.0, 0.00577445736315422, 0.03699291283586617, 0.0, 0.054643694670994745, 0.0, 0.0, 0.0, 0.08011353665096754, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1586425298490503, 0.5528778968703049, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22133004329744424, 0.198518145547811, 0.038821202773921835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18278114382714625, 0.1962641247777959, 0.33892974018746314, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013014677888550407, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6923776912647246, 0.0, 0.0, 0.0, 0.0, 0.03372992996786747, 0.0, 0.0, 0.0, 0.16613962476105457, 0.0, 0.21040813690603985, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12848588404681333, 0.44877210624067243, 0.0, 0.0, 0.00922143015221846, 0.0, 0.334367843457963, 0.0, 0.3344728737871945, 0.0, 0.007609184274463018, 0.0, 0.0783652603407444, 0.0, 0.008880000989208168, 0.6489339658812214, 0.0, 0.0, 0.4325454998052285, 0.24409841671258664, 0.0, 0.5857475361200408, 0.0, 0.006543890265716072, 0.0, 0.0, 0.0, 0.008425346810589148, 0.0, 0.0, 0.16622566484891482, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27228959440031625, 0.16863240701914226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16867493346874554, 0.42330165278515836, 0.0, 0.0, 0.6457463733709866, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12409061480277174, 0.0, 0.14869580809977814, 0.0, 0.0, 0.0, 0.0, 0.14872661472502066, 0.5062551989716859, 0.06509123699478471, 0.0, 0.026886171078575067, 0.0, 0.3503937118835625, 0.0, 0.13397275360612415, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07224002362266076, 0.0, 0.0, 0.014356019852976252, 0.2658719803791635, 0.13166323290951196, 0.0, 0.0, 0.0, 0.0, 0.020308821426184575, 0.025635708510132263, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007980130035150811, 0.4090538905999397, 0.0, 0.0, 0.008918289569362796, 0.31893009912431536, 0.0, 0.0, 0.0, 0.009525001505142072, 0.0, 0.0, 0.013278407346240287, 0.021967736559129595, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12136922915980554, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.279484838006678, 0.2506790805406071, 0.0, 0.0, 0.10182245842840729, 0.0, 0.0, 0.5745756063332743, 0.0, 0.5180568530499269, 0.0, 0.19018853437509187, 0.2984452378932974, 0.0, 0.0, 0.0, 0.2447378349547936, 0.0, 0.48522550177156104, 0.5906428332019245, 0.0, 0.0, 0.756905839792281, 0.0, 0.4128008966782496, 0.7173389246941403, 0.0, 0.4558370427008323, 0.6596592042653375, 0.33014948101925834, 0.0, 0.0, 0.0, 0.0, 0.17629318543732997, 0.5676926328184286, 0.6751757284159318, 0.18305092512120225, 0.0, 0.5526511646802545, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6816784313112039, 0.0, 0.0, 0.0, 0.0, 0.4384965381282249, 0.0, 0.5249476708016299, 0.5514304573066787, 0.0, 0.0, 0.0278361825874964, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.38619300372291254, 0.0, 0.5997636810868653, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20257599957103595, 0.0, 0.10844521538627999, 0.0, 1.2226089817686585, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04229463432907405, 0.0, 0.0, 0.0, 0.0, 0.19946965003853376, 0.048044013732540566, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.036066754029278635, 0.0, 0.16231330637401103, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6062852141104628, 0.40481276616279827, 0.0, 0.0, 0.2539845497787725, 0.0, 0.0, 0.5691049256163461, 0.572636917198234, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.026986118300396385, 0.0, 0.0, 0.2578776387682179, 0.2201878278780288, 0.3521973225117564, 0.49204059164875263, 0.20695213801377368, 0.3889153033164169, 0.12024801465448598, 0.5015222202004627, 0.0, 0.0, 0.0, 0.25511838171251733, 0.3395573400007252, 0.0, 0.14076018335068016, 0.006079069440287111, 0.3120090695218348, 0.02735798901473654, 0.23564550369089346, 0.0, 0.5201790908279286, 0.42693711336544043, 0.0, 0.0, 0.32942094250755716, 0.0, 0.0, 0.0, 0.0, 0.42208957099917227, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27253622974111436, 0.20091824181940837, 0.0, 0.0, 0.014451665708310772, 0.0648234167882922, 0.0, 0.0, 0.0, 0.0, 0.43998592755822313, 0.200499194569837, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10517105271620596, 0.4525079677294799, 0.0, 0.0, 0.0, 0.3992933674102322, 0.0, 0.6336315500314595, 0.5683248347075297, 0.0, 0.03911423543998137, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5618719435725092, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13717215169360888, 0.6023639264726295, 0.0, 0.16451246906039718, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2428824655720991, 0.0, 0.0, 0.0, 0.1307489669067023, 0.016490708613327746, 0.0, 0.5332965109179645, 0.0, 0.72835164827528, 0.5456771996801212, 0.20306669911566266, 0.8042852233793947, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08986996958215537, 0.0, 0.09372773893140769, 0.2910950907401526, 0.0, 0.0, 0.11400417325056827, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15073719147466486, 0.0, 0.0062702773417321524, 0.0, 0.11018067217476075, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03517847753788876, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012565073058016527, 0.0, 0.013511533493035587, 0.0, 0.0, 0.0, 0.1160032767395455, 0.3288776375406247, 0.009384967607632352, 0.013704851364696426, 0.0, 0.0, 0.0, 0.10679904950368906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5396299966121684, 0.0, 0.0, 0.0, 0.4120885163537877, 0.0, 0.0, 0.0, 0.0, 0.1374333138690627, 0.0, 0.0, 0.0, 0.0, 0.0, 0.308835769631579, 0.0, 0.0, 0.2935183046180839, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21641442424094412, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16667427577804808, 0.0, 0.0, 0.0, 0.0, 0.1072149118509185, 0.0, 0.0, 0.13482790109225204, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32702264908263307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17999984836884408, 0.0, 0.0, 0.0, 0.0, 0.17960575635997003, 0.0, 0.0, 0.0, 0.0, 0.08691531027429766, 0.0, 0.2893303365574358, 0.0, 0.21730094701941294, 0.07940443022930238, 0.0, 0.0, 0.01060094130869171, 0.0, 0.0, 0.21897146914670435, 0.3193413479843402, 0.11502306512986679, 0.16078084781731777, 0.0, 0.0, 0.029138065953053547, 0.12899773650646262, 0.0, 0.0, 0.0, 0.0, 0.35209003333845906, 0.0, 0.0, 0.42881832168904727, 0.0, 0.07551665376676242, 0.0, 0.10828996414360535, 0.0, 0.0, 0.0, 0.06175478190339224, 0.07317612880730584, 0.0, 0.4573326969055594, 0.13616590004764864, 0.1733951270114995, 0.0, 0.0, 0.0, 0.0, 0.004682414366264115, 0.0, 0.0, 0.014457116882680743, 0.477538324169661, 0.6690522150262886, 0.0, 0.0, 0.35074798851029604, 0.0, 0.6171788496488323, 0.0, 0.0, 0.0, 0.4598571719367441, 0.020424541646230095, 0.5014147387061915, 0.0, 0.0, 0.18767693158032267, 0.0, 0.08703683411330847, 0.0, 0.3388800152965881, 0.004390191394809013, 0.2131890539109787, 0.0, 0.43631258167790793, 0.0, 0.0, 0.0, 0.0, 0.011243656021000366, 0.0, 0.0, 0.004302223899340062, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3252052607129126, 0.10943362605720088, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4600533876598886, 0.0, 0.0, 0.0, 0.6238654988124779, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17327928789102656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3007942289792172, 0.49763260068933673, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1811653464990684, 0.0, 0.7596663692737102, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24115083978451807, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4132016125575898, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.30212649550849624, 0.45064666528381636, 0.0, 0.0, 1.0295831107021076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4924133218118872, 0.0, 0.0, 0.6123145833841932, 1.0074416855456372, 0.8974627446983252, 0.0, 0.0, 0.39387754214687604, 0.6443411524082342, 0.0, 0.6700619016074552, 0.0, 0.0, 0.4377096051189058, 0.3977735630344448, 0.0, 0.0, 0.0, 0.054395363429349475, 0.46912699721752377, 0.09054692381782561, 0.0, 0.6320539215802606, 0.33393006705956935, 0.614294006786966, 0.0, 0.0, 0.3986581318369477, 0.630705395059908, 0.0, 0.4266820903955834, 0.6230830913144384, 0.0, 1.0831760350408295, 0.21088758697054613, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7135848094649876, 0.06891111903866448, 0.0, 0.0, 0.0, 0.32920907856114856, 0.5438621275845595, 0.0, 0.4139962279186794, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13809169802867077, 0.0, 0.14849341459532064, 0.0, 0.0, 0.0, 0.5533499952652239, 0.0, 0.10314194807289108, 0.15061800177708015, 0.0, 0.5514890096422472, 0.23190068111070972, 0.0, 0.701822721136001, 0.6294877235260824, 0.0, 0.0, 0.0, 1.1059687974440209, 0.0, 0.0, 0.0, 0.96169924491107, 0.0, 0.0, 0.0, 0.0, 0.06628312350990566, 0.0, 0.37743722057381907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08765206247905213, 0.0, 0.5218343219200703, 0.0, 0.0, 0.0, 0.41461579287108996, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6671900885778136, 0.0, 0.0, 0.0, 0.028262864260027967, 0.7909490968152504, 0.04511006432931489, 0.0, 0.0, 0.0, 0.4564818142302977, 0.35113439772465294, 0.0, 0.0, 0.0, 0.03776705986522304, 0.6129906961879193, 0.0, 0.0, 0.0, 0.0, 0.0, 0.42034814190539166, 0.0, 0.0, 0.21638060933821013, 0.0, 0.0, 0.0, 0.2526638526129318, 0.03715346614099295, 0.6358424325058142, 0.0, 0.0, 0.04152129953915335, 0.04614236094065058, 0.0, 0.0, 0.07015688762319212, 0.05740172594394938, 0.18351207696535793, 0.0, 0.0, 0.008864400691370657, 0.0, 0.8651249823989057, 0.407678395491645, 0.845679782901505, 0.0, 0.0, 0.0, 0.0, 0.4753420456230014, 1.6107190368773618, 0.0, 0.0, 0.0638102435424578, 0.24041718723542044, 0.5177820553003522, 0.44619181405158337, 0.4489609795607919, 0.0, 0.07301898938869086, 0.0902695074635508, 0.4268753686592877, 0.0, 0.0, 0.0, 0.16408594940133228, 0.0, 0.0, 0.6736439849461895, 0.031382854877976686, 0.034000045192845295, 0.0, 0.0, 0.8630676095443404, 0.3932053636261741, 0.0, 0.0, 0.0, 0.23747714078691665, 0.2179752640521007, 0.0, 0.059020425946533776, 0.23507519209688904, 0.0, 0.6184935143504988, 0.18475168643298573, 0.3255150538360457, 0.0, 0.0, 0.10559996237493433, 0.258855010743052, 0.0, 0.0, 0.0, 0.0, 0.5931453366617909, 0.0, 0.012879882724963071, 0.34456425817306313, 0.0, 0.10363371305008902, 0.0, 0.0, 0.13032437159832005, 0.0, 0.0, 0.9136517678658531, 0.11645026631438779, 0.0, 0.0, 1.006195673838974, 0.0, 0.49183662858528426, 0.5242175629103283, 0.0, 0.0, 0.5842253265103485, 0.09175087532086124, 0.0, 0.0, 0.6346229344595797, 0.8483440750859271, 0.0, 0.0, 0.43168828228976236, 1.1746392457903614, 0.9786548179849323, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22466432888463708, 0.0, 0.0, 0.23383689086971213, 0.07868759200794351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5047067971651431, 0.0, 0.04466167239654772, 0.0, 0.18031282035336027, 0.06880056169195574, 0.0, 0.039760598424459205, 0.2052193360923158, 0.0, 0.1245954319549744, 0.0, 0.0, 0.3371869924989234, 0.10076818165067654, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21628428501390357, 0.41878760793751535, 0.0, 0.2743772579349106, 0.0, 0.0, 0.0, 0.23737336558608516, 0.0, 0.5609891075534331, 0.0, 0.0, 0.0, 0.0, 0.11206304892649907, 0.03770992438488979, 0.0, 0.0, 0.17383517173220803, 0.252655932334268, 0.0, 0.1585306005724301, 0.0, 0.1865298137691172, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05971061253533409, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15913149160221682, 0.0, 0.4030910369398146, 0.0, 0.32022434202130456, 0.23687245172165972, 0.0, 0.3599325265793679, 0.0, 0.4765964362382492, 0.0, 0.0, 0.0, 0.0, 0.6107120779202261, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.573260313114729, 0.5768180939544553, 0.0, 0.17499066339826644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7201822598869417, 0.0, 0.0, 0.0, 0.0, 0.08725982492406978, 0.5051841445138487, 0.0, 0.0, 0.0, 0.2569811590076317, 0.3964028116744446, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31198143094865527, 0.0, 0.0, 0.0, 0.0, 0.0, 0.42812611727229755, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.40187139064927635, 0.40436549376609193, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5533450612442088, 0.3541481069684905, 0.0, 0.0, 0.0, 0.1801509251180122, 0.0, 0.0, 0.0, 0.08899148634361333, 0.0, 0.4004935508153472, 0.1664002225353232, 0.6660561411338942, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5274372110974553, 0.0, 0.0, 0.11745018036413006, 0.03952272816910389, 0.654574176319042, 0.0, 0.0, 0.0, 0.0, 0.29642163335908794, 0.1274499525156547, 0.03345490004078419, 0.0, 0.0, 0.07653915037041417, 0.0, 0.02978363268713748, 0.15372448023095572, 0.03967384765850083, 0.06258104057589436, 0.0, 0.0, 0.044337981986606795, 0.07548283043416787, 0.0, 0.0, 0.0, 0.04735429836324166, 0.0, 0.0, 0.10863396357319674, 0.1797235339241024, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2053113311622715, 0.0, 0.0, 0.6339058613302375, 0.26881844559242324, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.047384735026192754, 0.0, 0.09394657688969027, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.41199723535819494, 0.0, 0.0, 0.19249813639294353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4930042077083581, 0.01569004651529754, 0.0, 0.1886409972802957, 0.0, 0.6504613027483894, 0.0, 0.0, 0.0, 0.4491308783325726, 0.0, 0.710600063326777, 0.0, 0.0, 0.0, 0.0, 0.6392481732081897, 0.0, 0.0, 0.21075790790635565, 0.143953089400151, 0.04517962123405222, 0.12446587761719602, 0.011602660433066613, 0.0, 0.021141064793252704, 0.017873704948355638, 0.0, 0.010329409925325224, 0.21730544865000528, 0.03874493610862431, 0.0, 0.0, 0.0, 0.04329986579681618, 0.02617857620219232, 0.0, 0.0, 0.14198488087669728, 0.04624555904799935, 0.33624630483183143, 0.0, 0.18065873234459212, 0.001167765010724762, 0.0, 0.41156050949339695, 0.06138103351330629, 0.0, 0.0, 0.0, 0.0, 0.0, 0.45223330011890606, 0.0, 0.42719481956226096, 0.0, 0.09390028378745956, 0.03159803909732349, 0.0, 0.0, 0.0, 0.14464053921535813, 0.0, 0.355579453624945, 0.27582314245244144, 0.0, 0.0, 0.4779189904559178, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6540512683692249, 0.16386729047093346, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08595593354654336, 0.0868518036911852, 0.1436872279500763, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08976911428083846, 0.0, 0.0, 0.0, 0.06561633035108058, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0209499775034313, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06082234101182034, 0.19585779687130894, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01528743323594608, 0.0, 0.0, 0.0, 0.011174280569280867, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3922416239543676, 0.0, 0.5308327922857218, 0.38811106643298066, 0.0, 0.5861742359361938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010357877371534024, 0.03335404406511716, 0.2121541431315042, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022181366862053645, 0.0, 0.0, 0.3232792127186002, 0.21780290259329063, 0.0, 0.0, 0.6724740217065875, 0.33454771410797746, 0.0, 0.0, 0.0, 0.0, 0.1517011050211833, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20421012621580428, 0.0, 0.0, 0.0, 0.09554663093280256, 0.0, 0.9066194809744413, 0.0, 0.6530356828494477, 0.0, 0.0, 0.4162310356447254, 0.2004622291406719, 0.0, 0.06531630990369706, 0.0, 0.0, 0.22516333873266767, 0.08751639468488193, 0.31273858097846424, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10565952264223451, 0.25213753900728575, 0.0, 0.13605306731422323, 0.0, 0.0, 0.20467409186458246, 0.0, 0.0, 0.0, 0.14062561725182204, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06654804138060808, 0.04780210989364991, 0.3312858803709398, 0.0, 0.09056986573031543, 0.0, 0.0, 0.22425218794100035, 0.6978010919630194, 0.5644822411311934, 0.0, 0.0, 0.0, 0.0, 0.25245963204466193, 0.5714387205668741, 0.0, 0.7794778768245345, 0.5229243836029062, 0.6068223768633771, 0.0, 0.0, 0.3439063861094934, 0.0, 0.5825998753106167, 0.07738278703840655, 0.0, 0.0, 0.45088731313609587, 0.020026145653267647, 0.49371220470576765, 0.0, 0.1967497337446331, 0.9333687888834381, 0.2687116640308751, 0.3109501951809905, 0.0, 0.628995837333548, 0.6431918870734405, 0.0, 0.0, 0.4278019777568812, 0.0, 0.0, 0.3426634565816645, 0.0, 0.7642079896603969, 0.15720215492969136, 0.0, 0.5627301734522071, 0.0, 0.0, 0.03992028432745955, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8902694418730629, 0.24785210085431258, 0.0, 0.0, 0.0, 0.19750384789873962, 0.0, 0.0, 0.24837057467209334, 0.0, 0.0, 0.06196917660673224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9196979517908253, 0.47523150409994275, 0.0, 0.05084636736867684, 0.30281629383729847, 0.0, 0.0, 0.0, 0.0, 0.7797375130571516, 0.0, 0.0, 0.009193696508787878, 0.7164774465476489, 0.37223214885126943, 0.36077619715591613]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = parallel(_)
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = ((ex.bodies[1]).bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    result = nothing
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = (Finch).moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = (Finch).moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            B_lvl_q = B_lvl_ptr[1]
            B_lvl_q_stop = B_lvl_ptr[1 + 1]
            if B_lvl_q < B_lvl_q_stop
                B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
            else
                B_lvl_i_stop = 0
            end
            phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                if B_lvl_tbl2[B_lvl_q] < phase_start_2
                    B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                end
                while true
                    B_lvl_i = B_lvl_tbl2[B_lvl_q]
                    B_lvl_q_step = B_lvl_q
                    if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                        B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    if B_lvl_i < phase_stop_2
                        Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                        val_4 = Ct_lvl_2_val
                        Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                        A_lvl_ptr_3 = A_lvl_ptr
                        A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                        A_lvl_tbl1_3 = A_lvl_tbl1
                        A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                        A_lvl_tbl2_3 = A_lvl_tbl2
                        A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                        val_5 = A_lvl_val
                        A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
                        B_lvl_ptr_3 = B_lvl_ptr
                        B_lvl_tbl1_3 = B_lvl_tbl1
                        B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                        B_lvl_tbl2_3 = B_lvl_tbl2
                        val_6 = B_lvl_val
                        B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
                        Threads.@threads for i_10 = 1:Threads.nthreads()
                                phase_start_7 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_10), Threads.nthreads()))
                                phase_stop_8 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_10, Threads.nthreads()))
                                if phase_stop_8 >= phase_start_7
                                    for i_13 = phase_start_7:phase_stop_8
                                        Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_13
                                        A_lvl_q = A_lvl_ptr[1]
                                        A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                        if A_lvl_q < A_lvl_q_stop
                                            A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                        else
                                            A_lvl_i_stop = 0
                                        end
                                        B_lvl_q_3 = B_lvl_q
                                        if B_lvl_q < B_lvl_q_step
                                            B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                        else
                                            B_lvl_i_stop_3 = 0
                                        end
                                        phase_stop_9 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                        if phase_stop_9 >= 1
                                            k = 1
                                            if A_lvl_tbl2[A_lvl_q] < 1
                                                A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            if B_lvl_tbl1[B_lvl_q] < 1
                                                B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                            end
                                            while k <= phase_stop_9
                                                A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                                A_lvl_q_step = A_lvl_q
                                                if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                    A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                                end
                                                B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                                phase_stop_10 = min(B_lvl_i_3, phase_stop_9, A_lvl_i)
                                                if A_lvl_i == phase_stop_10 && B_lvl_i_3 == phase_stop_10
                                                    B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                                    A_lvl_q_2 = A_lvl_q
                                                    if A_lvl_q < A_lvl_q_step
                                                        A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                    else
                                                        A_lvl_i_stop_2 = 0
                                                    end
                                                    phase_stop_11 = min(i_13, A_lvl_i_stop_2)
                                                    if phase_stop_11 >= i_13
                                                        if A_lvl_tbl1[A_lvl_q] < i_13
                                                            A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_13, A_lvl_q, A_lvl_q_step - 1)
                                                        end
                                                        while true
                                                            A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                            if A_lvl_i_2 < phase_stop_11
                                                                A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                                Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                                A_lvl_q_2 += 1
                                                            else
                                                                phase_stop_13 = min(A_lvl_i_2, phase_stop_11)
                                                                if A_lvl_i_2 == phase_stop_13
                                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                                    Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                                    A_lvl_q_2 += 1
                                                                end
                                                                break
                                                            end
                                                        end
                                                    end
                                                    A_lvl_q = A_lvl_q_step
                                                    B_lvl_q_3 += 1
                                                elseif B_lvl_i_3 == phase_stop_10
                                                    B_lvl_q_3 += 1
                                                elseif A_lvl_i == phase_stop_10
                                                    A_lvl_q = A_lvl_q_step
                                                end
                                                k = phase_stop_10 + 1
                                            end
                                        end
                                    end
                                end
                            end
                        Ct_lvl_2_val = val_4
                        A_lvl_ptr = A_lvl_ptr_3
                        A_lvl_tbl1 = A_lvl_tbl1_3
                        A_lvl_tbl2 = A_lvl_tbl2_3
                        A_lvl_val = val_5
                        B_lvl_ptr = B_lvl_ptr_3
                        B_lvl_tbl1 = B_lvl_tbl1_3
                        B_lvl_tbl2 = B_lvl_tbl2_3
                        B_lvl_val = val_6
                        B_lvl_q = B_lvl_q_step
                    else
                        phase_stop_19 = min(B_lvl_i, phase_stop_2)
                        if B_lvl_i == phase_stop_19
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_19
                            val_7 = Ct_lvl_2_val
                            Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                            A_lvl_ptr_4 = A_lvl_ptr
                            A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                            A_lvl_tbl1_4 = A_lvl_tbl1
                            A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                            A_lvl_tbl2_4 = A_lvl_tbl2
                            A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                            val_8 = A_lvl_val
                            A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
                            B_lvl_ptr_4 = B_lvl_ptr
                            B_lvl_tbl1_4 = B_lvl_tbl1
                            B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                            B_lvl_tbl2_4 = B_lvl_tbl2
                            val_9 = B_lvl_val
                            B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
                            Threads.@threads for i_20 = 1:Threads.nthreads()
                                    phase_start_22 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_20), Threads.nthreads()))
                                    phase_stop_24 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_20, Threads.nthreads()))
                                    if phase_stop_24 >= phase_start_22
                                        for i_23 = phase_start_22:phase_stop_24
                                            Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_23
                                            A_lvl_q = A_lvl_ptr[1]
                                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                            if A_lvl_q < A_lvl_q_stop
                                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                            else
                                                A_lvl_i_stop = 0
                                            end
                                            B_lvl_q_3 = B_lvl_q
                                            if B_lvl_q < B_lvl_q_step
                                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                            else
                                                B_lvl_i_stop_3 = 0
                                            end
                                            phase_stop_25 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                            if phase_stop_25 >= 1
                                                k = 1
                                                if A_lvl_tbl2[A_lvl_q] < 1
                                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                                end
                                                if B_lvl_tbl1[B_lvl_q] < 1
                                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                                end
                                                while k <= phase_stop_25
                                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                                    A_lvl_q_step = A_lvl_q
                                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                                    end
                                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                                    phase_stop_26 = min(B_lvl_i_3, A_lvl_i, phase_stop_25)
                                                    if A_lvl_i == phase_stop_26 && B_lvl_i_3 == phase_stop_26
                                                        B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                                        A_lvl_q_4 = A_lvl_q
                                                        if A_lvl_q < A_lvl_q_step
                                                            A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                        else
                                                            A_lvl_i_stop_4 = 0
                                                        end
                                                        phase_stop_27 = min(i_23, A_lvl_i_stop_4)
                                                        if phase_stop_27 >= i_23
                                                            if A_lvl_tbl1[A_lvl_q] < i_23
                                                                A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_23, A_lvl_q, A_lvl_q_step - 1)
                                                            end
                                                            while true
                                                                A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                                if A_lvl_i_4 < phase_stop_27
                                                                    A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                    Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                                    A_lvl_q_4 += 1
                                                                else
                                                                    phase_stop_29 = min(A_lvl_i_4, phase_stop_27)
                                                                    if A_lvl_i_4 == phase_stop_29
                                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                                        A_lvl_q_4 += 1
                                                                    end
                                                                    break
                                                                end
                                                            end
                                                        end
                                                        A_lvl_q = A_lvl_q_step
                                                        B_lvl_q_3 += 1
                                                    elseif B_lvl_i_3 == phase_stop_26
                                                        B_lvl_q_3 += 1
                                                    elseif A_lvl_i == phase_stop_26
                                                        A_lvl_q = A_lvl_q_step
                                                    end
                                                    k = phase_stop_26 + 1
                                                end
                                            end
                                        end
                                    end
                                end
                            Ct_lvl_2_val = val_7
                            A_lvl_ptr = A_lvl_ptr_4
                            A_lvl_tbl1 = A_lvl_tbl1_4
                            A_lvl_tbl2 = A_lvl_tbl2_4
                            A_lvl_val = val_8
                            B_lvl_ptr = B_lvl_ptr_4
                            B_lvl_tbl1 = B_lvl_tbl1_4
                            B_lvl_tbl2 = B_lvl_tbl2_4
                            B_lvl_val = val_9
                            B_lvl_q = B_lvl_q_step
                        end
                        break
                    end
                end
            end
        end
    resize!(val, A_lvl.shape[1] * B_lvl.shape[2])
    result = (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
    result
end
julia> @finch begin
        Ct .= 0
        for j = parallel(_)
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.70856859734722, 0.0, 0.0, 0.0, 0.0, 0.40683416426697433, 0.0, 0.0, 0.5116135014596547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11665081191597346, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6815260724911955, 0.0, 0.012792585308225086, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23414421666297028, 0.0, 0.049360715516608075, 0.0, 0.17114666211685992, 0.0, 0.0, 0.016462013473269064, 0.10889740119368015, 0.0, 0.0, 0.0, 0.00577445736315422, 0.03699291283586617, 0.0, 0.054643694670994745, 0.0, 0.0, 0.0, 0.08011353665096754, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1586425298490503, 0.5528778968703049, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22133004329744424, 0.198518145547811, 0.038821202773921835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18278114382714625, 0.1962641247777959, 0.33892974018746314, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013014677888550407, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6923776912647246, 0.0, 0.0, 0.0, 0.0, 0.03372992996786747, 0.0, 0.0, 0.0, 0.16613962476105457, 0.0, 0.21040813690603985, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12848588404681333, 0.44877210624067243, 0.0, 0.0, 0.00922143015221846, 0.0, 0.334367843457963, 0.0, 0.3344728737871945, 0.0, 0.007609184274463018, 0.0, 0.0783652603407444, 0.0, 0.008880000989208168, 0.6489339658812214, 0.0, 0.0, 0.4325454998052285, 0.24409841671258664, 0.0, 0.5857475361200408, 0.0, 0.006543890265716072, 0.0, 0.0, 0.0, 0.008425346810589148, 0.0, 0.0, 0.16622566484891482, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27228959440031625, 0.16863240701914226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16867493346874554, 0.42330165278515836, 0.0, 0.0, 0.6457463733709866, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12409061480277174, 0.0, 0.14869580809977814, 0.0, 0.0, 0.0, 0.0, 0.14872661472502066, 0.5062551989716859, 0.06509123699478471, 0.0, 0.026886171078575067, 0.0, 0.3503937118835625, 0.0, 0.13397275360612415, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07224002362266076, 0.0, 0.0, 0.014356019852976252, 0.2658719803791635, 0.13166323290951196, 0.0, 0.0, 0.0, 0.0, 0.020308821426184575, 0.025635708510132263, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007980130035150811, 0.4090538905999397, 0.0, 0.0, 0.008918289569362796, 0.31893009912431536, 0.0, 0.0, 0.0, 0.009525001505142072, 0.0, 0.0, 0.013278407346240287, 0.021967736559129595, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12136922915980554, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.279484838006678, 0.2506790805406071, 0.0, 0.0, 0.10182245842840729, 0.0, 0.0, 0.5745756063332743, 0.0, 0.5180568530499269, 0.0, 0.19018853437509187, 0.2984452378932974, 0.0, 0.0, 0.0, 0.2447378349547936, 0.0, 0.48522550177156104, 0.5906428332019245, 0.0, 0.0, 0.756905839792281, 0.0, 0.4128008966782496, 0.7173389246941403, 0.0, 0.4558370427008323, 0.6596592042653375, 0.33014948101925834, 0.0, 0.0, 0.0, 0.0, 0.17629318543732997, 0.5676926328184286, 0.6751757284159318, 0.18305092512120225, 0.0, 0.5526511646802545, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6816784313112039, 0.0, 0.0, 0.0, 0.0, 0.4384965381282249, 0.0, 0.5249476708016299, 0.5514304573066787, 0.0, 0.0, 0.0278361825874964, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.38619300372291254, 0.0, 0.5997636810868653, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20257599957103595, 0.0, 0.10844521538627999, 0.0, 1.2226089817686585, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04229463432907405, 0.0, 0.0, 0.0, 0.0, 0.19946965003853376, 0.048044013732540566, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.036066754029278635, 0.0, 0.16231330637401103, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6062852141104628, 0.40481276616279827, 0.0, 0.0, 0.2539845497787725, 0.0, 0.0, 0.5691049256163461, 0.572636917198234, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.026986118300396385, 0.0, 0.0, 0.2578776387682179, 0.2201878278780288, 0.3521973225117564, 0.49204059164875263, 0.20695213801377368, 0.3889153033164169, 0.12024801465448598, 0.5015222202004627, 0.0, 0.0, 0.0, 0.25511838171251733, 0.3395573400007252, 0.0, 0.14076018335068016, 0.006079069440287111, 0.3120090695218348, 0.02735798901473654, 0.23564550369089346, 0.0, 0.5201790908279286, 0.42693711336544043, 0.0, 0.0, 0.32942094250755716, 0.0, 0.0, 0.0, 0.0, 0.42208957099917227, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27253622974111436, 0.20091824181940837, 0.0, 0.0, 0.014451665708310772, 0.0648234167882922, 0.0, 0.0, 0.0, 0.0, 0.43998592755822313, 0.200499194569837, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10517105271620596, 0.4525079677294799, 0.0, 0.0, 0.0, 0.3992933674102322, 0.0, 0.6336315500314595, 0.5683248347075297, 0.0, 0.03911423543998137, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5618719435725092, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13717215169360888, 0.6023639264726295, 0.0, 0.16451246906039718, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2428824655720991, 0.0, 0.0, 0.0, 0.1307489669067023, 0.016490708613327746, 0.0, 0.5332965109179645, 0.0, 0.72835164827528, 0.5456771996801212, 0.20306669911566266, 0.8042852233793947, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08986996958215537, 0.0, 0.09372773893140769, 0.2910950907401526, 0.0, 0.0, 0.11400417325056827, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15073719147466486, 0.0, 0.0062702773417321524, 0.0, 0.11018067217476075, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03517847753788876, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012565073058016527, 0.0, 0.013511533493035587, 0.0, 0.0, 0.0, 0.1160032767395455, 0.3288776375406247, 0.009384967607632352, 0.013704851364696426, 0.0, 0.0, 0.0, 0.10679904950368906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5396299966121684, 0.0, 0.0, 0.0, 0.4120885163537877, 0.0, 0.0, 0.0, 0.0, 0.1374333138690627, 0.0, 0.0, 0.0, 0.0, 0.0, 0.308835769631579, 0.0, 0.0, 0.2935183046180839, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21641442424094412, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16667427577804808, 0.0, 0.0, 0.0, 0.0, 0.1072149118509185, 0.0, 0.0, 0.13482790109225204, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32702264908263307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17999984836884408, 0.0, 0.0, 0.0, 0.0, 0.17960575635997003, 0.0, 0.0, 0.0, 0.0, 0.08691531027429766, 0.0, 0.2893303365574358, 0.0, 0.21730094701941294, 0.07940443022930238, 0.0, 0.0, 0.01060094130869171, 0.0, 0.0, 0.21897146914670435, 0.3193413479843402, 0.11502306512986679, 0.16078084781731777, 0.0, 0.0, 0.029138065953053547, 0.12899773650646262, 0.0, 0.0, 0.0, 0.0, 0.35209003333845906, 0.0, 0.0, 0.42881832168904727, 0.0, 0.07551665376676242, 0.0, 0.10828996414360535, 0.0, 0.0, 0.0, 0.06175478190339224, 0.07317612880730584, 0.0, 0.4573326969055594, 0.13616590004764864, 0.1733951270114995, 0.0, 0.0, 0.0, 0.0, 0.004682414366264115, 0.0, 0.0, 0.014457116882680743, 0.477538324169661, 0.6690522150262886, 0.0, 0.0, 0.35074798851029604, 0.0, 0.6171788496488323, 0.0, 0.0, 0.0, 0.4598571719367441, 0.020424541646230095, 0.5014147387061915, 0.0, 0.0, 0.18767693158032267, 0.0, 0.08703683411330847, 0.0, 0.3388800152965881, 0.004390191394809013, 0.2131890539109787, 0.0, 0.43631258167790793, 0.0, 0.0, 0.0, 0.0, 0.011243656021000366, 0.0, 0.0, 0.004302223899340062, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3252052607129126, 0.10943362605720088, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4600533876598886, 0.0, 0.0, 0.0, 0.6238654988124779, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17327928789102656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3007942289792172, 0.49763260068933673, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1811653464990684, 0.0, 0.7596663692737102, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24115083978451807, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4132016125575898, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.30212649550849624, 0.45064666528381636, 0.0, 0.0, 1.0295831107021076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4924133218118872, 0.0, 0.0, 0.6123145833841932, 1.0074416855456372, 0.8974627446983252, 0.0, 0.0, 0.39387754214687604, 0.6443411524082342, 0.0, 0.6700619016074552, 0.0, 0.0, 0.4377096051189058, 0.3977735630344448, 0.0, 0.0, 0.0, 0.054395363429349475, 0.46912699721752377, 0.09054692381782561, 0.0, 0.6320539215802606, 0.33393006705956935, 0.614294006786966, 0.0, 0.0, 0.3986581318369477, 0.630705395059908, 0.0, 0.4266820903955834, 0.6230830913144384, 0.0, 1.0831760350408295, 0.21088758697054613, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7135848094649876, 0.06891111903866448, 0.0, 0.0, 0.0, 0.32920907856114856, 0.5438621275845595, 0.0, 0.4139962279186794, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13809169802867077, 0.0, 0.14849341459532064, 0.0, 0.0, 0.0, 0.5533499952652239, 0.0, 0.10314194807289108, 0.15061800177708015, 0.0, 0.5514890096422472, 0.23190068111070972, 0.0, 0.701822721136001, 0.6294877235260824, 0.0, 0.0, 0.0, 1.1059687974440209, 0.0, 0.0, 0.0, 0.96169924491107, 0.0, 0.0, 0.0, 0.0, 0.06628312350990566, 0.0, 0.37743722057381907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08765206247905213, 0.0, 0.5218343219200703, 0.0, 0.0, 0.0, 0.41461579287108996, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6671900885778136, 0.0, 0.0, 0.0, 0.028262864260027967, 0.7909490968152504, 0.04511006432931489, 0.0, 0.0, 0.0, 0.4564818142302977, 0.35113439772465294, 0.0, 0.0, 0.0, 0.03776705986522304, 0.6129906961879193, 0.0, 0.0, 0.0, 0.0, 0.0, 0.42034814190539166, 0.0, 0.0, 0.21638060933821013, 0.0, 0.0, 0.0, 0.2526638526129318, 0.03715346614099295, 0.6358424325058142, 0.0, 0.0, 0.04152129953915335, 0.04614236094065058, 0.0, 0.0, 0.07015688762319212, 0.05740172594394938, 0.18351207696535793, 0.0, 0.0, 0.008864400691370657, 0.0, 0.8651249823989057, 0.407678395491645, 0.845679782901505, 0.0, 0.0, 0.0, 0.0, 0.4753420456230014, 1.6107190368773618, 0.0, 0.0, 0.0638102435424578, 0.24041718723542044, 0.5177820553003522, 0.44619181405158337, 0.4489609795607919, 0.0, 0.07301898938869086, 0.0902695074635508, 0.4268753686592877, 0.0, 0.0, 0.0, 0.16408594940133228, 0.0, 0.0, 0.6736439849461895, 0.031382854877976686, 0.034000045192845295, 0.0, 0.0, 0.8630676095443404, 0.3932053636261741, 0.0, 0.0, 0.0, 0.23747714078691665, 0.2179752640521007, 0.0, 0.059020425946533776, 0.23507519209688904, 0.0, 0.6184935143504988, 0.18475168643298573, 0.3255150538360457, 0.0, 0.0, 0.10559996237493433, 0.258855010743052, 0.0, 0.0, 0.0, 0.0, 0.5931453366617909, 0.0, 0.012879882724963071, 0.34456425817306313, 0.0, 0.10363371305008902, 0.0, 0.0, 0.13032437159832005, 0.0, 0.0, 0.9136517678658531, 0.11645026631438779, 0.0, 0.0, 1.006195673838974, 0.0, 0.49183662858528426, 0.5242175629103283, 0.0, 0.0, 0.5842253265103485, 0.09175087532086124, 0.0, 0.0, 0.6346229344595797, 0.8483440750859271, 0.0, 0.0, 0.43168828228976236, 1.1746392457903614, 0.9786548179849323, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22466432888463708, 0.0, 0.0, 0.23383689086971213, 0.07868759200794351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5047067971651431, 0.0, 0.04466167239654772, 0.0, 0.18031282035336027, 0.06880056169195574, 0.0, 0.039760598424459205, 0.2052193360923158, 0.0, 0.1245954319549744, 0.0, 0.0, 0.3371869924989234, 0.10076818165067654, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21628428501390357, 0.41878760793751535, 0.0, 0.2743772579349106, 0.0, 0.0, 0.0, 0.23737336558608516, 0.0, 0.5609891075534331, 0.0, 0.0, 0.0, 0.0, 0.11206304892649907, 0.03770992438488979, 0.0, 0.0, 0.17383517173220803, 0.252655932334268, 0.0, 0.1585306005724301, 0.0, 0.1865298137691172, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05971061253533409, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15913149160221682, 0.0, 0.4030910369398146, 0.0, 0.32022434202130456, 0.23687245172165972, 0.0, 0.3599325265793679, 0.0, 0.4765964362382492, 0.0, 0.0, 0.0, 0.0, 0.6107120779202261, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.573260313114729, 0.5768180939544553, 0.0, 0.17499066339826644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7201822598869417, 0.0, 0.0, 0.0, 0.0, 0.08725982492406978, 0.5051841445138487, 0.0, 0.0, 0.0, 0.2569811590076317, 0.3964028116744446, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31198143094865527, 0.0, 0.0, 0.0, 0.0, 0.0, 0.42812611727229755, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.40187139064927635, 0.40436549376609193, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5533450612442088, 0.3541481069684905, 0.0, 0.0, 0.0, 0.1801509251180122, 0.0, 0.0, 0.0, 0.08899148634361333, 0.0, 0.4004935508153472, 0.1664002225353232, 0.6660561411338942, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5274372110974553, 0.0, 0.0, 0.11745018036413006, 0.03952272816910389, 0.654574176319042, 0.0, 0.0, 0.0, 0.0, 0.29642163335908794, 0.1274499525156547, 0.03345490004078419, 0.0, 0.0, 0.07653915037041417, 0.0, 0.02978363268713748, 0.15372448023095572, 0.03967384765850083, 0.06258104057589436, 0.0, 0.0, 0.044337981986606795, 0.07548283043416787, 0.0, 0.0, 0.0, 0.04735429836324166, 0.0, 0.0, 0.10863396357319674, 0.1797235339241024, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2053113311622715, 0.0, 0.0, 0.6339058613302375, 0.26881844559242324, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.047384735026192754, 0.0, 0.09394657688969027, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.41199723535819494, 0.0, 0.0, 0.19249813639294353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4930042077083581, 0.01569004651529754, 0.0, 0.1886409972802957, 0.0, 0.6504613027483894, 0.0, 0.0, 0.0, 0.4491308783325726, 0.0, 0.710600063326777, 0.0, 0.0, 0.0, 0.0, 0.6392481732081897, 0.0, 0.0, 0.21075790790635565, 0.143953089400151, 0.04517962123405222, 0.12446587761719602, 0.011602660433066613, 0.0, 0.021141064793252704, 0.017873704948355638, 0.0, 0.010329409925325224, 0.21730544865000528, 0.03874493610862431, 0.0, 0.0, 0.0, 0.04329986579681618, 0.02617857620219232, 0.0, 0.0, 0.14198488087669728, 0.04624555904799935, 0.33624630483183143, 0.0, 0.18065873234459212, 0.001167765010724762, 0.0, 0.41156050949339695, 0.06138103351330629, 0.0, 0.0, 0.0, 0.0, 0.0, 0.45223330011890606, 0.0, 0.42719481956226096, 0.0, 0.09390028378745956, 0.03159803909732349, 0.0, 0.0, 0.0, 0.14464053921535813, 0.0, 0.355579453624945, 0.27582314245244144, 0.0, 0.0, 0.4779189904559178, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6540512683692249, 0.16386729047093346, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08595593354654336, 0.0868518036911852, 0.1436872279500763, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08976911428083846, 0.0, 0.0, 0.0, 0.06561633035108058, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0209499775034313, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06082234101182034, 0.19585779687130894, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01528743323594608, 0.0, 0.0, 0.0, 0.011174280569280867, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3922416239543676, 0.0, 0.5308327922857218, 0.38811106643298066, 0.0, 0.5861742359361938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010357877371534024, 0.03335404406511716, 0.2121541431315042, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022181366862053645, 0.0, 0.0, 0.3232792127186002, 0.21780290259329063, 0.0, 0.0, 0.6724740217065875, 0.33454771410797746, 0.0, 0.0, 0.0, 0.0, 0.1517011050211833, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20421012621580428, 0.0, 0.0, 0.0, 0.09554663093280256, 0.0, 0.9066194809744413, 0.0, 0.6530356828494477, 0.0, 0.0, 0.4162310356447254, 0.2004622291406719, 0.0, 0.06531630990369706, 0.0, 0.0, 0.22516333873266767, 0.08751639468488193, 0.31273858097846424, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10565952264223451, 0.25213753900728575, 0.0, 0.13605306731422323, 0.0, 0.0, 0.20467409186458246, 0.0, 0.0, 0.0, 0.14062561725182204, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06654804138060808, 0.04780210989364991, 0.3312858803709398, 0.0, 0.09056986573031543, 0.0, 0.0, 0.22425218794100035, 0.6978010919630194, 0.5644822411311934, 0.0, 0.0, 0.0, 0.0, 0.25245963204466193, 0.5714387205668741, 0.0, 0.7794778768245345, 0.5229243836029062, 0.6068223768633771, 0.0, 0.0, 0.3439063861094934, 0.0, 0.5825998753106167, 0.07738278703840655, 0.0, 0.0, 0.45088731313609587, 0.020026145653267647, 0.49371220470576765, 0.0, 0.1967497337446331, 0.9333687888834381, 0.2687116640308751, 0.3109501951809905, 0.0, 0.628995837333548, 0.6431918870734405, 0.0, 0.0, 0.4278019777568812, 0.0, 0.0, 0.3426634565816645, 0.0, 0.7642079896603969, 0.15720215492969136, 0.0, 0.5627301734522071, 0.0, 0.0, 0.03992028432745955, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8902694418730629, 0.24785210085431258, 0.0, 0.0, 0.0, 0.19750384789873962, 0.0, 0.0, 0.24837057467209334, 0.0, 0.0, 0.06196917660673224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9196979517908253, 0.47523150409994275, 0.0, 0.05084636736867684, 0.30281629383729847, 0.0, 0.0, 0.0, 0.0, 0.7797375130571516, 0.0, 0.0, 0.009193696508787878, 0.7164774465476489, 0.37223214885126943, 0.36077619715591613]), 42), 42)),)

