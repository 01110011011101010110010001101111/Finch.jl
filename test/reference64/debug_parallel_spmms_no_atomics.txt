julia> @finch begin
        CR .= 0
        for i = _
            for j = _
                for k = _
                    CR[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(CR = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.18463927542167063, 0.16872726618738124, 0.0, 0.0, 0.0, 0.5894648191942263, 0.0, 0.0, 0.4399979683038884, 0.4872396182365841, 0.0, 0.0, 0.0, 0.5232587908717465, 0.0, 0.028458776980687873, 0.0, 0.1912882078151879, 0.5164204427507948, 0.25375520169397275, 0.7988048308846816, 0.548219404354131, 0.0, 0.0, 0.0, 0.08973042123875676, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16269102551813888, 0.0, 0.6368094288799442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03708423534085979, 0.16845626013473977, 0.6588748971102081, 0.010877416543996312, 0.0, 0.0, 0.0, 0.44123232519423083, 0.0, 0.24002669576156066, 0.0, 0.0, 0.0, 0.21689234576879324, 0.2098895841423717, 0.6161647640351655, 0.4776903273077015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6355594744363152, 0.0, 0.0, 0.08457949502263125, 0.0, 0.0, 0.6901459039432658, 0.0, 0.0, 0.0, 0.057641486402300816, 0.0, 0.0, 0.09549316762404748, 0.09211299074367744, 0.11795836585190977, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0467877508869465, 0.0, 0.0, 0.0, 0.0, 0.04370712736779799, 0.281244207688617, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2030486542718697, 0.0, 0.5182178756464231, 0.0, 0.0, 0.0, 0.057729640414041494, 0.0, 0.0, 0.0, 0.0, 0.19354540299708142, 0.06300918519480127, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1478355026964281, 0.0, 0.0, 0.0, 0.541165159810693, 0.0, 0.0, 0.0, 0.16407931870757664, 0.2057712545151301, 0.32812733654993936, 0.0, 0.24913050204963633, 0.20974603114415158, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5086673274457614, 0.32467254520642835, 0.0, 0.22357645210013752, 0.0, 0.0, 0.0, 0.15908397194054766, 0.0, 0.5866968618616202, 0.21349654033737236, 0.243214867615369, 0.0, 0.0, 0.27374855832219774, 0.0, 0.3075930851551728, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44087523604465795, 0.22505682646241645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2406824670174747, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4353583318013107, 0.0, 0.0, 0.0, 0.0, 0.42257756079609887, 0.0, 0.16328692344233348, 0.7239663574899861, 0.253917129058883, 0.0, 0.0, 0.0, 0.012941670285380298, 0.005948300091521438, 0.6648934966409066, 1.0229318994166612, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13578338949001834, 0.4197116456419966, 0.0, 0.947053670891027, 0.0, 0.13486088669396906, 0.0, 0.6620124726194103, 0.0, 0.6496849904621673, 0.0, 0.0, 0.7890806801928671, 0.01841465881846702, 0.27914669822772004, 0.0, 0.029516583729604803, 0.0, 0.0, 0.018864268719240455, 0.24394303159635272, 0.0, 0.5741603792214627, 0.3080365664801448, 0.6155081898538994, 0.2367816204152454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17416434327376132, 0.0, 0.0, 0.0, 0.0, 0.16269692366253372, 0.0, 0.3491221627973697, 0.0, 0.0, 0.0, 0.0, 0.0, 0.896220207410521, 0.13858473312625733, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2148948115594729, 0.1029957040349639, 0.0, 0.10833979410008034, 0.0, 0.0, 0.23454757178184932, 0.0, 0.2155791117194733, 0.0, 0.22311080318652476, 0.0, 0.0, 0.0, 0.0, 0.07959294795619429, 0.0, 0.17163567622890405, 0.07283994445558169, 0.0, 0.0, 0.0, 0.08654379427684032, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16368166253985994, 0.0, 0.17071385720442764, 0.34758616988919516, 0.0, 0.0, 0.0, 0.0, 0.25336804558610515, 0.0, 0.032257457723339725, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2136279714073461, 0.0, 0.24003957209260046, 0.21303254632455026, 0.006511170502708404, 0.383310313899178, 0.0, 0.0, 0.0, 0.1337677376744651, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0646519260588245, 0.0, 0.0, 0.0, 0.31350938881953055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20384728982176575, 0.0, 0.0, 0.0, 0.0, 0.05670575896106868, 0.0, 0.05964802423259749, 0.0, 0.0, 0.0, 0.4488563568162377, 0.11869016538841153, 0.0, 0.07667734536959822, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.040450967971474544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09480431131127824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0016369185703860613, 0.0, 0.0017218526009009176, 0.0, 0.0, 0.0, 0.0, 0.003426215245260554, 0.0, 0.002213436040060287, 0.0, 0.0, 0.0, 0.0, 0.028624781651490867, 0.0, 0.0, 0.0917221717112923, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03948493179664919, 0.0998678691104795, 0.0, 0.0, 0.0, 0.0, 0.0519314302968116, 0.019753641949088564, 0.0, 0.0, 0.08679325173985959, 0.0, 0.0, 0.44412542864463755, 0.0, 0.0, 0.0, 0.25440324223859473, 0.0, 0.06322962025433154, 0.0, 0.10055420420038351, 0.08043066558940987, 0.01930551218305079, 0.019941838639940274, 0.025537187307223392, 0.0, 0.2186623932992767, 0.06407227122111338, 0.0254835950299198, 0.0, 0.0, 0.6628636898477958, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5422666551724673, 0.6870145051652384, 0.0, 0.0, 0.1501651963322761, 0.019574736086580828, 0.0, 0.0, 0.03589870327691644, 0.0, 0.026565876114361182, 0.0, 0.0, 0.029264986204643254, 0.0, 0.0, 0.01893195304872942, 0.046727577005066426, 0.0, 0.0, 0.28025040572174376, 0.0, 0.018913093854322344, 0.0, 0.0, 0.0, 0.04894945277637971, 0.006583695531171352, 0.3875798367774556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3292155284261099, 0.04344067936293723, 0.0653720550702903, 0.0, 0.0, 0.22649528787365272, 0.018049730534704385, 0.14615138313733889, 0.0, 0.0, 0.0, 0.0, 0.20611785399916963, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31511052111518423, 0.0, 0.03672395965581389, 0.0, 0.0, 0.9552411806377301, 0.0, 0.0, 0.0, 0.3512720117094026, 0.0, 0.7814509194588117, 0.9900444949397926, 0.0, 0.2416698136762867, 0.0, 0.15594310090475377, 0.37345396038999507, 0.13378200524387487, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0037871796529251873, 0.0, 0.07132197093797825, 0.0, 0.0, 0.4038639203443338, 0.0, 0.007926901739731977, 0.0, 0.005121012178382578, 0.4483265473520345, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19545324009971515, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015822792815906598, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2654026801406409, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005338269605792754, 0.3841199941370228, 0.003089715236949895, 0.0, 0.0, 0.0, 0.2702896727602311, 0.0061480462629685435, 0.0, 0.0077716018517988575, 0.0, 0.0, 0.5060334906800565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5325221835704804, 0.4354711371945976, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4529112439588725, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.42175363431898494, 0.540090698333841, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1429447733494866, 0.0, 0.0, 0.17383910328410926, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7778438721037507, 0.7592283162924894, 0.27027978053916946, 0.9680808201913975, 0.0, 0.0, 0.0, 0.0, 0.23721687040615713, 0.0, 0.0, 0.0, 0.214724244927467, 0.0, 0.6713747276751851, 0.2691029964691732, 0.24957193666978766, 0.04174679102400021, 0.0, 0.0, 0.44623226966778873, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9739425217687773, 0.3878431688435761, 0.0, 0.0, 0.0, 0.0, 0.5905353658253105, 0.0, 0.41099451149099264, 0.46959062394023404, 0.38400872176268547, 0.0, 0.4974135902693747, 0.0, 0.0, 0.0, 0.9154557174508635, 0.2586557560688982, 0.09838729964211103, 0.4023736739495505, 0.0, 0.43229262167712557, 0.25909478937718255, 0.0, 0.0, 0.9527916935554207, 0.3993878193283669, 0.4377172055566821, 0.0, 0.0, 0.02448196261221554, 0.28590941544125414, 0.0, 0.4006023808751287, 0.03918308759508126, 0.0, 0.2882716435062163, 0.0, 0.0110856134215592, 0.37363494354124355, 0.049778285574341975, 0.0, 0.0, 0.9269077110105975, 0.31315811797299237, 0.0, 0.012303674452273154, 0.015429991708474198, 0.0, 0.0, 0.014446013487908171, 0.04123415495113678, 0.03371931705724562, 0.29538913191692134, 0.0, 0.3349006097039049, 0.23542836206424417, 0.23052114963877735, 0.31614042332280545, 0.0, 0.0, 0.11683083227872514, 0.0, 0.0, 0.0006114799040197484, 0.10975727273531571, 0.0, 0.004295508139252644, 0.05107901709661121, 0.0, 0.9334562674798996, 0.0, 0.0, 0.0, 0.0, 0.5880007983611341, 0.06450382436313898, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11800940272957328, 0.0, 0.1899751304440791, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.273744879534597, 0.0, 0.0, 0.2671580962806136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3492510086210155, 0.0, 0.3080038714015672, 0.0, 0.0, 0.2221605109577772, 0.0, 0.0, 0.0, 0.0, 0.07999688285015112, 0.06742807554519747, 0.08634727349135116, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10258077598728532, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5012203954355052, 0.0, 0.0, 0.0, 0.08111247475296846, 0.0, 0.0, 0.12138201096876389, 0.0, 0.08982551378065248, 0.0, 0.0, 0.12126628140327211, 0.0, 0.0, 0.07844895363581271, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07837080617454226, 0.0, 0.0, 0.0, 0.20283344995989894, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1282379216751728, 0.033796057874619194, 0.0, 0.09653017040380268, 0.0, 0.08822574416521244, 0.01404236183038609, 0.056929725378038414, 0.0, 0.15526439596527059, 0.0, 0.0, 0.0, 0.03781410167223484, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2529010219858623, 0.3238608478009693, 0.0, 0.0, 0.1407409609998849, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.329852425046806, 0.0, 0.0, 0.33341239592531074, 0.45526487856119674, 0.0, 0.33690660829115365, 0.08120142522000469, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08802821410799472, 0.39029129939250945, 0.13688708765693952, 0.0, 0.08496391292272984, 0.0, 0.0, 0.5183827483305287, 0.703065773125918, 0.4814340077714492, 0.0, 0.0, 0.0, 0.12053001147739661, 0.0, 0.07320101959394086, 0.0, 0.0, 0.5105579893076974, 0.6674329003953465, 0.7892434941755087, 0.051355650714931346, 0.4403973256963498, 0.0, 0.01884360978410253, 0.0, 0.0, 0.42539452394686605, 0.1591995319925085, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17375879569486846, 0.0, 0.0, 0.0, 0.0, 0.1721016330265864, 0.0, 0.00044047585032663307, 0.1672932604423309, 0.18102660031449758, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004373654797423859, 0.0, 0.0, 0.08031885873943684, 0.0, 0.0, 0.12088873452345646, 0.0, 0.25447674137867604, 0.0, 0.20210883042048103, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05466434515545004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005119594240948603, 0.0, 0.2461616206228127, 0.0, 0.0, 0.40062152777610743, 0.0, 0.0, 0.0, 0.4450914546665199, 0.0, 0.0, 0.0, 0.25078704499066257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6437873083636174, 0.0, 0.08986536930257488, 0.05459463157090123, 0.0, 0.0, 0.1617782592438168, 0.0, 0.0, 0.0, 0.0, 0.510293956173341, 0.0, 0.0, 0.0, 0.0, 0.0539939979224501, 0.06022554282071448, 0.0, 0.0, 0.0017903697912685446, 0.10539843894447092, 0.0, 0.007216901945678346, 0.0, 0.0, 0.0, 0.22684974207663636, 1.4564553082728662, 0.182720528082649, 0.06653876275957457, 0.0, 0.6274289004185675, 0.0, 0.5406773490882926, 0.27822503396119075, 0.09306814986560254, 0.29383387946651635, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6230315233142055, 0.9648450369390001, 0.5108864746688708, 0.0, 0.36840186712596623, 0.22805280632725022, 0.3227837297104921, 0.0, 0.0, 1.0240377781477865, 1.591691363735423, 0.0, 0.7314261723085571, 0.0, 0.0, 0.45960855697902075, 0.0, 0.43090812739194506, 0.0, 0.3151566241591431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3316537279743774, 0.27121053456867833, 0.0, 0.0, 0.0, 0.4001441487163798, 0.5176883689695168, 0.709966180421066, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2820721974309868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03908614977726766, 0.3083419381268706, 0.39485756407874356, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11511559935873378, 0.014949071584157462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03837527794969671, 0.5550679626138529, 0.0, 0.5726479000113842, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07921974732295421, 0.0, 0.0, 0.0, 0.0, 0.0035899619569867425, 0.0, 0.0, 0.2145099852146676, 0.0, 0.0, 0.0, 0.0, 0.21950879525087302, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09449503481586544, 0.21750142180635595, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32517302304866597, 0.0, 0.0, 0.21035924507266648, 0.0, 0.0, 0.0, 0.0, 0.23713929311152726, 0.21014969427312982, 0.0, 0.0, 0.0, 0.5438936968761621, 0.3401858622480347, 0.0, 0.0, 0.0016527476817863266, 0.0, 0.007935034693483254, 0.0, 0.0, 0.1293317927443745, 0.0, 0.0, 0.0, 0.14368792432790373, 0.0, 0.0, 0.0, 0.0, 0.35799313982296815, 0.29274964408302323, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029011045367568582, 0.0017598612427760815, 0.0, 0.0, 0.052226530139838453, 0.0, 0.0, 0.0, 0.0, 0.16473710872422428, 0.30447392294316244, 0.0, 0.0, 0.0, 0.01743076710707649, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0410977515599671, 0.0, 0.7547303898094697, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.30609022817115544, 0.0, 0.0, 0.0, 0.0, 0.2105854322132866, 0.5327502455990605, 0.13588517657689586, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24025222517683778, 0.0, 0.007535433506713796, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6979826303937671, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07410089446237557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4744178097104391, 0.0, 0.0, 0.0, 0.0, 0.1876471424480227, 0.28765608703745393, 0.2182401932056186, 0.04951496463333951, 0.0, 0.12909838655126593, 0.0, 0.3498317042975124, 0.0, 0.37087394843503463, 0.47981991008621333, 1.1039610273729945, 0.4455015727565212, 0.05271863458630144, 0.19600051626620313, 0.0, 0.2316343353202091, 0.0, 0.0, 0.0, 0.4641150152741525, 0.15018757716392944, 0.21321672818601478, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21465382837602517, 0.0, 0.0, 0.0, 0.00020573734093944128, 0.0, 0.0, 0.0, 0.16929708918389524, 0.0160994805386141, 0.0, 0.049657622486024976, 0.0, 0.017886560545272906, 0.1659947360027911, 0.0, 0.0, 0.0, 0.0, 0.11420185948155097, 0.0, 0.07369142147184715, 0.0, 0.33489361550116375, 0.46496602729779546, 0.0036113530199273977, 0.044355883980523376, 0.08156217107736623, 0.0, 0.006501263051750628, 0.24089225482763127, 0.0, 0.0, 0.3861226254326548, 0.020506805168434235, 0.0, 0.24677384860885976, 0.0, 0.0, 0.002169816793370778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.041148887129618034, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9769895490432172, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5923828848790257, 0.0, 0.10196835358593148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.328835975725113, 0.0, 0.0, 0.4036325198584592, 0.0, 0.0, 0.3602983928676083, 0.0, 0.0, 0.9557725494193282, 0.0, 0.43908662544955285, 0.0, 0.0, 0.46995823158461947, 0.0, 0.14454935060909507, 0.0, 0.010668331756011213, 0.6280409324984255, 0.03247332779409237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10592998533424139, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33399686192793854, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7833052225131492, 0.0, 0.0, 0.0, 0.24187208135853017, 0.0, 0.071730719549714, 0.0, 0.11407352111288803, 0.0, 0.44957678307373006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5736773783460691, 0.0692607726207939, 0.0, 0.0, 0.4731102083446558, 0.7389531951966185, 0.028778055517643567, 0.22717903301955947, 0.0, 0.1436855444728547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.49542697237169275, 0.0, 0.3454532351491738, 0.11660950163380673, 0.0, 0.0, 0.0, 0.0, 0.027794605242432064, 0.0, 0.0, 0.0, 0.132905538815286, 0.0, 0.05497259939520882, 0.0, 0.0, 1.2625457513449383, 0.39900789022393385, 0.0, 0.0, 0.0, 0.0, 0.4114559732861708, 0.4741201455820112, 0.0, 0.21077985466978477, 0.0, 0.0, 0.7439578224938339, 0.42232266256074136, 0.4876343844944301, 0.6687496612887465, 0.115118039848854, 0.0, 0.0, 0.0, 0.0, 0.05260063632046659, 0.07759581613885312, 0.028401563276091994, 0.0, 0.056334062495133036, 0.17033944709766455, 0.9153107777932757, 0.0, 0.004924424218813803, 0.057509247607595904, 0.0, 0.7491964746840728, 0.05934710743113981, 0.0, 0.0, 0.0, 0.22238712275143577, 0.4319949206677921, 0.025917686578643146, 0.031803778192995576, 0.0, 0.6741550137516903, 0.009328571558732282, 0.0, 0.0, 0.0, 0.0, 0.5515037103010108, 0.6987172178395075, 0.0, 0.0, 0.0, 0.060174729085221354, 0.0, 0.0, 0.08734749351416564, 0.6636235071641303, 0.09580942583448827, 0.0, 0.0, 0.08996354325075087, 0.04525348827838765, 0.5824434635228624, 0.05819874866882126, 0.12005968761069685, 0.0, 0.0, 0.3313826265510977, 0.0, 0.058140773587614956, 0.6127791119711469, 0.0, 0.0, 0.15047559500471339, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6662085079556846, 0.3400843601648346, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22423528728211575, 0.0, 0.0, 0.0, 0.0, 0.3636963343220375, 0.5756271502974593, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046502182852265775, 0.0, 0.04891501992848654, 0.0, 0.0, 0.0, 0.9407814186149676, 0.09733317875961926, 0.0, 0.11672934297033466, 0.0, 0.6385588104227404, 0.0, 0.022206283976929683, 0.09845615426665595, 0.03496357540242119, 0.0, 0.0, 0.0, 0.0, 0.03380456195194267, 0.18543528221865002, 0.12144811071150914, 0.0, 0.03755694741887004, 0.16907550573428723, 0.0, 0.0, 0.1443808645220955, 0.0, 0.1003230425739974, 0.12879502114342312, 0.0647357901808317, 0.0, 0.5084955858844, 0.08519692951707138, 0.007582866204885641, 0.0, 0.03198292312330168, 0.0, 0.1209623049539008, 0.0, 0.12387820451445683, 0.0, 0.0, 0.043058753609476905, 0.03493082633215086, 0.0, 0.0, 0.043777031505228374, 0.016261326799286477, 0.16693678030096587, 0.0, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = ((ex.bodies[1]).bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    result = nothing
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = (Finch).moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = (Finch).moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            phase_start_2 = max(1, 1 + fld(A_lvl.shape[1] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                for i_7 = phase_start_2:phase_stop_2
                    B_lvl_q = B_lvl_ptr[1]
                    B_lvl_q_stop = B_lvl_ptr[1 + 1]
                    if B_lvl_q < B_lvl_q_stop
                        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                    else
                        B_lvl_i_stop = 0
                    end
                    phase_stop_3 = min(B_lvl.shape[2], B_lvl_i_stop)
                    if phase_stop_3 >= 1
                        if B_lvl_tbl2[B_lvl_q] < 1
                            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        while true
                            B_lvl_i = B_lvl_tbl2[B_lvl_q]
                            B_lvl_q_step = B_lvl_q
                            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                            end
                            if B_lvl_i < phase_stop_3
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_5 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_5 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_5
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_6 = min(B_lvl_i_2, phase_stop_5, A_lvl_i)
                                        if A_lvl_i == phase_stop_6 && B_lvl_i_2 == phase_stop_6
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_7 = min(i_7, A_lvl_i_stop_2)
                                            if phase_stop_7 >= i_7
                                                if A_lvl_tbl1[A_lvl_q] < i_7
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_7
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_9 = min(A_lvl_i_2, phase_stop_7)
                                                        if A_lvl_i_2 == phase_stop_9
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_6
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_6
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_6 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            else
                                phase_stop_14 = min(B_lvl_i, phase_stop_3)
                                if B_lvl_i == phase_stop_14
                                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_14
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_15 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_15 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_15
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_16 = min(B_lvl_i_2, A_lvl_i, phase_stop_15)
                                            if A_lvl_i == phase_stop_16 && B_lvl_i_2 == phase_stop_16
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_17 = min(i_7, A_lvl_i_stop_4)
                                                if phase_stop_17 >= i_7
                                                    if A_lvl_tbl1[A_lvl_q] < i_7
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_17
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_19 = min(A_lvl_i_4, phase_stop_17)
                                                            if A_lvl_i_4 == phase_stop_19
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_16
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_16
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_16 + 1
                                        end
                                    end
                                    B_lvl_q = B_lvl_q_step
                                end
                                break
                            end
                        end
                    end
                end
            end
        end
    resize!(val, A_lvl.shape[1] * B_lvl.shape[2])
    result = (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
    result
end
julia> @finch begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.18463927542167063, 0.16872726618738124, 0.0, 0.0, 0.0, 0.5894648191942263, 0.0, 0.0, 0.4399979683038884, 0.4872396182365841, 0.0, 0.0, 0.0, 0.5232587908717465, 0.0, 0.028458776980687873, 0.0, 0.1912882078151879, 0.5164204427507948, 0.25375520169397275, 0.7988048308846816, 0.548219404354131, 0.0, 0.0, 0.0, 0.08973042123875676, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16269102551813888, 0.0, 0.6368094288799442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03708423534085979, 0.16845626013473977, 0.6588748971102081, 0.010877416543996312, 0.0, 0.0, 0.0, 0.44123232519423083, 0.0, 0.24002669576156066, 0.0, 0.0, 0.0, 0.21689234576879324, 0.2098895841423717, 0.6161647640351655, 0.4776903273077015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6355594744363152, 0.0, 0.0, 0.08457949502263125, 0.0, 0.0, 0.6901459039432658, 0.0, 0.0, 0.0, 0.057641486402300816, 0.0, 0.0, 0.09549316762404748, 0.09211299074367744, 0.11795836585190977, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0467877508869465, 0.0, 0.0, 0.0, 0.0, 0.04370712736779799, 0.281244207688617, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2030486542718697, 0.0, 0.5182178756464231, 0.0, 0.0, 0.0, 0.057729640414041494, 0.0, 0.0, 0.0, 0.0, 0.19354540299708142, 0.06300918519480127, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1478355026964281, 0.0, 0.0, 0.0, 0.541165159810693, 0.0, 0.0, 0.0, 0.16407931870757664, 0.2057712545151301, 0.32812733654993936, 0.0, 0.24913050204963633, 0.20974603114415158, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5086673274457614, 0.32467254520642835, 0.0, 0.22357645210013752, 0.0, 0.0, 0.0, 0.15908397194054766, 0.0, 0.5866968618616202, 0.21349654033737236, 0.243214867615369, 0.0, 0.0, 0.27374855832219774, 0.0, 0.3075930851551728, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44087523604465795, 0.22505682646241645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2406824670174747, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4353583318013107, 0.0, 0.0, 0.0, 0.0, 0.42257756079609887, 0.0, 0.16328692344233348, 0.7239663574899861, 0.253917129058883, 0.0, 0.0, 0.0, 0.012941670285380298, 0.005948300091521438, 0.6648934966409066, 1.0229318994166612, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13578338949001834, 0.4197116456419966, 0.0, 0.947053670891027, 0.0, 0.13486088669396906, 0.0, 0.6620124726194103, 0.0, 0.6496849904621673, 0.0, 0.0, 0.7890806801928671, 0.01841465881846702, 0.27914669822772004, 0.0, 0.029516583729604803, 0.0, 0.0, 0.018864268719240455, 0.24394303159635272, 0.0, 0.5741603792214627, 0.3080365664801448, 0.6155081898538994, 0.2367816204152454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17416434327376132, 0.0, 0.0, 0.0, 0.0, 0.16269692366253372, 0.0, 0.3491221627973697, 0.0, 0.0, 0.0, 0.0, 0.0, 0.896220207410521, 0.13858473312625733, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2148948115594729, 0.1029957040349639, 0.0, 0.10833979410008034, 0.0, 0.0, 0.23454757178184932, 0.0, 0.2155791117194733, 0.0, 0.22311080318652476, 0.0, 0.0, 0.0, 0.0, 0.07959294795619429, 0.0, 0.17163567622890405, 0.07283994445558169, 0.0, 0.0, 0.0, 0.08654379427684032, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16368166253985994, 0.0, 0.17071385720442764, 0.34758616988919516, 0.0, 0.0, 0.0, 0.0, 0.25336804558610515, 0.0, 0.032257457723339725, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2136279714073461, 0.0, 0.24003957209260046, 0.21303254632455026, 0.006511170502708404, 0.383310313899178, 0.0, 0.0, 0.0, 0.1337677376744651, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0646519260588245, 0.0, 0.0, 0.0, 0.31350938881953055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20384728982176575, 0.0, 0.0, 0.0, 0.0, 0.05670575896106868, 0.0, 0.05964802423259749, 0.0, 0.0, 0.0, 0.4488563568162377, 0.11869016538841153, 0.0, 0.07667734536959822, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.040450967971474544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09480431131127824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0016369185703860613, 0.0, 0.0017218526009009176, 0.0, 0.0, 0.0, 0.0, 0.003426215245260554, 0.0, 0.002213436040060287, 0.0, 0.0, 0.0, 0.0, 0.028624781651490867, 0.0, 0.0, 0.0917221717112923, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03948493179664919, 0.0998678691104795, 0.0, 0.0, 0.0, 0.0, 0.0519314302968116, 0.019753641949088564, 0.0, 0.0, 0.08679325173985959, 0.0, 0.0, 0.44412542864463755, 0.0, 0.0, 0.0, 0.25440324223859473, 0.0, 0.06322962025433154, 0.0, 0.10055420420038351, 0.08043066558940987, 0.01930551218305079, 0.019941838639940274, 0.025537187307223392, 0.0, 0.2186623932992767, 0.06407227122111338, 0.0254835950299198, 0.0, 0.0, 0.6628636898477958, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5422666551724673, 0.6870145051652384, 0.0, 0.0, 0.1501651963322761, 0.019574736086580828, 0.0, 0.0, 0.03589870327691644, 0.0, 0.026565876114361182, 0.0, 0.0, 0.029264986204643254, 0.0, 0.0, 0.01893195304872942, 0.046727577005066426, 0.0, 0.0, 0.28025040572174376, 0.0, 0.018913093854322344, 0.0, 0.0, 0.0, 0.04894945277637971, 0.006583695531171352, 0.3875798367774556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3292155284261099, 0.04344067936293723, 0.0653720550702903, 0.0, 0.0, 0.22649528787365272, 0.018049730534704385, 0.14615138313733889, 0.0, 0.0, 0.0, 0.0, 0.20611785399916963, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31511052111518423, 0.0, 0.03672395965581389, 0.0, 0.0, 0.9552411806377301, 0.0, 0.0, 0.0, 0.3512720117094026, 0.0, 0.7814509194588117, 0.9900444949397926, 0.0, 0.2416698136762867, 0.0, 0.15594310090475377, 0.37345396038999507, 0.13378200524387487, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0037871796529251873, 0.0, 0.07132197093797825, 0.0, 0.0, 0.4038639203443338, 0.0, 0.007926901739731977, 0.0, 0.005121012178382578, 0.4483265473520345, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19545324009971515, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015822792815906598, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2654026801406409, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005338269605792754, 0.3841199941370228, 0.003089715236949895, 0.0, 0.0, 0.0, 0.2702896727602311, 0.0061480462629685435, 0.0, 0.0077716018517988575, 0.0, 0.0, 0.5060334906800565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5325221835704804, 0.4354711371945976, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4529112439588725, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.42175363431898494, 0.540090698333841, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1429447733494866, 0.0, 0.0, 0.17383910328410926, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7778438721037507, 0.7592283162924894, 0.27027978053916946, 0.9680808201913975, 0.0, 0.0, 0.0, 0.0, 0.23721687040615713, 0.0, 0.0, 0.0, 0.214724244927467, 0.0, 0.6713747276751851, 0.2691029964691732, 0.24957193666978766, 0.04174679102400021, 0.0, 0.0, 0.44623226966778873, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9739425217687773, 0.3878431688435761, 0.0, 0.0, 0.0, 0.0, 0.5905353658253105, 0.0, 0.41099451149099264, 0.46959062394023404, 0.38400872176268547, 0.0, 0.4974135902693747, 0.0, 0.0, 0.0, 0.9154557174508635, 0.2586557560688982, 0.09838729964211103, 0.4023736739495505, 0.0, 0.43229262167712557, 0.25909478937718255, 0.0, 0.0, 0.9527916935554207, 0.3993878193283669, 0.4377172055566821, 0.0, 0.0, 0.02448196261221554, 0.28590941544125414, 0.0, 0.4006023808751287, 0.03918308759508126, 0.0, 0.2882716435062163, 0.0, 0.0110856134215592, 0.37363494354124355, 0.049778285574341975, 0.0, 0.0, 0.9269077110105975, 0.31315811797299237, 0.0, 0.012303674452273154, 0.015429991708474198, 0.0, 0.0, 0.014446013487908171, 0.04123415495113678, 0.03371931705724562, 0.29538913191692134, 0.0, 0.3349006097039049, 0.23542836206424417, 0.23052114963877735, 0.31614042332280545, 0.0, 0.0, 0.11683083227872514, 0.0, 0.0, 0.0006114799040197484, 0.10975727273531571, 0.0, 0.004295508139252644, 0.05107901709661121, 0.0, 0.9334562674798996, 0.0, 0.0, 0.0, 0.0, 0.5880007983611341, 0.06450382436313898, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11800940272957328, 0.0, 0.1899751304440791, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.273744879534597, 0.0, 0.0, 0.2671580962806136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3492510086210155, 0.0, 0.3080038714015672, 0.0, 0.0, 0.2221605109577772, 0.0, 0.0, 0.0, 0.0, 0.07999688285015112, 0.06742807554519747, 0.08634727349135116, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10258077598728532, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5012203954355052, 0.0, 0.0, 0.0, 0.08111247475296846, 0.0, 0.0, 0.12138201096876389, 0.0, 0.08982551378065248, 0.0, 0.0, 0.12126628140327211, 0.0, 0.0, 0.07844895363581271, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07837080617454226, 0.0, 0.0, 0.0, 0.20283344995989894, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1282379216751728, 0.033796057874619194, 0.0, 0.09653017040380268, 0.0, 0.08822574416521244, 0.01404236183038609, 0.056929725378038414, 0.0, 0.15526439596527059, 0.0, 0.0, 0.0, 0.03781410167223484, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2529010219858623, 0.3238608478009693, 0.0, 0.0, 0.1407409609998849, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.329852425046806, 0.0, 0.0, 0.33341239592531074, 0.45526487856119674, 0.0, 0.33690660829115365, 0.08120142522000469, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08802821410799472, 0.39029129939250945, 0.13688708765693952, 0.0, 0.08496391292272984, 0.0, 0.0, 0.5183827483305287, 0.703065773125918, 0.4814340077714492, 0.0, 0.0, 0.0, 0.12053001147739661, 0.0, 0.07320101959394086, 0.0, 0.0, 0.5105579893076974, 0.6674329003953465, 0.7892434941755087, 0.051355650714931346, 0.4403973256963498, 0.0, 0.01884360978410253, 0.0, 0.0, 0.42539452394686605, 0.1591995319925085, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17375879569486846, 0.0, 0.0, 0.0, 0.0, 0.1721016330265864, 0.0, 0.00044047585032663307, 0.1672932604423309, 0.18102660031449758, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004373654797423859, 0.0, 0.0, 0.08031885873943684, 0.0, 0.0, 0.12088873452345646, 0.0, 0.25447674137867604, 0.0, 0.20210883042048103, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05466434515545004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005119594240948603, 0.0, 0.2461616206228127, 0.0, 0.0, 0.40062152777610743, 0.0, 0.0, 0.0, 0.4450914546665199, 0.0, 0.0, 0.0, 0.25078704499066257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6437873083636174, 0.0, 0.08986536930257488, 0.05459463157090123, 0.0, 0.0, 0.1617782592438168, 0.0, 0.0, 0.0, 0.0, 0.510293956173341, 0.0, 0.0, 0.0, 0.0, 0.0539939979224501, 0.06022554282071448, 0.0, 0.0, 0.0017903697912685446, 0.10539843894447092, 0.0, 0.007216901945678346, 0.0, 0.0, 0.0, 0.22684974207663636, 1.4564553082728662, 0.182720528082649, 0.06653876275957457, 0.0, 0.6274289004185675, 0.0, 0.5406773490882926, 0.27822503396119075, 0.09306814986560254, 0.29383387946651635, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6230315233142055, 0.9648450369390001, 0.5108864746688708, 0.0, 0.36840186712596623, 0.22805280632725022, 0.3227837297104921, 0.0, 0.0, 1.0240377781477865, 1.591691363735423, 0.0, 0.7314261723085571, 0.0, 0.0, 0.45960855697902075, 0.0, 0.43090812739194506, 0.0, 0.3151566241591431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3316537279743774, 0.27121053456867833, 0.0, 0.0, 0.0, 0.4001441487163798, 0.5176883689695168, 0.709966180421066, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2820721974309868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03908614977726766, 0.3083419381268706, 0.39485756407874356, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11511559935873378, 0.014949071584157462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03837527794969671, 0.5550679626138529, 0.0, 0.5726479000113842, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07921974732295421, 0.0, 0.0, 0.0, 0.0, 0.0035899619569867425, 0.0, 0.0, 0.2145099852146676, 0.0, 0.0, 0.0, 0.0, 0.21950879525087302, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09449503481586544, 0.21750142180635595, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32517302304866597, 0.0, 0.0, 0.21035924507266648, 0.0, 0.0, 0.0, 0.0, 0.23713929311152726, 0.21014969427312982, 0.0, 0.0, 0.0, 0.5438936968761621, 0.3401858622480347, 0.0, 0.0, 0.0016527476817863266, 0.0, 0.007935034693483254, 0.0, 0.0, 0.1293317927443745, 0.0, 0.0, 0.0, 0.14368792432790373, 0.0, 0.0, 0.0, 0.0, 0.35799313982296815, 0.29274964408302323, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029011045367568582, 0.0017598612427760815, 0.0, 0.0, 0.052226530139838453, 0.0, 0.0, 0.0, 0.0, 0.16473710872422428, 0.30447392294316244, 0.0, 0.0, 0.0, 0.01743076710707649, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0410977515599671, 0.0, 0.7547303898094697, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.30609022817115544, 0.0, 0.0, 0.0, 0.0, 0.2105854322132866, 0.5327502455990605, 0.13588517657689586, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24025222517683778, 0.0, 0.007535433506713796, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6979826303937671, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07410089446237557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4744178097104391, 0.0, 0.0, 0.0, 0.0, 0.1876471424480227, 0.28765608703745393, 0.2182401932056186, 0.04951496463333951, 0.0, 0.12909838655126593, 0.0, 0.3498317042975124, 0.0, 0.37087394843503463, 0.47981991008621333, 1.1039610273729945, 0.4455015727565212, 0.05271863458630144, 0.19600051626620313, 0.0, 0.2316343353202091, 0.0, 0.0, 0.0, 0.4641150152741525, 0.15018757716392944, 0.21321672818601478, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21465382837602517, 0.0, 0.0, 0.0, 0.00020573734093944128, 0.0, 0.0, 0.0, 0.16929708918389524, 0.0160994805386141, 0.0, 0.049657622486024976, 0.0, 0.017886560545272906, 0.1659947360027911, 0.0, 0.0, 0.0, 0.0, 0.11420185948155097, 0.0, 0.07369142147184715, 0.0, 0.33489361550116375, 0.46496602729779546, 0.0036113530199273977, 0.044355883980523376, 0.08156217107736623, 0.0, 0.006501263051750628, 0.24089225482763127, 0.0, 0.0, 0.3861226254326548, 0.020506805168434235, 0.0, 0.24677384860885976, 0.0, 0.0, 0.002169816793370778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.041148887129618034, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9769895490432172, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5923828848790257, 0.0, 0.10196835358593148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.328835975725113, 0.0, 0.0, 0.4036325198584592, 0.0, 0.0, 0.3602983928676083, 0.0, 0.0, 0.9557725494193282, 0.0, 0.43908662544955285, 0.0, 0.0, 0.46995823158461947, 0.0, 0.14454935060909507, 0.0, 0.010668331756011213, 0.6280409324984255, 0.03247332779409237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10592998533424139, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33399686192793854, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7833052225131492, 0.0, 0.0, 0.0, 0.24187208135853017, 0.0, 0.071730719549714, 0.0, 0.11407352111288803, 0.0, 0.44957678307373006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5736773783460691, 0.0692607726207939, 0.0, 0.0, 0.4731102083446558, 0.7389531951966185, 0.028778055517643567, 0.22717903301955947, 0.0, 0.1436855444728547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.49542697237169275, 0.0, 0.3454532351491738, 0.11660950163380673, 0.0, 0.0, 0.0, 0.0, 0.027794605242432064, 0.0, 0.0, 0.0, 0.132905538815286, 0.0, 0.05497259939520882, 0.0, 0.0, 1.2625457513449383, 0.39900789022393385, 0.0, 0.0, 0.0, 0.0, 0.4114559732861708, 0.4741201455820112, 0.0, 0.21077985466978477, 0.0, 0.0, 0.7439578224938339, 0.42232266256074136, 0.4876343844944301, 0.6687496612887465, 0.115118039848854, 0.0, 0.0, 0.0, 0.0, 0.05260063632046659, 0.07759581613885312, 0.028401563276091994, 0.0, 0.056334062495133036, 0.17033944709766455, 0.9153107777932757, 0.0, 0.004924424218813803, 0.057509247607595904, 0.0, 0.7491964746840728, 0.05934710743113981, 0.0, 0.0, 0.0, 0.22238712275143577, 0.4319949206677921, 0.025917686578643146, 0.031803778192995576, 0.0, 0.6741550137516903, 0.009328571558732282, 0.0, 0.0, 0.0, 0.0, 0.5515037103010108, 0.6987172178395075, 0.0, 0.0, 0.0, 0.060174729085221354, 0.0, 0.0, 0.08734749351416564, 0.6636235071641303, 0.09580942583448827, 0.0, 0.0, 0.08996354325075087, 0.04525348827838765, 0.5824434635228624, 0.05819874866882126, 0.12005968761069685, 0.0, 0.0, 0.3313826265510977, 0.0, 0.058140773587614956, 0.6127791119711469, 0.0, 0.0, 0.15047559500471339, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6662085079556846, 0.3400843601648346, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22423528728211575, 0.0, 0.0, 0.0, 0.0, 0.3636963343220375, 0.5756271502974593, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046502182852265775, 0.0, 0.04891501992848654, 0.0, 0.0, 0.0, 0.9407814186149676, 0.09733317875961926, 0.0, 0.11672934297033466, 0.0, 0.6385588104227404, 0.0, 0.022206283976929683, 0.09845615426665595, 0.03496357540242119, 0.0, 0.0, 0.0, 0.0, 0.03380456195194267, 0.18543528221865002, 0.12144811071150914, 0.0, 0.03755694741887004, 0.16907550573428723, 0.0, 0.0, 0.1443808645220955, 0.0, 0.1003230425739974, 0.12879502114342312, 0.0647357901808317, 0.0, 0.5084955858844, 0.08519692951707138, 0.007582866204885641, 0.0, 0.03198292312330168, 0.0, 0.1209623049539008, 0.0, 0.12387820451445683, 0.0, 0.0, 0.043058753609476905, 0.03493082633215086, 0.0, 0.0, 0.043777031505228374, 0.016261326799286477, 0.16693678030096587, 0.0, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = ((ex.bodies[1]).bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    result = nothing
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    for i_4 = 1:A_lvl.shape[1]
        val = Ct_lvl_2_val
        Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
        B_lvl_ptr_2 = B_lvl_ptr
        B_lvl_ptr = (Finch).moveto(B_lvl_ptr, CPU(Threads.nthreads()))
        B_lvl_tbl1_2 = B_lvl_tbl1
        B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
        B_lvl_tbl2_2 = B_lvl_tbl2
        B_lvl_tbl2 = (Finch).moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
        val_2 = B_lvl_val
        B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
        A_lvl_ptr_2 = A_lvl_ptr
        A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
        A_lvl_tbl1_2 = A_lvl_tbl1
        A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
        A_lvl_tbl2_2 = A_lvl_tbl2
        A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
        val_3 = A_lvl_val
        A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
        Threads.@threads for i_5 = 1:Threads.nthreads()
                B_lvl_q = B_lvl_ptr[1]
                B_lvl_q_stop = B_lvl_ptr[1 + 1]
                if B_lvl_q < B_lvl_q_stop
                    B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                else
                    B_lvl_i_stop = 0
                end
                phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_5 + -1), Threads.nthreads()))
                phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_5, Threads.nthreads()))
                if phase_stop_2 >= phase_start_2
                    if B_lvl_tbl2[B_lvl_q] < phase_start_2
                        B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    while true
                        B_lvl_i = B_lvl_tbl2[B_lvl_q]
                        B_lvl_q_step = B_lvl_q
                        if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                            B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        if B_lvl_i < phase_stop_2
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_4, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_4
                                            if A_lvl_tbl1[A_lvl_q] < i_4
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        else
                            phase_stop_13 = min(B_lvl_i, phase_stop_2)
                            if B_lvl_i == phase_stop_13
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_4, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_4
                                                if A_lvl_tbl1[A_lvl_q] < i_4
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            end
                            break
                        end
                    end
                end
            end
        Ct_lvl_2_val = val
        B_lvl_ptr = B_lvl_ptr_2
        B_lvl_tbl1 = B_lvl_tbl1_2
        B_lvl_tbl2 = B_lvl_tbl2_2
        B_lvl_val = val_2
        A_lvl_ptr = A_lvl_ptr_2
        A_lvl_tbl1 = A_lvl_tbl1_2
        A_lvl_tbl2 = A_lvl_tbl2_2
        A_lvl_val = val_3
    end
    resize!(Ct_lvl_2_val, A_lvl.shape[1] * B_lvl.shape[2])
    result = (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
    result
end
julia> @finch begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.18463927542167063, 0.16872726618738124, 0.0, 0.0, 0.0, 0.5894648191942263, 0.0, 0.0, 0.4399979683038884, 0.4872396182365841, 0.0, 0.0, 0.0, 0.5232587908717465, 0.0, 0.028458776980687873, 0.0, 0.1912882078151879, 0.5164204427507948, 0.25375520169397275, 0.7988048308846816, 0.548219404354131, 0.0, 0.0, 0.0, 0.08973042123875676, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16269102551813888, 0.0, 0.6368094288799442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03708423534085979, 0.16845626013473977, 0.6588748971102081, 0.010877416543996312, 0.0, 0.0, 0.0, 0.44123232519423083, 0.0, 0.24002669576156066, 0.0, 0.0, 0.0, 0.21689234576879324, 0.2098895841423717, 0.6161647640351655, 0.4776903273077015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6355594744363152, 0.0, 0.0, 0.08457949502263125, 0.0, 0.0, 0.6901459039432658, 0.0, 0.0, 0.0, 0.057641486402300816, 0.0, 0.0, 0.09549316762404748, 0.09211299074367744, 0.11795836585190977, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0467877508869465, 0.0, 0.0, 0.0, 0.0, 0.04370712736779799, 0.281244207688617, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2030486542718697, 0.0, 0.5182178756464231, 0.0, 0.0, 0.0, 0.057729640414041494, 0.0, 0.0, 0.0, 0.0, 0.19354540299708142, 0.06300918519480127, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1478355026964281, 0.0, 0.0, 0.0, 0.541165159810693, 0.0, 0.0, 0.0, 0.16407931870757664, 0.2057712545151301, 0.32812733654993936, 0.0, 0.24913050204963633, 0.20974603114415158, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5086673274457614, 0.32467254520642835, 0.0, 0.22357645210013752, 0.0, 0.0, 0.0, 0.15908397194054766, 0.0, 0.5866968618616202, 0.21349654033737236, 0.243214867615369, 0.0, 0.0, 0.27374855832219774, 0.0, 0.3075930851551728, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44087523604465795, 0.22505682646241645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2406824670174747, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4353583318013107, 0.0, 0.0, 0.0, 0.0, 0.42257756079609887, 0.0, 0.16328692344233348, 0.7239663574899861, 0.253917129058883, 0.0, 0.0, 0.0, 0.012941670285380298, 0.005948300091521438, 0.6648934966409066, 1.0229318994166612, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13578338949001834, 0.4197116456419966, 0.0, 0.947053670891027, 0.0, 0.13486088669396906, 0.0, 0.6620124726194103, 0.0, 0.6496849904621673, 0.0, 0.0, 0.7890806801928671, 0.01841465881846702, 0.27914669822772004, 0.0, 0.029516583729604803, 0.0, 0.0, 0.018864268719240455, 0.24394303159635272, 0.0, 0.5741603792214627, 0.3080365664801448, 0.6155081898538994, 0.2367816204152454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17416434327376132, 0.0, 0.0, 0.0, 0.0, 0.16269692366253372, 0.0, 0.3491221627973697, 0.0, 0.0, 0.0, 0.0, 0.0, 0.896220207410521, 0.13858473312625733, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2148948115594729, 0.1029957040349639, 0.0, 0.10833979410008034, 0.0, 0.0, 0.23454757178184932, 0.0, 0.2155791117194733, 0.0, 0.22311080318652476, 0.0, 0.0, 0.0, 0.0, 0.07959294795619429, 0.0, 0.17163567622890405, 0.07283994445558169, 0.0, 0.0, 0.0, 0.08654379427684032, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16368166253985994, 0.0, 0.17071385720442764, 0.34758616988919516, 0.0, 0.0, 0.0, 0.0, 0.25336804558610515, 0.0, 0.032257457723339725, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2136279714073461, 0.0, 0.24003957209260046, 0.21303254632455026, 0.006511170502708404, 0.383310313899178, 0.0, 0.0, 0.0, 0.1337677376744651, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0646519260588245, 0.0, 0.0, 0.0, 0.31350938881953055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20384728982176575, 0.0, 0.0, 0.0, 0.0, 0.05670575896106868, 0.0, 0.05964802423259749, 0.0, 0.0, 0.0, 0.4488563568162377, 0.11869016538841153, 0.0, 0.07667734536959822, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.040450967971474544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09480431131127824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0016369185703860613, 0.0, 0.0017218526009009176, 0.0, 0.0, 0.0, 0.0, 0.003426215245260554, 0.0, 0.002213436040060287, 0.0, 0.0, 0.0, 0.0, 0.028624781651490867, 0.0, 0.0, 0.0917221717112923, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03948493179664919, 0.0998678691104795, 0.0, 0.0, 0.0, 0.0, 0.0519314302968116, 0.019753641949088564, 0.0, 0.0, 0.08679325173985959, 0.0, 0.0, 0.44412542864463755, 0.0, 0.0, 0.0, 0.25440324223859473, 0.0, 0.06322962025433154, 0.0, 0.10055420420038351, 0.08043066558940987, 0.01930551218305079, 0.019941838639940274, 0.025537187307223392, 0.0, 0.2186623932992767, 0.06407227122111338, 0.0254835950299198, 0.0, 0.0, 0.6628636898477958, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5422666551724673, 0.6870145051652384, 0.0, 0.0, 0.1501651963322761, 0.019574736086580828, 0.0, 0.0, 0.03589870327691644, 0.0, 0.026565876114361182, 0.0, 0.0, 0.029264986204643254, 0.0, 0.0, 0.01893195304872942, 0.046727577005066426, 0.0, 0.0, 0.28025040572174376, 0.0, 0.018913093854322344, 0.0, 0.0, 0.0, 0.04894945277637971, 0.006583695531171352, 0.3875798367774556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3292155284261099, 0.04344067936293723, 0.0653720550702903, 0.0, 0.0, 0.22649528787365272, 0.018049730534704385, 0.14615138313733889, 0.0, 0.0, 0.0, 0.0, 0.20611785399916963, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31511052111518423, 0.0, 0.03672395965581389, 0.0, 0.0, 0.9552411806377301, 0.0, 0.0, 0.0, 0.3512720117094026, 0.0, 0.7814509194588117, 0.9900444949397926, 0.0, 0.2416698136762867, 0.0, 0.15594310090475377, 0.37345396038999507, 0.13378200524387487, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0037871796529251873, 0.0, 0.07132197093797825, 0.0, 0.0, 0.4038639203443338, 0.0, 0.007926901739731977, 0.0, 0.005121012178382578, 0.4483265473520345, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19545324009971515, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015822792815906598, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2654026801406409, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005338269605792754, 0.3841199941370228, 0.003089715236949895, 0.0, 0.0, 0.0, 0.2702896727602311, 0.0061480462629685435, 0.0, 0.0077716018517988575, 0.0, 0.0, 0.5060334906800565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5325221835704804, 0.4354711371945976, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4529112439588725, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.42175363431898494, 0.540090698333841, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1429447733494866, 0.0, 0.0, 0.17383910328410926, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7778438721037507, 0.7592283162924894, 0.27027978053916946, 0.9680808201913975, 0.0, 0.0, 0.0, 0.0, 0.23721687040615713, 0.0, 0.0, 0.0, 0.214724244927467, 0.0, 0.6713747276751851, 0.2691029964691732, 0.24957193666978766, 0.04174679102400021, 0.0, 0.0, 0.44623226966778873, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9739425217687773, 0.3878431688435761, 0.0, 0.0, 0.0, 0.0, 0.5905353658253105, 0.0, 0.41099451149099264, 0.46959062394023404, 0.38400872176268547, 0.0, 0.4974135902693747, 0.0, 0.0, 0.0, 0.9154557174508635, 0.2586557560688982, 0.09838729964211103, 0.4023736739495505, 0.0, 0.43229262167712557, 0.25909478937718255, 0.0, 0.0, 0.9527916935554207, 0.3993878193283669, 0.4377172055566821, 0.0, 0.0, 0.02448196261221554, 0.28590941544125414, 0.0, 0.4006023808751287, 0.03918308759508126, 0.0, 0.2882716435062163, 0.0, 0.0110856134215592, 0.37363494354124355, 0.049778285574341975, 0.0, 0.0, 0.9269077110105975, 0.31315811797299237, 0.0, 0.012303674452273154, 0.015429991708474198, 0.0, 0.0, 0.014446013487908171, 0.04123415495113678, 0.03371931705724562, 0.29538913191692134, 0.0, 0.3349006097039049, 0.23542836206424417, 0.23052114963877735, 0.31614042332280545, 0.0, 0.0, 0.11683083227872514, 0.0, 0.0, 0.0006114799040197484, 0.10975727273531571, 0.0, 0.004295508139252644, 0.05107901709661121, 0.0, 0.9334562674798996, 0.0, 0.0, 0.0, 0.0, 0.5880007983611341, 0.06450382436313898, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11800940272957328, 0.0, 0.1899751304440791, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.273744879534597, 0.0, 0.0, 0.2671580962806136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3492510086210155, 0.0, 0.3080038714015672, 0.0, 0.0, 0.2221605109577772, 0.0, 0.0, 0.0, 0.0, 0.07999688285015112, 0.06742807554519747, 0.08634727349135116, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10258077598728532, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5012203954355052, 0.0, 0.0, 0.0, 0.08111247475296846, 0.0, 0.0, 0.12138201096876389, 0.0, 0.08982551378065248, 0.0, 0.0, 0.12126628140327211, 0.0, 0.0, 0.07844895363581271, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07837080617454226, 0.0, 0.0, 0.0, 0.20283344995989894, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1282379216751728, 0.033796057874619194, 0.0, 0.09653017040380268, 0.0, 0.08822574416521244, 0.01404236183038609, 0.056929725378038414, 0.0, 0.15526439596527059, 0.0, 0.0, 0.0, 0.03781410167223484, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2529010219858623, 0.3238608478009693, 0.0, 0.0, 0.1407409609998849, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.329852425046806, 0.0, 0.0, 0.33341239592531074, 0.45526487856119674, 0.0, 0.33690660829115365, 0.08120142522000469, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08802821410799472, 0.39029129939250945, 0.13688708765693952, 0.0, 0.08496391292272984, 0.0, 0.0, 0.5183827483305287, 0.703065773125918, 0.4814340077714492, 0.0, 0.0, 0.0, 0.12053001147739661, 0.0, 0.07320101959394086, 0.0, 0.0, 0.5105579893076974, 0.6674329003953465, 0.7892434941755087, 0.051355650714931346, 0.4403973256963498, 0.0, 0.01884360978410253, 0.0, 0.0, 0.42539452394686605, 0.1591995319925085, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17375879569486846, 0.0, 0.0, 0.0, 0.0, 0.1721016330265864, 0.0, 0.00044047585032663307, 0.1672932604423309, 0.18102660031449758, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004373654797423859, 0.0, 0.0, 0.08031885873943684, 0.0, 0.0, 0.12088873452345646, 0.0, 0.25447674137867604, 0.0, 0.20210883042048103, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05466434515545004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005119594240948603, 0.0, 0.2461616206228127, 0.0, 0.0, 0.40062152777610743, 0.0, 0.0, 0.0, 0.4450914546665199, 0.0, 0.0, 0.0, 0.25078704499066257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6437873083636174, 0.0, 0.08986536930257488, 0.05459463157090123, 0.0, 0.0, 0.1617782592438168, 0.0, 0.0, 0.0, 0.0, 0.510293956173341, 0.0, 0.0, 0.0, 0.0, 0.0539939979224501, 0.06022554282071448, 0.0, 0.0, 0.0017903697912685446, 0.10539843894447092, 0.0, 0.007216901945678346, 0.0, 0.0, 0.0, 0.22684974207663636, 1.4564553082728662, 0.182720528082649, 0.06653876275957457, 0.0, 0.6274289004185675, 0.0, 0.5406773490882926, 0.27822503396119075, 0.09306814986560254, 0.29383387946651635, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6230315233142055, 0.9648450369390001, 0.5108864746688708, 0.0, 0.36840186712596623, 0.22805280632725022, 0.3227837297104921, 0.0, 0.0, 1.0240377781477865, 1.591691363735423, 0.0, 0.7314261723085571, 0.0, 0.0, 0.45960855697902075, 0.0, 0.43090812739194506, 0.0, 0.3151566241591431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3316537279743774, 0.27121053456867833, 0.0, 0.0, 0.0, 0.4001441487163798, 0.5176883689695168, 0.709966180421066, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2820721974309868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03908614977726766, 0.3083419381268706, 0.39485756407874356, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11511559935873378, 0.014949071584157462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03837527794969671, 0.5550679626138529, 0.0, 0.5726479000113842, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07921974732295421, 0.0, 0.0, 0.0, 0.0, 0.0035899619569867425, 0.0, 0.0, 0.2145099852146676, 0.0, 0.0, 0.0, 0.0, 0.21950879525087302, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09449503481586544, 0.21750142180635595, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32517302304866597, 0.0, 0.0, 0.21035924507266648, 0.0, 0.0, 0.0, 0.0, 0.23713929311152726, 0.21014969427312982, 0.0, 0.0, 0.0, 0.5438936968761621, 0.3401858622480347, 0.0, 0.0, 0.0016527476817863266, 0.0, 0.007935034693483254, 0.0, 0.0, 0.1293317927443745, 0.0, 0.0, 0.0, 0.14368792432790373, 0.0, 0.0, 0.0, 0.0, 0.35799313982296815, 0.29274964408302323, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029011045367568582, 0.0017598612427760815, 0.0, 0.0, 0.052226530139838453, 0.0, 0.0, 0.0, 0.0, 0.16473710872422428, 0.30447392294316244, 0.0, 0.0, 0.0, 0.01743076710707649, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0410977515599671, 0.0, 0.7547303898094697, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.30609022817115544, 0.0, 0.0, 0.0, 0.0, 0.2105854322132866, 0.5327502455990605, 0.13588517657689586, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24025222517683778, 0.0, 0.007535433506713796, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6979826303937671, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07410089446237557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4744178097104391, 0.0, 0.0, 0.0, 0.0, 0.1876471424480227, 0.28765608703745393, 0.2182401932056186, 0.04951496463333951, 0.0, 0.12909838655126593, 0.0, 0.3498317042975124, 0.0, 0.37087394843503463, 0.47981991008621333, 1.1039610273729945, 0.4455015727565212, 0.05271863458630144, 0.19600051626620313, 0.0, 0.2316343353202091, 0.0, 0.0, 0.0, 0.4641150152741525, 0.15018757716392944, 0.21321672818601478, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21465382837602517, 0.0, 0.0, 0.0, 0.00020573734093944128, 0.0, 0.0, 0.0, 0.16929708918389524, 0.0160994805386141, 0.0, 0.049657622486024976, 0.0, 0.017886560545272906, 0.1659947360027911, 0.0, 0.0, 0.0, 0.0, 0.11420185948155097, 0.0, 0.07369142147184715, 0.0, 0.33489361550116375, 0.46496602729779546, 0.0036113530199273977, 0.044355883980523376, 0.08156217107736623, 0.0, 0.006501263051750628, 0.24089225482763127, 0.0, 0.0, 0.3861226254326548, 0.020506805168434235, 0.0, 0.24677384860885976, 0.0, 0.0, 0.002169816793370778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.041148887129618034, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9769895490432172, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5923828848790257, 0.0, 0.10196835358593148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.328835975725113, 0.0, 0.0, 0.4036325198584592, 0.0, 0.0, 0.3602983928676083, 0.0, 0.0, 0.9557725494193282, 0.0, 0.43908662544955285, 0.0, 0.0, 0.46995823158461947, 0.0, 0.14454935060909507, 0.0, 0.010668331756011213, 0.6280409324984255, 0.03247332779409237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10592998533424139, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33399686192793854, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7833052225131492, 0.0, 0.0, 0.0, 0.24187208135853017, 0.0, 0.071730719549714, 0.0, 0.11407352111288803, 0.0, 0.44957678307373006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5736773783460691, 0.0692607726207939, 0.0, 0.0, 0.4731102083446558, 0.7389531951966185, 0.028778055517643567, 0.22717903301955947, 0.0, 0.1436855444728547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.49542697237169275, 0.0, 0.3454532351491738, 0.11660950163380673, 0.0, 0.0, 0.0, 0.0, 0.027794605242432064, 0.0, 0.0, 0.0, 0.132905538815286, 0.0, 0.05497259939520882, 0.0, 0.0, 1.2625457513449383, 0.39900789022393385, 0.0, 0.0, 0.0, 0.0, 0.4114559732861708, 0.4741201455820112, 0.0, 0.21077985466978477, 0.0, 0.0, 0.7439578224938339, 0.42232266256074136, 0.4876343844944301, 0.6687496612887465, 0.115118039848854, 0.0, 0.0, 0.0, 0.0, 0.05260063632046659, 0.07759581613885312, 0.028401563276091994, 0.0, 0.056334062495133036, 0.17033944709766455, 0.9153107777932757, 0.0, 0.004924424218813803, 0.057509247607595904, 0.0, 0.7491964746840728, 0.05934710743113981, 0.0, 0.0, 0.0, 0.22238712275143577, 0.4319949206677921, 0.025917686578643146, 0.031803778192995576, 0.0, 0.6741550137516903, 0.009328571558732282, 0.0, 0.0, 0.0, 0.0, 0.5515037103010108, 0.6987172178395075, 0.0, 0.0, 0.0, 0.060174729085221354, 0.0, 0.0, 0.08734749351416564, 0.6636235071641303, 0.09580942583448827, 0.0, 0.0, 0.08996354325075087, 0.04525348827838765, 0.5824434635228624, 0.05819874866882126, 0.12005968761069685, 0.0, 0.0, 0.3313826265510977, 0.0, 0.058140773587614956, 0.6127791119711469, 0.0, 0.0, 0.15047559500471339, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6662085079556846, 0.3400843601648346, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22423528728211575, 0.0, 0.0, 0.0, 0.0, 0.3636963343220375, 0.5756271502974593, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046502182852265775, 0.0, 0.04891501992848654, 0.0, 0.0, 0.0, 0.9407814186149676, 0.09733317875961926, 0.0, 0.11672934297033466, 0.0, 0.6385588104227404, 0.0, 0.022206283976929683, 0.09845615426665595, 0.03496357540242119, 0.0, 0.0, 0.0, 0.0, 0.03380456195194267, 0.18543528221865002, 0.12144811071150914, 0.0, 0.03755694741887004, 0.16907550573428723, 0.0, 0.0, 0.1443808645220955, 0.0, 0.1003230425739974, 0.12879502114342312, 0.0647357901808317, 0.0, 0.5084955858844, 0.08519692951707138, 0.007582866204885641, 0.0, 0.03198292312330168, 0.0, 0.1209623049539008, 0.0, 0.12387820451445683, 0.0, 0.0, 0.043058753609476905, 0.03493082633215086, 0.0, 0.0, 0.043777031505228374, 0.016261326799286477, 0.16693678030096587, 0.0, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = ((ex.bodies[1]).bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    result = nothing
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = (Finch).moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = (Finch).moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            B_lvl_q = B_lvl_ptr[1]
            B_lvl_q_stop = B_lvl_ptr[1 + 1]
            if B_lvl_q < B_lvl_q_stop
                B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
            else
                B_lvl_i_stop = 0
            end
            phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                if B_lvl_tbl2[B_lvl_q] < phase_start_2
                    B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                end
                while true
                    B_lvl_i = B_lvl_tbl2[B_lvl_q]
                    B_lvl_q_step = B_lvl_q
                    if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                        B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    if B_lvl_i < phase_stop_2
                        Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                        for i_6 = 1:A_lvl.shape[1]
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_6
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_6, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_6
                                            if A_lvl_tbl1[A_lvl_q] < i_6
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_6, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                        end
                        B_lvl_q = B_lvl_q_step
                    else
                        phase_stop_13 = min(B_lvl_i, phase_stop_2)
                        if B_lvl_i == phase_stop_13
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                            for i_8 = 1:A_lvl.shape[1]
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_8
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_8, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_8
                                                if A_lvl_tbl1[A_lvl_q] < i_8
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_8, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        end
                        break
                    end
                end
            end
        end
    resize!(val, A_lvl.shape[1] * B_lvl.shape[2])
    result = (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
    result
end
julia> @finch begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.18463927542167063, 0.16872726618738124, 0.0, 0.0, 0.0, 0.5894648191942263, 0.0, 0.0, 0.4399979683038884, 0.4872396182365841, 0.0, 0.0, 0.0, 0.5232587908717465, 0.0, 0.028458776980687873, 0.0, 0.1912882078151879, 0.5164204427507948, 0.25375520169397275, 0.7988048308846816, 0.548219404354131, 0.0, 0.0, 0.0, 0.08973042123875676, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16269102551813888, 0.0, 0.6368094288799442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03708423534085979, 0.16845626013473977, 0.6588748971102081, 0.010877416543996312, 0.0, 0.0, 0.0, 0.44123232519423083, 0.0, 0.24002669576156066, 0.0, 0.0, 0.0, 0.21689234576879324, 0.2098895841423717, 0.6161647640351655, 0.4776903273077015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6355594744363152, 0.0, 0.0, 0.08457949502263125, 0.0, 0.0, 0.6901459039432658, 0.0, 0.0, 0.0, 0.057641486402300816, 0.0, 0.0, 0.09549316762404748, 0.09211299074367744, 0.11795836585190977, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0467877508869465, 0.0, 0.0, 0.0, 0.0, 0.04370712736779799, 0.281244207688617, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2030486542718697, 0.0, 0.5182178756464231, 0.0, 0.0, 0.0, 0.057729640414041494, 0.0, 0.0, 0.0, 0.0, 0.19354540299708142, 0.06300918519480127, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1478355026964281, 0.0, 0.0, 0.0, 0.541165159810693, 0.0, 0.0, 0.0, 0.16407931870757664, 0.2057712545151301, 0.32812733654993936, 0.0, 0.24913050204963633, 0.20974603114415158, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5086673274457614, 0.32467254520642835, 0.0, 0.22357645210013752, 0.0, 0.0, 0.0, 0.15908397194054766, 0.0, 0.5866968618616202, 0.21349654033737236, 0.243214867615369, 0.0, 0.0, 0.27374855832219774, 0.0, 0.3075930851551728, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44087523604465795, 0.22505682646241645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2406824670174747, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4353583318013107, 0.0, 0.0, 0.0, 0.0, 0.42257756079609887, 0.0, 0.16328692344233348, 0.7239663574899861, 0.253917129058883, 0.0, 0.0, 0.0, 0.012941670285380298, 0.005948300091521438, 0.6648934966409066, 1.0229318994166612, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13578338949001834, 0.4197116456419966, 0.0, 0.947053670891027, 0.0, 0.13486088669396906, 0.0, 0.6620124726194103, 0.0, 0.6496849904621673, 0.0, 0.0, 0.7890806801928671, 0.01841465881846702, 0.27914669822772004, 0.0, 0.029516583729604803, 0.0, 0.0, 0.018864268719240455, 0.24394303159635272, 0.0, 0.5741603792214627, 0.3080365664801448, 0.6155081898538994, 0.2367816204152454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17416434327376132, 0.0, 0.0, 0.0, 0.0, 0.16269692366253372, 0.0, 0.3491221627973697, 0.0, 0.0, 0.0, 0.0, 0.0, 0.896220207410521, 0.13858473312625733, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2148948115594729, 0.1029957040349639, 0.0, 0.10833979410008034, 0.0, 0.0, 0.23454757178184932, 0.0, 0.2155791117194733, 0.0, 0.22311080318652476, 0.0, 0.0, 0.0, 0.0, 0.07959294795619429, 0.0, 0.17163567622890405, 0.07283994445558169, 0.0, 0.0, 0.0, 0.08654379427684032, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16368166253985994, 0.0, 0.17071385720442764, 0.34758616988919516, 0.0, 0.0, 0.0, 0.0, 0.25336804558610515, 0.0, 0.032257457723339725, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2136279714073461, 0.0, 0.24003957209260046, 0.21303254632455026, 0.006511170502708404, 0.383310313899178, 0.0, 0.0, 0.0, 0.1337677376744651, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0646519260588245, 0.0, 0.0, 0.0, 0.31350938881953055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20384728982176575, 0.0, 0.0, 0.0, 0.0, 0.05670575896106868, 0.0, 0.05964802423259749, 0.0, 0.0, 0.0, 0.4488563568162377, 0.11869016538841153, 0.0, 0.07667734536959822, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.040450967971474544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09480431131127824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0016369185703860613, 0.0, 0.0017218526009009176, 0.0, 0.0, 0.0, 0.0, 0.003426215245260554, 0.0, 0.002213436040060287, 0.0, 0.0, 0.0, 0.0, 0.028624781651490867, 0.0, 0.0, 0.0917221717112923, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03948493179664919, 0.0998678691104795, 0.0, 0.0, 0.0, 0.0, 0.0519314302968116, 0.019753641949088564, 0.0, 0.0, 0.08679325173985959, 0.0, 0.0, 0.44412542864463755, 0.0, 0.0, 0.0, 0.25440324223859473, 0.0, 0.06322962025433154, 0.0, 0.10055420420038351, 0.08043066558940987, 0.01930551218305079, 0.019941838639940274, 0.025537187307223392, 0.0, 0.2186623932992767, 0.06407227122111338, 0.0254835950299198, 0.0, 0.0, 0.6628636898477958, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5422666551724673, 0.6870145051652384, 0.0, 0.0, 0.1501651963322761, 0.019574736086580828, 0.0, 0.0, 0.03589870327691644, 0.0, 0.026565876114361182, 0.0, 0.0, 0.029264986204643254, 0.0, 0.0, 0.01893195304872942, 0.046727577005066426, 0.0, 0.0, 0.28025040572174376, 0.0, 0.018913093854322344, 0.0, 0.0, 0.0, 0.04894945277637971, 0.006583695531171352, 0.3875798367774556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3292155284261099, 0.04344067936293723, 0.0653720550702903, 0.0, 0.0, 0.22649528787365272, 0.018049730534704385, 0.14615138313733889, 0.0, 0.0, 0.0, 0.0, 0.20611785399916963, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31511052111518423, 0.0, 0.03672395965581389, 0.0, 0.0, 0.9552411806377301, 0.0, 0.0, 0.0, 0.3512720117094026, 0.0, 0.7814509194588117, 0.9900444949397926, 0.0, 0.2416698136762867, 0.0, 0.15594310090475377, 0.37345396038999507, 0.13378200524387487, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0037871796529251873, 0.0, 0.07132197093797825, 0.0, 0.0, 0.4038639203443338, 0.0, 0.007926901739731977, 0.0, 0.005121012178382578, 0.4483265473520345, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19545324009971515, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015822792815906598, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2654026801406409, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005338269605792754, 0.3841199941370228, 0.003089715236949895, 0.0, 0.0, 0.0, 0.2702896727602311, 0.0061480462629685435, 0.0, 0.0077716018517988575, 0.0, 0.0, 0.5060334906800565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5325221835704804, 0.4354711371945976, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4529112439588725, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.42175363431898494, 0.540090698333841, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1429447733494866, 0.0, 0.0, 0.17383910328410926, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7778438721037507, 0.7592283162924894, 0.27027978053916946, 0.9680808201913975, 0.0, 0.0, 0.0, 0.0, 0.23721687040615713, 0.0, 0.0, 0.0, 0.214724244927467, 0.0, 0.6713747276751851, 0.2691029964691732, 0.24957193666978766, 0.04174679102400021, 0.0, 0.0, 0.44623226966778873, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9739425217687773, 0.3878431688435761, 0.0, 0.0, 0.0, 0.0, 0.5905353658253105, 0.0, 0.41099451149099264, 0.46959062394023404, 0.38400872176268547, 0.0, 0.4974135902693747, 0.0, 0.0, 0.0, 0.9154557174508635, 0.2586557560688982, 0.09838729964211103, 0.4023736739495505, 0.0, 0.43229262167712557, 0.25909478937718255, 0.0, 0.0, 0.9527916935554207, 0.3993878193283669, 0.4377172055566821, 0.0, 0.0, 0.02448196261221554, 0.28590941544125414, 0.0, 0.4006023808751287, 0.03918308759508126, 0.0, 0.2882716435062163, 0.0, 0.0110856134215592, 0.37363494354124355, 0.049778285574341975, 0.0, 0.0, 0.9269077110105975, 0.31315811797299237, 0.0, 0.012303674452273154, 0.015429991708474198, 0.0, 0.0, 0.014446013487908171, 0.04123415495113678, 0.03371931705724562, 0.29538913191692134, 0.0, 0.3349006097039049, 0.23542836206424417, 0.23052114963877735, 0.31614042332280545, 0.0, 0.0, 0.11683083227872514, 0.0, 0.0, 0.0006114799040197484, 0.10975727273531571, 0.0, 0.004295508139252644, 0.05107901709661121, 0.0, 0.9334562674798996, 0.0, 0.0, 0.0, 0.0, 0.5880007983611341, 0.06450382436313898, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11800940272957328, 0.0, 0.1899751304440791, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.273744879534597, 0.0, 0.0, 0.2671580962806136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3492510086210155, 0.0, 0.3080038714015672, 0.0, 0.0, 0.2221605109577772, 0.0, 0.0, 0.0, 0.0, 0.07999688285015112, 0.06742807554519747, 0.08634727349135116, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10258077598728532, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5012203954355052, 0.0, 0.0, 0.0, 0.08111247475296846, 0.0, 0.0, 0.12138201096876389, 0.0, 0.08982551378065248, 0.0, 0.0, 0.12126628140327211, 0.0, 0.0, 0.07844895363581271, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07837080617454226, 0.0, 0.0, 0.0, 0.20283344995989894, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1282379216751728, 0.033796057874619194, 0.0, 0.09653017040380268, 0.0, 0.08822574416521244, 0.01404236183038609, 0.056929725378038414, 0.0, 0.15526439596527059, 0.0, 0.0, 0.0, 0.03781410167223484, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2529010219858623, 0.3238608478009693, 0.0, 0.0, 0.1407409609998849, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.329852425046806, 0.0, 0.0, 0.33341239592531074, 0.45526487856119674, 0.0, 0.33690660829115365, 0.08120142522000469, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08802821410799472, 0.39029129939250945, 0.13688708765693952, 0.0, 0.08496391292272984, 0.0, 0.0, 0.5183827483305287, 0.703065773125918, 0.4814340077714492, 0.0, 0.0, 0.0, 0.12053001147739661, 0.0, 0.07320101959394086, 0.0, 0.0, 0.5105579893076974, 0.6674329003953465, 0.7892434941755087, 0.051355650714931346, 0.4403973256963498, 0.0, 0.01884360978410253, 0.0, 0.0, 0.42539452394686605, 0.1591995319925085, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17375879569486846, 0.0, 0.0, 0.0, 0.0, 0.1721016330265864, 0.0, 0.00044047585032663307, 0.1672932604423309, 0.18102660031449758, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004373654797423859, 0.0, 0.0, 0.08031885873943684, 0.0, 0.0, 0.12088873452345646, 0.0, 0.25447674137867604, 0.0, 0.20210883042048103, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05466434515545004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005119594240948603, 0.0, 0.2461616206228127, 0.0, 0.0, 0.40062152777610743, 0.0, 0.0, 0.0, 0.4450914546665199, 0.0, 0.0, 0.0, 0.25078704499066257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6437873083636174, 0.0, 0.08986536930257488, 0.05459463157090123, 0.0, 0.0, 0.1617782592438168, 0.0, 0.0, 0.0, 0.0, 0.510293956173341, 0.0, 0.0, 0.0, 0.0, 0.0539939979224501, 0.06022554282071448, 0.0, 0.0, 0.0017903697912685446, 0.10539843894447092, 0.0, 0.007216901945678346, 0.0, 0.0, 0.0, 0.22684974207663636, 1.4564553082728662, 0.182720528082649, 0.06653876275957457, 0.0, 0.6274289004185675, 0.0, 0.5406773490882926, 0.27822503396119075, 0.09306814986560254, 0.29383387946651635, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6230315233142055, 0.9648450369390001, 0.5108864746688708, 0.0, 0.36840186712596623, 0.22805280632725022, 0.3227837297104921, 0.0, 0.0, 1.0240377781477865, 1.591691363735423, 0.0, 0.7314261723085571, 0.0, 0.0, 0.45960855697902075, 0.0, 0.43090812739194506, 0.0, 0.3151566241591431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3316537279743774, 0.27121053456867833, 0.0, 0.0, 0.0, 0.4001441487163798, 0.5176883689695168, 0.709966180421066, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2820721974309868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03908614977726766, 0.3083419381268706, 0.39485756407874356, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11511559935873378, 0.014949071584157462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03837527794969671, 0.5550679626138529, 0.0, 0.5726479000113842, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07921974732295421, 0.0, 0.0, 0.0, 0.0, 0.0035899619569867425, 0.0, 0.0, 0.2145099852146676, 0.0, 0.0, 0.0, 0.0, 0.21950879525087302, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09449503481586544, 0.21750142180635595, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32517302304866597, 0.0, 0.0, 0.21035924507266648, 0.0, 0.0, 0.0, 0.0, 0.23713929311152726, 0.21014969427312982, 0.0, 0.0, 0.0, 0.5438936968761621, 0.3401858622480347, 0.0, 0.0, 0.0016527476817863266, 0.0, 0.007935034693483254, 0.0, 0.0, 0.1293317927443745, 0.0, 0.0, 0.0, 0.14368792432790373, 0.0, 0.0, 0.0, 0.0, 0.35799313982296815, 0.29274964408302323, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029011045367568582, 0.0017598612427760815, 0.0, 0.0, 0.052226530139838453, 0.0, 0.0, 0.0, 0.0, 0.16473710872422428, 0.30447392294316244, 0.0, 0.0, 0.0, 0.01743076710707649, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0410977515599671, 0.0, 0.7547303898094697, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.30609022817115544, 0.0, 0.0, 0.0, 0.0, 0.2105854322132866, 0.5327502455990605, 0.13588517657689586, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24025222517683778, 0.0, 0.007535433506713796, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6979826303937671, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07410089446237557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4744178097104391, 0.0, 0.0, 0.0, 0.0, 0.1876471424480227, 0.28765608703745393, 0.2182401932056186, 0.04951496463333951, 0.0, 0.12909838655126593, 0.0, 0.3498317042975124, 0.0, 0.37087394843503463, 0.47981991008621333, 1.1039610273729945, 0.4455015727565212, 0.05271863458630144, 0.19600051626620313, 0.0, 0.2316343353202091, 0.0, 0.0, 0.0, 0.4641150152741525, 0.15018757716392944, 0.21321672818601478, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21465382837602517, 0.0, 0.0, 0.0, 0.00020573734093944128, 0.0, 0.0, 0.0, 0.16929708918389524, 0.0160994805386141, 0.0, 0.049657622486024976, 0.0, 0.017886560545272906, 0.1659947360027911, 0.0, 0.0, 0.0, 0.0, 0.11420185948155097, 0.0, 0.07369142147184715, 0.0, 0.33489361550116375, 0.46496602729779546, 0.0036113530199273977, 0.044355883980523376, 0.08156217107736623, 0.0, 0.006501263051750628, 0.24089225482763127, 0.0, 0.0, 0.3861226254326548, 0.020506805168434235, 0.0, 0.24677384860885976, 0.0, 0.0, 0.002169816793370778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.041148887129618034, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9769895490432172, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5923828848790257, 0.0, 0.10196835358593148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.328835975725113, 0.0, 0.0, 0.4036325198584592, 0.0, 0.0, 0.3602983928676083, 0.0, 0.0, 0.9557725494193282, 0.0, 0.43908662544955285, 0.0, 0.0, 0.46995823158461947, 0.0, 0.14454935060909507, 0.0, 0.010668331756011213, 0.6280409324984255, 0.03247332779409237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10592998533424139, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33399686192793854, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7833052225131492, 0.0, 0.0, 0.0, 0.24187208135853017, 0.0, 0.071730719549714, 0.0, 0.11407352111288803, 0.0, 0.44957678307373006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5736773783460691, 0.0692607726207939, 0.0, 0.0, 0.4731102083446558, 0.7389531951966185, 0.028778055517643567, 0.22717903301955947, 0.0, 0.1436855444728547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.49542697237169275, 0.0, 0.3454532351491738, 0.11660950163380673, 0.0, 0.0, 0.0, 0.0, 0.027794605242432064, 0.0, 0.0, 0.0, 0.132905538815286, 0.0, 0.05497259939520882, 0.0, 0.0, 1.2625457513449383, 0.39900789022393385, 0.0, 0.0, 0.0, 0.0, 0.4114559732861708, 0.4741201455820112, 0.0, 0.21077985466978477, 0.0, 0.0, 0.7439578224938339, 0.42232266256074136, 0.4876343844944301, 0.6687496612887465, 0.115118039848854, 0.0, 0.0, 0.0, 0.0, 0.05260063632046659, 0.07759581613885312, 0.028401563276091994, 0.0, 0.056334062495133036, 0.17033944709766455, 0.9153107777932757, 0.0, 0.004924424218813803, 0.057509247607595904, 0.0, 0.7491964746840728, 0.05934710743113981, 0.0, 0.0, 0.0, 0.22238712275143577, 0.4319949206677921, 0.025917686578643146, 0.031803778192995576, 0.0, 0.6741550137516903, 0.009328571558732282, 0.0, 0.0, 0.0, 0.0, 0.5515037103010108, 0.6987172178395075, 0.0, 0.0, 0.0, 0.060174729085221354, 0.0, 0.0, 0.08734749351416564, 0.6636235071641303, 0.09580942583448827, 0.0, 0.0, 0.08996354325075087, 0.04525348827838765, 0.5824434635228624, 0.05819874866882126, 0.12005968761069685, 0.0, 0.0, 0.3313826265510977, 0.0, 0.058140773587614956, 0.6127791119711469, 0.0, 0.0, 0.15047559500471339, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6662085079556846, 0.3400843601648346, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22423528728211575, 0.0, 0.0, 0.0, 0.0, 0.3636963343220375, 0.5756271502974593, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046502182852265775, 0.0, 0.04891501992848654, 0.0, 0.0, 0.0, 0.9407814186149676, 0.09733317875961926, 0.0, 0.11672934297033466, 0.0, 0.6385588104227404, 0.0, 0.022206283976929683, 0.09845615426665595, 0.03496357540242119, 0.0, 0.0, 0.0, 0.0, 0.03380456195194267, 0.18543528221865002, 0.12144811071150914, 0.0, 0.03755694741887004, 0.16907550573428723, 0.0, 0.0, 0.1443808645220955, 0.0, 0.1003230425739974, 0.12879502114342312, 0.0647357901808317, 0.0, 0.5084955858844, 0.08519692951707138, 0.007582866204885641, 0.0, 0.03198292312330168, 0.0, 0.1209623049539008, 0.0, 0.12387820451445683, 0.0, 0.0, 0.043058753609476905, 0.03493082633215086, 0.0, 0.0, 0.043777031505228374, 0.016261326799286477, 0.16693678030096587, 0.0, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = ((ex.bodies[1]).bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    result = nothing
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    B_lvl_q = B_lvl_ptr[1]
    B_lvl_q_stop = B_lvl_ptr[1 + 1]
    if B_lvl_q < B_lvl_q_stop
        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
    else
        B_lvl_i_stop = 0
    end
    phase_stop = min(B_lvl.shape[2], B_lvl_i_stop)
    if phase_stop >= 1
        if B_lvl_tbl2[B_lvl_q] < 1
            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
        end
        while true
            B_lvl_i = B_lvl_tbl2[B_lvl_q]
            B_lvl_q_step = B_lvl_q
            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
            end
            if B_lvl_i < phase_stop
                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                val = Ct_lvl_2_val
                Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                B_lvl_tbl1_2 = B_lvl_tbl1
                B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                B_lvl_tbl2_2 = B_lvl_tbl2
                val_2 = B_lvl_val
                B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
                A_lvl_ptr_2 = A_lvl_ptr
                A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                A_lvl_tbl1_2 = A_lvl_tbl1
                A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                A_lvl_tbl2_2 = A_lvl_tbl2
                A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                val_3 = A_lvl_val
                A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
                Threads.@threads for i_9 = 1:Threads.nthreads()
                        phase_start_6 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_9), Threads.nthreads()))
                        phase_stop_7 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_9, Threads.nthreads()))
                        if phase_stop_7 >= phase_start_6
                            for i_12 = phase_start_6:phase_stop_7
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_12
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_8 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_8 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_8
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_9 = min(B_lvl_i_2, phase_stop_8, A_lvl_i)
                                        if A_lvl_i == phase_stop_9 && B_lvl_i_2 == phase_stop_9
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_10 = min(i_12, A_lvl_i_stop_2)
                                            if phase_stop_10 >= i_12
                                                if A_lvl_tbl1[A_lvl_q] < i_12
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_12, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_10
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_12 = min(A_lvl_i_2, phase_stop_10)
                                                        if A_lvl_i_2 == phase_stop_12
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_9
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_9
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_9 + 1
                                    end
                                end
                            end
                        end
                    end
                Ct_lvl_2_val = val
                B_lvl_tbl1 = B_lvl_tbl1_2
                B_lvl_tbl2 = B_lvl_tbl2_2
                B_lvl_val = val_2
                A_lvl_ptr = A_lvl_ptr_2
                A_lvl_tbl1 = A_lvl_tbl1_2
                A_lvl_tbl2 = A_lvl_tbl2_2
                A_lvl_val = val_3
                B_lvl_q = B_lvl_q_step
            else
                phase_stop_18 = min(B_lvl_i, phase_stop)
                if B_lvl_i == phase_stop_18
                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_18
                    val_4 = Ct_lvl_2_val
                    Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                    B_lvl_tbl1_3 = B_lvl_tbl1
                    B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                    B_lvl_tbl2_3 = B_lvl_tbl2
                    val_5 = B_lvl_val
                    B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
                    A_lvl_ptr_3 = A_lvl_ptr
                    A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                    A_lvl_tbl1_3 = A_lvl_tbl1
                    A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                    A_lvl_tbl2_3 = A_lvl_tbl2
                    A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                    val_6 = A_lvl_val
                    A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
                    Threads.@threads for i_19 = 1:Threads.nthreads()
                            phase_start_21 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_19), Threads.nthreads()))
                            phase_stop_23 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_19, Threads.nthreads()))
                            if phase_stop_23 >= phase_start_21
                                for i_22 = phase_start_21:phase_stop_23
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_22
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_24 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_24 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_24
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_25 = min(B_lvl_i_2, A_lvl_i, phase_stop_24)
                                            if A_lvl_i == phase_stop_25 && B_lvl_i_2 == phase_stop_25
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_26 = min(i_22, A_lvl_i_stop_4)
                                                if phase_stop_26 >= i_22
                                                    if A_lvl_tbl1[A_lvl_q] < i_22
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_22, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_26
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_28 = min(A_lvl_i_4, phase_stop_26)
                                                            if A_lvl_i_4 == phase_stop_28
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_25
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_25
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_25 + 1
                                        end
                                    end
                                end
                            end
                        end
                    Ct_lvl_2_val = val_4
                    B_lvl_tbl1 = B_lvl_tbl1_3
                    B_lvl_tbl2 = B_lvl_tbl2_3
                    B_lvl_val = val_5
                    A_lvl_ptr = A_lvl_ptr_3
                    A_lvl_tbl1 = A_lvl_tbl1_3
                    A_lvl_tbl2 = A_lvl_tbl2_3
                    A_lvl_val = val_6
                    B_lvl_q = B_lvl_q_step
                end
                break
            end
        end
    end
    resize!(Ct_lvl_2_val, A_lvl.shape[1] * B_lvl.shape[2])
    result = (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
    result
end
julia> @finch begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.18463927542167063, 0.16872726618738124, 0.0, 0.0, 0.0, 0.5894648191942263, 0.0, 0.0, 0.4399979683038884, 0.4872396182365841, 0.0, 0.0, 0.0, 0.5232587908717465, 0.0, 0.028458776980687873, 0.0, 0.1912882078151879, 0.5164204427507948, 0.25375520169397275, 0.7988048308846816, 0.548219404354131, 0.0, 0.0, 0.0, 0.08973042123875676, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16269102551813888, 0.0, 0.6368094288799442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03708423534085979, 0.16845626013473977, 0.6588748971102081, 0.010877416543996312, 0.0, 0.0, 0.0, 0.44123232519423083, 0.0, 0.24002669576156066, 0.0, 0.0, 0.0, 0.21689234576879324, 0.2098895841423717, 0.6161647640351655, 0.4776903273077015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6355594744363152, 0.0, 0.0, 0.08457949502263125, 0.0, 0.0, 0.6901459039432658, 0.0, 0.0, 0.0, 0.057641486402300816, 0.0, 0.0, 0.09549316762404748, 0.09211299074367744, 0.11795836585190977, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0467877508869465, 0.0, 0.0, 0.0, 0.0, 0.04370712736779799, 0.281244207688617, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2030486542718697, 0.0, 0.5182178756464231, 0.0, 0.0, 0.0, 0.057729640414041494, 0.0, 0.0, 0.0, 0.0, 0.19354540299708142, 0.06300918519480127, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1478355026964281, 0.0, 0.0, 0.0, 0.541165159810693, 0.0, 0.0, 0.0, 0.16407931870757664, 0.2057712545151301, 0.32812733654993936, 0.0, 0.24913050204963633, 0.20974603114415158, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5086673274457614, 0.32467254520642835, 0.0, 0.22357645210013752, 0.0, 0.0, 0.0, 0.15908397194054766, 0.0, 0.5866968618616202, 0.21349654033737236, 0.243214867615369, 0.0, 0.0, 0.27374855832219774, 0.0, 0.3075930851551728, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44087523604465795, 0.22505682646241645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2406824670174747, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4353583318013107, 0.0, 0.0, 0.0, 0.0, 0.42257756079609887, 0.0, 0.16328692344233348, 0.7239663574899861, 0.253917129058883, 0.0, 0.0, 0.0, 0.012941670285380298, 0.005948300091521438, 0.6648934966409066, 1.0229318994166612, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13578338949001834, 0.4197116456419966, 0.0, 0.947053670891027, 0.0, 0.13486088669396906, 0.0, 0.6620124726194103, 0.0, 0.6496849904621673, 0.0, 0.0, 0.7890806801928671, 0.01841465881846702, 0.27914669822772004, 0.0, 0.029516583729604803, 0.0, 0.0, 0.018864268719240455, 0.24394303159635272, 0.0, 0.5741603792214627, 0.3080365664801448, 0.6155081898538994, 0.2367816204152454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17416434327376132, 0.0, 0.0, 0.0, 0.0, 0.16269692366253372, 0.0, 0.3491221627973697, 0.0, 0.0, 0.0, 0.0, 0.0, 0.896220207410521, 0.13858473312625733, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2148948115594729, 0.1029957040349639, 0.0, 0.10833979410008034, 0.0, 0.0, 0.23454757178184932, 0.0, 0.2155791117194733, 0.0, 0.22311080318652476, 0.0, 0.0, 0.0, 0.0, 0.07959294795619429, 0.0, 0.17163567622890405, 0.07283994445558169, 0.0, 0.0, 0.0, 0.08654379427684032, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16368166253985994, 0.0, 0.17071385720442764, 0.34758616988919516, 0.0, 0.0, 0.0, 0.0, 0.25336804558610515, 0.0, 0.032257457723339725, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2136279714073461, 0.0, 0.24003957209260046, 0.21303254632455026, 0.006511170502708404, 0.383310313899178, 0.0, 0.0, 0.0, 0.1337677376744651, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0646519260588245, 0.0, 0.0, 0.0, 0.31350938881953055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20384728982176575, 0.0, 0.0, 0.0, 0.0, 0.05670575896106868, 0.0, 0.05964802423259749, 0.0, 0.0, 0.0, 0.4488563568162377, 0.11869016538841153, 0.0, 0.07667734536959822, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.040450967971474544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09480431131127824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0016369185703860613, 0.0, 0.0017218526009009176, 0.0, 0.0, 0.0, 0.0, 0.003426215245260554, 0.0, 0.002213436040060287, 0.0, 0.0, 0.0, 0.0, 0.028624781651490867, 0.0, 0.0, 0.0917221717112923, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03948493179664919, 0.0998678691104795, 0.0, 0.0, 0.0, 0.0, 0.0519314302968116, 0.019753641949088564, 0.0, 0.0, 0.08679325173985959, 0.0, 0.0, 0.44412542864463755, 0.0, 0.0, 0.0, 0.25440324223859473, 0.0, 0.06322962025433154, 0.0, 0.10055420420038351, 0.08043066558940987, 0.01930551218305079, 0.019941838639940274, 0.025537187307223392, 0.0, 0.2186623932992767, 0.06407227122111338, 0.0254835950299198, 0.0, 0.0, 0.6628636898477958, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5422666551724673, 0.6870145051652384, 0.0, 0.0, 0.1501651963322761, 0.019574736086580828, 0.0, 0.0, 0.03589870327691644, 0.0, 0.026565876114361182, 0.0, 0.0, 0.029264986204643254, 0.0, 0.0, 0.01893195304872942, 0.046727577005066426, 0.0, 0.0, 0.28025040572174376, 0.0, 0.018913093854322344, 0.0, 0.0, 0.0, 0.04894945277637971, 0.006583695531171352, 0.3875798367774556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3292155284261099, 0.04344067936293723, 0.0653720550702903, 0.0, 0.0, 0.22649528787365272, 0.018049730534704385, 0.14615138313733889, 0.0, 0.0, 0.0, 0.0, 0.20611785399916963, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31511052111518423, 0.0, 0.03672395965581389, 0.0, 0.0, 0.9552411806377301, 0.0, 0.0, 0.0, 0.3512720117094026, 0.0, 0.7814509194588117, 0.9900444949397926, 0.0, 0.2416698136762867, 0.0, 0.15594310090475377, 0.37345396038999507, 0.13378200524387487, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0037871796529251873, 0.0, 0.07132197093797825, 0.0, 0.0, 0.4038639203443338, 0.0, 0.007926901739731977, 0.0, 0.005121012178382578, 0.4483265473520345, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19545324009971515, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015822792815906598, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2654026801406409, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005338269605792754, 0.3841199941370228, 0.003089715236949895, 0.0, 0.0, 0.0, 0.2702896727602311, 0.0061480462629685435, 0.0, 0.0077716018517988575, 0.0, 0.0, 0.5060334906800565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5325221835704804, 0.4354711371945976, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4529112439588725, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.42175363431898494, 0.540090698333841, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1429447733494866, 0.0, 0.0, 0.17383910328410926, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7778438721037507, 0.7592283162924894, 0.27027978053916946, 0.9680808201913975, 0.0, 0.0, 0.0, 0.0, 0.23721687040615713, 0.0, 0.0, 0.0, 0.214724244927467, 0.0, 0.6713747276751851, 0.2691029964691732, 0.24957193666978766, 0.04174679102400021, 0.0, 0.0, 0.44623226966778873, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9739425217687773, 0.3878431688435761, 0.0, 0.0, 0.0, 0.0, 0.5905353658253105, 0.0, 0.41099451149099264, 0.46959062394023404, 0.38400872176268547, 0.0, 0.4974135902693747, 0.0, 0.0, 0.0, 0.9154557174508635, 0.2586557560688982, 0.09838729964211103, 0.4023736739495505, 0.0, 0.43229262167712557, 0.25909478937718255, 0.0, 0.0, 0.9527916935554207, 0.3993878193283669, 0.4377172055566821, 0.0, 0.0, 0.02448196261221554, 0.28590941544125414, 0.0, 0.4006023808751287, 0.03918308759508126, 0.0, 0.2882716435062163, 0.0, 0.0110856134215592, 0.37363494354124355, 0.049778285574341975, 0.0, 0.0, 0.9269077110105975, 0.31315811797299237, 0.0, 0.012303674452273154, 0.015429991708474198, 0.0, 0.0, 0.014446013487908171, 0.04123415495113678, 0.03371931705724562, 0.29538913191692134, 0.0, 0.3349006097039049, 0.23542836206424417, 0.23052114963877735, 0.31614042332280545, 0.0, 0.0, 0.11683083227872514, 0.0, 0.0, 0.0006114799040197484, 0.10975727273531571, 0.0, 0.004295508139252644, 0.05107901709661121, 0.0, 0.9334562674798996, 0.0, 0.0, 0.0, 0.0, 0.5880007983611341, 0.06450382436313898, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11800940272957328, 0.0, 0.1899751304440791, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.273744879534597, 0.0, 0.0, 0.2671580962806136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3492510086210155, 0.0, 0.3080038714015672, 0.0, 0.0, 0.2221605109577772, 0.0, 0.0, 0.0, 0.0, 0.07999688285015112, 0.06742807554519747, 0.08634727349135116, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10258077598728532, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5012203954355052, 0.0, 0.0, 0.0, 0.08111247475296846, 0.0, 0.0, 0.12138201096876389, 0.0, 0.08982551378065248, 0.0, 0.0, 0.12126628140327211, 0.0, 0.0, 0.07844895363581271, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07837080617454226, 0.0, 0.0, 0.0, 0.20283344995989894, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1282379216751728, 0.033796057874619194, 0.0, 0.09653017040380268, 0.0, 0.08822574416521244, 0.01404236183038609, 0.056929725378038414, 0.0, 0.15526439596527059, 0.0, 0.0, 0.0, 0.03781410167223484, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2529010219858623, 0.3238608478009693, 0.0, 0.0, 0.1407409609998849, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.329852425046806, 0.0, 0.0, 0.33341239592531074, 0.45526487856119674, 0.0, 0.33690660829115365, 0.08120142522000469, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08802821410799472, 0.39029129939250945, 0.13688708765693952, 0.0, 0.08496391292272984, 0.0, 0.0, 0.5183827483305287, 0.703065773125918, 0.4814340077714492, 0.0, 0.0, 0.0, 0.12053001147739661, 0.0, 0.07320101959394086, 0.0, 0.0, 0.5105579893076974, 0.6674329003953465, 0.7892434941755087, 0.051355650714931346, 0.4403973256963498, 0.0, 0.01884360978410253, 0.0, 0.0, 0.42539452394686605, 0.1591995319925085, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17375879569486846, 0.0, 0.0, 0.0, 0.0, 0.1721016330265864, 0.0, 0.00044047585032663307, 0.1672932604423309, 0.18102660031449758, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004373654797423859, 0.0, 0.0, 0.08031885873943684, 0.0, 0.0, 0.12088873452345646, 0.0, 0.25447674137867604, 0.0, 0.20210883042048103, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05466434515545004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005119594240948603, 0.0, 0.2461616206228127, 0.0, 0.0, 0.40062152777610743, 0.0, 0.0, 0.0, 0.4450914546665199, 0.0, 0.0, 0.0, 0.25078704499066257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6437873083636174, 0.0, 0.08986536930257488, 0.05459463157090123, 0.0, 0.0, 0.1617782592438168, 0.0, 0.0, 0.0, 0.0, 0.510293956173341, 0.0, 0.0, 0.0, 0.0, 0.0539939979224501, 0.06022554282071448, 0.0, 0.0, 0.0017903697912685446, 0.10539843894447092, 0.0, 0.007216901945678346, 0.0, 0.0, 0.0, 0.22684974207663636, 1.4564553082728662, 0.182720528082649, 0.06653876275957457, 0.0, 0.6274289004185675, 0.0, 0.5406773490882926, 0.27822503396119075, 0.09306814986560254, 0.29383387946651635, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6230315233142055, 0.9648450369390001, 0.5108864746688708, 0.0, 0.36840186712596623, 0.22805280632725022, 0.3227837297104921, 0.0, 0.0, 1.0240377781477865, 1.591691363735423, 0.0, 0.7314261723085571, 0.0, 0.0, 0.45960855697902075, 0.0, 0.43090812739194506, 0.0, 0.3151566241591431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3316537279743774, 0.27121053456867833, 0.0, 0.0, 0.0, 0.4001441487163798, 0.5176883689695168, 0.709966180421066, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2820721974309868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03908614977726766, 0.3083419381268706, 0.39485756407874356, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11511559935873378, 0.014949071584157462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03837527794969671, 0.5550679626138529, 0.0, 0.5726479000113842, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07921974732295421, 0.0, 0.0, 0.0, 0.0, 0.0035899619569867425, 0.0, 0.0, 0.2145099852146676, 0.0, 0.0, 0.0, 0.0, 0.21950879525087302, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09449503481586544, 0.21750142180635595, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32517302304866597, 0.0, 0.0, 0.21035924507266648, 0.0, 0.0, 0.0, 0.0, 0.23713929311152726, 0.21014969427312982, 0.0, 0.0, 0.0, 0.5438936968761621, 0.3401858622480347, 0.0, 0.0, 0.0016527476817863266, 0.0, 0.007935034693483254, 0.0, 0.0, 0.1293317927443745, 0.0, 0.0, 0.0, 0.14368792432790373, 0.0, 0.0, 0.0, 0.0, 0.35799313982296815, 0.29274964408302323, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029011045367568582, 0.0017598612427760815, 0.0, 0.0, 0.052226530139838453, 0.0, 0.0, 0.0, 0.0, 0.16473710872422428, 0.30447392294316244, 0.0, 0.0, 0.0, 0.01743076710707649, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0410977515599671, 0.0, 0.7547303898094697, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.30609022817115544, 0.0, 0.0, 0.0, 0.0, 0.2105854322132866, 0.5327502455990605, 0.13588517657689586, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24025222517683778, 0.0, 0.007535433506713796, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6979826303937671, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07410089446237557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4744178097104391, 0.0, 0.0, 0.0, 0.0, 0.1876471424480227, 0.28765608703745393, 0.2182401932056186, 0.04951496463333951, 0.0, 0.12909838655126593, 0.0, 0.3498317042975124, 0.0, 0.37087394843503463, 0.47981991008621333, 1.1039610273729945, 0.4455015727565212, 0.05271863458630144, 0.19600051626620313, 0.0, 0.2316343353202091, 0.0, 0.0, 0.0, 0.4641150152741525, 0.15018757716392944, 0.21321672818601478, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21465382837602517, 0.0, 0.0, 0.0, 0.00020573734093944128, 0.0, 0.0, 0.0, 0.16929708918389524, 0.0160994805386141, 0.0, 0.049657622486024976, 0.0, 0.017886560545272906, 0.1659947360027911, 0.0, 0.0, 0.0, 0.0, 0.11420185948155097, 0.0, 0.07369142147184715, 0.0, 0.33489361550116375, 0.46496602729779546, 0.0036113530199273977, 0.044355883980523376, 0.08156217107736623, 0.0, 0.006501263051750628, 0.24089225482763127, 0.0, 0.0, 0.3861226254326548, 0.020506805168434235, 0.0, 0.24677384860885976, 0.0, 0.0, 0.002169816793370778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.041148887129618034, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9769895490432172, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5923828848790257, 0.0, 0.10196835358593148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.328835975725113, 0.0, 0.0, 0.4036325198584592, 0.0, 0.0, 0.3602983928676083, 0.0, 0.0, 0.9557725494193282, 0.0, 0.43908662544955285, 0.0, 0.0, 0.46995823158461947, 0.0, 0.14454935060909507, 0.0, 0.010668331756011213, 0.6280409324984255, 0.03247332779409237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10592998533424139, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33399686192793854, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7833052225131492, 0.0, 0.0, 0.0, 0.24187208135853017, 0.0, 0.071730719549714, 0.0, 0.11407352111288803, 0.0, 0.44957678307373006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5736773783460691, 0.0692607726207939, 0.0, 0.0, 0.4731102083446558, 0.7389531951966185, 0.028778055517643567, 0.22717903301955947, 0.0, 0.1436855444728547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.49542697237169275, 0.0, 0.3454532351491738, 0.11660950163380673, 0.0, 0.0, 0.0, 0.0, 0.027794605242432064, 0.0, 0.0, 0.0, 0.132905538815286, 0.0, 0.05497259939520882, 0.0, 0.0, 1.2625457513449383, 0.39900789022393385, 0.0, 0.0, 0.0, 0.0, 0.4114559732861708, 0.4741201455820112, 0.0, 0.21077985466978477, 0.0, 0.0, 0.7439578224938339, 0.42232266256074136, 0.4876343844944301, 0.6687496612887465, 0.115118039848854, 0.0, 0.0, 0.0, 0.0, 0.05260063632046659, 0.07759581613885312, 0.028401563276091994, 0.0, 0.056334062495133036, 0.17033944709766455, 0.9153107777932757, 0.0, 0.004924424218813803, 0.057509247607595904, 0.0, 0.7491964746840728, 0.05934710743113981, 0.0, 0.0, 0.0, 0.22238712275143577, 0.4319949206677921, 0.025917686578643146, 0.031803778192995576, 0.0, 0.6741550137516903, 0.009328571558732282, 0.0, 0.0, 0.0, 0.0, 0.5515037103010108, 0.6987172178395075, 0.0, 0.0, 0.0, 0.060174729085221354, 0.0, 0.0, 0.08734749351416564, 0.6636235071641303, 0.09580942583448827, 0.0, 0.0, 0.08996354325075087, 0.04525348827838765, 0.5824434635228624, 0.05819874866882126, 0.12005968761069685, 0.0, 0.0, 0.3313826265510977, 0.0, 0.058140773587614956, 0.6127791119711469, 0.0, 0.0, 0.15047559500471339, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6662085079556846, 0.3400843601648346, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22423528728211575, 0.0, 0.0, 0.0, 0.0, 0.3636963343220375, 0.5756271502974593, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046502182852265775, 0.0, 0.04891501992848654, 0.0, 0.0, 0.0, 0.9407814186149676, 0.09733317875961926, 0.0, 0.11672934297033466, 0.0, 0.6385588104227404, 0.0, 0.022206283976929683, 0.09845615426665595, 0.03496357540242119, 0.0, 0.0, 0.0, 0.0, 0.03380456195194267, 0.18543528221865002, 0.12144811071150914, 0.0, 0.03755694741887004, 0.16907550573428723, 0.0, 0.0, 0.1443808645220955, 0.0, 0.1003230425739974, 0.12879502114342312, 0.0647357901808317, 0.0, 0.5084955858844, 0.08519692951707138, 0.007582866204885641, 0.0, 0.03198292312330168, 0.0, 0.1209623049539008, 0.0, 0.12387820451445683, 0.0, 0.0, 0.043058753609476905, 0.03493082633215086, 0.0, 0.0, 0.043777031505228374, 0.016261326799286477, 0.16693678030096587, 0.0, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = parallel(_)
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = ((ex.bodies[1]).bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    result = nothing
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = (Finch).moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = (Finch).moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            B_lvl_q = B_lvl_ptr[1]
            B_lvl_q_stop = B_lvl_ptr[1 + 1]
            if B_lvl_q < B_lvl_q_stop
                B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
            else
                B_lvl_i_stop = 0
            end
            phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                if B_lvl_tbl2[B_lvl_q] < phase_start_2
                    B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                end
                while true
                    B_lvl_i = B_lvl_tbl2[B_lvl_q]
                    B_lvl_q_step = B_lvl_q
                    if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                        B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    if B_lvl_i < phase_stop_2
                        Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                        val_4 = Ct_lvl_2_val
                        Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                        A_lvl_ptr_3 = A_lvl_ptr
                        A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                        A_lvl_tbl1_3 = A_lvl_tbl1
                        A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                        A_lvl_tbl2_3 = A_lvl_tbl2
                        A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                        val_5 = A_lvl_val
                        A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
                        B_lvl_ptr_3 = B_lvl_ptr
                        B_lvl_tbl1_3 = B_lvl_tbl1
                        B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                        B_lvl_tbl2_3 = B_lvl_tbl2
                        val_6 = B_lvl_val
                        B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
                        Threads.@threads for i_10 = 1:Threads.nthreads()
                                phase_start_7 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_10), Threads.nthreads()))
                                phase_stop_8 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_10, Threads.nthreads()))
                                if phase_stop_8 >= phase_start_7
                                    for i_13 = phase_start_7:phase_stop_8
                                        Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_13
                                        A_lvl_q = A_lvl_ptr[1]
                                        A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                        if A_lvl_q < A_lvl_q_stop
                                            A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                        else
                                            A_lvl_i_stop = 0
                                        end
                                        B_lvl_q_3 = B_lvl_q
                                        if B_lvl_q < B_lvl_q_step
                                            B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                        else
                                            B_lvl_i_stop_3 = 0
                                        end
                                        phase_stop_9 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                        if phase_stop_9 >= 1
                                            k = 1
                                            if A_lvl_tbl2[A_lvl_q] < 1
                                                A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            if B_lvl_tbl1[B_lvl_q] < 1
                                                B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                            end
                                            while k <= phase_stop_9
                                                A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                                A_lvl_q_step = A_lvl_q
                                                if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                    A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                                end
                                                B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                                phase_stop_10 = min(B_lvl_i_3, phase_stop_9, A_lvl_i)
                                                if A_lvl_i == phase_stop_10 && B_lvl_i_3 == phase_stop_10
                                                    B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                                    A_lvl_q_2 = A_lvl_q
                                                    if A_lvl_q < A_lvl_q_step
                                                        A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                    else
                                                        A_lvl_i_stop_2 = 0
                                                    end
                                                    phase_stop_11 = min(i_13, A_lvl_i_stop_2)
                                                    if phase_stop_11 >= i_13
                                                        if A_lvl_tbl1[A_lvl_q] < i_13
                                                            A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_13, A_lvl_q, A_lvl_q_step - 1)
                                                        end
                                                        while true
                                                            A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                            if A_lvl_i_2 < phase_stop_11
                                                                A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                                Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                                A_lvl_q_2 += 1
                                                            else
                                                                phase_stop_13 = min(A_lvl_i_2, phase_stop_11)
                                                                if A_lvl_i_2 == phase_stop_13
                                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                                    Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                                    A_lvl_q_2 += 1
                                                                end
                                                                break
                                                            end
                                                        end
                                                    end
                                                    A_lvl_q = A_lvl_q_step
                                                    B_lvl_q_3 += 1
                                                elseif B_lvl_i_3 == phase_stop_10
                                                    B_lvl_q_3 += 1
                                                elseif A_lvl_i == phase_stop_10
                                                    A_lvl_q = A_lvl_q_step
                                                end
                                                k = phase_stop_10 + 1
                                            end
                                        end
                                    end
                                end
                            end
                        Ct_lvl_2_val = val_4
                        A_lvl_ptr = A_lvl_ptr_3
                        A_lvl_tbl1 = A_lvl_tbl1_3
                        A_lvl_tbl2 = A_lvl_tbl2_3
                        A_lvl_val = val_5
                        B_lvl_ptr = B_lvl_ptr_3
                        B_lvl_tbl1 = B_lvl_tbl1_3
                        B_lvl_tbl2 = B_lvl_tbl2_3
                        B_lvl_val = val_6
                        B_lvl_q = B_lvl_q_step
                    else
                        phase_stop_19 = min(B_lvl_i, phase_stop_2)
                        if B_lvl_i == phase_stop_19
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_19
                            val_7 = Ct_lvl_2_val
                            Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                            A_lvl_ptr_4 = A_lvl_ptr
                            A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                            A_lvl_tbl1_4 = A_lvl_tbl1
                            A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                            A_lvl_tbl2_4 = A_lvl_tbl2
                            A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                            val_8 = A_lvl_val
                            A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
                            B_lvl_ptr_4 = B_lvl_ptr
                            B_lvl_tbl1_4 = B_lvl_tbl1
                            B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                            B_lvl_tbl2_4 = B_lvl_tbl2
                            val_9 = B_lvl_val
                            B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
                            Threads.@threads for i_20 = 1:Threads.nthreads()
                                    phase_start_22 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_20), Threads.nthreads()))
                                    phase_stop_24 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_20, Threads.nthreads()))
                                    if phase_stop_24 >= phase_start_22
                                        for i_23 = phase_start_22:phase_stop_24
                                            Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_23
                                            A_lvl_q = A_lvl_ptr[1]
                                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                            if A_lvl_q < A_lvl_q_stop
                                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                            else
                                                A_lvl_i_stop = 0
                                            end
                                            B_lvl_q_3 = B_lvl_q
                                            if B_lvl_q < B_lvl_q_step
                                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                            else
                                                B_lvl_i_stop_3 = 0
                                            end
                                            phase_stop_25 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                            if phase_stop_25 >= 1
                                                k = 1
                                                if A_lvl_tbl2[A_lvl_q] < 1
                                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                                end
                                                if B_lvl_tbl1[B_lvl_q] < 1
                                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                                end
                                                while k <= phase_stop_25
                                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                                    A_lvl_q_step = A_lvl_q
                                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                                    end
                                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                                    phase_stop_26 = min(B_lvl_i_3, A_lvl_i, phase_stop_25)
                                                    if A_lvl_i == phase_stop_26 && B_lvl_i_3 == phase_stop_26
                                                        B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                                        A_lvl_q_4 = A_lvl_q
                                                        if A_lvl_q < A_lvl_q_step
                                                            A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                        else
                                                            A_lvl_i_stop_4 = 0
                                                        end
                                                        phase_stop_27 = min(i_23, A_lvl_i_stop_4)
                                                        if phase_stop_27 >= i_23
                                                            if A_lvl_tbl1[A_lvl_q] < i_23
                                                                A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_23, A_lvl_q, A_lvl_q_step - 1)
                                                            end
                                                            while true
                                                                A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                                if A_lvl_i_4 < phase_stop_27
                                                                    A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                    Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                                    A_lvl_q_4 += 1
                                                                else
                                                                    phase_stop_29 = min(A_lvl_i_4, phase_stop_27)
                                                                    if A_lvl_i_4 == phase_stop_29
                                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                                        A_lvl_q_4 += 1
                                                                    end
                                                                    break
                                                                end
                                                            end
                                                        end
                                                        A_lvl_q = A_lvl_q_step
                                                        B_lvl_q_3 += 1
                                                    elseif B_lvl_i_3 == phase_stop_26
                                                        B_lvl_q_3 += 1
                                                    elseif A_lvl_i == phase_stop_26
                                                        A_lvl_q = A_lvl_q_step
                                                    end
                                                    k = phase_stop_26 + 1
                                                end
                                            end
                                        end
                                    end
                                end
                            Ct_lvl_2_val = val_7
                            A_lvl_ptr = A_lvl_ptr_4
                            A_lvl_tbl1 = A_lvl_tbl1_4
                            A_lvl_tbl2 = A_lvl_tbl2_4
                            A_lvl_val = val_8
                            B_lvl_ptr = B_lvl_ptr_4
                            B_lvl_tbl1 = B_lvl_tbl1_4
                            B_lvl_tbl2 = B_lvl_tbl2_4
                            B_lvl_val = val_9
                            B_lvl_q = B_lvl_q_step
                        end
                        break
                    end
                end
            end
        end
    resize!(val, A_lvl.shape[1] * B_lvl.shape[2])
    result = (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
    result
end
julia> @finch begin
        Ct .= 0
        for j = parallel(_)
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.18463927542167063, 0.16872726618738124, 0.0, 0.0, 0.0, 0.5894648191942263, 0.0, 0.0, 0.4399979683038884, 0.4872396182365841, 0.0, 0.0, 0.0, 0.5232587908717465, 0.0, 0.028458776980687873, 0.0, 0.1912882078151879, 0.5164204427507948, 0.25375520169397275, 0.7988048308846816, 0.548219404354131, 0.0, 0.0, 0.0, 0.08973042123875676, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16269102551813888, 0.0, 0.6368094288799442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03708423534085979, 0.16845626013473977, 0.6588748971102081, 0.010877416543996312, 0.0, 0.0, 0.0, 0.44123232519423083, 0.0, 0.24002669576156066, 0.0, 0.0, 0.0, 0.21689234576879324, 0.2098895841423717, 0.6161647640351655, 0.4776903273077015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6355594744363152, 0.0, 0.0, 0.08457949502263125, 0.0, 0.0, 0.6901459039432658, 0.0, 0.0, 0.0, 0.057641486402300816, 0.0, 0.0, 0.09549316762404748, 0.09211299074367744, 0.11795836585190977, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0467877508869465, 0.0, 0.0, 0.0, 0.0, 0.04370712736779799, 0.281244207688617, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2030486542718697, 0.0, 0.5182178756464231, 0.0, 0.0, 0.0, 0.057729640414041494, 0.0, 0.0, 0.0, 0.0, 0.19354540299708142, 0.06300918519480127, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1478355026964281, 0.0, 0.0, 0.0, 0.541165159810693, 0.0, 0.0, 0.0, 0.16407931870757664, 0.2057712545151301, 0.32812733654993936, 0.0, 0.24913050204963633, 0.20974603114415158, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5086673274457614, 0.32467254520642835, 0.0, 0.22357645210013752, 0.0, 0.0, 0.0, 0.15908397194054766, 0.0, 0.5866968618616202, 0.21349654033737236, 0.243214867615369, 0.0, 0.0, 0.27374855832219774, 0.0, 0.3075930851551728, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44087523604465795, 0.22505682646241645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2406824670174747, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4353583318013107, 0.0, 0.0, 0.0, 0.0, 0.42257756079609887, 0.0, 0.16328692344233348, 0.7239663574899861, 0.253917129058883, 0.0, 0.0, 0.0, 0.012941670285380298, 0.005948300091521438, 0.6648934966409066, 1.0229318994166612, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13578338949001834, 0.4197116456419966, 0.0, 0.947053670891027, 0.0, 0.13486088669396906, 0.0, 0.6620124726194103, 0.0, 0.6496849904621673, 0.0, 0.0, 0.7890806801928671, 0.01841465881846702, 0.27914669822772004, 0.0, 0.029516583729604803, 0.0, 0.0, 0.018864268719240455, 0.24394303159635272, 0.0, 0.5741603792214627, 0.3080365664801448, 0.6155081898538994, 0.2367816204152454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17416434327376132, 0.0, 0.0, 0.0, 0.0, 0.16269692366253372, 0.0, 0.3491221627973697, 0.0, 0.0, 0.0, 0.0, 0.0, 0.896220207410521, 0.13858473312625733, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2148948115594729, 0.1029957040349639, 0.0, 0.10833979410008034, 0.0, 0.0, 0.23454757178184932, 0.0, 0.2155791117194733, 0.0, 0.22311080318652476, 0.0, 0.0, 0.0, 0.0, 0.07959294795619429, 0.0, 0.17163567622890405, 0.07283994445558169, 0.0, 0.0, 0.0, 0.08654379427684032, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16368166253985994, 0.0, 0.17071385720442764, 0.34758616988919516, 0.0, 0.0, 0.0, 0.0, 0.25336804558610515, 0.0, 0.032257457723339725, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2136279714073461, 0.0, 0.24003957209260046, 0.21303254632455026, 0.006511170502708404, 0.383310313899178, 0.0, 0.0, 0.0, 0.1337677376744651, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0646519260588245, 0.0, 0.0, 0.0, 0.31350938881953055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20384728982176575, 0.0, 0.0, 0.0, 0.0, 0.05670575896106868, 0.0, 0.05964802423259749, 0.0, 0.0, 0.0, 0.4488563568162377, 0.11869016538841153, 0.0, 0.07667734536959822, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.040450967971474544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09480431131127824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0016369185703860613, 0.0, 0.0017218526009009176, 0.0, 0.0, 0.0, 0.0, 0.003426215245260554, 0.0, 0.002213436040060287, 0.0, 0.0, 0.0, 0.0, 0.028624781651490867, 0.0, 0.0, 0.0917221717112923, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03948493179664919, 0.0998678691104795, 0.0, 0.0, 0.0, 0.0, 0.0519314302968116, 0.019753641949088564, 0.0, 0.0, 0.08679325173985959, 0.0, 0.0, 0.44412542864463755, 0.0, 0.0, 0.0, 0.25440324223859473, 0.0, 0.06322962025433154, 0.0, 0.10055420420038351, 0.08043066558940987, 0.01930551218305079, 0.019941838639940274, 0.025537187307223392, 0.0, 0.2186623932992767, 0.06407227122111338, 0.0254835950299198, 0.0, 0.0, 0.6628636898477958, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5422666551724673, 0.6870145051652384, 0.0, 0.0, 0.1501651963322761, 0.019574736086580828, 0.0, 0.0, 0.03589870327691644, 0.0, 0.026565876114361182, 0.0, 0.0, 0.029264986204643254, 0.0, 0.0, 0.01893195304872942, 0.046727577005066426, 0.0, 0.0, 0.28025040572174376, 0.0, 0.018913093854322344, 0.0, 0.0, 0.0, 0.04894945277637971, 0.006583695531171352, 0.3875798367774556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3292155284261099, 0.04344067936293723, 0.0653720550702903, 0.0, 0.0, 0.22649528787365272, 0.018049730534704385, 0.14615138313733889, 0.0, 0.0, 0.0, 0.0, 0.20611785399916963, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31511052111518423, 0.0, 0.03672395965581389, 0.0, 0.0, 0.9552411806377301, 0.0, 0.0, 0.0, 0.3512720117094026, 0.0, 0.7814509194588117, 0.9900444949397926, 0.0, 0.2416698136762867, 0.0, 0.15594310090475377, 0.37345396038999507, 0.13378200524387487, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0037871796529251873, 0.0, 0.07132197093797825, 0.0, 0.0, 0.4038639203443338, 0.0, 0.007926901739731977, 0.0, 0.005121012178382578, 0.4483265473520345, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19545324009971515, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015822792815906598, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2654026801406409, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005338269605792754, 0.3841199941370228, 0.003089715236949895, 0.0, 0.0, 0.0, 0.2702896727602311, 0.0061480462629685435, 0.0, 0.0077716018517988575, 0.0, 0.0, 0.5060334906800565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5325221835704804, 0.4354711371945976, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4529112439588725, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.42175363431898494, 0.540090698333841, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1429447733494866, 0.0, 0.0, 0.17383910328410926, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7778438721037507, 0.7592283162924894, 0.27027978053916946, 0.9680808201913975, 0.0, 0.0, 0.0, 0.0, 0.23721687040615713, 0.0, 0.0, 0.0, 0.214724244927467, 0.0, 0.6713747276751851, 0.2691029964691732, 0.24957193666978766, 0.04174679102400021, 0.0, 0.0, 0.44623226966778873, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9739425217687773, 0.3878431688435761, 0.0, 0.0, 0.0, 0.0, 0.5905353658253105, 0.0, 0.41099451149099264, 0.46959062394023404, 0.38400872176268547, 0.0, 0.4974135902693747, 0.0, 0.0, 0.0, 0.9154557174508635, 0.2586557560688982, 0.09838729964211103, 0.4023736739495505, 0.0, 0.43229262167712557, 0.25909478937718255, 0.0, 0.0, 0.9527916935554207, 0.3993878193283669, 0.4377172055566821, 0.0, 0.0, 0.02448196261221554, 0.28590941544125414, 0.0, 0.4006023808751287, 0.03918308759508126, 0.0, 0.2882716435062163, 0.0, 0.0110856134215592, 0.37363494354124355, 0.049778285574341975, 0.0, 0.0, 0.9269077110105975, 0.31315811797299237, 0.0, 0.012303674452273154, 0.015429991708474198, 0.0, 0.0, 0.014446013487908171, 0.04123415495113678, 0.03371931705724562, 0.29538913191692134, 0.0, 0.3349006097039049, 0.23542836206424417, 0.23052114963877735, 0.31614042332280545, 0.0, 0.0, 0.11683083227872514, 0.0, 0.0, 0.0006114799040197484, 0.10975727273531571, 0.0, 0.004295508139252644, 0.05107901709661121, 0.0, 0.9334562674798996, 0.0, 0.0, 0.0, 0.0, 0.5880007983611341, 0.06450382436313898, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11800940272957328, 0.0, 0.1899751304440791, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.273744879534597, 0.0, 0.0, 0.2671580962806136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3492510086210155, 0.0, 0.3080038714015672, 0.0, 0.0, 0.2221605109577772, 0.0, 0.0, 0.0, 0.0, 0.07999688285015112, 0.06742807554519747, 0.08634727349135116, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10258077598728532, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5012203954355052, 0.0, 0.0, 0.0, 0.08111247475296846, 0.0, 0.0, 0.12138201096876389, 0.0, 0.08982551378065248, 0.0, 0.0, 0.12126628140327211, 0.0, 0.0, 0.07844895363581271, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07837080617454226, 0.0, 0.0, 0.0, 0.20283344995989894, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1282379216751728, 0.033796057874619194, 0.0, 0.09653017040380268, 0.0, 0.08822574416521244, 0.01404236183038609, 0.056929725378038414, 0.0, 0.15526439596527059, 0.0, 0.0, 0.0, 0.03781410167223484, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2529010219858623, 0.3238608478009693, 0.0, 0.0, 0.1407409609998849, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.329852425046806, 0.0, 0.0, 0.33341239592531074, 0.45526487856119674, 0.0, 0.33690660829115365, 0.08120142522000469, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08802821410799472, 0.39029129939250945, 0.13688708765693952, 0.0, 0.08496391292272984, 0.0, 0.0, 0.5183827483305287, 0.703065773125918, 0.4814340077714492, 0.0, 0.0, 0.0, 0.12053001147739661, 0.0, 0.07320101959394086, 0.0, 0.0, 0.5105579893076974, 0.6674329003953465, 0.7892434941755087, 0.051355650714931346, 0.4403973256963498, 0.0, 0.01884360978410253, 0.0, 0.0, 0.42539452394686605, 0.1591995319925085, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17375879569486846, 0.0, 0.0, 0.0, 0.0, 0.1721016330265864, 0.0, 0.00044047585032663307, 0.1672932604423309, 0.18102660031449758, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004373654797423859, 0.0, 0.0, 0.08031885873943684, 0.0, 0.0, 0.12088873452345646, 0.0, 0.25447674137867604, 0.0, 0.20210883042048103, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05466434515545004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005119594240948603, 0.0, 0.2461616206228127, 0.0, 0.0, 0.40062152777610743, 0.0, 0.0, 0.0, 0.4450914546665199, 0.0, 0.0, 0.0, 0.25078704499066257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6437873083636174, 0.0, 0.08986536930257488, 0.05459463157090123, 0.0, 0.0, 0.1617782592438168, 0.0, 0.0, 0.0, 0.0, 0.510293956173341, 0.0, 0.0, 0.0, 0.0, 0.0539939979224501, 0.06022554282071448, 0.0, 0.0, 0.0017903697912685446, 0.10539843894447092, 0.0, 0.007216901945678346, 0.0, 0.0, 0.0, 0.22684974207663636, 1.4564553082728662, 0.182720528082649, 0.06653876275957457, 0.0, 0.6274289004185675, 0.0, 0.5406773490882926, 0.27822503396119075, 0.09306814986560254, 0.29383387946651635, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6230315233142055, 0.9648450369390001, 0.5108864746688708, 0.0, 0.36840186712596623, 0.22805280632725022, 0.3227837297104921, 0.0, 0.0, 1.0240377781477865, 1.591691363735423, 0.0, 0.7314261723085571, 0.0, 0.0, 0.45960855697902075, 0.0, 0.43090812739194506, 0.0, 0.3151566241591431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3316537279743774, 0.27121053456867833, 0.0, 0.0, 0.0, 0.4001441487163798, 0.5176883689695168, 0.709966180421066, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2820721974309868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03908614977726766, 0.3083419381268706, 0.39485756407874356, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11511559935873378, 0.014949071584157462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03837527794969671, 0.5550679626138529, 0.0, 0.5726479000113842, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07921974732295421, 0.0, 0.0, 0.0, 0.0, 0.0035899619569867425, 0.0, 0.0, 0.2145099852146676, 0.0, 0.0, 0.0, 0.0, 0.21950879525087302, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09449503481586544, 0.21750142180635595, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32517302304866597, 0.0, 0.0, 0.21035924507266648, 0.0, 0.0, 0.0, 0.0, 0.23713929311152726, 0.21014969427312982, 0.0, 0.0, 0.0, 0.5438936968761621, 0.3401858622480347, 0.0, 0.0, 0.0016527476817863266, 0.0, 0.007935034693483254, 0.0, 0.0, 0.1293317927443745, 0.0, 0.0, 0.0, 0.14368792432790373, 0.0, 0.0, 0.0, 0.0, 0.35799313982296815, 0.29274964408302323, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029011045367568582, 0.0017598612427760815, 0.0, 0.0, 0.052226530139838453, 0.0, 0.0, 0.0, 0.0, 0.16473710872422428, 0.30447392294316244, 0.0, 0.0, 0.0, 0.01743076710707649, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0410977515599671, 0.0, 0.7547303898094697, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.30609022817115544, 0.0, 0.0, 0.0, 0.0, 0.2105854322132866, 0.5327502455990605, 0.13588517657689586, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24025222517683778, 0.0, 0.007535433506713796, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6979826303937671, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07410089446237557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4744178097104391, 0.0, 0.0, 0.0, 0.0, 0.1876471424480227, 0.28765608703745393, 0.2182401932056186, 0.04951496463333951, 0.0, 0.12909838655126593, 0.0, 0.3498317042975124, 0.0, 0.37087394843503463, 0.47981991008621333, 1.1039610273729945, 0.4455015727565212, 0.05271863458630144, 0.19600051626620313, 0.0, 0.2316343353202091, 0.0, 0.0, 0.0, 0.4641150152741525, 0.15018757716392944, 0.21321672818601478, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21465382837602517, 0.0, 0.0, 0.0, 0.00020573734093944128, 0.0, 0.0, 0.0, 0.16929708918389524, 0.0160994805386141, 0.0, 0.049657622486024976, 0.0, 0.017886560545272906, 0.1659947360027911, 0.0, 0.0, 0.0, 0.0, 0.11420185948155097, 0.0, 0.07369142147184715, 0.0, 0.33489361550116375, 0.46496602729779546, 0.0036113530199273977, 0.044355883980523376, 0.08156217107736623, 0.0, 0.006501263051750628, 0.24089225482763127, 0.0, 0.0, 0.3861226254326548, 0.020506805168434235, 0.0, 0.24677384860885976, 0.0, 0.0, 0.002169816793370778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.041148887129618034, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9769895490432172, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5923828848790257, 0.0, 0.10196835358593148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.328835975725113, 0.0, 0.0, 0.4036325198584592, 0.0, 0.0, 0.3602983928676083, 0.0, 0.0, 0.9557725494193282, 0.0, 0.43908662544955285, 0.0, 0.0, 0.46995823158461947, 0.0, 0.14454935060909507, 0.0, 0.010668331756011213, 0.6280409324984255, 0.03247332779409237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10592998533424139, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33399686192793854, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7833052225131492, 0.0, 0.0, 0.0, 0.24187208135853017, 0.0, 0.071730719549714, 0.0, 0.11407352111288803, 0.0, 0.44957678307373006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5736773783460691, 0.0692607726207939, 0.0, 0.0, 0.4731102083446558, 0.7389531951966185, 0.028778055517643567, 0.22717903301955947, 0.0, 0.1436855444728547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.49542697237169275, 0.0, 0.3454532351491738, 0.11660950163380673, 0.0, 0.0, 0.0, 0.0, 0.027794605242432064, 0.0, 0.0, 0.0, 0.132905538815286, 0.0, 0.05497259939520882, 0.0, 0.0, 1.2625457513449383, 0.39900789022393385, 0.0, 0.0, 0.0, 0.0, 0.4114559732861708, 0.4741201455820112, 0.0, 0.21077985466978477, 0.0, 0.0, 0.7439578224938339, 0.42232266256074136, 0.4876343844944301, 0.6687496612887465, 0.115118039848854, 0.0, 0.0, 0.0, 0.0, 0.05260063632046659, 0.07759581613885312, 0.028401563276091994, 0.0, 0.056334062495133036, 0.17033944709766455, 0.9153107777932757, 0.0, 0.004924424218813803, 0.057509247607595904, 0.0, 0.7491964746840728, 0.05934710743113981, 0.0, 0.0, 0.0, 0.22238712275143577, 0.4319949206677921, 0.025917686578643146, 0.031803778192995576, 0.0, 0.6741550137516903, 0.009328571558732282, 0.0, 0.0, 0.0, 0.0, 0.5515037103010108, 0.6987172178395075, 0.0, 0.0, 0.0, 0.060174729085221354, 0.0, 0.0, 0.08734749351416564, 0.6636235071641303, 0.09580942583448827, 0.0, 0.0, 0.08996354325075087, 0.04525348827838765, 0.5824434635228624, 0.05819874866882126, 0.12005968761069685, 0.0, 0.0, 0.3313826265510977, 0.0, 0.058140773587614956, 0.6127791119711469, 0.0, 0.0, 0.15047559500471339, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6662085079556846, 0.3400843601648346, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22423528728211575, 0.0, 0.0, 0.0, 0.0, 0.3636963343220375, 0.5756271502974593, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046502182852265775, 0.0, 0.04891501992848654, 0.0, 0.0, 0.0, 0.9407814186149676, 0.09733317875961926, 0.0, 0.11672934297033466, 0.0, 0.6385588104227404, 0.0, 0.022206283976929683, 0.09845615426665595, 0.03496357540242119, 0.0, 0.0, 0.0, 0.0, 0.03380456195194267, 0.18543528221865002, 0.12144811071150914, 0.0, 0.03755694741887004, 0.16907550573428723, 0.0, 0.0, 0.1443808645220955, 0.0, 0.1003230425739974, 0.12879502114342312, 0.0647357901808317, 0.0, 0.5084955858844, 0.08519692951707138, 0.007582866204885641, 0.0, 0.03198292312330168, 0.0, 0.1209623049539008, 0.0, 0.12387820451445683, 0.0, 0.0, 0.043058753609476905, 0.03493082633215086, 0.0, 0.0, 0.043777031505228374, 0.016261326799286477, 0.16693678030096587, 0.0, 0.0]), 42), 42)),)

