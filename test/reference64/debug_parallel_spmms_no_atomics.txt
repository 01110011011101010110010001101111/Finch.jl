julia> @finch begin
        CR .= 0
        for i = _
            for j = _
                for k = _
                    CR[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(CR = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0, 0.0, 0.0, 0.004334271763144113, 0.19866796375858964, 0.05511378591134807, 0.37983318302160474, 0.0, 0.8329001018500983, 0.30166090609024687, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3289219868624648, 0.0, 0.15511617694464214, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18499887719318273, 0.0, 0.0, 0.0, 0.19259223593916178, 0.0, 0.0, 0.0638915620449496, 0.08125561754012361, 0.17659717140450376, 0.0, 0.0, 0.1525779960421776, 0.0, 0.0, 0.0, 0.0, 0.29196799055295863, 0.0, 0.0, 0.0, 0.5551667524029037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4594337414860906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3019846765891098, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18892258579898563, 0.0, 0.2395310548375412, 0.0, 0.359229579293753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00028478282759787334, 0.0, 0.3432275013790356, 0.0, 0.0, 0.2972839943189292, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19540404365846636, 0.0, 0.0, 0.0, 0.0, 0.18546216890582629, 0.0, 0.0, 0.00916099623463154, 0.0, 0.034731738855626984, 0.4853088859188271, 0.0076713909171271105, 0.0, 0.0, 0.5640422262893099, 0.0, 0.5950501234938147, 0.040920156633969085, 0.0014920661060395996, 0.0, 0.08081663901361853, 0.8825202410318094, 0.0, 0.006218090636031004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3280555235834662, 0.03812149119614576, 0.0, 0.05190957475021724, 0.0, 0.06033708819060236, 0.0, 0.10124291140603807, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5085302772239245, 0.0, 0.5334307954394755, 0.0, 0.0, 0.2328402544634388, 0.0, 0.005816161517311894, 1.5317318593263975, 0.0, 0.46738125332221536, 0.0, 0.09813163178688389, 0.4394739409933983, 0.0, 0.26102098784262384, 0.0, 0.0, 0.0, 0.0, 0.8804155352247423, 0.0, 0.0, 0.48240400321886606, 0.3763091458911055, 0.6151496795666793, 0.0, 0.27268296540705483, 0.0, 0.0014290584461454537, 0.0, 0.0, 0.0, 0.5294637260878591, 0.0, 0.0, 0.0, 0.31181862629226015, 0.0, 0.34152287293383227, 0.1986997482112381, 0.2527011428302912, 0.0, 0.434749031361829, 0.0, 0.4472075705110786, 0.4092092126159339, 0.2807873539680881, 0.17401521117970595, 0.0, 0.0, 0.43733948883443596, 0.5485693747724284, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0983266821553666, 0.05373620000037918, 0.49612300100963586, 0.0, 0.0, 1.075151264033199, 0.0, 0.18716105879611752, 0.03950202184626144, 0.0, 0.0, 0.7852572057826378, 0.0, 0.0, 0.08756438716142145, 0.27446263899702106, 0.0, 0.0, 0.07204260281080108, 0.0, 0.09291620627436281, 0.23270017215835156, 0.23824566618105675, 0.0, 0.0, 0.0, 0.13665872084048475, 0.0, 0.0, 0.0067937121060301624, 0.1164801427986044, 0.0, 0.5691621002508566, 0.4713388661713567, 0.41358181631025637, 0.0, 0.3128607440091297, 0.0, 0.0, 0.20424594336339066, 0.0, 0.0, 0.7442286193021344, 0.10553727794892508, 0.0, 0.30186901767997887, 0.0, 0.0, 0.0, 0.32291683563950796, 0.07758151598185183, 0.0, 0.347786535534979, 0.7360230359944353, 0.0, 0.0, 0.0, 0.8757273805535595, 0.41322710295268655, 0.0, 0.25679112297862333, 0.17774302673574222, 0.5841939760860391, 0.0, 0.0, 0.0, 0.13300779742001662, 0.16915583797139946, 0.0, 0.0, 0.0, 0.7164740353603845, 0.45156468002631484, 0.0, 0.27217311439601866, 0.7084784766971438, 0.06835326871026481, 0.0, 0.0, 0.0, 0.2861016237491002, 1.0700436562849107, 0.135740840434777, 0.2599664462430113, 0.45536369102376945, 0.0, 0.0, 0.0, 0.16460938932017066, 0.9424546774296569, 0.4980075807530193, 0.501649117240962, 0.9215918221852557, 0.0, 0.5227646022766553, 0.46865433734477785, 0.0, 0.1899494468105599, 0.0, 0.14204872822901377, 0.7891527088162923, 0.2800775160398023, 0.0, 0.0, 0.6038144125549446, 0.0, 0.4101280976499608, 0.15813547044885462, 0.4381740566617935, 0.0025678619479343042, 0.5913196381458786, 0.01581382625018507, 0.4317825799661969, 0.0, 0.5361217114844467, 0.16404576969544252, 0.0, 0.03623369485950333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19327505078226082, 0.1125279795645968, 0.0, 0.38171505913646114, 0.0, 0.0012820843611261621, 0.0, 0.0, 0.017018867107690273, 0.0, 0.018573326277702022, 0.7755881641075008, 0.12996991545193906, 0.0, 0.18005632805213245, 0.0, 0.2451805301156947, 0.0, 0.3747092870507497, 0.0, 0.4781929115858312, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.35214721497251084, 0.0, 0.020304118651362432, 0.14466279308909658, 0.626862644216891, 0.0, 0.0, 1.5549351131665947, 0.30737844662289215, 0.0, 0.0, 0.0, 0.0, 0.009701873820892715, 0.2520412914553624, 0.018396624198184493, 0.0, 0.7982268230101296, 0.0, 0.9201682818000929, 0.0, 0.0, 0.7342011144064367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09396246898802936, 0.0, 0.5073258744317697, 0.22284199906817412, 0.10151898941699102, 0.03180992568768135, 0.0, 0.5543686635557524, 0.0, 0.0, 0.0, 0.22367207355552704, 0.0, 0.0, 0.0, 0.40880306935546484, 0.0, 0.42113696363931935, 0.0, 0.0, 0.0, 0.43326306412219423, 0.39232717474974355, 0.0, 0.0, 0.0, 0.013675206005982276, 0.0, 0.2603519242467926, 0.3502025254743841, 0.14312660068897712, 0.34775162340880994, 0.20310882214946477, 0.0, 0.27127891969267615, 0.0, 0.0, 0.020541404680713544, 0.0, 0.0, 0.1991870263338475, 0.0, 0.0, 0.2456237488004554, 0.12123860127246498, 0.2079074067296303, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24646978770136238, 0.15225808974571345, 0.0, 0.0, 0.46988488666249073, 0.16461741428636234, 0.4415386194506142, 0.05631807188893752, 0.0611318669096905, 0.0, 0.0, 0.0, 0.5215534136524774, 0.0, 0.0, 0.6274626509368973, 0.038256585847320265, 0.530070634494204, 0.0, 0.7877809268797783, 0.25601250351282806, 0.37534186876141573, 0.700013095645165, 0.0, 0.29431110179251513, 0.22882177649965688, 1.0569304890861846, 0.0, 0.0, 0.0, 0.0, 0.02833923205698933, 0.02572736738919585, 0.0, 0.1150475064446459, 0.24371977071087828, 0.0, 0.0, 0.044121908414131224, 0.0771055384425919, 0.09695789652483547, 0.0, 0.0, 0.0, 0.12020713596947674, 0.045249684273424934, 0.8721213827038133, 0.0, 0.11731035534021127, 0.37569616707373005, 0.0, 0.0, 0.9570606189270042, 0.9086284112098039, 0.20027308709305888, 0.4268041763647709, 0.0, 0.0, 0.015909944043088663, 0.0, 0.7658973441527888, 0.0, 0.19507759787979717, 0.3226660152512004, 0.23629991340839918, 0.0, 0.13848101841219138, 0.0, 0.0, 0.24578615843203896, 0.0, 0.0, 0.10167993256702677, 0.0, 0.0, 0.12538470338440244, 0.9934363459809055, 0.10613146591697438, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6421265863141966, 0.0, 0.5400344618364347, 0.025731716559651077, 0.0, 0.0, 0.0, 0.007521097208030019, 0.5455660054203588, 0.0, 1.1209603429913297, 0.0, 0.0, 0.5234607155600556, 0.0, 0.023977651748011766, 0.0, 0.0, 0.0, 0.27798322874225667, 0.0, 0.35612425891926536, 0.0, 0.7529913555346315, 0.6214420882570796, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12146506806205652, 0.0, 0.3210215267482459, 0.0, 0.0, 0.0, 0.33419799384234544, 0.0, 0.0, 0.15359727591834285, 0.19534099821264672, 0.525863863274946, 0.0, 0.0, 0.34569678733330644, 0.0, 0.0, 0.3415674397857982, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.523004359259986, 0.0, 0.22235180304591923, 0.0, 0.00635695661054767, 0.0, 0.0, 0.0843846185513356, 0.0, 0.0, 0.0, 0.8855039996037297, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27294552017018114, 0.0, 0.3412878895624823, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5864608505347954, 0.0, 0.10380099346078346, 0.02524889717437322, 0.0, 0.0, 0.38157075730498896, 0.0, 0.0, 0.0, 0.45469484904050655, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26459586877877683, 0.18314523505716396, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22372814141562178, 0.0, 0.0, 0.0, 0.6276845275284666, 0.18331203344516112, 0.25689720406507865, 0.04048914090414101, 0.0, 0.016969058333515752, 0.0, 0.5911567891086366, 0.37358991094373184, 0.21597413111529512, 0.09535966106843696, 0.021909623318208237, 0.49412659112391777, 0.39229708641560146, 0.0, 0.0, 0.5239905526116051, 0.22383442432789932, 0.0, 0.408761833081477, 0.7858251636352953, 0.0, 0.03095169444374481, 1.0134328993353359, 0.0, 0.5783663988186404, 0.023349871375665725, 0.4068090650141155, 0.3701047391436106, 0.7170357796952346, 0.31327392774306395, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.39350494784782397, 0.07595368484327951, 0.4868292236818003, 0.046986031631648606, 0.530207151960747, 0.01037806421927761, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05535788985307855, 0.7394134258901907, 0.17106635844034493, 0.9821443208953105, 0.0, 0.008962807708770178, 0.0, 0.0, 0.11897566020800357, 0.0, 0.0, 0.0, 1.2828090910806784, 0.0, 0.4691443493685695, 0.0, 0.07022467062008277, 0.18231155361607748, 0.41302392682414213, 0.0, 0.13696413696930718, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.423686950315378, 0.0, 0.5297732126841366, 0.0, 0.10086209606266386, 0.4105665706595916, 0.2899236467445781, 0.08479167779863508, 0.0, 1.1607169616195545, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44056670549109944, 0.0036426275414389976, 0.22268017654499572, 0.70259131988822, 0.021981713208965886, 0.0, 0.0, 0.0, 0.6644482698446481, 0.5559910462571956, 0.9081368976941899, 0.9545466439826781, 0.0, 0.5800820874802947, 0.6131061961157059, 0.0, 0.3387938943904664, 0.0, 0.41233232521209184, 0.6892320892052487, 0.9168247018477867, 0.7592095444656833, 0.0, 0.7080274090242662, 0.0, 0.0, 0.7409103227795685, 0.4891910517462003, 0.0, 0.0, 0.6996898878138225, 0.18201720925245757, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.033060035104523045, 0.0, 0.0, 0.0, 0.0, 0.0010434833431631984, 0.0, 0.0, 0.0, 0.010921241238865733, 0.0, 0.015498170387323498, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011618017340433275, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2104054739321471, 0.0, 0.0, 0.0, 0.051249741424478176, 0.0, 0.0, 0.0, 0.09744956100917274, 0.0, 0.0, 0.0, 0.0, 0.10766993925868745, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18951483497887683, 0.5969374604132631, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6248091001560899, 0.0, 0.0, 0.0, 0.0, 0.18617398114562853, 0.0530079909103574, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14076422137873648, 0.3823151552281139, 0.28535858102685935, 0.3054264047421474, 0.0, 0.0, 0.5425991952026914, 0.16566917168527906, 0.0, 0.0, 0.24572721112234908, 0.0, 0.0, 0.311993347128524, 0.0, 1.0231061248638629, 0.5939954507294103, 0.5983388707717968, 0.7851882761904205, 0.0, 0.28209866162610053, 0.46470151179975694, 0.0, 0.22767102275324697, 0.0, 0.0, 0.3351791998251741, 0.0, 0.0, 0.0, 0.3258354466053841, 0.29514851487698524, 0.25858133097678015, 0.1886151008053812, 0.5226293862659805, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0442888167727012, 0.0, 0.002505192641426779, 0.0, 0.0, 0.07664740890754863, 0.0, 0.0, 0.013363010227595206, 0.00048725362450601585, 0.002419244088482629, 0.026391726291740327, 0.0, 0.0, 0.03460717393330891, 0.0, 0.13624247986958654, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012449069569846, 0.0, 0.016951747875799564, 0.0, 0.0197038621774241, 0.0, 0.033062191640471625, 0.07310509907314719, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06556034095953286, 0.0, 0.02434740964367002, 0.0, 0.0, 0.13592547532623306, 0.0, 0.030022610188286405, 0.03795734899490777, 0.008699370747195389, 0.0, 0.0, 0.18940737674761182, 0.16014434992779022, 0.0058393216511455045, 0.0, 0.3162824676834449, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14919154588027264, 0.0, 0.20315232851529888, 0.0, 0.27582528532656464, 0.0, 0.3962223404212856, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00142596300027198, 0.0, 0.008781597914028754, 0.2917830655969515, 0.0, 0.0, 0.3010196130390429, 1.085161420061463, 0.0, 0.8818413438369698, 0.0, 0.14807253735972709, 0.6682230613653689, 0.6738718955158736, 0.0, 0.2921120382906695, 0.0, 0.23016863424694362, 0.44343897402347887, 0.0, 0.0, 1.2859761704109314, 1.188418751121335, 0.0, 0.4620501506594876, 0.23396469855670338, 0.0, 0.03498671823904876, 0.0, 0.0, 0.3392611399043929, 0.20497564811318061, 0.0, 0.4183535169904413, 0.20649711391931772, 0.520902106226398, 0.0, 0.0, 0.16125806820737418, 0.5576988707246249, 0.0, 0.0, 0.0, 0.11391623120681071, 0.0, 0.0, 0.09151851725619714, 0.0, 0.0, 1.197254000111945, 0.0, 0.8775820846458859, 0.0, 0.5629797627532018, 0.6687057520979063, 0.341227000414515, 0.0, 0.25482576175079585, 0.0, 0.39537376259476137, 0.4437592920364036, 0.0, 0.0, 0.5927285333298421, 0.651986303075751, 0.0, 1.0434591895456948, 0.0, 0.0, 0.03501199088471498, 0.0, 0.0, 0.3395062050893736, 0.15619471223870085, 0.0, 0.41865571453083017, 0.20664627704903807, 0.48238135206809674, 0.08860575544817058, 0.0, 0.005695035181786341, 0.42497543210880456, 0.0, 0.23934194260454025, 0.30438882267274336, 0.2874361482000896, 0.0, 0.0, 0.6245971595264426, 0.0, 0.3639559852559372, 0.0, 0.0, 0.20575691959192335, 0.0, 0.5994751724692355, 0.0, 0.0, 0.0, 0.0, 0.002904710270589501, 0.0, 0.27722048222724693, 0.15720637186052522, 0.0, 0.40582979792580204, 0.3276141191929195, 0.0, 0.5216472712210337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022828121623522435, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.111114554054197, 0.0, 0.0, 0.005523835481493179, 0.12589753624924738, 0.4223352609451343, 0.19512225997871732, 0.9646462752755526, 0.0, 0.0, 0.908234546074678, 0.0, 0.1303083320340461, 0.4180115467796097, 0.3502002628164635, 0.005797584702932196, 0.620011620521792, 0.6673813159226286, 0.0030737631986123407, 0.0, 0.42144642014638856, 1.0414378536891689, 0.6289622414694364, 1.5563606265339154, 0.08385396416867873, 0.3115994197854627, 0.7117829870894571, 0.6381078265340546, 0.0, 0.23989801450652898, 0.0, 0.5826228436020011, 0.8457142287971168, 0.0, 0.0, 0.0, 1.0193786877434905, 0.0, 0.0, 0.19971832516875276, 0.5533950636153777, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21472741909783344, 0.39082851888093567, 0.44716245502350144, 0.2879713896328549, 0.5511368948723377, 0.33565164359388877, 1.3517828371888283, 0.0, 0.09957607530690628, 0.0, 0.0, 0.26103163435011795, 0.3965287786994862, 0.6597151598792015, 0.0, 0.34286845579856556, 0.3900059867245693, 0.34423839922938515, 0.0, 1.2842745238771571, 0.3694211002080512, 0.0, 0.05489702502604951, 0.3645945046142821, 0.0, 0.0, 0.08041277613458127, 0.2893532857853331, 0.0, 0.5419495915870312, 0.70973704371772, 0.06597297072421819, 0.0, 0.0, 0.28417074423649047, 0.45673560467870195, 0.1772454725351981, 0.22541615615699262, 0.08077018346809645, 0.5662548477384789, 0.07678831671440492, 0.7573984424681688, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4826509895037858, 0.0, 0.7156459126461995, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013563432420802306, 0.4559162518278489, 0.0, 0.0, 0.0, 0.0, 0.26262283944881243, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16505781512417061, 0.2933206286622639, 0.0, 0.0, 0.0, 0.0, 0.40289003757510067, 0.0, 0.0, 0.0, 0.6372941893353361, 0.0, 0.6237934507673457, 0.8070702584067075, 0.0, 0.0, 0.3379623279083573, 0.0, 0.46129991809862586, 0.7333015288992237, 0.21031442023566344, 0.0, 0.7041420713561054, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7760176053652288, 0.0, 1.0534236649773678, 0.0, 0.0, 0.8070780011372765, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4823716017908521, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6208306891807599, 0.0, 0.0, 0.01732988822073383, 0.2738833349341264, 0.10672374403860475, 0.0, 0.008043590060480383, 0.0, 0.055277084044920086, 0.0, 0.0, 0.17943961322559207, 0.0, 0.0, 0.0, 0.0, 0.0, 0.718108290096912, 0.0, 0.0, 0.0, 0.008728384689335622, 0.0, 0.1697763404331079, 0.11586384141065409, 0.0, 0.0, 0.0, 0.8848305575683707, 0.0, 0.8449968950182224, 0.0, 0.0, 0.0, 0.6706152313255382, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8308196053978146, 0.0, 0.9175002824131868, 0.7530109173431534, 0.022342425693822682, 0.0, 0.0035915524463436964, 0.15288943835905894, 0.0, 0.5199425421439203, 0.11917728725476327, 0.3521011644457409, 0.0, 0.822488739774635, 0.513543493422637, 0.2981570434312181, 0.0, 0.0, 1.2701086711578897, 0.0, 0.11247893732510689, 0.0, 0.039332390318575425, 0.0, 0.11102635670488926, 0.0, 0.15118325075382752, 0.0, 0.1757278339799435, 0.0, 0.2948633760880991, 0.15371975197149906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04453230950399372, 0.0, 0.05568267952698087, 0.0, 0.21714106205060016, 0.0, 0.0, 0.0, 0.0, 0.7213227431120439, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2701718137836067, 0.0, 0.06123281377737304, 0.4007297534572718, 0.0, 0.0, 0.02350606599699272, 0.0, 0.47270164239805296, 0.39554302818161685, 0.39843532224568856, 0.0, 0.0, 0.46004399190355705, 0.41242580548884994, 0.0, 0.15086754157289264, 0.0, 0.0, 0.5466072621606027, 0.18414300130678235, 0.2087681866233734, 0.0, 0.5313695523968773, 0.0, 0.0, 0.2807883504872339, 0.34802019073798685, 0.0, 0.09776175774612722, 0.1924014129464173, 0.0, 0.0, 0.0, 0.2995245510564702, 0.1439415371971276, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09612835370305864, 0.0, 0.07178793274997196, 0.0, 0.0, 0.0, 0.529229742886101, 0.0, 0.0, 0.5898615183638936, 0.0, 0.1996504875270556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07685007162501065, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4724449949223617, 0.0, 0.21985358976027652, 0.0, 0.0, 0.0, 0.6511036698044336, 0.5394783324223208, 0.9048355234024855, 0.0, 0.0, 0.0, 0.008134079103649166, 0.17236516549262873, 0.0, 0.7108281927774538, 0.0, 0.0, 0.0, 0.8245834715993663, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22989626507902333, 0.12442621872484774, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3037122713618223, 0.0, 0.0, 0.008259351424073042, 0.4378900919766154, 0.05086408498890661, 0.31974807102322866, 0.0, 0.0, 0.7960980638875322, 0.0689135689704732, 0.0, 0.0, 0.0, 0.20359294387478752, 0.9097633301509839, 0.0, 0.6841090251831579, 0.0, 0.6154584825649704, 0.7335150023327748, 1.015637287349907, 0.0, 0.0779165260149525, 1.1277817941657742, 0.0, 0.0, 0.3457097793442966, 0.6958114560471119, 0.22297907055084815, 0.02617735461213243, 0.8271866794310433, 0.0, 0.25383801660715655, 0.0, 0.6564805026462143, 0.4519424562793308, 0.8175733700968422, 0.26495100977979114, 0.0, 0.0, 0.0, 0.33910693050583907, 0.0, 0.0, 0.0, 0.4889218480481046, 0.0, 0.3925649791995639, 1.5627863234111485, 0.8201598086100989, 0.28474440170274185, 1.0514838770367962, 0.010290531779792767, 0.0, 0.0, 1.5282247448720891, 0.4202365445740981, 0.38631591015261757, 0.07827208426350975, 0.002001482371009478, 0.19978631068641475, 0.10840878806592323, 0.8849580765325739, 0.7025926608640433, 0.21468725597950544, 0.6515266028885637, 0.304659479667613, 0.0, 0.5203661079080213, 0.1989832785613875, 0.22866153178235926, 0.2129788116497342, 0.0511368043793991, 0.0, 0.06963236972450947, 0.0, 0.08093717687353573, 0.0, 0.3254291820176872, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4944791980843742, 0.5417407856384763, 0.0, 0.4343792686365096, 1.5442785077006684, 0.0, 0.13746522377571999, 0.0, 0.0, 0.0, 0.0, 0.01933245239450143, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44623451745418, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.30502005455201636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014872981553739924, 0.0, 0.0, 0.0007393802182031884, 0.0, 0.14653699929908884, 0.0, 0.0, 0.26188312990151436, 0.028159286104530473, 0.0, 0.4084686341089707, 0.9235151006259961, 0.5957064186534903, 0.0, 0.0, 0.5535709817494249, 0.0, 0.25891613288066156, 0.07783302925410085, 0.2939591160629263, 0.1987201643010643, 0.0, 0.09767541007114018, 0.4120929351702914, 0.22225830848448824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13739476210353194, 0.0, 0.037597515302596925, 0.0, 0.0, 0.31874062595288627, 0.33536722163001637, 0.0, 0.0, 0.0, 0.48352996362981304, 0.0, 0.38851289261478805, 0.6830254018960756, 0.3364461001285706, 0.8253161071254982, 0.0, 0.2348841918025421, 0.0, 0.4205219693225024, 0.5314960091024892, 0.0, 0.0, 0.10178270195612651, 0.0, 0.0, 0.780835547454152, 0.5872223143240599, 0.2503853190515238, 0.0, 0.0, 0.6150976289535472, 0.0, 0.09445718365268194, 0.0, 0.0, 0.17331386223779208, 1.5139967025892487, 0.0, 0.0031631037123804793, 0.0, 0.0, 0.20592512324808152, 0.0, 0.03635874582185372, 0.0, 0.20018457152414249, 0.22068196040992705, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = ((ex.bodies[1]).bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    result = nothing
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = (Finch).moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = (Finch).moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            phase_start_2 = max(1, 1 + fld(A_lvl.shape[1] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                for i_7 = phase_start_2:phase_stop_2
                    B_lvl_q = B_lvl_ptr[1]
                    B_lvl_q_stop = B_lvl_ptr[1 + 1]
                    if B_lvl_q < B_lvl_q_stop
                        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                    else
                        B_lvl_i_stop = 0
                    end
                    phase_stop_3 = min(B_lvl.shape[2], B_lvl_i_stop)
                    if phase_stop_3 >= 1
                        if B_lvl_tbl2[B_lvl_q] < 1
                            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        while true
                            B_lvl_i = B_lvl_tbl2[B_lvl_q]
                            B_lvl_q_step = B_lvl_q
                            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                            end
                            if B_lvl_i < phase_stop_3
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_5 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_5 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_5
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_6 = min(B_lvl_i_2, phase_stop_5, A_lvl_i)
                                        if A_lvl_i == phase_stop_6 && B_lvl_i_2 == phase_stop_6
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_7 = min(i_7, A_lvl_i_stop_2)
                                            if phase_stop_7 >= i_7
                                                if A_lvl_tbl1[A_lvl_q] < i_7
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_7
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_9 = min(A_lvl_i_2, phase_stop_7)
                                                        if A_lvl_i_2 == phase_stop_9
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_6
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_6
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_6 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            else
                                phase_stop_14 = min(B_lvl_i, phase_stop_3)
                                if B_lvl_i == phase_stop_14
                                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_14
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_15 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_15 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_15
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_16 = min(B_lvl_i_2, A_lvl_i, phase_stop_15)
                                            if A_lvl_i == phase_stop_16 && B_lvl_i_2 == phase_stop_16
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_17 = min(i_7, A_lvl_i_stop_4)
                                                if phase_stop_17 >= i_7
                                                    if A_lvl_tbl1[A_lvl_q] < i_7
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_17
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_19 = min(A_lvl_i_4, phase_stop_17)
                                                            if A_lvl_i_4 == phase_stop_19
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_16
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_16
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_16 + 1
                                        end
                                    end
                                    B_lvl_q = B_lvl_q_step
                                end
                                break
                            end
                        end
                    end
                end
            end
        end
    resize!(val, A_lvl.shape[1] * B_lvl.shape[2])
    result = (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
    result
end
julia> @finch begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0, 0.0, 0.0, 0.004334271763144113, 0.19866796375858964, 0.05511378591134807, 0.37983318302160474, 0.0, 0.8329001018500983, 0.30166090609024687, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3289219868624648, 0.0, 0.15511617694464214, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18499887719318273, 0.0, 0.0, 0.0, 0.19259223593916178, 0.0, 0.0, 0.0638915620449496, 0.08125561754012361, 0.17659717140450376, 0.0, 0.0, 0.1525779960421776, 0.0, 0.0, 0.0, 0.0, 0.29196799055295863, 0.0, 0.0, 0.0, 0.5551667524029037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4594337414860906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3019846765891098, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18892258579898563, 0.0, 0.2395310548375412, 0.0, 0.359229579293753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00028478282759787334, 0.0, 0.3432275013790356, 0.0, 0.0, 0.2972839943189292, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19540404365846636, 0.0, 0.0, 0.0, 0.0, 0.18546216890582629, 0.0, 0.0, 0.00916099623463154, 0.0, 0.034731738855626984, 0.4853088859188271, 0.0076713909171271105, 0.0, 0.0, 0.5640422262893099, 0.0, 0.5950501234938147, 0.040920156633969085, 0.0014920661060395996, 0.0, 0.08081663901361853, 0.8825202410318094, 0.0, 0.006218090636031004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3280555235834662, 0.03812149119614576, 0.0, 0.05190957475021724, 0.0, 0.06033708819060236, 0.0, 0.10124291140603807, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5085302772239245, 0.0, 0.5334307954394755, 0.0, 0.0, 0.2328402544634388, 0.0, 0.005816161517311894, 1.5317318593263975, 0.0, 0.46738125332221536, 0.0, 0.09813163178688389, 0.4394739409933983, 0.0, 0.26102098784262384, 0.0, 0.0, 0.0, 0.0, 0.8804155352247423, 0.0, 0.0, 0.48240400321886606, 0.3763091458911055, 0.6151496795666793, 0.0, 0.27268296540705483, 0.0, 0.0014290584461454537, 0.0, 0.0, 0.0, 0.5294637260878591, 0.0, 0.0, 0.0, 0.31181862629226015, 0.0, 0.34152287293383227, 0.1986997482112381, 0.2527011428302912, 0.0, 0.434749031361829, 0.0, 0.4472075705110786, 0.4092092126159339, 0.2807873539680881, 0.17401521117970595, 0.0, 0.0, 0.43733948883443596, 0.5485693747724284, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0983266821553666, 0.05373620000037918, 0.49612300100963586, 0.0, 0.0, 1.075151264033199, 0.0, 0.18716105879611752, 0.03950202184626144, 0.0, 0.0, 0.7852572057826378, 0.0, 0.0, 0.08756438716142145, 0.27446263899702106, 0.0, 0.0, 0.07204260281080108, 0.0, 0.09291620627436281, 0.23270017215835156, 0.23824566618105675, 0.0, 0.0, 0.0, 0.13665872084048475, 0.0, 0.0, 0.0067937121060301624, 0.1164801427986044, 0.0, 0.5691621002508566, 0.4713388661713567, 0.41358181631025637, 0.0, 0.3128607440091297, 0.0, 0.0, 0.20424594336339066, 0.0, 0.0, 0.7442286193021344, 0.10553727794892508, 0.0, 0.30186901767997887, 0.0, 0.0, 0.0, 0.32291683563950796, 0.07758151598185183, 0.0, 0.347786535534979, 0.7360230359944353, 0.0, 0.0, 0.0, 0.8757273805535595, 0.41322710295268655, 0.0, 0.25679112297862333, 0.17774302673574222, 0.5841939760860391, 0.0, 0.0, 0.0, 0.13300779742001662, 0.16915583797139946, 0.0, 0.0, 0.0, 0.7164740353603845, 0.45156468002631484, 0.0, 0.27217311439601866, 0.7084784766971438, 0.06835326871026481, 0.0, 0.0, 0.0, 0.2861016237491002, 1.0700436562849107, 0.135740840434777, 0.2599664462430113, 0.45536369102376945, 0.0, 0.0, 0.0, 0.16460938932017066, 0.9424546774296569, 0.4980075807530193, 0.501649117240962, 0.9215918221852557, 0.0, 0.5227646022766553, 0.46865433734477785, 0.0, 0.1899494468105599, 0.0, 0.14204872822901377, 0.7891527088162923, 0.2800775160398023, 0.0, 0.0, 0.6038144125549446, 0.0, 0.4101280976499608, 0.15813547044885462, 0.4381740566617935, 0.0025678619479343042, 0.5913196381458786, 0.01581382625018507, 0.4317825799661969, 0.0, 0.5361217114844467, 0.16404576969544252, 0.0, 0.03623369485950333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19327505078226082, 0.1125279795645968, 0.0, 0.38171505913646114, 0.0, 0.0012820843611261621, 0.0, 0.0, 0.017018867107690273, 0.0, 0.018573326277702022, 0.7755881641075008, 0.12996991545193906, 0.0, 0.18005632805213245, 0.0, 0.2451805301156947, 0.0, 0.3747092870507497, 0.0, 0.4781929115858312, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.35214721497251084, 0.0, 0.020304118651362432, 0.14466279308909658, 0.626862644216891, 0.0, 0.0, 1.5549351131665947, 0.30737844662289215, 0.0, 0.0, 0.0, 0.0, 0.009701873820892715, 0.2520412914553624, 0.018396624198184493, 0.0, 0.7982268230101296, 0.0, 0.9201682818000929, 0.0, 0.0, 0.7342011144064367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09396246898802936, 0.0, 0.5073258744317697, 0.22284199906817412, 0.10151898941699102, 0.03180992568768135, 0.0, 0.5543686635557524, 0.0, 0.0, 0.0, 0.22367207355552704, 0.0, 0.0, 0.0, 0.40880306935546484, 0.0, 0.42113696363931935, 0.0, 0.0, 0.0, 0.43326306412219423, 0.39232717474974355, 0.0, 0.0, 0.0, 0.013675206005982276, 0.0, 0.2603519242467926, 0.3502025254743841, 0.14312660068897712, 0.34775162340880994, 0.20310882214946477, 0.0, 0.27127891969267615, 0.0, 0.0, 0.020541404680713544, 0.0, 0.0, 0.1991870263338475, 0.0, 0.0, 0.2456237488004554, 0.12123860127246498, 0.2079074067296303, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24646978770136238, 0.15225808974571345, 0.0, 0.0, 0.46988488666249073, 0.16461741428636234, 0.4415386194506142, 0.05631807188893752, 0.0611318669096905, 0.0, 0.0, 0.0, 0.5215534136524774, 0.0, 0.0, 0.6274626509368973, 0.038256585847320265, 0.530070634494204, 0.0, 0.7877809268797783, 0.25601250351282806, 0.37534186876141573, 0.700013095645165, 0.0, 0.29431110179251513, 0.22882177649965688, 1.0569304890861846, 0.0, 0.0, 0.0, 0.0, 0.02833923205698933, 0.02572736738919585, 0.0, 0.1150475064446459, 0.24371977071087828, 0.0, 0.0, 0.044121908414131224, 0.0771055384425919, 0.09695789652483547, 0.0, 0.0, 0.0, 0.12020713596947674, 0.045249684273424934, 0.8721213827038133, 0.0, 0.11731035534021127, 0.37569616707373005, 0.0, 0.0, 0.9570606189270042, 0.9086284112098039, 0.20027308709305888, 0.4268041763647709, 0.0, 0.0, 0.015909944043088663, 0.0, 0.7658973441527888, 0.0, 0.19507759787979717, 0.3226660152512004, 0.23629991340839918, 0.0, 0.13848101841219138, 0.0, 0.0, 0.24578615843203896, 0.0, 0.0, 0.10167993256702677, 0.0, 0.0, 0.12538470338440244, 0.9934363459809055, 0.10613146591697438, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6421265863141966, 0.0, 0.5400344618364347, 0.025731716559651077, 0.0, 0.0, 0.0, 0.007521097208030019, 0.5455660054203588, 0.0, 1.1209603429913297, 0.0, 0.0, 0.5234607155600556, 0.0, 0.023977651748011766, 0.0, 0.0, 0.0, 0.27798322874225667, 0.0, 0.35612425891926536, 0.0, 0.7529913555346315, 0.6214420882570796, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12146506806205652, 0.0, 0.3210215267482459, 0.0, 0.0, 0.0, 0.33419799384234544, 0.0, 0.0, 0.15359727591834285, 0.19534099821264672, 0.525863863274946, 0.0, 0.0, 0.34569678733330644, 0.0, 0.0, 0.3415674397857982, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.523004359259986, 0.0, 0.22235180304591923, 0.0, 0.00635695661054767, 0.0, 0.0, 0.0843846185513356, 0.0, 0.0, 0.0, 0.8855039996037297, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27294552017018114, 0.0, 0.3412878895624823, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5864608505347954, 0.0, 0.10380099346078346, 0.02524889717437322, 0.0, 0.0, 0.38157075730498896, 0.0, 0.0, 0.0, 0.45469484904050655, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26459586877877683, 0.18314523505716396, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22372814141562178, 0.0, 0.0, 0.0, 0.6276845275284666, 0.18331203344516112, 0.25689720406507865, 0.04048914090414101, 0.0, 0.016969058333515752, 0.0, 0.5911567891086366, 0.37358991094373184, 0.21597413111529512, 0.09535966106843696, 0.021909623318208237, 0.49412659112391777, 0.39229708641560146, 0.0, 0.0, 0.5239905526116051, 0.22383442432789932, 0.0, 0.408761833081477, 0.7858251636352953, 0.0, 0.03095169444374481, 1.0134328993353359, 0.0, 0.5783663988186404, 0.023349871375665725, 0.4068090650141155, 0.3701047391436106, 0.7170357796952346, 0.31327392774306395, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.39350494784782397, 0.07595368484327951, 0.4868292236818003, 0.046986031631648606, 0.530207151960747, 0.01037806421927761, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05535788985307855, 0.7394134258901907, 0.17106635844034493, 0.9821443208953105, 0.0, 0.008962807708770178, 0.0, 0.0, 0.11897566020800357, 0.0, 0.0, 0.0, 1.2828090910806784, 0.0, 0.4691443493685695, 0.0, 0.07022467062008277, 0.18231155361607748, 0.41302392682414213, 0.0, 0.13696413696930718, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.423686950315378, 0.0, 0.5297732126841366, 0.0, 0.10086209606266386, 0.4105665706595916, 0.2899236467445781, 0.08479167779863508, 0.0, 1.1607169616195545, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44056670549109944, 0.0036426275414389976, 0.22268017654499572, 0.70259131988822, 0.021981713208965886, 0.0, 0.0, 0.0, 0.6644482698446481, 0.5559910462571956, 0.9081368976941899, 0.9545466439826781, 0.0, 0.5800820874802947, 0.6131061961157059, 0.0, 0.3387938943904664, 0.0, 0.41233232521209184, 0.6892320892052487, 0.9168247018477867, 0.7592095444656833, 0.0, 0.7080274090242662, 0.0, 0.0, 0.7409103227795685, 0.4891910517462003, 0.0, 0.0, 0.6996898878138225, 0.18201720925245757, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.033060035104523045, 0.0, 0.0, 0.0, 0.0, 0.0010434833431631984, 0.0, 0.0, 0.0, 0.010921241238865733, 0.0, 0.015498170387323498, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011618017340433275, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2104054739321471, 0.0, 0.0, 0.0, 0.051249741424478176, 0.0, 0.0, 0.0, 0.09744956100917274, 0.0, 0.0, 0.0, 0.0, 0.10766993925868745, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18951483497887683, 0.5969374604132631, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6248091001560899, 0.0, 0.0, 0.0, 0.0, 0.18617398114562853, 0.0530079909103574, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14076422137873648, 0.3823151552281139, 0.28535858102685935, 0.3054264047421474, 0.0, 0.0, 0.5425991952026914, 0.16566917168527906, 0.0, 0.0, 0.24572721112234908, 0.0, 0.0, 0.311993347128524, 0.0, 1.0231061248638629, 0.5939954507294103, 0.5983388707717968, 0.7851882761904205, 0.0, 0.28209866162610053, 0.46470151179975694, 0.0, 0.22767102275324697, 0.0, 0.0, 0.3351791998251741, 0.0, 0.0, 0.0, 0.3258354466053841, 0.29514851487698524, 0.25858133097678015, 0.1886151008053812, 0.5226293862659805, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0442888167727012, 0.0, 0.002505192641426779, 0.0, 0.0, 0.07664740890754863, 0.0, 0.0, 0.013363010227595206, 0.00048725362450601585, 0.002419244088482629, 0.026391726291740327, 0.0, 0.0, 0.03460717393330891, 0.0, 0.13624247986958654, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012449069569846, 0.0, 0.016951747875799564, 0.0, 0.0197038621774241, 0.0, 0.033062191640471625, 0.07310509907314719, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06556034095953286, 0.0, 0.02434740964367002, 0.0, 0.0, 0.13592547532623306, 0.0, 0.030022610188286405, 0.03795734899490777, 0.008699370747195389, 0.0, 0.0, 0.18940737674761182, 0.16014434992779022, 0.0058393216511455045, 0.0, 0.3162824676834449, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14919154588027264, 0.0, 0.20315232851529888, 0.0, 0.27582528532656464, 0.0, 0.3962223404212856, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00142596300027198, 0.0, 0.008781597914028754, 0.2917830655969515, 0.0, 0.0, 0.3010196130390429, 1.085161420061463, 0.0, 0.8818413438369698, 0.0, 0.14807253735972709, 0.6682230613653689, 0.6738718955158736, 0.0, 0.2921120382906695, 0.0, 0.23016863424694362, 0.44343897402347887, 0.0, 0.0, 1.2859761704109314, 1.188418751121335, 0.0, 0.4620501506594876, 0.23396469855670338, 0.0, 0.03498671823904876, 0.0, 0.0, 0.3392611399043929, 0.20497564811318061, 0.0, 0.4183535169904413, 0.20649711391931772, 0.520902106226398, 0.0, 0.0, 0.16125806820737418, 0.5576988707246249, 0.0, 0.0, 0.0, 0.11391623120681071, 0.0, 0.0, 0.09151851725619714, 0.0, 0.0, 1.197254000111945, 0.0, 0.8775820846458859, 0.0, 0.5629797627532018, 0.6687057520979063, 0.341227000414515, 0.0, 0.25482576175079585, 0.0, 0.39537376259476137, 0.4437592920364036, 0.0, 0.0, 0.5927285333298421, 0.651986303075751, 0.0, 1.0434591895456948, 0.0, 0.0, 0.03501199088471498, 0.0, 0.0, 0.3395062050893736, 0.15619471223870085, 0.0, 0.41865571453083017, 0.20664627704903807, 0.48238135206809674, 0.08860575544817058, 0.0, 0.005695035181786341, 0.42497543210880456, 0.0, 0.23934194260454025, 0.30438882267274336, 0.2874361482000896, 0.0, 0.0, 0.6245971595264426, 0.0, 0.3639559852559372, 0.0, 0.0, 0.20575691959192335, 0.0, 0.5994751724692355, 0.0, 0.0, 0.0, 0.0, 0.002904710270589501, 0.0, 0.27722048222724693, 0.15720637186052522, 0.0, 0.40582979792580204, 0.3276141191929195, 0.0, 0.5216472712210337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022828121623522435, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.111114554054197, 0.0, 0.0, 0.005523835481493179, 0.12589753624924738, 0.4223352609451343, 0.19512225997871732, 0.9646462752755526, 0.0, 0.0, 0.908234546074678, 0.0, 0.1303083320340461, 0.4180115467796097, 0.3502002628164635, 0.005797584702932196, 0.620011620521792, 0.6673813159226286, 0.0030737631986123407, 0.0, 0.42144642014638856, 1.0414378536891689, 0.6289622414694364, 1.5563606265339154, 0.08385396416867873, 0.3115994197854627, 0.7117829870894571, 0.6381078265340546, 0.0, 0.23989801450652898, 0.0, 0.5826228436020011, 0.8457142287971168, 0.0, 0.0, 0.0, 1.0193786877434905, 0.0, 0.0, 0.19971832516875276, 0.5533950636153777, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21472741909783344, 0.39082851888093567, 0.44716245502350144, 0.2879713896328549, 0.5511368948723377, 0.33565164359388877, 1.3517828371888283, 0.0, 0.09957607530690628, 0.0, 0.0, 0.26103163435011795, 0.3965287786994862, 0.6597151598792015, 0.0, 0.34286845579856556, 0.3900059867245693, 0.34423839922938515, 0.0, 1.2842745238771571, 0.3694211002080512, 0.0, 0.05489702502604951, 0.3645945046142821, 0.0, 0.0, 0.08041277613458127, 0.2893532857853331, 0.0, 0.5419495915870312, 0.70973704371772, 0.06597297072421819, 0.0, 0.0, 0.28417074423649047, 0.45673560467870195, 0.1772454725351981, 0.22541615615699262, 0.08077018346809645, 0.5662548477384789, 0.07678831671440492, 0.7573984424681688, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4826509895037858, 0.0, 0.7156459126461995, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013563432420802306, 0.4559162518278489, 0.0, 0.0, 0.0, 0.0, 0.26262283944881243, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16505781512417061, 0.2933206286622639, 0.0, 0.0, 0.0, 0.0, 0.40289003757510067, 0.0, 0.0, 0.0, 0.6372941893353361, 0.0, 0.6237934507673457, 0.8070702584067075, 0.0, 0.0, 0.3379623279083573, 0.0, 0.46129991809862586, 0.7333015288992237, 0.21031442023566344, 0.0, 0.7041420713561054, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7760176053652288, 0.0, 1.0534236649773678, 0.0, 0.0, 0.8070780011372765, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4823716017908521, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6208306891807599, 0.0, 0.0, 0.01732988822073383, 0.2738833349341264, 0.10672374403860475, 0.0, 0.008043590060480383, 0.0, 0.055277084044920086, 0.0, 0.0, 0.17943961322559207, 0.0, 0.0, 0.0, 0.0, 0.0, 0.718108290096912, 0.0, 0.0, 0.0, 0.008728384689335622, 0.0, 0.1697763404331079, 0.11586384141065409, 0.0, 0.0, 0.0, 0.8848305575683707, 0.0, 0.8449968950182224, 0.0, 0.0, 0.0, 0.6706152313255382, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8308196053978146, 0.0, 0.9175002824131868, 0.7530109173431534, 0.022342425693822682, 0.0, 0.0035915524463436964, 0.15288943835905894, 0.0, 0.5199425421439203, 0.11917728725476327, 0.3521011644457409, 0.0, 0.822488739774635, 0.513543493422637, 0.2981570434312181, 0.0, 0.0, 1.2701086711578897, 0.0, 0.11247893732510689, 0.0, 0.039332390318575425, 0.0, 0.11102635670488926, 0.0, 0.15118325075382752, 0.0, 0.1757278339799435, 0.0, 0.2948633760880991, 0.15371975197149906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04453230950399372, 0.0, 0.05568267952698087, 0.0, 0.21714106205060016, 0.0, 0.0, 0.0, 0.0, 0.7213227431120439, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2701718137836067, 0.0, 0.06123281377737304, 0.4007297534572718, 0.0, 0.0, 0.02350606599699272, 0.0, 0.47270164239805296, 0.39554302818161685, 0.39843532224568856, 0.0, 0.0, 0.46004399190355705, 0.41242580548884994, 0.0, 0.15086754157289264, 0.0, 0.0, 0.5466072621606027, 0.18414300130678235, 0.2087681866233734, 0.0, 0.5313695523968773, 0.0, 0.0, 0.2807883504872339, 0.34802019073798685, 0.0, 0.09776175774612722, 0.1924014129464173, 0.0, 0.0, 0.0, 0.2995245510564702, 0.1439415371971276, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09612835370305864, 0.0, 0.07178793274997196, 0.0, 0.0, 0.0, 0.529229742886101, 0.0, 0.0, 0.5898615183638936, 0.0, 0.1996504875270556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07685007162501065, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4724449949223617, 0.0, 0.21985358976027652, 0.0, 0.0, 0.0, 0.6511036698044336, 0.5394783324223208, 0.9048355234024855, 0.0, 0.0, 0.0, 0.008134079103649166, 0.17236516549262873, 0.0, 0.7108281927774538, 0.0, 0.0, 0.0, 0.8245834715993663, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22989626507902333, 0.12442621872484774, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3037122713618223, 0.0, 0.0, 0.008259351424073042, 0.4378900919766154, 0.05086408498890661, 0.31974807102322866, 0.0, 0.0, 0.7960980638875322, 0.0689135689704732, 0.0, 0.0, 0.0, 0.20359294387478752, 0.9097633301509839, 0.0, 0.6841090251831579, 0.0, 0.6154584825649704, 0.7335150023327748, 1.015637287349907, 0.0, 0.0779165260149525, 1.1277817941657742, 0.0, 0.0, 0.3457097793442966, 0.6958114560471119, 0.22297907055084815, 0.02617735461213243, 0.8271866794310433, 0.0, 0.25383801660715655, 0.0, 0.6564805026462143, 0.4519424562793308, 0.8175733700968422, 0.26495100977979114, 0.0, 0.0, 0.0, 0.33910693050583907, 0.0, 0.0, 0.0, 0.4889218480481046, 0.0, 0.3925649791995639, 1.5627863234111485, 0.8201598086100989, 0.28474440170274185, 1.0514838770367962, 0.010290531779792767, 0.0, 0.0, 1.5282247448720891, 0.4202365445740981, 0.38631591015261757, 0.07827208426350975, 0.002001482371009478, 0.19978631068641475, 0.10840878806592323, 0.8849580765325739, 0.7025926608640433, 0.21468725597950544, 0.6515266028885637, 0.304659479667613, 0.0, 0.5203661079080213, 0.1989832785613875, 0.22866153178235926, 0.2129788116497342, 0.0511368043793991, 0.0, 0.06963236972450947, 0.0, 0.08093717687353573, 0.0, 0.3254291820176872, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4944791980843742, 0.5417407856384763, 0.0, 0.4343792686365096, 1.5442785077006684, 0.0, 0.13746522377571999, 0.0, 0.0, 0.0, 0.0, 0.01933245239450143, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44623451745418, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.30502005455201636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014872981553739924, 0.0, 0.0, 0.0007393802182031884, 0.0, 0.14653699929908884, 0.0, 0.0, 0.26188312990151436, 0.028159286104530473, 0.0, 0.4084686341089707, 0.9235151006259961, 0.5957064186534903, 0.0, 0.0, 0.5535709817494249, 0.0, 0.25891613288066156, 0.07783302925410085, 0.2939591160629263, 0.1987201643010643, 0.0, 0.09767541007114018, 0.4120929351702914, 0.22225830848448824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13739476210353194, 0.0, 0.037597515302596925, 0.0, 0.0, 0.31874062595288627, 0.33536722163001637, 0.0, 0.0, 0.0, 0.48352996362981304, 0.0, 0.38851289261478805, 0.6830254018960756, 0.3364461001285706, 0.8253161071254982, 0.0, 0.2348841918025421, 0.0, 0.4205219693225024, 0.5314960091024892, 0.0, 0.0, 0.10178270195612651, 0.0, 0.0, 0.780835547454152, 0.5872223143240599, 0.2503853190515238, 0.0, 0.0, 0.6150976289535472, 0.0, 0.09445718365268194, 0.0, 0.0, 0.17331386223779208, 1.5139967025892487, 0.0, 0.0031631037123804793, 0.0, 0.0, 0.20592512324808152, 0.0, 0.03635874582185372, 0.0, 0.20018457152414249, 0.22068196040992705, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = ((ex.bodies[1]).bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    result = nothing
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    for i_4 = 1:A_lvl.shape[1]
        val = Ct_lvl_2_val
        Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
        B_lvl_ptr_2 = B_lvl_ptr
        B_lvl_ptr = (Finch).moveto(B_lvl_ptr, CPU(Threads.nthreads()))
        B_lvl_tbl1_2 = B_lvl_tbl1
        B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
        B_lvl_tbl2_2 = B_lvl_tbl2
        B_lvl_tbl2 = (Finch).moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
        val_2 = B_lvl_val
        B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
        A_lvl_ptr_2 = A_lvl_ptr
        A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
        A_lvl_tbl1_2 = A_lvl_tbl1
        A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
        A_lvl_tbl2_2 = A_lvl_tbl2
        A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
        val_3 = A_lvl_val
        A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
        Threads.@threads for i_5 = 1:Threads.nthreads()
                B_lvl_q = B_lvl_ptr[1]
                B_lvl_q_stop = B_lvl_ptr[1 + 1]
                if B_lvl_q < B_lvl_q_stop
                    B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                else
                    B_lvl_i_stop = 0
                end
                phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_5 + -1), Threads.nthreads()))
                phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_5, Threads.nthreads()))
                if phase_stop_2 >= phase_start_2
                    if B_lvl_tbl2[B_lvl_q] < phase_start_2
                        B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    while true
                        B_lvl_i = B_lvl_tbl2[B_lvl_q]
                        B_lvl_q_step = B_lvl_q
                        if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                            B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        if B_lvl_i < phase_stop_2
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_4, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_4
                                            if A_lvl_tbl1[A_lvl_q] < i_4
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        else
                            phase_stop_13 = min(B_lvl_i, phase_stop_2)
                            if B_lvl_i == phase_stop_13
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_4, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_4
                                                if A_lvl_tbl1[A_lvl_q] < i_4
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            end
                            break
                        end
                    end
                end
            end
        Ct_lvl_2_val = val
        B_lvl_ptr = B_lvl_ptr_2
        B_lvl_tbl1 = B_lvl_tbl1_2
        B_lvl_tbl2 = B_lvl_tbl2_2
        B_lvl_val = val_2
        A_lvl_ptr = A_lvl_ptr_2
        A_lvl_tbl1 = A_lvl_tbl1_2
        A_lvl_tbl2 = A_lvl_tbl2_2
        A_lvl_val = val_3
    end
    resize!(Ct_lvl_2_val, A_lvl.shape[1] * B_lvl.shape[2])
    result = (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
    result
end
julia> @finch begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0, 0.0, 0.0, 0.004334271763144113, 0.19866796375858964, 0.05511378591134807, 0.37983318302160474, 0.0, 0.8329001018500983, 0.30166090609024687, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3289219868624648, 0.0, 0.15511617694464214, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18499887719318273, 0.0, 0.0, 0.0, 0.19259223593916178, 0.0, 0.0, 0.0638915620449496, 0.08125561754012361, 0.17659717140450376, 0.0, 0.0, 0.1525779960421776, 0.0, 0.0, 0.0, 0.0, 0.29196799055295863, 0.0, 0.0, 0.0, 0.5551667524029037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4594337414860906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3019846765891098, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18892258579898563, 0.0, 0.2395310548375412, 0.0, 0.359229579293753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00028478282759787334, 0.0, 0.3432275013790356, 0.0, 0.0, 0.2972839943189292, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19540404365846636, 0.0, 0.0, 0.0, 0.0, 0.18546216890582629, 0.0, 0.0, 0.00916099623463154, 0.0, 0.034731738855626984, 0.4853088859188271, 0.0076713909171271105, 0.0, 0.0, 0.5640422262893099, 0.0, 0.5950501234938147, 0.040920156633969085, 0.0014920661060395996, 0.0, 0.08081663901361853, 0.8825202410318094, 0.0, 0.006218090636031004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3280555235834662, 0.03812149119614576, 0.0, 0.05190957475021724, 0.0, 0.06033708819060236, 0.0, 0.10124291140603807, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5085302772239245, 0.0, 0.5334307954394755, 0.0, 0.0, 0.2328402544634388, 0.0, 0.005816161517311894, 1.5317318593263975, 0.0, 0.46738125332221536, 0.0, 0.09813163178688389, 0.4394739409933983, 0.0, 0.26102098784262384, 0.0, 0.0, 0.0, 0.0, 0.8804155352247423, 0.0, 0.0, 0.48240400321886606, 0.3763091458911055, 0.6151496795666793, 0.0, 0.27268296540705483, 0.0, 0.0014290584461454537, 0.0, 0.0, 0.0, 0.5294637260878591, 0.0, 0.0, 0.0, 0.31181862629226015, 0.0, 0.34152287293383227, 0.1986997482112381, 0.2527011428302912, 0.0, 0.434749031361829, 0.0, 0.4472075705110786, 0.4092092126159339, 0.2807873539680881, 0.17401521117970595, 0.0, 0.0, 0.43733948883443596, 0.5485693747724284, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0983266821553666, 0.05373620000037918, 0.49612300100963586, 0.0, 0.0, 1.075151264033199, 0.0, 0.18716105879611752, 0.03950202184626144, 0.0, 0.0, 0.7852572057826378, 0.0, 0.0, 0.08756438716142145, 0.27446263899702106, 0.0, 0.0, 0.07204260281080108, 0.0, 0.09291620627436281, 0.23270017215835156, 0.23824566618105675, 0.0, 0.0, 0.0, 0.13665872084048475, 0.0, 0.0, 0.0067937121060301624, 0.1164801427986044, 0.0, 0.5691621002508566, 0.4713388661713567, 0.41358181631025637, 0.0, 0.3128607440091297, 0.0, 0.0, 0.20424594336339066, 0.0, 0.0, 0.7442286193021344, 0.10553727794892508, 0.0, 0.30186901767997887, 0.0, 0.0, 0.0, 0.32291683563950796, 0.07758151598185183, 0.0, 0.347786535534979, 0.7360230359944353, 0.0, 0.0, 0.0, 0.8757273805535595, 0.41322710295268655, 0.0, 0.25679112297862333, 0.17774302673574222, 0.5841939760860391, 0.0, 0.0, 0.0, 0.13300779742001662, 0.16915583797139946, 0.0, 0.0, 0.0, 0.7164740353603845, 0.45156468002631484, 0.0, 0.27217311439601866, 0.7084784766971438, 0.06835326871026481, 0.0, 0.0, 0.0, 0.2861016237491002, 1.0700436562849107, 0.135740840434777, 0.2599664462430113, 0.45536369102376945, 0.0, 0.0, 0.0, 0.16460938932017066, 0.9424546774296569, 0.4980075807530193, 0.501649117240962, 0.9215918221852557, 0.0, 0.5227646022766553, 0.46865433734477785, 0.0, 0.1899494468105599, 0.0, 0.14204872822901377, 0.7891527088162923, 0.2800775160398023, 0.0, 0.0, 0.6038144125549446, 0.0, 0.4101280976499608, 0.15813547044885462, 0.4381740566617935, 0.0025678619479343042, 0.5913196381458786, 0.01581382625018507, 0.4317825799661969, 0.0, 0.5361217114844467, 0.16404576969544252, 0.0, 0.03623369485950333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19327505078226082, 0.1125279795645968, 0.0, 0.38171505913646114, 0.0, 0.0012820843611261621, 0.0, 0.0, 0.017018867107690273, 0.0, 0.018573326277702022, 0.7755881641075008, 0.12996991545193906, 0.0, 0.18005632805213245, 0.0, 0.2451805301156947, 0.0, 0.3747092870507497, 0.0, 0.4781929115858312, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.35214721497251084, 0.0, 0.020304118651362432, 0.14466279308909658, 0.626862644216891, 0.0, 0.0, 1.5549351131665947, 0.30737844662289215, 0.0, 0.0, 0.0, 0.0, 0.009701873820892715, 0.2520412914553624, 0.018396624198184493, 0.0, 0.7982268230101296, 0.0, 0.9201682818000929, 0.0, 0.0, 0.7342011144064367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09396246898802936, 0.0, 0.5073258744317697, 0.22284199906817412, 0.10151898941699102, 0.03180992568768135, 0.0, 0.5543686635557524, 0.0, 0.0, 0.0, 0.22367207355552704, 0.0, 0.0, 0.0, 0.40880306935546484, 0.0, 0.42113696363931935, 0.0, 0.0, 0.0, 0.43326306412219423, 0.39232717474974355, 0.0, 0.0, 0.0, 0.013675206005982276, 0.0, 0.2603519242467926, 0.3502025254743841, 0.14312660068897712, 0.34775162340880994, 0.20310882214946477, 0.0, 0.27127891969267615, 0.0, 0.0, 0.020541404680713544, 0.0, 0.0, 0.1991870263338475, 0.0, 0.0, 0.2456237488004554, 0.12123860127246498, 0.2079074067296303, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24646978770136238, 0.15225808974571345, 0.0, 0.0, 0.46988488666249073, 0.16461741428636234, 0.4415386194506142, 0.05631807188893752, 0.0611318669096905, 0.0, 0.0, 0.0, 0.5215534136524774, 0.0, 0.0, 0.6274626509368973, 0.038256585847320265, 0.530070634494204, 0.0, 0.7877809268797783, 0.25601250351282806, 0.37534186876141573, 0.700013095645165, 0.0, 0.29431110179251513, 0.22882177649965688, 1.0569304890861846, 0.0, 0.0, 0.0, 0.0, 0.02833923205698933, 0.02572736738919585, 0.0, 0.1150475064446459, 0.24371977071087828, 0.0, 0.0, 0.044121908414131224, 0.0771055384425919, 0.09695789652483547, 0.0, 0.0, 0.0, 0.12020713596947674, 0.045249684273424934, 0.8721213827038133, 0.0, 0.11731035534021127, 0.37569616707373005, 0.0, 0.0, 0.9570606189270042, 0.9086284112098039, 0.20027308709305888, 0.4268041763647709, 0.0, 0.0, 0.015909944043088663, 0.0, 0.7658973441527888, 0.0, 0.19507759787979717, 0.3226660152512004, 0.23629991340839918, 0.0, 0.13848101841219138, 0.0, 0.0, 0.24578615843203896, 0.0, 0.0, 0.10167993256702677, 0.0, 0.0, 0.12538470338440244, 0.9934363459809055, 0.10613146591697438, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6421265863141966, 0.0, 0.5400344618364347, 0.025731716559651077, 0.0, 0.0, 0.0, 0.007521097208030019, 0.5455660054203588, 0.0, 1.1209603429913297, 0.0, 0.0, 0.5234607155600556, 0.0, 0.023977651748011766, 0.0, 0.0, 0.0, 0.27798322874225667, 0.0, 0.35612425891926536, 0.0, 0.7529913555346315, 0.6214420882570796, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12146506806205652, 0.0, 0.3210215267482459, 0.0, 0.0, 0.0, 0.33419799384234544, 0.0, 0.0, 0.15359727591834285, 0.19534099821264672, 0.525863863274946, 0.0, 0.0, 0.34569678733330644, 0.0, 0.0, 0.3415674397857982, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.523004359259986, 0.0, 0.22235180304591923, 0.0, 0.00635695661054767, 0.0, 0.0, 0.0843846185513356, 0.0, 0.0, 0.0, 0.8855039996037297, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27294552017018114, 0.0, 0.3412878895624823, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5864608505347954, 0.0, 0.10380099346078346, 0.02524889717437322, 0.0, 0.0, 0.38157075730498896, 0.0, 0.0, 0.0, 0.45469484904050655, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26459586877877683, 0.18314523505716396, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22372814141562178, 0.0, 0.0, 0.0, 0.6276845275284666, 0.18331203344516112, 0.25689720406507865, 0.04048914090414101, 0.0, 0.016969058333515752, 0.0, 0.5911567891086366, 0.37358991094373184, 0.21597413111529512, 0.09535966106843696, 0.021909623318208237, 0.49412659112391777, 0.39229708641560146, 0.0, 0.0, 0.5239905526116051, 0.22383442432789932, 0.0, 0.408761833081477, 0.7858251636352953, 0.0, 0.03095169444374481, 1.0134328993353359, 0.0, 0.5783663988186404, 0.023349871375665725, 0.4068090650141155, 0.3701047391436106, 0.7170357796952346, 0.31327392774306395, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.39350494784782397, 0.07595368484327951, 0.4868292236818003, 0.046986031631648606, 0.530207151960747, 0.01037806421927761, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05535788985307855, 0.7394134258901907, 0.17106635844034493, 0.9821443208953105, 0.0, 0.008962807708770178, 0.0, 0.0, 0.11897566020800357, 0.0, 0.0, 0.0, 1.2828090910806784, 0.0, 0.4691443493685695, 0.0, 0.07022467062008277, 0.18231155361607748, 0.41302392682414213, 0.0, 0.13696413696930718, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.423686950315378, 0.0, 0.5297732126841366, 0.0, 0.10086209606266386, 0.4105665706595916, 0.2899236467445781, 0.08479167779863508, 0.0, 1.1607169616195545, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44056670549109944, 0.0036426275414389976, 0.22268017654499572, 0.70259131988822, 0.021981713208965886, 0.0, 0.0, 0.0, 0.6644482698446481, 0.5559910462571956, 0.9081368976941899, 0.9545466439826781, 0.0, 0.5800820874802947, 0.6131061961157059, 0.0, 0.3387938943904664, 0.0, 0.41233232521209184, 0.6892320892052487, 0.9168247018477867, 0.7592095444656833, 0.0, 0.7080274090242662, 0.0, 0.0, 0.7409103227795685, 0.4891910517462003, 0.0, 0.0, 0.6996898878138225, 0.18201720925245757, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.033060035104523045, 0.0, 0.0, 0.0, 0.0, 0.0010434833431631984, 0.0, 0.0, 0.0, 0.010921241238865733, 0.0, 0.015498170387323498, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011618017340433275, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2104054739321471, 0.0, 0.0, 0.0, 0.051249741424478176, 0.0, 0.0, 0.0, 0.09744956100917274, 0.0, 0.0, 0.0, 0.0, 0.10766993925868745, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18951483497887683, 0.5969374604132631, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6248091001560899, 0.0, 0.0, 0.0, 0.0, 0.18617398114562853, 0.0530079909103574, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14076422137873648, 0.3823151552281139, 0.28535858102685935, 0.3054264047421474, 0.0, 0.0, 0.5425991952026914, 0.16566917168527906, 0.0, 0.0, 0.24572721112234908, 0.0, 0.0, 0.311993347128524, 0.0, 1.0231061248638629, 0.5939954507294103, 0.5983388707717968, 0.7851882761904205, 0.0, 0.28209866162610053, 0.46470151179975694, 0.0, 0.22767102275324697, 0.0, 0.0, 0.3351791998251741, 0.0, 0.0, 0.0, 0.3258354466053841, 0.29514851487698524, 0.25858133097678015, 0.1886151008053812, 0.5226293862659805, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0442888167727012, 0.0, 0.002505192641426779, 0.0, 0.0, 0.07664740890754863, 0.0, 0.0, 0.013363010227595206, 0.00048725362450601585, 0.002419244088482629, 0.026391726291740327, 0.0, 0.0, 0.03460717393330891, 0.0, 0.13624247986958654, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012449069569846, 0.0, 0.016951747875799564, 0.0, 0.0197038621774241, 0.0, 0.033062191640471625, 0.07310509907314719, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06556034095953286, 0.0, 0.02434740964367002, 0.0, 0.0, 0.13592547532623306, 0.0, 0.030022610188286405, 0.03795734899490777, 0.008699370747195389, 0.0, 0.0, 0.18940737674761182, 0.16014434992779022, 0.0058393216511455045, 0.0, 0.3162824676834449, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14919154588027264, 0.0, 0.20315232851529888, 0.0, 0.27582528532656464, 0.0, 0.3962223404212856, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00142596300027198, 0.0, 0.008781597914028754, 0.2917830655969515, 0.0, 0.0, 0.3010196130390429, 1.085161420061463, 0.0, 0.8818413438369698, 0.0, 0.14807253735972709, 0.6682230613653689, 0.6738718955158736, 0.0, 0.2921120382906695, 0.0, 0.23016863424694362, 0.44343897402347887, 0.0, 0.0, 1.2859761704109314, 1.188418751121335, 0.0, 0.4620501506594876, 0.23396469855670338, 0.0, 0.03498671823904876, 0.0, 0.0, 0.3392611399043929, 0.20497564811318061, 0.0, 0.4183535169904413, 0.20649711391931772, 0.520902106226398, 0.0, 0.0, 0.16125806820737418, 0.5576988707246249, 0.0, 0.0, 0.0, 0.11391623120681071, 0.0, 0.0, 0.09151851725619714, 0.0, 0.0, 1.197254000111945, 0.0, 0.8775820846458859, 0.0, 0.5629797627532018, 0.6687057520979063, 0.341227000414515, 0.0, 0.25482576175079585, 0.0, 0.39537376259476137, 0.4437592920364036, 0.0, 0.0, 0.5927285333298421, 0.651986303075751, 0.0, 1.0434591895456948, 0.0, 0.0, 0.03501199088471498, 0.0, 0.0, 0.3395062050893736, 0.15619471223870085, 0.0, 0.41865571453083017, 0.20664627704903807, 0.48238135206809674, 0.08860575544817058, 0.0, 0.005695035181786341, 0.42497543210880456, 0.0, 0.23934194260454025, 0.30438882267274336, 0.2874361482000896, 0.0, 0.0, 0.6245971595264426, 0.0, 0.3639559852559372, 0.0, 0.0, 0.20575691959192335, 0.0, 0.5994751724692355, 0.0, 0.0, 0.0, 0.0, 0.002904710270589501, 0.0, 0.27722048222724693, 0.15720637186052522, 0.0, 0.40582979792580204, 0.3276141191929195, 0.0, 0.5216472712210337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022828121623522435, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.111114554054197, 0.0, 0.0, 0.005523835481493179, 0.12589753624924738, 0.4223352609451343, 0.19512225997871732, 0.9646462752755526, 0.0, 0.0, 0.908234546074678, 0.0, 0.1303083320340461, 0.4180115467796097, 0.3502002628164635, 0.005797584702932196, 0.620011620521792, 0.6673813159226286, 0.0030737631986123407, 0.0, 0.42144642014638856, 1.0414378536891689, 0.6289622414694364, 1.5563606265339154, 0.08385396416867873, 0.3115994197854627, 0.7117829870894571, 0.6381078265340546, 0.0, 0.23989801450652898, 0.0, 0.5826228436020011, 0.8457142287971168, 0.0, 0.0, 0.0, 1.0193786877434905, 0.0, 0.0, 0.19971832516875276, 0.5533950636153777, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21472741909783344, 0.39082851888093567, 0.44716245502350144, 0.2879713896328549, 0.5511368948723377, 0.33565164359388877, 1.3517828371888283, 0.0, 0.09957607530690628, 0.0, 0.0, 0.26103163435011795, 0.3965287786994862, 0.6597151598792015, 0.0, 0.34286845579856556, 0.3900059867245693, 0.34423839922938515, 0.0, 1.2842745238771571, 0.3694211002080512, 0.0, 0.05489702502604951, 0.3645945046142821, 0.0, 0.0, 0.08041277613458127, 0.2893532857853331, 0.0, 0.5419495915870312, 0.70973704371772, 0.06597297072421819, 0.0, 0.0, 0.28417074423649047, 0.45673560467870195, 0.1772454725351981, 0.22541615615699262, 0.08077018346809645, 0.5662548477384789, 0.07678831671440492, 0.7573984424681688, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4826509895037858, 0.0, 0.7156459126461995, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013563432420802306, 0.4559162518278489, 0.0, 0.0, 0.0, 0.0, 0.26262283944881243, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16505781512417061, 0.2933206286622639, 0.0, 0.0, 0.0, 0.0, 0.40289003757510067, 0.0, 0.0, 0.0, 0.6372941893353361, 0.0, 0.6237934507673457, 0.8070702584067075, 0.0, 0.0, 0.3379623279083573, 0.0, 0.46129991809862586, 0.7333015288992237, 0.21031442023566344, 0.0, 0.7041420713561054, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7760176053652288, 0.0, 1.0534236649773678, 0.0, 0.0, 0.8070780011372765, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4823716017908521, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6208306891807599, 0.0, 0.0, 0.01732988822073383, 0.2738833349341264, 0.10672374403860475, 0.0, 0.008043590060480383, 0.0, 0.055277084044920086, 0.0, 0.0, 0.17943961322559207, 0.0, 0.0, 0.0, 0.0, 0.0, 0.718108290096912, 0.0, 0.0, 0.0, 0.008728384689335622, 0.0, 0.1697763404331079, 0.11586384141065409, 0.0, 0.0, 0.0, 0.8848305575683707, 0.0, 0.8449968950182224, 0.0, 0.0, 0.0, 0.6706152313255382, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8308196053978146, 0.0, 0.9175002824131868, 0.7530109173431534, 0.022342425693822682, 0.0, 0.0035915524463436964, 0.15288943835905894, 0.0, 0.5199425421439203, 0.11917728725476327, 0.3521011644457409, 0.0, 0.822488739774635, 0.513543493422637, 0.2981570434312181, 0.0, 0.0, 1.2701086711578897, 0.0, 0.11247893732510689, 0.0, 0.039332390318575425, 0.0, 0.11102635670488926, 0.0, 0.15118325075382752, 0.0, 0.1757278339799435, 0.0, 0.2948633760880991, 0.15371975197149906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04453230950399372, 0.0, 0.05568267952698087, 0.0, 0.21714106205060016, 0.0, 0.0, 0.0, 0.0, 0.7213227431120439, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2701718137836067, 0.0, 0.06123281377737304, 0.4007297534572718, 0.0, 0.0, 0.02350606599699272, 0.0, 0.47270164239805296, 0.39554302818161685, 0.39843532224568856, 0.0, 0.0, 0.46004399190355705, 0.41242580548884994, 0.0, 0.15086754157289264, 0.0, 0.0, 0.5466072621606027, 0.18414300130678235, 0.2087681866233734, 0.0, 0.5313695523968773, 0.0, 0.0, 0.2807883504872339, 0.34802019073798685, 0.0, 0.09776175774612722, 0.1924014129464173, 0.0, 0.0, 0.0, 0.2995245510564702, 0.1439415371971276, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09612835370305864, 0.0, 0.07178793274997196, 0.0, 0.0, 0.0, 0.529229742886101, 0.0, 0.0, 0.5898615183638936, 0.0, 0.1996504875270556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07685007162501065, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4724449949223617, 0.0, 0.21985358976027652, 0.0, 0.0, 0.0, 0.6511036698044336, 0.5394783324223208, 0.9048355234024855, 0.0, 0.0, 0.0, 0.008134079103649166, 0.17236516549262873, 0.0, 0.7108281927774538, 0.0, 0.0, 0.0, 0.8245834715993663, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22989626507902333, 0.12442621872484774, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3037122713618223, 0.0, 0.0, 0.008259351424073042, 0.4378900919766154, 0.05086408498890661, 0.31974807102322866, 0.0, 0.0, 0.7960980638875322, 0.0689135689704732, 0.0, 0.0, 0.0, 0.20359294387478752, 0.9097633301509839, 0.0, 0.6841090251831579, 0.0, 0.6154584825649704, 0.7335150023327748, 1.015637287349907, 0.0, 0.0779165260149525, 1.1277817941657742, 0.0, 0.0, 0.3457097793442966, 0.6958114560471119, 0.22297907055084815, 0.02617735461213243, 0.8271866794310433, 0.0, 0.25383801660715655, 0.0, 0.6564805026462143, 0.4519424562793308, 0.8175733700968422, 0.26495100977979114, 0.0, 0.0, 0.0, 0.33910693050583907, 0.0, 0.0, 0.0, 0.4889218480481046, 0.0, 0.3925649791995639, 1.5627863234111485, 0.8201598086100989, 0.28474440170274185, 1.0514838770367962, 0.010290531779792767, 0.0, 0.0, 1.5282247448720891, 0.4202365445740981, 0.38631591015261757, 0.07827208426350975, 0.002001482371009478, 0.19978631068641475, 0.10840878806592323, 0.8849580765325739, 0.7025926608640433, 0.21468725597950544, 0.6515266028885637, 0.304659479667613, 0.0, 0.5203661079080213, 0.1989832785613875, 0.22866153178235926, 0.2129788116497342, 0.0511368043793991, 0.0, 0.06963236972450947, 0.0, 0.08093717687353573, 0.0, 0.3254291820176872, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4944791980843742, 0.5417407856384763, 0.0, 0.4343792686365096, 1.5442785077006684, 0.0, 0.13746522377571999, 0.0, 0.0, 0.0, 0.0, 0.01933245239450143, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44623451745418, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.30502005455201636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014872981553739924, 0.0, 0.0, 0.0007393802182031884, 0.0, 0.14653699929908884, 0.0, 0.0, 0.26188312990151436, 0.028159286104530473, 0.0, 0.4084686341089707, 0.9235151006259961, 0.5957064186534903, 0.0, 0.0, 0.5535709817494249, 0.0, 0.25891613288066156, 0.07783302925410085, 0.2939591160629263, 0.1987201643010643, 0.0, 0.09767541007114018, 0.4120929351702914, 0.22225830848448824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13739476210353194, 0.0, 0.037597515302596925, 0.0, 0.0, 0.31874062595288627, 0.33536722163001637, 0.0, 0.0, 0.0, 0.48352996362981304, 0.0, 0.38851289261478805, 0.6830254018960756, 0.3364461001285706, 0.8253161071254982, 0.0, 0.2348841918025421, 0.0, 0.4205219693225024, 0.5314960091024892, 0.0, 0.0, 0.10178270195612651, 0.0, 0.0, 0.780835547454152, 0.5872223143240599, 0.2503853190515238, 0.0, 0.0, 0.6150976289535472, 0.0, 0.09445718365268194, 0.0, 0.0, 0.17331386223779208, 1.5139967025892487, 0.0, 0.0031631037123804793, 0.0, 0.0, 0.20592512324808152, 0.0, 0.03635874582185372, 0.0, 0.20018457152414249, 0.22068196040992705, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = ((ex.bodies[1]).bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    result = nothing
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = (Finch).moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = (Finch).moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            B_lvl_q = B_lvl_ptr[1]
            B_lvl_q_stop = B_lvl_ptr[1 + 1]
            if B_lvl_q < B_lvl_q_stop
                B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
            else
                B_lvl_i_stop = 0
            end
            phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                if B_lvl_tbl2[B_lvl_q] < phase_start_2
                    B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                end
                while true
                    B_lvl_i = B_lvl_tbl2[B_lvl_q]
                    B_lvl_q_step = B_lvl_q
                    if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                        B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    if B_lvl_i < phase_stop_2
                        Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                        for i_6 = 1:A_lvl.shape[1]
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_6
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_6, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_6
                                            if A_lvl_tbl1[A_lvl_q] < i_6
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_6, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                        end
                        B_lvl_q = B_lvl_q_step
                    else
                        phase_stop_13 = min(B_lvl_i, phase_stop_2)
                        if B_lvl_i == phase_stop_13
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                            for i_8 = 1:A_lvl.shape[1]
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_8
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_8, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_8
                                                if A_lvl_tbl1[A_lvl_q] < i_8
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_8, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        end
                        break
                    end
                end
            end
        end
    resize!(val, A_lvl.shape[1] * B_lvl.shape[2])
    result = (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
    result
end
julia> @finch begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0, 0.0, 0.0, 0.004334271763144113, 0.19866796375858964, 0.05511378591134807, 0.37983318302160474, 0.0, 0.8329001018500983, 0.30166090609024687, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3289219868624648, 0.0, 0.15511617694464214, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18499887719318273, 0.0, 0.0, 0.0, 0.19259223593916178, 0.0, 0.0, 0.0638915620449496, 0.08125561754012361, 0.17659717140450376, 0.0, 0.0, 0.1525779960421776, 0.0, 0.0, 0.0, 0.0, 0.29196799055295863, 0.0, 0.0, 0.0, 0.5551667524029037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4594337414860906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3019846765891098, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18892258579898563, 0.0, 0.2395310548375412, 0.0, 0.359229579293753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00028478282759787334, 0.0, 0.3432275013790356, 0.0, 0.0, 0.2972839943189292, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19540404365846636, 0.0, 0.0, 0.0, 0.0, 0.18546216890582629, 0.0, 0.0, 0.00916099623463154, 0.0, 0.034731738855626984, 0.4853088859188271, 0.0076713909171271105, 0.0, 0.0, 0.5640422262893099, 0.0, 0.5950501234938147, 0.040920156633969085, 0.0014920661060395996, 0.0, 0.08081663901361853, 0.8825202410318094, 0.0, 0.006218090636031004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3280555235834662, 0.03812149119614576, 0.0, 0.05190957475021724, 0.0, 0.06033708819060236, 0.0, 0.10124291140603807, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5085302772239245, 0.0, 0.5334307954394755, 0.0, 0.0, 0.2328402544634388, 0.0, 0.005816161517311894, 1.5317318593263975, 0.0, 0.46738125332221536, 0.0, 0.09813163178688389, 0.4394739409933983, 0.0, 0.26102098784262384, 0.0, 0.0, 0.0, 0.0, 0.8804155352247423, 0.0, 0.0, 0.48240400321886606, 0.3763091458911055, 0.6151496795666793, 0.0, 0.27268296540705483, 0.0, 0.0014290584461454537, 0.0, 0.0, 0.0, 0.5294637260878591, 0.0, 0.0, 0.0, 0.31181862629226015, 0.0, 0.34152287293383227, 0.1986997482112381, 0.2527011428302912, 0.0, 0.434749031361829, 0.0, 0.4472075705110786, 0.4092092126159339, 0.2807873539680881, 0.17401521117970595, 0.0, 0.0, 0.43733948883443596, 0.5485693747724284, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0983266821553666, 0.05373620000037918, 0.49612300100963586, 0.0, 0.0, 1.075151264033199, 0.0, 0.18716105879611752, 0.03950202184626144, 0.0, 0.0, 0.7852572057826378, 0.0, 0.0, 0.08756438716142145, 0.27446263899702106, 0.0, 0.0, 0.07204260281080108, 0.0, 0.09291620627436281, 0.23270017215835156, 0.23824566618105675, 0.0, 0.0, 0.0, 0.13665872084048475, 0.0, 0.0, 0.0067937121060301624, 0.1164801427986044, 0.0, 0.5691621002508566, 0.4713388661713567, 0.41358181631025637, 0.0, 0.3128607440091297, 0.0, 0.0, 0.20424594336339066, 0.0, 0.0, 0.7442286193021344, 0.10553727794892508, 0.0, 0.30186901767997887, 0.0, 0.0, 0.0, 0.32291683563950796, 0.07758151598185183, 0.0, 0.347786535534979, 0.7360230359944353, 0.0, 0.0, 0.0, 0.8757273805535595, 0.41322710295268655, 0.0, 0.25679112297862333, 0.17774302673574222, 0.5841939760860391, 0.0, 0.0, 0.0, 0.13300779742001662, 0.16915583797139946, 0.0, 0.0, 0.0, 0.7164740353603845, 0.45156468002631484, 0.0, 0.27217311439601866, 0.7084784766971438, 0.06835326871026481, 0.0, 0.0, 0.0, 0.2861016237491002, 1.0700436562849107, 0.135740840434777, 0.2599664462430113, 0.45536369102376945, 0.0, 0.0, 0.0, 0.16460938932017066, 0.9424546774296569, 0.4980075807530193, 0.501649117240962, 0.9215918221852557, 0.0, 0.5227646022766553, 0.46865433734477785, 0.0, 0.1899494468105599, 0.0, 0.14204872822901377, 0.7891527088162923, 0.2800775160398023, 0.0, 0.0, 0.6038144125549446, 0.0, 0.4101280976499608, 0.15813547044885462, 0.4381740566617935, 0.0025678619479343042, 0.5913196381458786, 0.01581382625018507, 0.4317825799661969, 0.0, 0.5361217114844467, 0.16404576969544252, 0.0, 0.03623369485950333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19327505078226082, 0.1125279795645968, 0.0, 0.38171505913646114, 0.0, 0.0012820843611261621, 0.0, 0.0, 0.017018867107690273, 0.0, 0.018573326277702022, 0.7755881641075008, 0.12996991545193906, 0.0, 0.18005632805213245, 0.0, 0.2451805301156947, 0.0, 0.3747092870507497, 0.0, 0.4781929115858312, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.35214721497251084, 0.0, 0.020304118651362432, 0.14466279308909658, 0.626862644216891, 0.0, 0.0, 1.5549351131665947, 0.30737844662289215, 0.0, 0.0, 0.0, 0.0, 0.009701873820892715, 0.2520412914553624, 0.018396624198184493, 0.0, 0.7982268230101296, 0.0, 0.9201682818000929, 0.0, 0.0, 0.7342011144064367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09396246898802936, 0.0, 0.5073258744317697, 0.22284199906817412, 0.10151898941699102, 0.03180992568768135, 0.0, 0.5543686635557524, 0.0, 0.0, 0.0, 0.22367207355552704, 0.0, 0.0, 0.0, 0.40880306935546484, 0.0, 0.42113696363931935, 0.0, 0.0, 0.0, 0.43326306412219423, 0.39232717474974355, 0.0, 0.0, 0.0, 0.013675206005982276, 0.0, 0.2603519242467926, 0.3502025254743841, 0.14312660068897712, 0.34775162340880994, 0.20310882214946477, 0.0, 0.27127891969267615, 0.0, 0.0, 0.020541404680713544, 0.0, 0.0, 0.1991870263338475, 0.0, 0.0, 0.2456237488004554, 0.12123860127246498, 0.2079074067296303, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24646978770136238, 0.15225808974571345, 0.0, 0.0, 0.46988488666249073, 0.16461741428636234, 0.4415386194506142, 0.05631807188893752, 0.0611318669096905, 0.0, 0.0, 0.0, 0.5215534136524774, 0.0, 0.0, 0.6274626509368973, 0.038256585847320265, 0.530070634494204, 0.0, 0.7877809268797783, 0.25601250351282806, 0.37534186876141573, 0.700013095645165, 0.0, 0.29431110179251513, 0.22882177649965688, 1.0569304890861846, 0.0, 0.0, 0.0, 0.0, 0.02833923205698933, 0.02572736738919585, 0.0, 0.1150475064446459, 0.24371977071087828, 0.0, 0.0, 0.044121908414131224, 0.0771055384425919, 0.09695789652483547, 0.0, 0.0, 0.0, 0.12020713596947674, 0.045249684273424934, 0.8721213827038133, 0.0, 0.11731035534021127, 0.37569616707373005, 0.0, 0.0, 0.9570606189270042, 0.9086284112098039, 0.20027308709305888, 0.4268041763647709, 0.0, 0.0, 0.015909944043088663, 0.0, 0.7658973441527888, 0.0, 0.19507759787979717, 0.3226660152512004, 0.23629991340839918, 0.0, 0.13848101841219138, 0.0, 0.0, 0.24578615843203896, 0.0, 0.0, 0.10167993256702677, 0.0, 0.0, 0.12538470338440244, 0.9934363459809055, 0.10613146591697438, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6421265863141966, 0.0, 0.5400344618364347, 0.025731716559651077, 0.0, 0.0, 0.0, 0.007521097208030019, 0.5455660054203588, 0.0, 1.1209603429913297, 0.0, 0.0, 0.5234607155600556, 0.0, 0.023977651748011766, 0.0, 0.0, 0.0, 0.27798322874225667, 0.0, 0.35612425891926536, 0.0, 0.7529913555346315, 0.6214420882570796, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12146506806205652, 0.0, 0.3210215267482459, 0.0, 0.0, 0.0, 0.33419799384234544, 0.0, 0.0, 0.15359727591834285, 0.19534099821264672, 0.525863863274946, 0.0, 0.0, 0.34569678733330644, 0.0, 0.0, 0.3415674397857982, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.523004359259986, 0.0, 0.22235180304591923, 0.0, 0.00635695661054767, 0.0, 0.0, 0.0843846185513356, 0.0, 0.0, 0.0, 0.8855039996037297, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27294552017018114, 0.0, 0.3412878895624823, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5864608505347954, 0.0, 0.10380099346078346, 0.02524889717437322, 0.0, 0.0, 0.38157075730498896, 0.0, 0.0, 0.0, 0.45469484904050655, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26459586877877683, 0.18314523505716396, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22372814141562178, 0.0, 0.0, 0.0, 0.6276845275284666, 0.18331203344516112, 0.25689720406507865, 0.04048914090414101, 0.0, 0.016969058333515752, 0.0, 0.5911567891086366, 0.37358991094373184, 0.21597413111529512, 0.09535966106843696, 0.021909623318208237, 0.49412659112391777, 0.39229708641560146, 0.0, 0.0, 0.5239905526116051, 0.22383442432789932, 0.0, 0.408761833081477, 0.7858251636352953, 0.0, 0.03095169444374481, 1.0134328993353359, 0.0, 0.5783663988186404, 0.023349871375665725, 0.4068090650141155, 0.3701047391436106, 0.7170357796952346, 0.31327392774306395, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.39350494784782397, 0.07595368484327951, 0.4868292236818003, 0.046986031631648606, 0.530207151960747, 0.01037806421927761, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05535788985307855, 0.7394134258901907, 0.17106635844034493, 0.9821443208953105, 0.0, 0.008962807708770178, 0.0, 0.0, 0.11897566020800357, 0.0, 0.0, 0.0, 1.2828090910806784, 0.0, 0.4691443493685695, 0.0, 0.07022467062008277, 0.18231155361607748, 0.41302392682414213, 0.0, 0.13696413696930718, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.423686950315378, 0.0, 0.5297732126841366, 0.0, 0.10086209606266386, 0.4105665706595916, 0.2899236467445781, 0.08479167779863508, 0.0, 1.1607169616195545, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44056670549109944, 0.0036426275414389976, 0.22268017654499572, 0.70259131988822, 0.021981713208965886, 0.0, 0.0, 0.0, 0.6644482698446481, 0.5559910462571956, 0.9081368976941899, 0.9545466439826781, 0.0, 0.5800820874802947, 0.6131061961157059, 0.0, 0.3387938943904664, 0.0, 0.41233232521209184, 0.6892320892052487, 0.9168247018477867, 0.7592095444656833, 0.0, 0.7080274090242662, 0.0, 0.0, 0.7409103227795685, 0.4891910517462003, 0.0, 0.0, 0.6996898878138225, 0.18201720925245757, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.033060035104523045, 0.0, 0.0, 0.0, 0.0, 0.0010434833431631984, 0.0, 0.0, 0.0, 0.010921241238865733, 0.0, 0.015498170387323498, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011618017340433275, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2104054739321471, 0.0, 0.0, 0.0, 0.051249741424478176, 0.0, 0.0, 0.0, 0.09744956100917274, 0.0, 0.0, 0.0, 0.0, 0.10766993925868745, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18951483497887683, 0.5969374604132631, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6248091001560899, 0.0, 0.0, 0.0, 0.0, 0.18617398114562853, 0.0530079909103574, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14076422137873648, 0.3823151552281139, 0.28535858102685935, 0.3054264047421474, 0.0, 0.0, 0.5425991952026914, 0.16566917168527906, 0.0, 0.0, 0.24572721112234908, 0.0, 0.0, 0.311993347128524, 0.0, 1.0231061248638629, 0.5939954507294103, 0.5983388707717968, 0.7851882761904205, 0.0, 0.28209866162610053, 0.46470151179975694, 0.0, 0.22767102275324697, 0.0, 0.0, 0.3351791998251741, 0.0, 0.0, 0.0, 0.3258354466053841, 0.29514851487698524, 0.25858133097678015, 0.1886151008053812, 0.5226293862659805, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0442888167727012, 0.0, 0.002505192641426779, 0.0, 0.0, 0.07664740890754863, 0.0, 0.0, 0.013363010227595206, 0.00048725362450601585, 0.002419244088482629, 0.026391726291740327, 0.0, 0.0, 0.03460717393330891, 0.0, 0.13624247986958654, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012449069569846, 0.0, 0.016951747875799564, 0.0, 0.0197038621774241, 0.0, 0.033062191640471625, 0.07310509907314719, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06556034095953286, 0.0, 0.02434740964367002, 0.0, 0.0, 0.13592547532623306, 0.0, 0.030022610188286405, 0.03795734899490777, 0.008699370747195389, 0.0, 0.0, 0.18940737674761182, 0.16014434992779022, 0.0058393216511455045, 0.0, 0.3162824676834449, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14919154588027264, 0.0, 0.20315232851529888, 0.0, 0.27582528532656464, 0.0, 0.3962223404212856, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00142596300027198, 0.0, 0.008781597914028754, 0.2917830655969515, 0.0, 0.0, 0.3010196130390429, 1.085161420061463, 0.0, 0.8818413438369698, 0.0, 0.14807253735972709, 0.6682230613653689, 0.6738718955158736, 0.0, 0.2921120382906695, 0.0, 0.23016863424694362, 0.44343897402347887, 0.0, 0.0, 1.2859761704109314, 1.188418751121335, 0.0, 0.4620501506594876, 0.23396469855670338, 0.0, 0.03498671823904876, 0.0, 0.0, 0.3392611399043929, 0.20497564811318061, 0.0, 0.4183535169904413, 0.20649711391931772, 0.520902106226398, 0.0, 0.0, 0.16125806820737418, 0.5576988707246249, 0.0, 0.0, 0.0, 0.11391623120681071, 0.0, 0.0, 0.09151851725619714, 0.0, 0.0, 1.197254000111945, 0.0, 0.8775820846458859, 0.0, 0.5629797627532018, 0.6687057520979063, 0.341227000414515, 0.0, 0.25482576175079585, 0.0, 0.39537376259476137, 0.4437592920364036, 0.0, 0.0, 0.5927285333298421, 0.651986303075751, 0.0, 1.0434591895456948, 0.0, 0.0, 0.03501199088471498, 0.0, 0.0, 0.3395062050893736, 0.15619471223870085, 0.0, 0.41865571453083017, 0.20664627704903807, 0.48238135206809674, 0.08860575544817058, 0.0, 0.005695035181786341, 0.42497543210880456, 0.0, 0.23934194260454025, 0.30438882267274336, 0.2874361482000896, 0.0, 0.0, 0.6245971595264426, 0.0, 0.3639559852559372, 0.0, 0.0, 0.20575691959192335, 0.0, 0.5994751724692355, 0.0, 0.0, 0.0, 0.0, 0.002904710270589501, 0.0, 0.27722048222724693, 0.15720637186052522, 0.0, 0.40582979792580204, 0.3276141191929195, 0.0, 0.5216472712210337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022828121623522435, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.111114554054197, 0.0, 0.0, 0.005523835481493179, 0.12589753624924738, 0.4223352609451343, 0.19512225997871732, 0.9646462752755526, 0.0, 0.0, 0.908234546074678, 0.0, 0.1303083320340461, 0.4180115467796097, 0.3502002628164635, 0.005797584702932196, 0.620011620521792, 0.6673813159226286, 0.0030737631986123407, 0.0, 0.42144642014638856, 1.0414378536891689, 0.6289622414694364, 1.5563606265339154, 0.08385396416867873, 0.3115994197854627, 0.7117829870894571, 0.6381078265340546, 0.0, 0.23989801450652898, 0.0, 0.5826228436020011, 0.8457142287971168, 0.0, 0.0, 0.0, 1.0193786877434905, 0.0, 0.0, 0.19971832516875276, 0.5533950636153777, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21472741909783344, 0.39082851888093567, 0.44716245502350144, 0.2879713896328549, 0.5511368948723377, 0.33565164359388877, 1.3517828371888283, 0.0, 0.09957607530690628, 0.0, 0.0, 0.26103163435011795, 0.3965287786994862, 0.6597151598792015, 0.0, 0.34286845579856556, 0.3900059867245693, 0.34423839922938515, 0.0, 1.2842745238771571, 0.3694211002080512, 0.0, 0.05489702502604951, 0.3645945046142821, 0.0, 0.0, 0.08041277613458127, 0.2893532857853331, 0.0, 0.5419495915870312, 0.70973704371772, 0.06597297072421819, 0.0, 0.0, 0.28417074423649047, 0.45673560467870195, 0.1772454725351981, 0.22541615615699262, 0.08077018346809645, 0.5662548477384789, 0.07678831671440492, 0.7573984424681688, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4826509895037858, 0.0, 0.7156459126461995, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013563432420802306, 0.4559162518278489, 0.0, 0.0, 0.0, 0.0, 0.26262283944881243, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16505781512417061, 0.2933206286622639, 0.0, 0.0, 0.0, 0.0, 0.40289003757510067, 0.0, 0.0, 0.0, 0.6372941893353361, 0.0, 0.6237934507673457, 0.8070702584067075, 0.0, 0.0, 0.3379623279083573, 0.0, 0.46129991809862586, 0.7333015288992237, 0.21031442023566344, 0.0, 0.7041420713561054, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7760176053652288, 0.0, 1.0534236649773678, 0.0, 0.0, 0.8070780011372765, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4823716017908521, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6208306891807599, 0.0, 0.0, 0.01732988822073383, 0.2738833349341264, 0.10672374403860475, 0.0, 0.008043590060480383, 0.0, 0.055277084044920086, 0.0, 0.0, 0.17943961322559207, 0.0, 0.0, 0.0, 0.0, 0.0, 0.718108290096912, 0.0, 0.0, 0.0, 0.008728384689335622, 0.0, 0.1697763404331079, 0.11586384141065409, 0.0, 0.0, 0.0, 0.8848305575683707, 0.0, 0.8449968950182224, 0.0, 0.0, 0.0, 0.6706152313255382, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8308196053978146, 0.0, 0.9175002824131868, 0.7530109173431534, 0.022342425693822682, 0.0, 0.0035915524463436964, 0.15288943835905894, 0.0, 0.5199425421439203, 0.11917728725476327, 0.3521011644457409, 0.0, 0.822488739774635, 0.513543493422637, 0.2981570434312181, 0.0, 0.0, 1.2701086711578897, 0.0, 0.11247893732510689, 0.0, 0.039332390318575425, 0.0, 0.11102635670488926, 0.0, 0.15118325075382752, 0.0, 0.1757278339799435, 0.0, 0.2948633760880991, 0.15371975197149906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04453230950399372, 0.0, 0.05568267952698087, 0.0, 0.21714106205060016, 0.0, 0.0, 0.0, 0.0, 0.7213227431120439, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2701718137836067, 0.0, 0.06123281377737304, 0.4007297534572718, 0.0, 0.0, 0.02350606599699272, 0.0, 0.47270164239805296, 0.39554302818161685, 0.39843532224568856, 0.0, 0.0, 0.46004399190355705, 0.41242580548884994, 0.0, 0.15086754157289264, 0.0, 0.0, 0.5466072621606027, 0.18414300130678235, 0.2087681866233734, 0.0, 0.5313695523968773, 0.0, 0.0, 0.2807883504872339, 0.34802019073798685, 0.0, 0.09776175774612722, 0.1924014129464173, 0.0, 0.0, 0.0, 0.2995245510564702, 0.1439415371971276, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09612835370305864, 0.0, 0.07178793274997196, 0.0, 0.0, 0.0, 0.529229742886101, 0.0, 0.0, 0.5898615183638936, 0.0, 0.1996504875270556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07685007162501065, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4724449949223617, 0.0, 0.21985358976027652, 0.0, 0.0, 0.0, 0.6511036698044336, 0.5394783324223208, 0.9048355234024855, 0.0, 0.0, 0.0, 0.008134079103649166, 0.17236516549262873, 0.0, 0.7108281927774538, 0.0, 0.0, 0.0, 0.8245834715993663, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22989626507902333, 0.12442621872484774, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3037122713618223, 0.0, 0.0, 0.008259351424073042, 0.4378900919766154, 0.05086408498890661, 0.31974807102322866, 0.0, 0.0, 0.7960980638875322, 0.0689135689704732, 0.0, 0.0, 0.0, 0.20359294387478752, 0.9097633301509839, 0.0, 0.6841090251831579, 0.0, 0.6154584825649704, 0.7335150023327748, 1.015637287349907, 0.0, 0.0779165260149525, 1.1277817941657742, 0.0, 0.0, 0.3457097793442966, 0.6958114560471119, 0.22297907055084815, 0.02617735461213243, 0.8271866794310433, 0.0, 0.25383801660715655, 0.0, 0.6564805026462143, 0.4519424562793308, 0.8175733700968422, 0.26495100977979114, 0.0, 0.0, 0.0, 0.33910693050583907, 0.0, 0.0, 0.0, 0.4889218480481046, 0.0, 0.3925649791995639, 1.5627863234111485, 0.8201598086100989, 0.28474440170274185, 1.0514838770367962, 0.010290531779792767, 0.0, 0.0, 1.5282247448720891, 0.4202365445740981, 0.38631591015261757, 0.07827208426350975, 0.002001482371009478, 0.19978631068641475, 0.10840878806592323, 0.8849580765325739, 0.7025926608640433, 0.21468725597950544, 0.6515266028885637, 0.304659479667613, 0.0, 0.5203661079080213, 0.1989832785613875, 0.22866153178235926, 0.2129788116497342, 0.0511368043793991, 0.0, 0.06963236972450947, 0.0, 0.08093717687353573, 0.0, 0.3254291820176872, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4944791980843742, 0.5417407856384763, 0.0, 0.4343792686365096, 1.5442785077006684, 0.0, 0.13746522377571999, 0.0, 0.0, 0.0, 0.0, 0.01933245239450143, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44623451745418, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.30502005455201636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014872981553739924, 0.0, 0.0, 0.0007393802182031884, 0.0, 0.14653699929908884, 0.0, 0.0, 0.26188312990151436, 0.028159286104530473, 0.0, 0.4084686341089707, 0.9235151006259961, 0.5957064186534903, 0.0, 0.0, 0.5535709817494249, 0.0, 0.25891613288066156, 0.07783302925410085, 0.2939591160629263, 0.1987201643010643, 0.0, 0.09767541007114018, 0.4120929351702914, 0.22225830848448824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13739476210353194, 0.0, 0.037597515302596925, 0.0, 0.0, 0.31874062595288627, 0.33536722163001637, 0.0, 0.0, 0.0, 0.48352996362981304, 0.0, 0.38851289261478805, 0.6830254018960756, 0.3364461001285706, 0.8253161071254982, 0.0, 0.2348841918025421, 0.0, 0.4205219693225024, 0.5314960091024892, 0.0, 0.0, 0.10178270195612651, 0.0, 0.0, 0.780835547454152, 0.5872223143240599, 0.2503853190515238, 0.0, 0.0, 0.6150976289535472, 0.0, 0.09445718365268194, 0.0, 0.0, 0.17331386223779208, 1.5139967025892487, 0.0, 0.0031631037123804793, 0.0, 0.0, 0.20592512324808152, 0.0, 0.03635874582185372, 0.0, 0.20018457152414249, 0.22068196040992705, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = ((ex.bodies[1]).bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    result = nothing
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    B_lvl_q = B_lvl_ptr[1]
    B_lvl_q_stop = B_lvl_ptr[1 + 1]
    if B_lvl_q < B_lvl_q_stop
        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
    else
        B_lvl_i_stop = 0
    end
    phase_stop = min(B_lvl.shape[2], B_lvl_i_stop)
    if phase_stop >= 1
        if B_lvl_tbl2[B_lvl_q] < 1
            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
        end
        while true
            B_lvl_i = B_lvl_tbl2[B_lvl_q]
            B_lvl_q_step = B_lvl_q
            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
            end
            if B_lvl_i < phase_stop
                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                val = Ct_lvl_2_val
                Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                B_lvl_tbl1_2 = B_lvl_tbl1
                B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                B_lvl_tbl2_2 = B_lvl_tbl2
                val_2 = B_lvl_val
                B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
                A_lvl_ptr_2 = A_lvl_ptr
                A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                A_lvl_tbl1_2 = A_lvl_tbl1
                A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                A_lvl_tbl2_2 = A_lvl_tbl2
                A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                val_3 = A_lvl_val
                A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
                Threads.@threads for i_9 = 1:Threads.nthreads()
                        phase_start_6 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_9), Threads.nthreads()))
                        phase_stop_7 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_9, Threads.nthreads()))
                        if phase_stop_7 >= phase_start_6
                            for i_12 = phase_start_6:phase_stop_7
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_12
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_8 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_8 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_8
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_9 = min(B_lvl_i_2, phase_stop_8, A_lvl_i)
                                        if A_lvl_i == phase_stop_9 && B_lvl_i_2 == phase_stop_9
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_10 = min(i_12, A_lvl_i_stop_2)
                                            if phase_stop_10 >= i_12
                                                if A_lvl_tbl1[A_lvl_q] < i_12
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_12, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_10
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_12 = min(A_lvl_i_2, phase_stop_10)
                                                        if A_lvl_i_2 == phase_stop_12
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_9
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_9
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_9 + 1
                                    end
                                end
                            end
                        end
                    end
                Ct_lvl_2_val = val
                B_lvl_tbl1 = B_lvl_tbl1_2
                B_lvl_tbl2 = B_lvl_tbl2_2
                B_lvl_val = val_2
                A_lvl_ptr = A_lvl_ptr_2
                A_lvl_tbl1 = A_lvl_tbl1_2
                A_lvl_tbl2 = A_lvl_tbl2_2
                A_lvl_val = val_3
                B_lvl_q = B_lvl_q_step
            else
                phase_stop_18 = min(B_lvl_i, phase_stop)
                if B_lvl_i == phase_stop_18
                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_18
                    val_4 = Ct_lvl_2_val
                    Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                    B_lvl_tbl1_3 = B_lvl_tbl1
                    B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                    B_lvl_tbl2_3 = B_lvl_tbl2
                    val_5 = B_lvl_val
                    B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
                    A_lvl_ptr_3 = A_lvl_ptr
                    A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                    A_lvl_tbl1_3 = A_lvl_tbl1
                    A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                    A_lvl_tbl2_3 = A_lvl_tbl2
                    A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                    val_6 = A_lvl_val
                    A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
                    Threads.@threads for i_19 = 1:Threads.nthreads()
                            phase_start_21 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_19), Threads.nthreads()))
                            phase_stop_23 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_19, Threads.nthreads()))
                            if phase_stop_23 >= phase_start_21
                                for i_22 = phase_start_21:phase_stop_23
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_22
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_24 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_24 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_24
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_25 = min(B_lvl_i_2, A_lvl_i, phase_stop_24)
                                            if A_lvl_i == phase_stop_25 && B_lvl_i_2 == phase_stop_25
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_26 = min(i_22, A_lvl_i_stop_4)
                                                if phase_stop_26 >= i_22
                                                    if A_lvl_tbl1[A_lvl_q] < i_22
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_22, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_26
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_28 = min(A_lvl_i_4, phase_stop_26)
                                                            if A_lvl_i_4 == phase_stop_28
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_25
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_25
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_25 + 1
                                        end
                                    end
                                end
                            end
                        end
                    Ct_lvl_2_val = val_4
                    B_lvl_tbl1 = B_lvl_tbl1_3
                    B_lvl_tbl2 = B_lvl_tbl2_3
                    B_lvl_val = val_5
                    A_lvl_ptr = A_lvl_ptr_3
                    A_lvl_tbl1 = A_lvl_tbl1_3
                    A_lvl_tbl2 = A_lvl_tbl2_3
                    A_lvl_val = val_6
                    B_lvl_q = B_lvl_q_step
                end
                break
            end
        end
    end
    resize!(Ct_lvl_2_val, A_lvl.shape[1] * B_lvl.shape[2])
    result = (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
    result
end
julia> @finch begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0, 0.0, 0.0, 0.004334271763144113, 0.19866796375858964, 0.05511378591134807, 0.37983318302160474, 0.0, 0.8329001018500983, 0.30166090609024687, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3289219868624648, 0.0, 0.15511617694464214, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18499887719318273, 0.0, 0.0, 0.0, 0.19259223593916178, 0.0, 0.0, 0.0638915620449496, 0.08125561754012361, 0.17659717140450376, 0.0, 0.0, 0.1525779960421776, 0.0, 0.0, 0.0, 0.0, 0.29196799055295863, 0.0, 0.0, 0.0, 0.5551667524029037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4594337414860906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3019846765891098, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18892258579898563, 0.0, 0.2395310548375412, 0.0, 0.359229579293753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00028478282759787334, 0.0, 0.3432275013790356, 0.0, 0.0, 0.2972839943189292, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19540404365846636, 0.0, 0.0, 0.0, 0.0, 0.18546216890582629, 0.0, 0.0, 0.00916099623463154, 0.0, 0.034731738855626984, 0.4853088859188271, 0.0076713909171271105, 0.0, 0.0, 0.5640422262893099, 0.0, 0.5950501234938147, 0.040920156633969085, 0.0014920661060395996, 0.0, 0.08081663901361853, 0.8825202410318094, 0.0, 0.006218090636031004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3280555235834662, 0.03812149119614576, 0.0, 0.05190957475021724, 0.0, 0.06033708819060236, 0.0, 0.10124291140603807, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5085302772239245, 0.0, 0.5334307954394755, 0.0, 0.0, 0.2328402544634388, 0.0, 0.005816161517311894, 1.5317318593263975, 0.0, 0.46738125332221536, 0.0, 0.09813163178688389, 0.4394739409933983, 0.0, 0.26102098784262384, 0.0, 0.0, 0.0, 0.0, 0.8804155352247423, 0.0, 0.0, 0.48240400321886606, 0.3763091458911055, 0.6151496795666793, 0.0, 0.27268296540705483, 0.0, 0.0014290584461454537, 0.0, 0.0, 0.0, 0.5294637260878591, 0.0, 0.0, 0.0, 0.31181862629226015, 0.0, 0.34152287293383227, 0.1986997482112381, 0.2527011428302912, 0.0, 0.434749031361829, 0.0, 0.4472075705110786, 0.4092092126159339, 0.2807873539680881, 0.17401521117970595, 0.0, 0.0, 0.43733948883443596, 0.5485693747724284, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0983266821553666, 0.05373620000037918, 0.49612300100963586, 0.0, 0.0, 1.075151264033199, 0.0, 0.18716105879611752, 0.03950202184626144, 0.0, 0.0, 0.7852572057826378, 0.0, 0.0, 0.08756438716142145, 0.27446263899702106, 0.0, 0.0, 0.07204260281080108, 0.0, 0.09291620627436281, 0.23270017215835156, 0.23824566618105675, 0.0, 0.0, 0.0, 0.13665872084048475, 0.0, 0.0, 0.0067937121060301624, 0.1164801427986044, 0.0, 0.5691621002508566, 0.4713388661713567, 0.41358181631025637, 0.0, 0.3128607440091297, 0.0, 0.0, 0.20424594336339066, 0.0, 0.0, 0.7442286193021344, 0.10553727794892508, 0.0, 0.30186901767997887, 0.0, 0.0, 0.0, 0.32291683563950796, 0.07758151598185183, 0.0, 0.347786535534979, 0.7360230359944353, 0.0, 0.0, 0.0, 0.8757273805535595, 0.41322710295268655, 0.0, 0.25679112297862333, 0.17774302673574222, 0.5841939760860391, 0.0, 0.0, 0.0, 0.13300779742001662, 0.16915583797139946, 0.0, 0.0, 0.0, 0.7164740353603845, 0.45156468002631484, 0.0, 0.27217311439601866, 0.7084784766971438, 0.06835326871026481, 0.0, 0.0, 0.0, 0.2861016237491002, 1.0700436562849107, 0.135740840434777, 0.2599664462430113, 0.45536369102376945, 0.0, 0.0, 0.0, 0.16460938932017066, 0.9424546774296569, 0.4980075807530193, 0.501649117240962, 0.9215918221852557, 0.0, 0.5227646022766553, 0.46865433734477785, 0.0, 0.1899494468105599, 0.0, 0.14204872822901377, 0.7891527088162923, 0.2800775160398023, 0.0, 0.0, 0.6038144125549446, 0.0, 0.4101280976499608, 0.15813547044885462, 0.4381740566617935, 0.0025678619479343042, 0.5913196381458786, 0.01581382625018507, 0.4317825799661969, 0.0, 0.5361217114844467, 0.16404576969544252, 0.0, 0.03623369485950333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19327505078226082, 0.1125279795645968, 0.0, 0.38171505913646114, 0.0, 0.0012820843611261621, 0.0, 0.0, 0.017018867107690273, 0.0, 0.018573326277702022, 0.7755881641075008, 0.12996991545193906, 0.0, 0.18005632805213245, 0.0, 0.2451805301156947, 0.0, 0.3747092870507497, 0.0, 0.4781929115858312, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.35214721497251084, 0.0, 0.020304118651362432, 0.14466279308909658, 0.626862644216891, 0.0, 0.0, 1.5549351131665947, 0.30737844662289215, 0.0, 0.0, 0.0, 0.0, 0.009701873820892715, 0.2520412914553624, 0.018396624198184493, 0.0, 0.7982268230101296, 0.0, 0.9201682818000929, 0.0, 0.0, 0.7342011144064367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09396246898802936, 0.0, 0.5073258744317697, 0.22284199906817412, 0.10151898941699102, 0.03180992568768135, 0.0, 0.5543686635557524, 0.0, 0.0, 0.0, 0.22367207355552704, 0.0, 0.0, 0.0, 0.40880306935546484, 0.0, 0.42113696363931935, 0.0, 0.0, 0.0, 0.43326306412219423, 0.39232717474974355, 0.0, 0.0, 0.0, 0.013675206005982276, 0.0, 0.2603519242467926, 0.3502025254743841, 0.14312660068897712, 0.34775162340880994, 0.20310882214946477, 0.0, 0.27127891969267615, 0.0, 0.0, 0.020541404680713544, 0.0, 0.0, 0.1991870263338475, 0.0, 0.0, 0.2456237488004554, 0.12123860127246498, 0.2079074067296303, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24646978770136238, 0.15225808974571345, 0.0, 0.0, 0.46988488666249073, 0.16461741428636234, 0.4415386194506142, 0.05631807188893752, 0.0611318669096905, 0.0, 0.0, 0.0, 0.5215534136524774, 0.0, 0.0, 0.6274626509368973, 0.038256585847320265, 0.530070634494204, 0.0, 0.7877809268797783, 0.25601250351282806, 0.37534186876141573, 0.700013095645165, 0.0, 0.29431110179251513, 0.22882177649965688, 1.0569304890861846, 0.0, 0.0, 0.0, 0.0, 0.02833923205698933, 0.02572736738919585, 0.0, 0.1150475064446459, 0.24371977071087828, 0.0, 0.0, 0.044121908414131224, 0.0771055384425919, 0.09695789652483547, 0.0, 0.0, 0.0, 0.12020713596947674, 0.045249684273424934, 0.8721213827038133, 0.0, 0.11731035534021127, 0.37569616707373005, 0.0, 0.0, 0.9570606189270042, 0.9086284112098039, 0.20027308709305888, 0.4268041763647709, 0.0, 0.0, 0.015909944043088663, 0.0, 0.7658973441527888, 0.0, 0.19507759787979717, 0.3226660152512004, 0.23629991340839918, 0.0, 0.13848101841219138, 0.0, 0.0, 0.24578615843203896, 0.0, 0.0, 0.10167993256702677, 0.0, 0.0, 0.12538470338440244, 0.9934363459809055, 0.10613146591697438, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6421265863141966, 0.0, 0.5400344618364347, 0.025731716559651077, 0.0, 0.0, 0.0, 0.007521097208030019, 0.5455660054203588, 0.0, 1.1209603429913297, 0.0, 0.0, 0.5234607155600556, 0.0, 0.023977651748011766, 0.0, 0.0, 0.0, 0.27798322874225667, 0.0, 0.35612425891926536, 0.0, 0.7529913555346315, 0.6214420882570796, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12146506806205652, 0.0, 0.3210215267482459, 0.0, 0.0, 0.0, 0.33419799384234544, 0.0, 0.0, 0.15359727591834285, 0.19534099821264672, 0.525863863274946, 0.0, 0.0, 0.34569678733330644, 0.0, 0.0, 0.3415674397857982, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.523004359259986, 0.0, 0.22235180304591923, 0.0, 0.00635695661054767, 0.0, 0.0, 0.0843846185513356, 0.0, 0.0, 0.0, 0.8855039996037297, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27294552017018114, 0.0, 0.3412878895624823, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5864608505347954, 0.0, 0.10380099346078346, 0.02524889717437322, 0.0, 0.0, 0.38157075730498896, 0.0, 0.0, 0.0, 0.45469484904050655, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26459586877877683, 0.18314523505716396, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22372814141562178, 0.0, 0.0, 0.0, 0.6276845275284666, 0.18331203344516112, 0.25689720406507865, 0.04048914090414101, 0.0, 0.016969058333515752, 0.0, 0.5911567891086366, 0.37358991094373184, 0.21597413111529512, 0.09535966106843696, 0.021909623318208237, 0.49412659112391777, 0.39229708641560146, 0.0, 0.0, 0.5239905526116051, 0.22383442432789932, 0.0, 0.408761833081477, 0.7858251636352953, 0.0, 0.03095169444374481, 1.0134328993353359, 0.0, 0.5783663988186404, 0.023349871375665725, 0.4068090650141155, 0.3701047391436106, 0.7170357796952346, 0.31327392774306395, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.39350494784782397, 0.07595368484327951, 0.4868292236818003, 0.046986031631648606, 0.530207151960747, 0.01037806421927761, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05535788985307855, 0.7394134258901907, 0.17106635844034493, 0.9821443208953105, 0.0, 0.008962807708770178, 0.0, 0.0, 0.11897566020800357, 0.0, 0.0, 0.0, 1.2828090910806784, 0.0, 0.4691443493685695, 0.0, 0.07022467062008277, 0.18231155361607748, 0.41302392682414213, 0.0, 0.13696413696930718, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.423686950315378, 0.0, 0.5297732126841366, 0.0, 0.10086209606266386, 0.4105665706595916, 0.2899236467445781, 0.08479167779863508, 0.0, 1.1607169616195545, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44056670549109944, 0.0036426275414389976, 0.22268017654499572, 0.70259131988822, 0.021981713208965886, 0.0, 0.0, 0.0, 0.6644482698446481, 0.5559910462571956, 0.9081368976941899, 0.9545466439826781, 0.0, 0.5800820874802947, 0.6131061961157059, 0.0, 0.3387938943904664, 0.0, 0.41233232521209184, 0.6892320892052487, 0.9168247018477867, 0.7592095444656833, 0.0, 0.7080274090242662, 0.0, 0.0, 0.7409103227795685, 0.4891910517462003, 0.0, 0.0, 0.6996898878138225, 0.18201720925245757, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.033060035104523045, 0.0, 0.0, 0.0, 0.0, 0.0010434833431631984, 0.0, 0.0, 0.0, 0.010921241238865733, 0.0, 0.015498170387323498, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011618017340433275, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2104054739321471, 0.0, 0.0, 0.0, 0.051249741424478176, 0.0, 0.0, 0.0, 0.09744956100917274, 0.0, 0.0, 0.0, 0.0, 0.10766993925868745, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18951483497887683, 0.5969374604132631, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6248091001560899, 0.0, 0.0, 0.0, 0.0, 0.18617398114562853, 0.0530079909103574, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14076422137873648, 0.3823151552281139, 0.28535858102685935, 0.3054264047421474, 0.0, 0.0, 0.5425991952026914, 0.16566917168527906, 0.0, 0.0, 0.24572721112234908, 0.0, 0.0, 0.311993347128524, 0.0, 1.0231061248638629, 0.5939954507294103, 0.5983388707717968, 0.7851882761904205, 0.0, 0.28209866162610053, 0.46470151179975694, 0.0, 0.22767102275324697, 0.0, 0.0, 0.3351791998251741, 0.0, 0.0, 0.0, 0.3258354466053841, 0.29514851487698524, 0.25858133097678015, 0.1886151008053812, 0.5226293862659805, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0442888167727012, 0.0, 0.002505192641426779, 0.0, 0.0, 0.07664740890754863, 0.0, 0.0, 0.013363010227595206, 0.00048725362450601585, 0.002419244088482629, 0.026391726291740327, 0.0, 0.0, 0.03460717393330891, 0.0, 0.13624247986958654, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012449069569846, 0.0, 0.016951747875799564, 0.0, 0.0197038621774241, 0.0, 0.033062191640471625, 0.07310509907314719, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06556034095953286, 0.0, 0.02434740964367002, 0.0, 0.0, 0.13592547532623306, 0.0, 0.030022610188286405, 0.03795734899490777, 0.008699370747195389, 0.0, 0.0, 0.18940737674761182, 0.16014434992779022, 0.0058393216511455045, 0.0, 0.3162824676834449, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14919154588027264, 0.0, 0.20315232851529888, 0.0, 0.27582528532656464, 0.0, 0.3962223404212856, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00142596300027198, 0.0, 0.008781597914028754, 0.2917830655969515, 0.0, 0.0, 0.3010196130390429, 1.085161420061463, 0.0, 0.8818413438369698, 0.0, 0.14807253735972709, 0.6682230613653689, 0.6738718955158736, 0.0, 0.2921120382906695, 0.0, 0.23016863424694362, 0.44343897402347887, 0.0, 0.0, 1.2859761704109314, 1.188418751121335, 0.0, 0.4620501506594876, 0.23396469855670338, 0.0, 0.03498671823904876, 0.0, 0.0, 0.3392611399043929, 0.20497564811318061, 0.0, 0.4183535169904413, 0.20649711391931772, 0.520902106226398, 0.0, 0.0, 0.16125806820737418, 0.5576988707246249, 0.0, 0.0, 0.0, 0.11391623120681071, 0.0, 0.0, 0.09151851725619714, 0.0, 0.0, 1.197254000111945, 0.0, 0.8775820846458859, 0.0, 0.5629797627532018, 0.6687057520979063, 0.341227000414515, 0.0, 0.25482576175079585, 0.0, 0.39537376259476137, 0.4437592920364036, 0.0, 0.0, 0.5927285333298421, 0.651986303075751, 0.0, 1.0434591895456948, 0.0, 0.0, 0.03501199088471498, 0.0, 0.0, 0.3395062050893736, 0.15619471223870085, 0.0, 0.41865571453083017, 0.20664627704903807, 0.48238135206809674, 0.08860575544817058, 0.0, 0.005695035181786341, 0.42497543210880456, 0.0, 0.23934194260454025, 0.30438882267274336, 0.2874361482000896, 0.0, 0.0, 0.6245971595264426, 0.0, 0.3639559852559372, 0.0, 0.0, 0.20575691959192335, 0.0, 0.5994751724692355, 0.0, 0.0, 0.0, 0.0, 0.002904710270589501, 0.0, 0.27722048222724693, 0.15720637186052522, 0.0, 0.40582979792580204, 0.3276141191929195, 0.0, 0.5216472712210337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022828121623522435, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.111114554054197, 0.0, 0.0, 0.005523835481493179, 0.12589753624924738, 0.4223352609451343, 0.19512225997871732, 0.9646462752755526, 0.0, 0.0, 0.908234546074678, 0.0, 0.1303083320340461, 0.4180115467796097, 0.3502002628164635, 0.005797584702932196, 0.620011620521792, 0.6673813159226286, 0.0030737631986123407, 0.0, 0.42144642014638856, 1.0414378536891689, 0.6289622414694364, 1.5563606265339154, 0.08385396416867873, 0.3115994197854627, 0.7117829870894571, 0.6381078265340546, 0.0, 0.23989801450652898, 0.0, 0.5826228436020011, 0.8457142287971168, 0.0, 0.0, 0.0, 1.0193786877434905, 0.0, 0.0, 0.19971832516875276, 0.5533950636153777, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21472741909783344, 0.39082851888093567, 0.44716245502350144, 0.2879713896328549, 0.5511368948723377, 0.33565164359388877, 1.3517828371888283, 0.0, 0.09957607530690628, 0.0, 0.0, 0.26103163435011795, 0.3965287786994862, 0.6597151598792015, 0.0, 0.34286845579856556, 0.3900059867245693, 0.34423839922938515, 0.0, 1.2842745238771571, 0.3694211002080512, 0.0, 0.05489702502604951, 0.3645945046142821, 0.0, 0.0, 0.08041277613458127, 0.2893532857853331, 0.0, 0.5419495915870312, 0.70973704371772, 0.06597297072421819, 0.0, 0.0, 0.28417074423649047, 0.45673560467870195, 0.1772454725351981, 0.22541615615699262, 0.08077018346809645, 0.5662548477384789, 0.07678831671440492, 0.7573984424681688, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4826509895037858, 0.0, 0.7156459126461995, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013563432420802306, 0.4559162518278489, 0.0, 0.0, 0.0, 0.0, 0.26262283944881243, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16505781512417061, 0.2933206286622639, 0.0, 0.0, 0.0, 0.0, 0.40289003757510067, 0.0, 0.0, 0.0, 0.6372941893353361, 0.0, 0.6237934507673457, 0.8070702584067075, 0.0, 0.0, 0.3379623279083573, 0.0, 0.46129991809862586, 0.7333015288992237, 0.21031442023566344, 0.0, 0.7041420713561054, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7760176053652288, 0.0, 1.0534236649773678, 0.0, 0.0, 0.8070780011372765, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4823716017908521, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6208306891807599, 0.0, 0.0, 0.01732988822073383, 0.2738833349341264, 0.10672374403860475, 0.0, 0.008043590060480383, 0.0, 0.055277084044920086, 0.0, 0.0, 0.17943961322559207, 0.0, 0.0, 0.0, 0.0, 0.0, 0.718108290096912, 0.0, 0.0, 0.0, 0.008728384689335622, 0.0, 0.1697763404331079, 0.11586384141065409, 0.0, 0.0, 0.0, 0.8848305575683707, 0.0, 0.8449968950182224, 0.0, 0.0, 0.0, 0.6706152313255382, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8308196053978146, 0.0, 0.9175002824131868, 0.7530109173431534, 0.022342425693822682, 0.0, 0.0035915524463436964, 0.15288943835905894, 0.0, 0.5199425421439203, 0.11917728725476327, 0.3521011644457409, 0.0, 0.822488739774635, 0.513543493422637, 0.2981570434312181, 0.0, 0.0, 1.2701086711578897, 0.0, 0.11247893732510689, 0.0, 0.039332390318575425, 0.0, 0.11102635670488926, 0.0, 0.15118325075382752, 0.0, 0.1757278339799435, 0.0, 0.2948633760880991, 0.15371975197149906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04453230950399372, 0.0, 0.05568267952698087, 0.0, 0.21714106205060016, 0.0, 0.0, 0.0, 0.0, 0.7213227431120439, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2701718137836067, 0.0, 0.06123281377737304, 0.4007297534572718, 0.0, 0.0, 0.02350606599699272, 0.0, 0.47270164239805296, 0.39554302818161685, 0.39843532224568856, 0.0, 0.0, 0.46004399190355705, 0.41242580548884994, 0.0, 0.15086754157289264, 0.0, 0.0, 0.5466072621606027, 0.18414300130678235, 0.2087681866233734, 0.0, 0.5313695523968773, 0.0, 0.0, 0.2807883504872339, 0.34802019073798685, 0.0, 0.09776175774612722, 0.1924014129464173, 0.0, 0.0, 0.0, 0.2995245510564702, 0.1439415371971276, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09612835370305864, 0.0, 0.07178793274997196, 0.0, 0.0, 0.0, 0.529229742886101, 0.0, 0.0, 0.5898615183638936, 0.0, 0.1996504875270556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07685007162501065, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4724449949223617, 0.0, 0.21985358976027652, 0.0, 0.0, 0.0, 0.6511036698044336, 0.5394783324223208, 0.9048355234024855, 0.0, 0.0, 0.0, 0.008134079103649166, 0.17236516549262873, 0.0, 0.7108281927774538, 0.0, 0.0, 0.0, 0.8245834715993663, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22989626507902333, 0.12442621872484774, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3037122713618223, 0.0, 0.0, 0.008259351424073042, 0.4378900919766154, 0.05086408498890661, 0.31974807102322866, 0.0, 0.0, 0.7960980638875322, 0.0689135689704732, 0.0, 0.0, 0.0, 0.20359294387478752, 0.9097633301509839, 0.0, 0.6841090251831579, 0.0, 0.6154584825649704, 0.7335150023327748, 1.015637287349907, 0.0, 0.0779165260149525, 1.1277817941657742, 0.0, 0.0, 0.3457097793442966, 0.6958114560471119, 0.22297907055084815, 0.02617735461213243, 0.8271866794310433, 0.0, 0.25383801660715655, 0.0, 0.6564805026462143, 0.4519424562793308, 0.8175733700968422, 0.26495100977979114, 0.0, 0.0, 0.0, 0.33910693050583907, 0.0, 0.0, 0.0, 0.4889218480481046, 0.0, 0.3925649791995639, 1.5627863234111485, 0.8201598086100989, 0.28474440170274185, 1.0514838770367962, 0.010290531779792767, 0.0, 0.0, 1.5282247448720891, 0.4202365445740981, 0.38631591015261757, 0.07827208426350975, 0.002001482371009478, 0.19978631068641475, 0.10840878806592323, 0.8849580765325739, 0.7025926608640433, 0.21468725597950544, 0.6515266028885637, 0.304659479667613, 0.0, 0.5203661079080213, 0.1989832785613875, 0.22866153178235926, 0.2129788116497342, 0.0511368043793991, 0.0, 0.06963236972450947, 0.0, 0.08093717687353573, 0.0, 0.3254291820176872, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4944791980843742, 0.5417407856384763, 0.0, 0.4343792686365096, 1.5442785077006684, 0.0, 0.13746522377571999, 0.0, 0.0, 0.0, 0.0, 0.01933245239450143, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44623451745418, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.30502005455201636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014872981553739924, 0.0, 0.0, 0.0007393802182031884, 0.0, 0.14653699929908884, 0.0, 0.0, 0.26188312990151436, 0.028159286104530473, 0.0, 0.4084686341089707, 0.9235151006259961, 0.5957064186534903, 0.0, 0.0, 0.5535709817494249, 0.0, 0.25891613288066156, 0.07783302925410085, 0.2939591160629263, 0.1987201643010643, 0.0, 0.09767541007114018, 0.4120929351702914, 0.22225830848448824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13739476210353194, 0.0, 0.037597515302596925, 0.0, 0.0, 0.31874062595288627, 0.33536722163001637, 0.0, 0.0, 0.0, 0.48352996362981304, 0.0, 0.38851289261478805, 0.6830254018960756, 0.3364461001285706, 0.8253161071254982, 0.0, 0.2348841918025421, 0.0, 0.4205219693225024, 0.5314960091024892, 0.0, 0.0, 0.10178270195612651, 0.0, 0.0, 0.780835547454152, 0.5872223143240599, 0.2503853190515238, 0.0, 0.0, 0.6150976289535472, 0.0, 0.09445718365268194, 0.0, 0.0, 0.17331386223779208, 1.5139967025892487, 0.0, 0.0031631037123804793, 0.0, 0.0, 0.20592512324808152, 0.0, 0.03635874582185372, 0.0, 0.20018457152414249, 0.22068196040992705, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = parallel(_)
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = ((ex.bodies[1]).bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    result = nothing
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = (Finch).moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = (Finch).moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            B_lvl_q = B_lvl_ptr[1]
            B_lvl_q_stop = B_lvl_ptr[1 + 1]
            if B_lvl_q < B_lvl_q_stop
                B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
            else
                B_lvl_i_stop = 0
            end
            phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                if B_lvl_tbl2[B_lvl_q] < phase_start_2
                    B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                end
                while true
                    B_lvl_i = B_lvl_tbl2[B_lvl_q]
                    B_lvl_q_step = B_lvl_q
                    if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                        B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    if B_lvl_i < phase_stop_2
                        Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                        val_4 = Ct_lvl_2_val
                        Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                        A_lvl_ptr_3 = A_lvl_ptr
                        A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                        A_lvl_tbl1_3 = A_lvl_tbl1
                        A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                        A_lvl_tbl2_3 = A_lvl_tbl2
                        A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                        val_5 = A_lvl_val
                        A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
                        B_lvl_ptr_3 = B_lvl_ptr
                        B_lvl_tbl1_3 = B_lvl_tbl1
                        B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                        B_lvl_tbl2_3 = B_lvl_tbl2
                        val_6 = B_lvl_val
                        B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
                        Threads.@threads for i_10 = 1:Threads.nthreads()
                                phase_start_7 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_10), Threads.nthreads()))
                                phase_stop_8 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_10, Threads.nthreads()))
                                if phase_stop_8 >= phase_start_7
                                    for i_13 = phase_start_7:phase_stop_8
                                        Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_13
                                        A_lvl_q = A_lvl_ptr[1]
                                        A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                        if A_lvl_q < A_lvl_q_stop
                                            A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                        else
                                            A_lvl_i_stop = 0
                                        end
                                        B_lvl_q_3 = B_lvl_q
                                        if B_lvl_q < B_lvl_q_step
                                            B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                        else
                                            B_lvl_i_stop_3 = 0
                                        end
                                        phase_stop_9 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                        if phase_stop_9 >= 1
                                            k = 1
                                            if A_lvl_tbl2[A_lvl_q] < 1
                                                A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            if B_lvl_tbl1[B_lvl_q] < 1
                                                B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                            end
                                            while k <= phase_stop_9
                                                A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                                A_lvl_q_step = A_lvl_q
                                                if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                    A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                                end
                                                B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                                phase_stop_10 = min(B_lvl_i_3, phase_stop_9, A_lvl_i)
                                                if A_lvl_i == phase_stop_10 && B_lvl_i_3 == phase_stop_10
                                                    B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                                    A_lvl_q_2 = A_lvl_q
                                                    if A_lvl_q < A_lvl_q_step
                                                        A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                    else
                                                        A_lvl_i_stop_2 = 0
                                                    end
                                                    phase_stop_11 = min(i_13, A_lvl_i_stop_2)
                                                    if phase_stop_11 >= i_13
                                                        if A_lvl_tbl1[A_lvl_q] < i_13
                                                            A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_13, A_lvl_q, A_lvl_q_step - 1)
                                                        end
                                                        while true
                                                            A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                            if A_lvl_i_2 < phase_stop_11
                                                                A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                                Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                                A_lvl_q_2 += 1
                                                            else
                                                                phase_stop_13 = min(A_lvl_i_2, phase_stop_11)
                                                                if A_lvl_i_2 == phase_stop_13
                                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                                    Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                                    A_lvl_q_2 += 1
                                                                end
                                                                break
                                                            end
                                                        end
                                                    end
                                                    A_lvl_q = A_lvl_q_step
                                                    B_lvl_q_3 += 1
                                                elseif B_lvl_i_3 == phase_stop_10
                                                    B_lvl_q_3 += 1
                                                elseif A_lvl_i == phase_stop_10
                                                    A_lvl_q = A_lvl_q_step
                                                end
                                                k = phase_stop_10 + 1
                                            end
                                        end
                                    end
                                end
                            end
                        Ct_lvl_2_val = val_4
                        A_lvl_ptr = A_lvl_ptr_3
                        A_lvl_tbl1 = A_lvl_tbl1_3
                        A_lvl_tbl2 = A_lvl_tbl2_3
                        A_lvl_val = val_5
                        B_lvl_ptr = B_lvl_ptr_3
                        B_lvl_tbl1 = B_lvl_tbl1_3
                        B_lvl_tbl2 = B_lvl_tbl2_3
                        B_lvl_val = val_6
                        B_lvl_q = B_lvl_q_step
                    else
                        phase_stop_19 = min(B_lvl_i, phase_stop_2)
                        if B_lvl_i == phase_stop_19
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_19
                            val_7 = Ct_lvl_2_val
                            Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                            A_lvl_ptr_4 = A_lvl_ptr
                            A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                            A_lvl_tbl1_4 = A_lvl_tbl1
                            A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                            A_lvl_tbl2_4 = A_lvl_tbl2
                            A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                            val_8 = A_lvl_val
                            A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
                            B_lvl_ptr_4 = B_lvl_ptr
                            B_lvl_tbl1_4 = B_lvl_tbl1
                            B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                            B_lvl_tbl2_4 = B_lvl_tbl2
                            val_9 = B_lvl_val
                            B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
                            Threads.@threads for i_20 = 1:Threads.nthreads()
                                    phase_start_22 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_20), Threads.nthreads()))
                                    phase_stop_24 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_20, Threads.nthreads()))
                                    if phase_stop_24 >= phase_start_22
                                        for i_23 = phase_start_22:phase_stop_24
                                            Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_23
                                            A_lvl_q = A_lvl_ptr[1]
                                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                            if A_lvl_q < A_lvl_q_stop
                                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                            else
                                                A_lvl_i_stop = 0
                                            end
                                            B_lvl_q_3 = B_lvl_q
                                            if B_lvl_q < B_lvl_q_step
                                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                            else
                                                B_lvl_i_stop_3 = 0
                                            end
                                            phase_stop_25 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                            if phase_stop_25 >= 1
                                                k = 1
                                                if A_lvl_tbl2[A_lvl_q] < 1
                                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                                end
                                                if B_lvl_tbl1[B_lvl_q] < 1
                                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                                end
                                                while k <= phase_stop_25
                                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                                    A_lvl_q_step = A_lvl_q
                                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                                    end
                                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                                    phase_stop_26 = min(B_lvl_i_3, A_lvl_i, phase_stop_25)
                                                    if A_lvl_i == phase_stop_26 && B_lvl_i_3 == phase_stop_26
                                                        B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                                        A_lvl_q_4 = A_lvl_q
                                                        if A_lvl_q < A_lvl_q_step
                                                            A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                        else
                                                            A_lvl_i_stop_4 = 0
                                                        end
                                                        phase_stop_27 = min(i_23, A_lvl_i_stop_4)
                                                        if phase_stop_27 >= i_23
                                                            if A_lvl_tbl1[A_lvl_q] < i_23
                                                                A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_23, A_lvl_q, A_lvl_q_step - 1)
                                                            end
                                                            while true
                                                                A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                                if A_lvl_i_4 < phase_stop_27
                                                                    A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                    Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                                    A_lvl_q_4 += 1
                                                                else
                                                                    phase_stop_29 = min(A_lvl_i_4, phase_stop_27)
                                                                    if A_lvl_i_4 == phase_stop_29
                                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                                        A_lvl_q_4 += 1
                                                                    end
                                                                    break
                                                                end
                                                            end
                                                        end
                                                        A_lvl_q = A_lvl_q_step
                                                        B_lvl_q_3 += 1
                                                    elseif B_lvl_i_3 == phase_stop_26
                                                        B_lvl_q_3 += 1
                                                    elseif A_lvl_i == phase_stop_26
                                                        A_lvl_q = A_lvl_q_step
                                                    end
                                                    k = phase_stop_26 + 1
                                                end
                                            end
                                        end
                                    end
                                end
                            Ct_lvl_2_val = val_7
                            A_lvl_ptr = A_lvl_ptr_4
                            A_lvl_tbl1 = A_lvl_tbl1_4
                            A_lvl_tbl2 = A_lvl_tbl2_4
                            A_lvl_val = val_8
                            B_lvl_ptr = B_lvl_ptr_4
                            B_lvl_tbl1 = B_lvl_tbl1_4
                            B_lvl_tbl2 = B_lvl_tbl2_4
                            B_lvl_val = val_9
                            B_lvl_q = B_lvl_q_step
                        end
                        break
                    end
                end
            end
        end
    resize!(val, A_lvl.shape[1] * B_lvl.shape[2])
    result = (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
    result
end
julia> @finch begin
        Ct .= 0
        for j = parallel(_)
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.0, 0.0, 0.0, 0.004334271763144113, 0.19866796375858964, 0.05511378591134807, 0.37983318302160474, 0.0, 0.8329001018500983, 0.30166090609024687, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3289219868624648, 0.0, 0.15511617694464214, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18499887719318273, 0.0, 0.0, 0.0, 0.19259223593916178, 0.0, 0.0, 0.0638915620449496, 0.08125561754012361, 0.17659717140450376, 0.0, 0.0, 0.1525779960421776, 0.0, 0.0, 0.0, 0.0, 0.29196799055295863, 0.0, 0.0, 0.0, 0.5551667524029037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4594337414860906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3019846765891098, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18892258579898563, 0.0, 0.2395310548375412, 0.0, 0.359229579293753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00028478282759787334, 0.0, 0.3432275013790356, 0.0, 0.0, 0.2972839943189292, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19540404365846636, 0.0, 0.0, 0.0, 0.0, 0.18546216890582629, 0.0, 0.0, 0.00916099623463154, 0.0, 0.034731738855626984, 0.4853088859188271, 0.0076713909171271105, 0.0, 0.0, 0.5640422262893099, 0.0, 0.5950501234938147, 0.040920156633969085, 0.0014920661060395996, 0.0, 0.08081663901361853, 0.8825202410318094, 0.0, 0.006218090636031004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3280555235834662, 0.03812149119614576, 0.0, 0.05190957475021724, 0.0, 0.06033708819060236, 0.0, 0.10124291140603807, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5085302772239245, 0.0, 0.5334307954394755, 0.0, 0.0, 0.2328402544634388, 0.0, 0.005816161517311894, 1.5317318593263975, 0.0, 0.46738125332221536, 0.0, 0.09813163178688389, 0.4394739409933983, 0.0, 0.26102098784262384, 0.0, 0.0, 0.0, 0.0, 0.8804155352247423, 0.0, 0.0, 0.48240400321886606, 0.3763091458911055, 0.6151496795666793, 0.0, 0.27268296540705483, 0.0, 0.0014290584461454537, 0.0, 0.0, 0.0, 0.5294637260878591, 0.0, 0.0, 0.0, 0.31181862629226015, 0.0, 0.34152287293383227, 0.1986997482112381, 0.2527011428302912, 0.0, 0.434749031361829, 0.0, 0.4472075705110786, 0.4092092126159339, 0.2807873539680881, 0.17401521117970595, 0.0, 0.0, 0.43733948883443596, 0.5485693747724284, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0983266821553666, 0.05373620000037918, 0.49612300100963586, 0.0, 0.0, 1.075151264033199, 0.0, 0.18716105879611752, 0.03950202184626144, 0.0, 0.0, 0.7852572057826378, 0.0, 0.0, 0.08756438716142145, 0.27446263899702106, 0.0, 0.0, 0.07204260281080108, 0.0, 0.09291620627436281, 0.23270017215835156, 0.23824566618105675, 0.0, 0.0, 0.0, 0.13665872084048475, 0.0, 0.0, 0.0067937121060301624, 0.1164801427986044, 0.0, 0.5691621002508566, 0.4713388661713567, 0.41358181631025637, 0.0, 0.3128607440091297, 0.0, 0.0, 0.20424594336339066, 0.0, 0.0, 0.7442286193021344, 0.10553727794892508, 0.0, 0.30186901767997887, 0.0, 0.0, 0.0, 0.32291683563950796, 0.07758151598185183, 0.0, 0.347786535534979, 0.7360230359944353, 0.0, 0.0, 0.0, 0.8757273805535595, 0.41322710295268655, 0.0, 0.25679112297862333, 0.17774302673574222, 0.5841939760860391, 0.0, 0.0, 0.0, 0.13300779742001662, 0.16915583797139946, 0.0, 0.0, 0.0, 0.7164740353603845, 0.45156468002631484, 0.0, 0.27217311439601866, 0.7084784766971438, 0.06835326871026481, 0.0, 0.0, 0.0, 0.2861016237491002, 1.0700436562849107, 0.135740840434777, 0.2599664462430113, 0.45536369102376945, 0.0, 0.0, 0.0, 0.16460938932017066, 0.9424546774296569, 0.4980075807530193, 0.501649117240962, 0.9215918221852557, 0.0, 0.5227646022766553, 0.46865433734477785, 0.0, 0.1899494468105599, 0.0, 0.14204872822901377, 0.7891527088162923, 0.2800775160398023, 0.0, 0.0, 0.6038144125549446, 0.0, 0.4101280976499608, 0.15813547044885462, 0.4381740566617935, 0.0025678619479343042, 0.5913196381458786, 0.01581382625018507, 0.4317825799661969, 0.0, 0.5361217114844467, 0.16404576969544252, 0.0, 0.03623369485950333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19327505078226082, 0.1125279795645968, 0.0, 0.38171505913646114, 0.0, 0.0012820843611261621, 0.0, 0.0, 0.017018867107690273, 0.0, 0.018573326277702022, 0.7755881641075008, 0.12996991545193906, 0.0, 0.18005632805213245, 0.0, 0.2451805301156947, 0.0, 0.3747092870507497, 0.0, 0.4781929115858312, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.35214721497251084, 0.0, 0.020304118651362432, 0.14466279308909658, 0.626862644216891, 0.0, 0.0, 1.5549351131665947, 0.30737844662289215, 0.0, 0.0, 0.0, 0.0, 0.009701873820892715, 0.2520412914553624, 0.018396624198184493, 0.0, 0.7982268230101296, 0.0, 0.9201682818000929, 0.0, 0.0, 0.7342011144064367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09396246898802936, 0.0, 0.5073258744317697, 0.22284199906817412, 0.10151898941699102, 0.03180992568768135, 0.0, 0.5543686635557524, 0.0, 0.0, 0.0, 0.22367207355552704, 0.0, 0.0, 0.0, 0.40880306935546484, 0.0, 0.42113696363931935, 0.0, 0.0, 0.0, 0.43326306412219423, 0.39232717474974355, 0.0, 0.0, 0.0, 0.013675206005982276, 0.0, 0.2603519242467926, 0.3502025254743841, 0.14312660068897712, 0.34775162340880994, 0.20310882214946477, 0.0, 0.27127891969267615, 0.0, 0.0, 0.020541404680713544, 0.0, 0.0, 0.1991870263338475, 0.0, 0.0, 0.2456237488004554, 0.12123860127246498, 0.2079074067296303, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24646978770136238, 0.15225808974571345, 0.0, 0.0, 0.46988488666249073, 0.16461741428636234, 0.4415386194506142, 0.05631807188893752, 0.0611318669096905, 0.0, 0.0, 0.0, 0.5215534136524774, 0.0, 0.0, 0.6274626509368973, 0.038256585847320265, 0.530070634494204, 0.0, 0.7877809268797783, 0.25601250351282806, 0.37534186876141573, 0.700013095645165, 0.0, 0.29431110179251513, 0.22882177649965688, 1.0569304890861846, 0.0, 0.0, 0.0, 0.0, 0.02833923205698933, 0.02572736738919585, 0.0, 0.1150475064446459, 0.24371977071087828, 0.0, 0.0, 0.044121908414131224, 0.0771055384425919, 0.09695789652483547, 0.0, 0.0, 0.0, 0.12020713596947674, 0.045249684273424934, 0.8721213827038133, 0.0, 0.11731035534021127, 0.37569616707373005, 0.0, 0.0, 0.9570606189270042, 0.9086284112098039, 0.20027308709305888, 0.4268041763647709, 0.0, 0.0, 0.015909944043088663, 0.0, 0.7658973441527888, 0.0, 0.19507759787979717, 0.3226660152512004, 0.23629991340839918, 0.0, 0.13848101841219138, 0.0, 0.0, 0.24578615843203896, 0.0, 0.0, 0.10167993256702677, 0.0, 0.0, 0.12538470338440244, 0.9934363459809055, 0.10613146591697438, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6421265863141966, 0.0, 0.5400344618364347, 0.025731716559651077, 0.0, 0.0, 0.0, 0.007521097208030019, 0.5455660054203588, 0.0, 1.1209603429913297, 0.0, 0.0, 0.5234607155600556, 0.0, 0.023977651748011766, 0.0, 0.0, 0.0, 0.27798322874225667, 0.0, 0.35612425891926536, 0.0, 0.7529913555346315, 0.6214420882570796, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12146506806205652, 0.0, 0.3210215267482459, 0.0, 0.0, 0.0, 0.33419799384234544, 0.0, 0.0, 0.15359727591834285, 0.19534099821264672, 0.525863863274946, 0.0, 0.0, 0.34569678733330644, 0.0, 0.0, 0.3415674397857982, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.523004359259986, 0.0, 0.22235180304591923, 0.0, 0.00635695661054767, 0.0, 0.0, 0.0843846185513356, 0.0, 0.0, 0.0, 0.8855039996037297, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27294552017018114, 0.0, 0.3412878895624823, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5864608505347954, 0.0, 0.10380099346078346, 0.02524889717437322, 0.0, 0.0, 0.38157075730498896, 0.0, 0.0, 0.0, 0.45469484904050655, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26459586877877683, 0.18314523505716396, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22372814141562178, 0.0, 0.0, 0.0, 0.6276845275284666, 0.18331203344516112, 0.25689720406507865, 0.04048914090414101, 0.0, 0.016969058333515752, 0.0, 0.5911567891086366, 0.37358991094373184, 0.21597413111529512, 0.09535966106843696, 0.021909623318208237, 0.49412659112391777, 0.39229708641560146, 0.0, 0.0, 0.5239905526116051, 0.22383442432789932, 0.0, 0.408761833081477, 0.7858251636352953, 0.0, 0.03095169444374481, 1.0134328993353359, 0.0, 0.5783663988186404, 0.023349871375665725, 0.4068090650141155, 0.3701047391436106, 0.7170357796952346, 0.31327392774306395, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.39350494784782397, 0.07595368484327951, 0.4868292236818003, 0.046986031631648606, 0.530207151960747, 0.01037806421927761, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05535788985307855, 0.7394134258901907, 0.17106635844034493, 0.9821443208953105, 0.0, 0.008962807708770178, 0.0, 0.0, 0.11897566020800357, 0.0, 0.0, 0.0, 1.2828090910806784, 0.0, 0.4691443493685695, 0.0, 0.07022467062008277, 0.18231155361607748, 0.41302392682414213, 0.0, 0.13696413696930718, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.423686950315378, 0.0, 0.5297732126841366, 0.0, 0.10086209606266386, 0.4105665706595916, 0.2899236467445781, 0.08479167779863508, 0.0, 1.1607169616195545, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44056670549109944, 0.0036426275414389976, 0.22268017654499572, 0.70259131988822, 0.021981713208965886, 0.0, 0.0, 0.0, 0.6644482698446481, 0.5559910462571956, 0.9081368976941899, 0.9545466439826781, 0.0, 0.5800820874802947, 0.6131061961157059, 0.0, 0.3387938943904664, 0.0, 0.41233232521209184, 0.6892320892052487, 0.9168247018477867, 0.7592095444656833, 0.0, 0.7080274090242662, 0.0, 0.0, 0.7409103227795685, 0.4891910517462003, 0.0, 0.0, 0.6996898878138225, 0.18201720925245757, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.033060035104523045, 0.0, 0.0, 0.0, 0.0, 0.0010434833431631984, 0.0, 0.0, 0.0, 0.010921241238865733, 0.0, 0.015498170387323498, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011618017340433275, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2104054739321471, 0.0, 0.0, 0.0, 0.051249741424478176, 0.0, 0.0, 0.0, 0.09744956100917274, 0.0, 0.0, 0.0, 0.0, 0.10766993925868745, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18951483497887683, 0.5969374604132631, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6248091001560899, 0.0, 0.0, 0.0, 0.0, 0.18617398114562853, 0.0530079909103574, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14076422137873648, 0.3823151552281139, 0.28535858102685935, 0.3054264047421474, 0.0, 0.0, 0.5425991952026914, 0.16566917168527906, 0.0, 0.0, 0.24572721112234908, 0.0, 0.0, 0.311993347128524, 0.0, 1.0231061248638629, 0.5939954507294103, 0.5983388707717968, 0.7851882761904205, 0.0, 0.28209866162610053, 0.46470151179975694, 0.0, 0.22767102275324697, 0.0, 0.0, 0.3351791998251741, 0.0, 0.0, 0.0, 0.3258354466053841, 0.29514851487698524, 0.25858133097678015, 0.1886151008053812, 0.5226293862659805, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0442888167727012, 0.0, 0.002505192641426779, 0.0, 0.0, 0.07664740890754863, 0.0, 0.0, 0.013363010227595206, 0.00048725362450601585, 0.002419244088482629, 0.026391726291740327, 0.0, 0.0, 0.03460717393330891, 0.0, 0.13624247986958654, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012449069569846, 0.0, 0.016951747875799564, 0.0, 0.0197038621774241, 0.0, 0.033062191640471625, 0.07310509907314719, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06556034095953286, 0.0, 0.02434740964367002, 0.0, 0.0, 0.13592547532623306, 0.0, 0.030022610188286405, 0.03795734899490777, 0.008699370747195389, 0.0, 0.0, 0.18940737674761182, 0.16014434992779022, 0.0058393216511455045, 0.0, 0.3162824676834449, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14919154588027264, 0.0, 0.20315232851529888, 0.0, 0.27582528532656464, 0.0, 0.3962223404212856, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00142596300027198, 0.0, 0.008781597914028754, 0.2917830655969515, 0.0, 0.0, 0.3010196130390429, 1.085161420061463, 0.0, 0.8818413438369698, 0.0, 0.14807253735972709, 0.6682230613653689, 0.6738718955158736, 0.0, 0.2921120382906695, 0.0, 0.23016863424694362, 0.44343897402347887, 0.0, 0.0, 1.2859761704109314, 1.188418751121335, 0.0, 0.4620501506594876, 0.23396469855670338, 0.0, 0.03498671823904876, 0.0, 0.0, 0.3392611399043929, 0.20497564811318061, 0.0, 0.4183535169904413, 0.20649711391931772, 0.520902106226398, 0.0, 0.0, 0.16125806820737418, 0.5576988707246249, 0.0, 0.0, 0.0, 0.11391623120681071, 0.0, 0.0, 0.09151851725619714, 0.0, 0.0, 1.197254000111945, 0.0, 0.8775820846458859, 0.0, 0.5629797627532018, 0.6687057520979063, 0.341227000414515, 0.0, 0.25482576175079585, 0.0, 0.39537376259476137, 0.4437592920364036, 0.0, 0.0, 0.5927285333298421, 0.651986303075751, 0.0, 1.0434591895456948, 0.0, 0.0, 0.03501199088471498, 0.0, 0.0, 0.3395062050893736, 0.15619471223870085, 0.0, 0.41865571453083017, 0.20664627704903807, 0.48238135206809674, 0.08860575544817058, 0.0, 0.005695035181786341, 0.42497543210880456, 0.0, 0.23934194260454025, 0.30438882267274336, 0.2874361482000896, 0.0, 0.0, 0.6245971595264426, 0.0, 0.3639559852559372, 0.0, 0.0, 0.20575691959192335, 0.0, 0.5994751724692355, 0.0, 0.0, 0.0, 0.0, 0.002904710270589501, 0.0, 0.27722048222724693, 0.15720637186052522, 0.0, 0.40582979792580204, 0.3276141191929195, 0.0, 0.5216472712210337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022828121623522435, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.111114554054197, 0.0, 0.0, 0.005523835481493179, 0.12589753624924738, 0.4223352609451343, 0.19512225997871732, 0.9646462752755526, 0.0, 0.0, 0.908234546074678, 0.0, 0.1303083320340461, 0.4180115467796097, 0.3502002628164635, 0.005797584702932196, 0.620011620521792, 0.6673813159226286, 0.0030737631986123407, 0.0, 0.42144642014638856, 1.0414378536891689, 0.6289622414694364, 1.5563606265339154, 0.08385396416867873, 0.3115994197854627, 0.7117829870894571, 0.6381078265340546, 0.0, 0.23989801450652898, 0.0, 0.5826228436020011, 0.8457142287971168, 0.0, 0.0, 0.0, 1.0193786877434905, 0.0, 0.0, 0.19971832516875276, 0.5533950636153777, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21472741909783344, 0.39082851888093567, 0.44716245502350144, 0.2879713896328549, 0.5511368948723377, 0.33565164359388877, 1.3517828371888283, 0.0, 0.09957607530690628, 0.0, 0.0, 0.26103163435011795, 0.3965287786994862, 0.6597151598792015, 0.0, 0.34286845579856556, 0.3900059867245693, 0.34423839922938515, 0.0, 1.2842745238771571, 0.3694211002080512, 0.0, 0.05489702502604951, 0.3645945046142821, 0.0, 0.0, 0.08041277613458127, 0.2893532857853331, 0.0, 0.5419495915870312, 0.70973704371772, 0.06597297072421819, 0.0, 0.0, 0.28417074423649047, 0.45673560467870195, 0.1772454725351981, 0.22541615615699262, 0.08077018346809645, 0.5662548477384789, 0.07678831671440492, 0.7573984424681688, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4826509895037858, 0.0, 0.7156459126461995, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013563432420802306, 0.4559162518278489, 0.0, 0.0, 0.0, 0.0, 0.26262283944881243, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16505781512417061, 0.2933206286622639, 0.0, 0.0, 0.0, 0.0, 0.40289003757510067, 0.0, 0.0, 0.0, 0.6372941893353361, 0.0, 0.6237934507673457, 0.8070702584067075, 0.0, 0.0, 0.3379623279083573, 0.0, 0.46129991809862586, 0.7333015288992237, 0.21031442023566344, 0.0, 0.7041420713561054, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7760176053652288, 0.0, 1.0534236649773678, 0.0, 0.0, 0.8070780011372765, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4823716017908521, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6208306891807599, 0.0, 0.0, 0.01732988822073383, 0.2738833349341264, 0.10672374403860475, 0.0, 0.008043590060480383, 0.0, 0.055277084044920086, 0.0, 0.0, 0.17943961322559207, 0.0, 0.0, 0.0, 0.0, 0.0, 0.718108290096912, 0.0, 0.0, 0.0, 0.008728384689335622, 0.0, 0.1697763404331079, 0.11586384141065409, 0.0, 0.0, 0.0, 0.8848305575683707, 0.0, 0.8449968950182224, 0.0, 0.0, 0.0, 0.6706152313255382, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8308196053978146, 0.0, 0.9175002824131868, 0.7530109173431534, 0.022342425693822682, 0.0, 0.0035915524463436964, 0.15288943835905894, 0.0, 0.5199425421439203, 0.11917728725476327, 0.3521011644457409, 0.0, 0.822488739774635, 0.513543493422637, 0.2981570434312181, 0.0, 0.0, 1.2701086711578897, 0.0, 0.11247893732510689, 0.0, 0.039332390318575425, 0.0, 0.11102635670488926, 0.0, 0.15118325075382752, 0.0, 0.1757278339799435, 0.0, 0.2948633760880991, 0.15371975197149906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04453230950399372, 0.0, 0.05568267952698087, 0.0, 0.21714106205060016, 0.0, 0.0, 0.0, 0.0, 0.7213227431120439, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2701718137836067, 0.0, 0.06123281377737304, 0.4007297534572718, 0.0, 0.0, 0.02350606599699272, 0.0, 0.47270164239805296, 0.39554302818161685, 0.39843532224568856, 0.0, 0.0, 0.46004399190355705, 0.41242580548884994, 0.0, 0.15086754157289264, 0.0, 0.0, 0.5466072621606027, 0.18414300130678235, 0.2087681866233734, 0.0, 0.5313695523968773, 0.0, 0.0, 0.2807883504872339, 0.34802019073798685, 0.0, 0.09776175774612722, 0.1924014129464173, 0.0, 0.0, 0.0, 0.2995245510564702, 0.1439415371971276, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09612835370305864, 0.0, 0.07178793274997196, 0.0, 0.0, 0.0, 0.529229742886101, 0.0, 0.0, 0.5898615183638936, 0.0, 0.1996504875270556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07685007162501065, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4724449949223617, 0.0, 0.21985358976027652, 0.0, 0.0, 0.0, 0.6511036698044336, 0.5394783324223208, 0.9048355234024855, 0.0, 0.0, 0.0, 0.008134079103649166, 0.17236516549262873, 0.0, 0.7108281927774538, 0.0, 0.0, 0.0, 0.8245834715993663, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22989626507902333, 0.12442621872484774, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3037122713618223, 0.0, 0.0, 0.008259351424073042, 0.4378900919766154, 0.05086408498890661, 0.31974807102322866, 0.0, 0.0, 0.7960980638875322, 0.0689135689704732, 0.0, 0.0, 0.0, 0.20359294387478752, 0.9097633301509839, 0.0, 0.6841090251831579, 0.0, 0.6154584825649704, 0.7335150023327748, 1.015637287349907, 0.0, 0.0779165260149525, 1.1277817941657742, 0.0, 0.0, 0.3457097793442966, 0.6958114560471119, 0.22297907055084815, 0.02617735461213243, 0.8271866794310433, 0.0, 0.25383801660715655, 0.0, 0.6564805026462143, 0.4519424562793308, 0.8175733700968422, 0.26495100977979114, 0.0, 0.0, 0.0, 0.33910693050583907, 0.0, 0.0, 0.0, 0.4889218480481046, 0.0, 0.3925649791995639, 1.5627863234111485, 0.8201598086100989, 0.28474440170274185, 1.0514838770367962, 0.010290531779792767, 0.0, 0.0, 1.5282247448720891, 0.4202365445740981, 0.38631591015261757, 0.07827208426350975, 0.002001482371009478, 0.19978631068641475, 0.10840878806592323, 0.8849580765325739, 0.7025926608640433, 0.21468725597950544, 0.6515266028885637, 0.304659479667613, 0.0, 0.5203661079080213, 0.1989832785613875, 0.22866153178235926, 0.2129788116497342, 0.0511368043793991, 0.0, 0.06963236972450947, 0.0, 0.08093717687353573, 0.0, 0.3254291820176872, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4944791980843742, 0.5417407856384763, 0.0, 0.4343792686365096, 1.5442785077006684, 0.0, 0.13746522377571999, 0.0, 0.0, 0.0, 0.0, 0.01933245239450143, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44623451745418, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.30502005455201636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014872981553739924, 0.0, 0.0, 0.0007393802182031884, 0.0, 0.14653699929908884, 0.0, 0.0, 0.26188312990151436, 0.028159286104530473, 0.0, 0.4084686341089707, 0.9235151006259961, 0.5957064186534903, 0.0, 0.0, 0.5535709817494249, 0.0, 0.25891613288066156, 0.07783302925410085, 0.2939591160629263, 0.1987201643010643, 0.0, 0.09767541007114018, 0.4120929351702914, 0.22225830848448824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13739476210353194, 0.0, 0.037597515302596925, 0.0, 0.0, 0.31874062595288627, 0.33536722163001637, 0.0, 0.0, 0.0, 0.48352996362981304, 0.0, 0.38851289261478805, 0.6830254018960756, 0.3364461001285706, 0.8253161071254982, 0.0, 0.2348841918025421, 0.0, 0.4205219693225024, 0.5314960091024892, 0.0, 0.0, 0.10178270195612651, 0.0, 0.0, 0.780835547454152, 0.5872223143240599, 0.2503853190515238, 0.0, 0.0, 0.6150976289535472, 0.0, 0.09445718365268194, 0.0, 0.0, 0.17331386223779208, 1.5139967025892487, 0.0, 0.0031631037123804793, 0.0, 0.0, 0.20592512324808152, 0.0, 0.03635874582185372, 0.0, 0.20018457152414249, 0.22068196040992705, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 42), 42)),)

