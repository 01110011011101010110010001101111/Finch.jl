julia> @finch begin
        CR .= 0
        for i = _
            for j = _
                for k = _
                    CR[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(CR = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.24622058278177836, 0.0, 0.0, 0.1261597862603059, 0.07342085928314178, 0.12871077894157168, 0.0, 0.0, 0.8034179790657067, 0.0, 0.0, 0.056025142012607894, 1.0596276779513034, 0.0, 0.12141212330168466, 0.0, 0.5574009533856183, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3327944495595785, 0.0, 0.8694342868598712, 0.0, 0.7416396915765561, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7569986463219165, 0.05683836078786318, 0.0, 0.0, 0.8725529080345216, 0.0, 0.2047777547094641, 0.5614660149749067, 0.0, 0.0, 0.36223551246779223, 0.0, 0.0, 0.0, 0.22107392894943306, 0.0, 0.0, 0.0, 0.3259321271119929, 0.5055955427859341, 0.27515457195781157, 0.4169876847813729, 0.018631329176858533, 0.3876953885710935, 0.0, 0.021531556327917045, 0.0, 0.0, 0.28223766305057085, 0.48287552686851865, 0.0, 0.0, 0.10175887870070799, 0.0, 0.0, 0.0, 0.06032870137556734, 0.018726053338837593, 0.045366945649236726, 0.0, 0.0, 0.0, 0.5790914418316278, 0.6815353846022115, 0.0, 0.1332216207413034, 0.0, 0.04017082185996856, 0.0, 0.0, 0.0, 0.0, 0.03484741576880692, 0.893850322287328, 0.4479341395605485, 0.5025126461241762, 0.1987665376735214, 0.31067608319719825, 0.0, 0.0, 0.0, 0.46442597522784795, 0.0, 0.0, 0.4431570212945692, 0.0, 0.0, 0.5121406140989466, 0.0, 0.49477960883115907, 0.0, 0.0, 0.0, 0.0, 0.47025922462023484, 0.3616621631765011, 0.5907253850051586, 0.0, 0.0, 0.44541009068477694, 0.03445733103321199, 0.0, 0.02309502433018302, 0.0, 0.03777890472311954, 0.16345317623186756, 0.16813336048226513, 0.0, 0.0, 0.09235605661534649, 0.0, 0.0, 0.01896246145159963, 0.02753978144349941, 0.0, 0.0, 0.0, 0.30135789429977294, 0.6764584765695312, 0.0, 0.0, 0.3151319553463997, 0.0, 0.04554105431298512, 0.0, 0.08378021558986462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09849622821166114, 0.0, 0.25488776315346307, 0.0, 0.2781851808227525, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2839462446541717, 0.00854309647598918, 0.0, 0.0, 0.3272900456325508, 0.10375527349591217, 0.8492672297194694, 0.0, 0.5876718077641173, 0.03093892983747342, 0.0, 0.0, 0.0, 0.3385535490482335, 0.0, 0.0, 0.0, 0.29218416128587404, 0.0, 0.6966394562456512, 0.0, 0.0, 0.0, 0.522006486105503, 0.0, 0.0, 0.0, 0.11065330711314218, 0.0, 0.23311787887979893, 0.0, 0.31252070060750486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06134998798063221, 0.0, 0.0, 0.3657519298489218, 0.31899283438369924, 0.0, 0.0, 0.0, 0.3676864240590821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0010478172394749591, 0.5287716819761857, 0.0, 0.3903081303690744, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0017717866111426558, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5892681933608603, 0.002977075804572329, 0.0, 0.0, 0.0, 0.0, 0.43072916849878135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.045384364903179854, 0.0, 0.0, 0.0018618199412216315, 0.0, 0.17111256306326456, 0.2672837889083413, 0.0, 0.0, 0.19576072012760673, 0.5948736598005219, 0.08413627091493711, 0.7573648130796338, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33101786250771764, 0.16289357736810944, 0.16515314938780928, 0.0, 0.0, 0.0, 0.0, 0.0, 0.662932866371622, 0.6905568008213746, 0.0, 0.0, 0.0, 0.0, 0.4845748090935882, 0.061923683856559844, 0.0, 0.028719251839850298, 0.0, 0.02009827057946138, 0.0, 0.0, 0.0, 0.11124419756806017, 0.4147463454400596, 0.0, 0.0, 0.34783853396429565, 0.0, 0.09260170356894369, 0.0, 0.0, 0.0, 1.1401462738363544, 0.915922599074042, 0.0, 0.0, 0.0, 0.0, 0.6274522680730893, 0.0, 0.3139030754242789, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3751134192842195, 0.0, 1.257728265907189, 0.0, 0.35074493350028346, 0.268137845429166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016899841379555174, 0.7155448975358032, 0.1680074523773585, 0.0, 0.0, 0.30581125296894957, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9366277987154731, 0.0, 0.0, 0.0, 0.0, 0.07522986257761975, 0.06328419096078892, 0.0, 0.02479071082883737, 0.0, 0.7915629916935427, 0.031935353301522654, 0.0, 0.0, 0.070458258165472, 0.0, 0.0, 0.0, 0.0, 0.052737602183104026, 0.0, 0.013647273418699125, 0.3169788666986562, 0.17235734856254364, 0.0, 0.0, 0.0, 0.0661528775144355, 0.0, 0.027308490724219276, 0.0, 0.0, 0.0, 0.7971915175033586, 0.0, 0.5343175730821937, 0.0, 0.8740381649643706, 0.04598025590229294, 0.029012248678093225, 0.0, 0.0, 0.11141095490761138, 0.0, 0.0, 0.13001430753376275, 0.0, 0.0, 0.0, 0.22549966134722088, 0.0, 0.0, 0.0, 0.3726692665495664, 0.3774382964295704, 0.20411971872540274, 0.06057654493015839, 0.0, 0.5744310538348768, 0.0, 0.0, 0.6232315714035893, 0.0, 0.5072896743166587, 0.0, 0.005553340050755054, 0.5689868928162961, 0.06836125145830099, 0.0, 0.0, 0.49734376521104756, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27904969776436467, 0.0, 0.7814242677772039, 0.5117851675292832, 0.0, 0.0, 0.04886194301383263, 0.0, 0.3067365939854962, 0.0, 0.0, 0.4115088709036164, 1.405104579225751, 0.42840174089528493, 0.48760195358213543, 0.0, 0.0, 0.663844265848651, 0.0, 0.38778436442248837, 0.7776635401136125, 0.15032072106029123, 0.18953081567758903, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7044650329571657, 0.7235946500790119, 0.12987194286479317, 0.19647866132545186, 0.45582545423087456, 0.0, 0.0, 0.0, 0.032958397990191235, 0.0, 0.0, 0.7570462349991403, 0.0941136312544236, 0.0, 0.1276645560961348, 0.0, 0.0, 0.0, 0.755417985546979, 0.8458204729185679, 0.12543050025210123, 0.006393660363144921, 0.0, 0.0, 0.28926738179087036, 0.0, 0.0, 0.4731345211962404, 0.0, 0.7203970460167203, 0.0, 0.0, 0.0, 0.4891311717243447, 0.0031333060836164625, 0.003950606766695812, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8250852706661534, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1940012409565178, 0.0, 0.7255195358366966, 0.0, 0.4862794311033925, 0.0, 0.7981182971725221, 0.0, 0.26385403496014914, 0.0, 0.5139863704027979, 0.35381636124647037, 0.06016159688024159, 0.0, 0.0, 0.0, 0.07675121814682248, 0.0, 0.0, 0.12553662505708466, 0.0, 0.301350309711505, 0.0, 0.0, 0.0, 0.12978101101826656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2180668401971053, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09272141980768678, 0.0, 0.3034931056908601, 0.0, 0.203416238280828, 0.0, 0.33274884560755447, 0.0, 0.12655518438173288, 0.0, 0.13637583261219158, 0.0, 0.0, 0.0, 0.0, 0.020115042222633606, 0.0, 0.11478942012015081, 0.0, 0.044967175485766994, 0.0, 0.27969782401606164, 0.1383242818414881, 0.0, 0.0, 0.18685369035983804, 0.0, 0.06810955828519266, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17579267922314348, 0.0, 0.0, 0.11999285025017485, 0.0, 0.0, 0.14209169595972287, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11399414163060874, 0.019825113051252268, 0.0, 0.049971986914216976, 0.07235420154739257, 0.0, 0.0, 0.08331368376199759, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07280640355864025, 0.0, 0.0, 0.35152520960031586, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13676901662991092, 0.0, 0.0, 0.0, 0.015204774707799058, 0.0, 0.0, 0.0, 0.0, 0.5304031136005922, 0.0, 0.0, 0.0, 0.32819195551919517, 0.1622165373359131, 0.0, 0.0, 0.0, 0.1321652778478459, 0.0, 0.0, 0.2161735452720934, 0.0, 0.27952564740678887, 0.0, 0.0, 0.0, 0.22348236021207268, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375510190195534, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2500091160813631, 0.0, 0.2815132558946093, 0.0, 0.18868424509978618, 0.0, 0.30865021038590384, 0.0, 0.341236683480638, 0.0, 0.23483861551802387, 0.0, 0.0, 0.0, 0.12117745385937667, 0.0, 0.0, 0.3973467888918919, 0.0, 0.2920101397985984, 0.0, 0.12988448302264086, 0.0, 0.3517844506342411, 0.0, 0.0, 0.0, 0.6519761543079575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4428070420734244, 0.0, 0.3964317892338393, 0.0, 0.0, 0.0, 0.3236724995283946, 0.0, 0.0567229736921611, 0.0, 0.0, 0.0, 0.25146743826064083, 0.0, 0.0, 0.011207888208677623, 0.08869787998517195, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0436070974131627, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05218480148875932, 0.0, 0.0, 0.0, 0.28668060177386556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16806127510824798, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1010038649276795, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24411150965510042, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3252831829639798, 0.0, 0.13696611429551586, 0.0, 0.08809099936621054, 0.0, 0.0, 0.0, 0.5834039633369968, 0.2573783852497724, 0.0, 0.34353142253711105, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8226681209013311, 0.43635481618924155, 0.0, 0.21222851603946533, 0.0, 0.6640973088333184, 0.0, 0.1716321278580885, 0.7312656094018032, 0.46618173092618587, 0.7638215749927391, 0.5281765059497923, 0.012259008816529623, 0.9144713112403178, 0.0, 0.0, 0.0, 0.0, 0.7953741970864907, 0.0, 0.0, 0.5850872742568098, 0.0, 0.0, 0.0, 0.7307808207285703, 0.0, 0.6199309753815017, 0.37291372178799176, 0.0, 0.0, 0.22356724549482568, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3221254739496775, 0.11781870844111127, 0.04460774210115785, 0.7133319766560491, 0.17115949268039302, 0.47397294440775345, 0.0, 0.10101149129956093, 0.11755718548933392, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.057090584067732354, 0.6447844891876003, 0.0, 0.0, 0.033829519264607456, 0.0, 0.1190337187097033, 0.30523421948716617, 0.0, 0.0, 0.0, 0.0, 0.6213057774307251, 0.842768502451811, 0.0, 0.3906048759812801, 0.7719833104847825, 0.41168363040706235, 0.1712307445975339, 0.0, 0.05992936357237096, 0.04890539039186202, 0.0, 0.0, 0.012380496199171342, 0.0, 0.2674194407541521, 0.0, 0.0014443996714777229, 0.0, 0.16387003646802972, 0.0, 0.0, 0.03594122433913304, 0.24159569520366886, 0.3747703785250739, 0.0, 0.054699683547073226, 0.0, 0.2873774296478163, 0.0, 0.0, 0.0, 0.14956132991470428, 0.0, 0.3888645476970781, 0.0, 0.0, 0.07542830239130101, 0.07080076609705026, 0.0, 0.0, 0.0, 0.0, 0.09852911382155678, 0.19548481725379033, 0.0, 0.0, 0.4350977845419587, 0.00557774494203208, 0.0, 0.09874991570897035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6034262468294622, 0.2713368059843151, 0.6497013107648627, 0.0, 0.8007029601130917, 0.0, 0.0, 0.0, 0.0, 0.01616961592030159, 0.0, 0.0, 0.0, 0.0, 0.2502876057454169, 0.49212467358206446, 0.2134864810859512, 0.1765833209669342, 0.0, 0.0, 0.0, 0.5401972297237567, 0.03318857704329382, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12219566143679067, 0.0, 0.04829725298673487, 0.0, 0.0, 0.0, 0.0, 0.0, 1.2356793204214815, 0.02263120223723859, 0.7256596881722004, 0.3263003275955702, 0.7654157892976137, 0.08145618747845872, 0.6387218661983024, 0.0, 0.5276231408957645, 0.8184646008092085, 0.0, 0.0, 0.0, 0.6276063069977658, 0.0, 0.6132063830791735, 0.006204111701464058, 0.18851722870999638, 0.375149258411478, 0.7816851453263511, 0.0, 0.587776400734769, 0.16472858834087942, 1.1016458158754883, 0.1459604994495737, 0.0, 0.0, 0.0, 0.037075917094288376, 0.0, 0.2815459504132854, 0.2536212145780303, 0.559150690052304, 0.48027689889482117, 0.0, 0.21566088189458643, 0.058080618345779114, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008439821738835876, 0.0, 0.36408664087256903, 0.0, 0.16156000319694572, 0.22471190561828702, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10732980278519261, 0.0, 0.0, 0.12403718878006863, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27901056312251676, 0.4801492052284416, 0.0, 0.0, 0.10787548181472921, 0.0, 0.0, 0.0, 0.15654727928350437, 0.0, 0.13104284110293218, 0.13666096202593647, 0.0, 0.0, 0.0, 0.07276575816635268, 0.0, 0.0, 0.0, 0.0, 0.6100450928472326, 0.0, 0.0, 0.0, 0.4238237484103886, 0.9941317988293357, 0.0, 0.0, 0.0, 0.0, 0.3355479309040112, 0.0, 0.0, 0.0, 0.0, 0.5350029094771915, 0.0, 0.008586567333100323, 1.1385513331902584, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1850917274468032, 0.0, 0.0, 0.06716099014107918, 1.1525784735402484, 0.0, 0.0, 0.036572260841491885, 0.0, 0.26119964749940466, 0.0, 0.0, 0.8459335464772313, 0.0, 0.0, 0.0, 0.0, 0.38438887602945404, 0.0, 0.0, 0.0, 0.0, 0.7204933523373307, 0.0, 0.0, 0.16060697060474402, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7112792652633437, 0.0, 0.935521710131386, 0.26536974069831215, 0.2935825392525973, 0.0, 0.0, 0.13719141489981915, 0.0, 0.10324823792839577, 0.0, 0.0, 0.0, 0.7256165269576538, 0.0, 0.48634443939716193, 0.0, 0.7955635800994527, 0.1927019559074891, 0.0, 0.0, 0.7058981437597585, 0.0, 0.5236895266156559, 0.0, 0.08128104063747256, 0.0, 0.0, 0.0, 0.5731212091054763, 0.0, 0.0, 0.0419831484236368, 0.11625926299639212, 0.0, 0.0, 0.08927614896350113, 0.217346651106267, 0.13128361330254273, 0.0, 0.07219885291348893, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4218130442222412, 0.0, 0.0, 0.0, 0.0, 0.1699892215644571, 0.0, 0.14639742856012483, 0.0, 0.0, 0.04200062557179813, 0.0, 0.0, 0.0, 0.31146013043119736, 0.0, 0.0, 0.0, 0.24384030368566587, 0.9260558729858251, 0.4296955936776912, 0.0, 0.0, 0.0, 0.73790516731229, 0.0, 0.0, 0.0, 0.16295488074282302, 0.26550524353964333, 0.0, 0.0, 0.0, 0.31144107126852844, 0.0, 0.485858439126786, 0.0, 0.6914937323909329, 0.0, 0.0, 0.0, 0.0, 0.3162528337434342, 0.0, 0.0, 0.0, 0.0, 1.5785215576611167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5161924043112004, 0.0, 0.0, 0.2568302026164213, 0.0, 0.3023415151909343, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5030068615133427, 0.0, 0.0, 0.0, 0.2741080207900236, 0.17120972783029909, 0.0, 0.0, 0.0, 0.0, 0.43685119598979993, 0.02712666041421048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2451085199735546, 0.0, 0.1576437689621315, 0.0, 0.0, 0.0, 0.6857487349281931, 0.05917052173435364, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6537568461306704, 0.0, 0.0, 0.5395442735782994, 0.05374109131453472, 0.0, 0.5917732326059808, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0008206716541601089, 0.19818937147807536, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.535053682582317, 0.0, 0.3513819151899916, 0.04333888481337121, 0.5747915094439204, 0.19540689526232177, 0.01899860952318465, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06595709778063945, 0.0, 0.08799217533170228, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03735300199555871, 0.19147688509049948, 0.0, 0.0, 0.0, 0.2914125830050192, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04799459307428656, 0.0, 0.16480769798416883, 0.0, 0.0, 0.0, 0.04840048223798238, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027156548216558336, 0.0, 0.047181418535814645, 0.0, 0.0, 0.0, 0.0, 0.35341760898781915, 0.5999981948389704, 0.024572219243726254, 0.0, 0.0, 0.08667332426569417, 0.4875808312926843, 0.06369622265280388, 0.0, 0.0, 0.0, 0.07133443039288194, 0.0, 0.0, 0.47894471690358564, 0.4793008357816607, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09658957720985865, 0.3016070141903076, 0.061398864173637636, 0.0, 0.0, 0.0, 0.07060273869520314, 0.35885594789448355, 0.0, 0.06446892769447045, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2497207855356876, 0.018509596151814908, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2926774045716465, 0.0, 0.013651758929637155, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03597721563910695, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07068090962707653, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03642909404914556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2749443358980585, 0.0, 0.0, 0.0, 0.0, 0.5096842842109697, 0.0, 0.0, 0.0, 0.028086239659725638, 0.7421154878137212, 0.0, 0.48123810905471315, 0.0, 0.3067030382176026, 0.6866619523091189, 0.0008295150087239233, 0.5016528107597976, 0.0, 0.08848804808651539, 1.3805931747462472, 0.0, 0.0, 1.04718815925786, 0.0, 0.0, 0.0, 0.42746548493362363, 0.0, 0.5817357352379013, 0.8450882691967682, 0.41665385220387907, 1.0915687546509307, 1.108964330219497, 0.0, 0.0, 0.0, 0.04066069749695377, 0.032733248007639555, 0.0, 0.0, 0.0, 0.07163077100441487, 0.6487063582321942, 0.0, 0.2995103130321618, 0.10283702762113958, 0.33384701220391544, 0.41258982728857174, 0.0, 0.5449670143554067, 0.0, 0.3217489265559145, 0.0, 0.0, 0.0, 0.2849908553676538, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01610871320352385, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2570528176713379, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2969662795916102, 0.0, 0.12312876081315716, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09075655657480966, 0.0, 0.3633373647258237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06617087939273322, 0.0, 0.0, 0.0, 0.00923594008230594, 0.17805875048711786, 0.22450411347361834, 0.0, 0.0, 0.09502241267674551, 0.0, 0.024589605765570302, 0.0, 0.1826423790293924, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039040067948029694, 0.0, 0.06664139722445166, 0.0, 0.04466639301844483, 0.0, 0.22428735118617732, 0.08284705163804024, 0.05227416022819031, 0.0, 0.19522102914015915, 0.8685310451611696, 0.0, 0.0, 0.0, 0.0, 0.8702352191794189, 0.0, 0.0, 0.15684527219370442, 0.0, 0.9243788213991903, 0.22207735215974056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.40507055158794986, 0.30718446392066534, 0.0, 0.46812545232304087, 0.0, 0.5780009938921578, 0.18279458657604497, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02896217836710352, 0.0, 0.0, 0.40712998428742797, 0.7449999863100496, 0.0, 0.49933620201845186, 0.0, 0.8168155413547457, 0.11457346224809285, 0.0, 0.0, 0.0, 0.15332753334456128, 0.11864376215788304, 0.0, 0.0, 0.04572095120049149, 0.0, 0.0, 0.31683394463012315, 0.5003078767139268, 0.14059206617597214, 0.19554784897288408, 0.0, 0.43178409369310355, 0.0, 0.07560627618395319, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16352131380333873, 0.0, 0.34449708571742155, 0.0, 0.4618370375665847, 0.0, 0.0, 0.41783342102021986, 0.0, 0.0, 0.0, 0.02165252564332241, 0.0, 0.0, 0.0, 0.47140143149032104, 0.39181349979635294, 0.11892452733738064, 0.0, 0.5433598750764469, 0.0, 0.0, 0.0, 0.07266999506080335, 0.0, 0.6735755940113932, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0041630866962457744, 0.2109647750127205, 0.0, 0.0, 0.0, 0.3715024914369038, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1845463840294256, 0.0, 0.6075440706050301, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.29101469974401967, 0.0, 0.0, 0.032739778024003775, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4875593392929034, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20371408580482323, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0439075644510851, 0.0, 0.5496827874572944, 0.40327797474842975, 0.37238046622077875, 0.0, 0.0, 0.17401376516444506, 0.0, 0.11278871866256229, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15394336215607962, 0.0, 0.0, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = ((ex.bodies[1]).bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    result = nothing
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = (Finch).moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = (Finch).moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            phase_start_2 = max(1, 1 + fld(A_lvl.shape[1] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                for i_7 = phase_start_2:phase_stop_2
                    B_lvl_q = B_lvl_ptr[1]
                    B_lvl_q_stop = B_lvl_ptr[1 + 1]
                    if B_lvl_q < B_lvl_q_stop
                        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                    else
                        B_lvl_i_stop = 0
                    end
                    phase_stop_3 = min(B_lvl.shape[2], B_lvl_i_stop)
                    if phase_stop_3 >= 1
                        if B_lvl_tbl2[B_lvl_q] < 1
                            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        while true
                            B_lvl_i = B_lvl_tbl2[B_lvl_q]
                            B_lvl_q_step = B_lvl_q
                            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                            end
                            if B_lvl_i < phase_stop_3
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_5 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_5 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_5
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_6 = min(B_lvl_i_2, phase_stop_5, A_lvl_i)
                                        if A_lvl_i == phase_stop_6 && B_lvl_i_2 == phase_stop_6
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_7 = min(i_7, A_lvl_i_stop_2)
                                            if phase_stop_7 >= i_7
                                                if A_lvl_tbl1[A_lvl_q] < i_7
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_7
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_9 = min(A_lvl_i_2, phase_stop_7)
                                                        if A_lvl_i_2 == phase_stop_9
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_6
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_6
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_6 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            else
                                phase_stop_14 = min(B_lvl_i, phase_stop_3)
                                if B_lvl_i == phase_stop_14
                                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_14
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_7
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_15 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_15 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_15
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_16 = min(B_lvl_i_2, A_lvl_i, phase_stop_15)
                                            if A_lvl_i == phase_stop_16 && B_lvl_i_2 == phase_stop_16
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_17 = min(i_7, A_lvl_i_stop_4)
                                                if phase_stop_17 >= i_7
                                                    if A_lvl_tbl1[A_lvl_q] < i_7
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_7, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_17
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_19 = min(A_lvl_i_4, phase_stop_17)
                                                            if A_lvl_i_4 == phase_stop_19
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_16
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_16
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_16 + 1
                                        end
                                    end
                                    B_lvl_q = B_lvl_q_step
                                end
                                break
                            end
                        end
                    end
                end
            end
        end
    resize!(val, A_lvl.shape[1] * B_lvl.shape[2])
    result = (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
    result
end
julia> @finch begin
        Ct .= 0
        for i = parallel(_)
            for j = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.24622058278177836, 0.0, 0.0, 0.1261597862603059, 0.07342085928314178, 0.12871077894157168, 0.0, 0.0, 0.8034179790657067, 0.0, 0.0, 0.056025142012607894, 1.0596276779513034, 0.0, 0.12141212330168466, 0.0, 0.5574009533856183, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3327944495595785, 0.0, 0.8694342868598712, 0.0, 0.7416396915765561, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7569986463219165, 0.05683836078786318, 0.0, 0.0, 0.8725529080345216, 0.0, 0.2047777547094641, 0.5614660149749067, 0.0, 0.0, 0.36223551246779223, 0.0, 0.0, 0.0, 0.22107392894943306, 0.0, 0.0, 0.0, 0.3259321271119929, 0.5055955427859341, 0.27515457195781157, 0.4169876847813729, 0.018631329176858533, 0.3876953885710935, 0.0, 0.021531556327917045, 0.0, 0.0, 0.28223766305057085, 0.48287552686851865, 0.0, 0.0, 0.10175887870070799, 0.0, 0.0, 0.0, 0.06032870137556734, 0.018726053338837593, 0.045366945649236726, 0.0, 0.0, 0.0, 0.5790914418316278, 0.6815353846022115, 0.0, 0.1332216207413034, 0.0, 0.04017082185996856, 0.0, 0.0, 0.0, 0.0, 0.03484741576880692, 0.893850322287328, 0.4479341395605485, 0.5025126461241762, 0.1987665376735214, 0.31067608319719825, 0.0, 0.0, 0.0, 0.46442597522784795, 0.0, 0.0, 0.4431570212945692, 0.0, 0.0, 0.5121406140989466, 0.0, 0.49477960883115907, 0.0, 0.0, 0.0, 0.0, 0.47025922462023484, 0.3616621631765011, 0.5907253850051586, 0.0, 0.0, 0.44541009068477694, 0.03445733103321199, 0.0, 0.02309502433018302, 0.0, 0.03777890472311954, 0.16345317623186756, 0.16813336048226513, 0.0, 0.0, 0.09235605661534649, 0.0, 0.0, 0.01896246145159963, 0.02753978144349941, 0.0, 0.0, 0.0, 0.30135789429977294, 0.6764584765695312, 0.0, 0.0, 0.3151319553463997, 0.0, 0.04554105431298512, 0.0, 0.08378021558986462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09849622821166114, 0.0, 0.25488776315346307, 0.0, 0.2781851808227525, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2839462446541717, 0.00854309647598918, 0.0, 0.0, 0.3272900456325508, 0.10375527349591217, 0.8492672297194694, 0.0, 0.5876718077641173, 0.03093892983747342, 0.0, 0.0, 0.0, 0.3385535490482335, 0.0, 0.0, 0.0, 0.29218416128587404, 0.0, 0.6966394562456512, 0.0, 0.0, 0.0, 0.522006486105503, 0.0, 0.0, 0.0, 0.11065330711314218, 0.0, 0.23311787887979893, 0.0, 0.31252070060750486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06134998798063221, 0.0, 0.0, 0.3657519298489218, 0.31899283438369924, 0.0, 0.0, 0.0, 0.3676864240590821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0010478172394749591, 0.5287716819761857, 0.0, 0.3903081303690744, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0017717866111426558, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5892681933608603, 0.002977075804572329, 0.0, 0.0, 0.0, 0.0, 0.43072916849878135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.045384364903179854, 0.0, 0.0, 0.0018618199412216315, 0.0, 0.17111256306326456, 0.2672837889083413, 0.0, 0.0, 0.19576072012760673, 0.5948736598005219, 0.08413627091493711, 0.7573648130796338, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33101786250771764, 0.16289357736810944, 0.16515314938780928, 0.0, 0.0, 0.0, 0.0, 0.0, 0.662932866371622, 0.6905568008213746, 0.0, 0.0, 0.0, 0.0, 0.4845748090935882, 0.061923683856559844, 0.0, 0.028719251839850298, 0.0, 0.02009827057946138, 0.0, 0.0, 0.0, 0.11124419756806017, 0.4147463454400596, 0.0, 0.0, 0.34783853396429565, 0.0, 0.09260170356894369, 0.0, 0.0, 0.0, 1.1401462738363544, 0.915922599074042, 0.0, 0.0, 0.0, 0.0, 0.6274522680730893, 0.0, 0.3139030754242789, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3751134192842195, 0.0, 1.257728265907189, 0.0, 0.35074493350028346, 0.268137845429166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016899841379555174, 0.7155448975358032, 0.1680074523773585, 0.0, 0.0, 0.30581125296894957, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9366277987154731, 0.0, 0.0, 0.0, 0.0, 0.07522986257761975, 0.06328419096078892, 0.0, 0.02479071082883737, 0.0, 0.7915629916935427, 0.031935353301522654, 0.0, 0.0, 0.070458258165472, 0.0, 0.0, 0.0, 0.0, 0.052737602183104026, 0.0, 0.013647273418699125, 0.3169788666986562, 0.17235734856254364, 0.0, 0.0, 0.0, 0.0661528775144355, 0.0, 0.027308490724219276, 0.0, 0.0, 0.0, 0.7971915175033586, 0.0, 0.5343175730821937, 0.0, 0.8740381649643706, 0.04598025590229294, 0.029012248678093225, 0.0, 0.0, 0.11141095490761138, 0.0, 0.0, 0.13001430753376275, 0.0, 0.0, 0.0, 0.22549966134722088, 0.0, 0.0, 0.0, 0.3726692665495664, 0.3774382964295704, 0.20411971872540274, 0.06057654493015839, 0.0, 0.5744310538348768, 0.0, 0.0, 0.6232315714035893, 0.0, 0.5072896743166587, 0.0, 0.005553340050755054, 0.5689868928162961, 0.06836125145830099, 0.0, 0.0, 0.49734376521104756, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27904969776436467, 0.0, 0.7814242677772039, 0.5117851675292832, 0.0, 0.0, 0.04886194301383263, 0.0, 0.3067365939854962, 0.0, 0.0, 0.4115088709036164, 1.405104579225751, 0.42840174089528493, 0.48760195358213543, 0.0, 0.0, 0.663844265848651, 0.0, 0.38778436442248837, 0.7776635401136125, 0.15032072106029123, 0.18953081567758903, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7044650329571657, 0.7235946500790119, 0.12987194286479317, 0.19647866132545186, 0.45582545423087456, 0.0, 0.0, 0.0, 0.032958397990191235, 0.0, 0.0, 0.7570462349991403, 0.0941136312544236, 0.0, 0.1276645560961348, 0.0, 0.0, 0.0, 0.755417985546979, 0.8458204729185679, 0.12543050025210123, 0.006393660363144921, 0.0, 0.0, 0.28926738179087036, 0.0, 0.0, 0.4731345211962404, 0.0, 0.7203970460167203, 0.0, 0.0, 0.0, 0.4891311717243447, 0.0031333060836164625, 0.003950606766695812, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8250852706661534, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1940012409565178, 0.0, 0.7255195358366966, 0.0, 0.4862794311033925, 0.0, 0.7981182971725221, 0.0, 0.26385403496014914, 0.0, 0.5139863704027979, 0.35381636124647037, 0.06016159688024159, 0.0, 0.0, 0.0, 0.07675121814682248, 0.0, 0.0, 0.12553662505708466, 0.0, 0.301350309711505, 0.0, 0.0, 0.0, 0.12978101101826656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2180668401971053, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09272141980768678, 0.0, 0.3034931056908601, 0.0, 0.203416238280828, 0.0, 0.33274884560755447, 0.0, 0.12655518438173288, 0.0, 0.13637583261219158, 0.0, 0.0, 0.0, 0.0, 0.020115042222633606, 0.0, 0.11478942012015081, 0.0, 0.044967175485766994, 0.0, 0.27969782401606164, 0.1383242818414881, 0.0, 0.0, 0.18685369035983804, 0.0, 0.06810955828519266, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17579267922314348, 0.0, 0.0, 0.11999285025017485, 0.0, 0.0, 0.14209169595972287, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11399414163060874, 0.019825113051252268, 0.0, 0.049971986914216976, 0.07235420154739257, 0.0, 0.0, 0.08331368376199759, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07280640355864025, 0.0, 0.0, 0.35152520960031586, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13676901662991092, 0.0, 0.0, 0.0, 0.015204774707799058, 0.0, 0.0, 0.0, 0.0, 0.5304031136005922, 0.0, 0.0, 0.0, 0.32819195551919517, 0.1622165373359131, 0.0, 0.0, 0.0, 0.1321652778478459, 0.0, 0.0, 0.2161735452720934, 0.0, 0.27952564740678887, 0.0, 0.0, 0.0, 0.22348236021207268, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375510190195534, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2500091160813631, 0.0, 0.2815132558946093, 0.0, 0.18868424509978618, 0.0, 0.30865021038590384, 0.0, 0.341236683480638, 0.0, 0.23483861551802387, 0.0, 0.0, 0.0, 0.12117745385937667, 0.0, 0.0, 0.3973467888918919, 0.0, 0.2920101397985984, 0.0, 0.12988448302264086, 0.0, 0.3517844506342411, 0.0, 0.0, 0.0, 0.6519761543079575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4428070420734244, 0.0, 0.3964317892338393, 0.0, 0.0, 0.0, 0.3236724995283946, 0.0, 0.0567229736921611, 0.0, 0.0, 0.0, 0.25146743826064083, 0.0, 0.0, 0.011207888208677623, 0.08869787998517195, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0436070974131627, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05218480148875932, 0.0, 0.0, 0.0, 0.28668060177386556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16806127510824798, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1010038649276795, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24411150965510042, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3252831829639798, 0.0, 0.13696611429551586, 0.0, 0.08809099936621054, 0.0, 0.0, 0.0, 0.5834039633369968, 0.2573783852497724, 0.0, 0.34353142253711105, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8226681209013311, 0.43635481618924155, 0.0, 0.21222851603946533, 0.0, 0.6640973088333184, 0.0, 0.1716321278580885, 0.7312656094018032, 0.46618173092618587, 0.7638215749927391, 0.5281765059497923, 0.012259008816529623, 0.9144713112403178, 0.0, 0.0, 0.0, 0.0, 0.7953741970864907, 0.0, 0.0, 0.5850872742568098, 0.0, 0.0, 0.0, 0.7307808207285703, 0.0, 0.6199309753815017, 0.37291372178799176, 0.0, 0.0, 0.22356724549482568, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3221254739496775, 0.11781870844111127, 0.04460774210115785, 0.7133319766560491, 0.17115949268039302, 0.47397294440775345, 0.0, 0.10101149129956093, 0.11755718548933392, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.057090584067732354, 0.6447844891876003, 0.0, 0.0, 0.033829519264607456, 0.0, 0.1190337187097033, 0.30523421948716617, 0.0, 0.0, 0.0, 0.0, 0.6213057774307251, 0.842768502451811, 0.0, 0.3906048759812801, 0.7719833104847825, 0.41168363040706235, 0.1712307445975339, 0.0, 0.05992936357237096, 0.04890539039186202, 0.0, 0.0, 0.012380496199171342, 0.0, 0.2674194407541521, 0.0, 0.0014443996714777229, 0.0, 0.16387003646802972, 0.0, 0.0, 0.03594122433913304, 0.24159569520366886, 0.3747703785250739, 0.0, 0.054699683547073226, 0.0, 0.2873774296478163, 0.0, 0.0, 0.0, 0.14956132991470428, 0.0, 0.3888645476970781, 0.0, 0.0, 0.07542830239130101, 0.07080076609705026, 0.0, 0.0, 0.0, 0.0, 0.09852911382155678, 0.19548481725379033, 0.0, 0.0, 0.4350977845419587, 0.00557774494203208, 0.0, 0.09874991570897035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6034262468294622, 0.2713368059843151, 0.6497013107648627, 0.0, 0.8007029601130917, 0.0, 0.0, 0.0, 0.0, 0.01616961592030159, 0.0, 0.0, 0.0, 0.0, 0.2502876057454169, 0.49212467358206446, 0.2134864810859512, 0.1765833209669342, 0.0, 0.0, 0.0, 0.5401972297237567, 0.03318857704329382, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12219566143679067, 0.0, 0.04829725298673487, 0.0, 0.0, 0.0, 0.0, 0.0, 1.2356793204214815, 0.02263120223723859, 0.7256596881722004, 0.3263003275955702, 0.7654157892976137, 0.08145618747845872, 0.6387218661983024, 0.0, 0.5276231408957645, 0.8184646008092085, 0.0, 0.0, 0.0, 0.6276063069977658, 0.0, 0.6132063830791735, 0.006204111701464058, 0.18851722870999638, 0.375149258411478, 0.7816851453263511, 0.0, 0.587776400734769, 0.16472858834087942, 1.1016458158754883, 0.1459604994495737, 0.0, 0.0, 0.0, 0.037075917094288376, 0.0, 0.2815459504132854, 0.2536212145780303, 0.559150690052304, 0.48027689889482117, 0.0, 0.21566088189458643, 0.058080618345779114, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008439821738835876, 0.0, 0.36408664087256903, 0.0, 0.16156000319694572, 0.22471190561828702, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10732980278519261, 0.0, 0.0, 0.12403718878006863, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27901056312251676, 0.4801492052284416, 0.0, 0.0, 0.10787548181472921, 0.0, 0.0, 0.0, 0.15654727928350437, 0.0, 0.13104284110293218, 0.13666096202593647, 0.0, 0.0, 0.0, 0.07276575816635268, 0.0, 0.0, 0.0, 0.0, 0.6100450928472326, 0.0, 0.0, 0.0, 0.4238237484103886, 0.9941317988293357, 0.0, 0.0, 0.0, 0.0, 0.3355479309040112, 0.0, 0.0, 0.0, 0.0, 0.5350029094771915, 0.0, 0.008586567333100323, 1.1385513331902584, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1850917274468032, 0.0, 0.0, 0.06716099014107918, 1.1525784735402484, 0.0, 0.0, 0.036572260841491885, 0.0, 0.26119964749940466, 0.0, 0.0, 0.8459335464772313, 0.0, 0.0, 0.0, 0.0, 0.38438887602945404, 0.0, 0.0, 0.0, 0.0, 0.7204933523373307, 0.0, 0.0, 0.16060697060474402, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7112792652633437, 0.0, 0.935521710131386, 0.26536974069831215, 0.2935825392525973, 0.0, 0.0, 0.13719141489981915, 0.0, 0.10324823792839577, 0.0, 0.0, 0.0, 0.7256165269576538, 0.0, 0.48634443939716193, 0.0, 0.7955635800994527, 0.1927019559074891, 0.0, 0.0, 0.7058981437597585, 0.0, 0.5236895266156559, 0.0, 0.08128104063747256, 0.0, 0.0, 0.0, 0.5731212091054763, 0.0, 0.0, 0.0419831484236368, 0.11625926299639212, 0.0, 0.0, 0.08927614896350113, 0.217346651106267, 0.13128361330254273, 0.0, 0.07219885291348893, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4218130442222412, 0.0, 0.0, 0.0, 0.0, 0.1699892215644571, 0.0, 0.14639742856012483, 0.0, 0.0, 0.04200062557179813, 0.0, 0.0, 0.0, 0.31146013043119736, 0.0, 0.0, 0.0, 0.24384030368566587, 0.9260558729858251, 0.4296955936776912, 0.0, 0.0, 0.0, 0.73790516731229, 0.0, 0.0, 0.0, 0.16295488074282302, 0.26550524353964333, 0.0, 0.0, 0.0, 0.31144107126852844, 0.0, 0.485858439126786, 0.0, 0.6914937323909329, 0.0, 0.0, 0.0, 0.0, 0.3162528337434342, 0.0, 0.0, 0.0, 0.0, 1.5785215576611167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5161924043112004, 0.0, 0.0, 0.2568302026164213, 0.0, 0.3023415151909343, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5030068615133427, 0.0, 0.0, 0.0, 0.2741080207900236, 0.17120972783029909, 0.0, 0.0, 0.0, 0.0, 0.43685119598979993, 0.02712666041421048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2451085199735546, 0.0, 0.1576437689621315, 0.0, 0.0, 0.0, 0.6857487349281931, 0.05917052173435364, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6537568461306704, 0.0, 0.0, 0.5395442735782994, 0.05374109131453472, 0.0, 0.5917732326059808, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0008206716541601089, 0.19818937147807536, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.535053682582317, 0.0, 0.3513819151899916, 0.04333888481337121, 0.5747915094439204, 0.19540689526232177, 0.01899860952318465, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06595709778063945, 0.0, 0.08799217533170228, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03735300199555871, 0.19147688509049948, 0.0, 0.0, 0.0, 0.2914125830050192, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04799459307428656, 0.0, 0.16480769798416883, 0.0, 0.0, 0.0, 0.04840048223798238, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027156548216558336, 0.0, 0.047181418535814645, 0.0, 0.0, 0.0, 0.0, 0.35341760898781915, 0.5999981948389704, 0.024572219243726254, 0.0, 0.0, 0.08667332426569417, 0.4875808312926843, 0.06369622265280388, 0.0, 0.0, 0.0, 0.07133443039288194, 0.0, 0.0, 0.47894471690358564, 0.4793008357816607, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09658957720985865, 0.3016070141903076, 0.061398864173637636, 0.0, 0.0, 0.0, 0.07060273869520314, 0.35885594789448355, 0.0, 0.06446892769447045, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2497207855356876, 0.018509596151814908, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2926774045716465, 0.0, 0.013651758929637155, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03597721563910695, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07068090962707653, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03642909404914556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2749443358980585, 0.0, 0.0, 0.0, 0.0, 0.5096842842109697, 0.0, 0.0, 0.0, 0.028086239659725638, 0.7421154878137212, 0.0, 0.48123810905471315, 0.0, 0.3067030382176026, 0.6866619523091189, 0.0008295150087239233, 0.5016528107597976, 0.0, 0.08848804808651539, 1.3805931747462472, 0.0, 0.0, 1.04718815925786, 0.0, 0.0, 0.0, 0.42746548493362363, 0.0, 0.5817357352379013, 0.8450882691967682, 0.41665385220387907, 1.0915687546509307, 1.108964330219497, 0.0, 0.0, 0.0, 0.04066069749695377, 0.032733248007639555, 0.0, 0.0, 0.0, 0.07163077100441487, 0.6487063582321942, 0.0, 0.2995103130321618, 0.10283702762113958, 0.33384701220391544, 0.41258982728857174, 0.0, 0.5449670143554067, 0.0, 0.3217489265559145, 0.0, 0.0, 0.0, 0.2849908553676538, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01610871320352385, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2570528176713379, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2969662795916102, 0.0, 0.12312876081315716, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09075655657480966, 0.0, 0.3633373647258237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06617087939273322, 0.0, 0.0, 0.0, 0.00923594008230594, 0.17805875048711786, 0.22450411347361834, 0.0, 0.0, 0.09502241267674551, 0.0, 0.024589605765570302, 0.0, 0.1826423790293924, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039040067948029694, 0.0, 0.06664139722445166, 0.0, 0.04466639301844483, 0.0, 0.22428735118617732, 0.08284705163804024, 0.05227416022819031, 0.0, 0.19522102914015915, 0.8685310451611696, 0.0, 0.0, 0.0, 0.0, 0.8702352191794189, 0.0, 0.0, 0.15684527219370442, 0.0, 0.9243788213991903, 0.22207735215974056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.40507055158794986, 0.30718446392066534, 0.0, 0.46812545232304087, 0.0, 0.5780009938921578, 0.18279458657604497, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02896217836710352, 0.0, 0.0, 0.40712998428742797, 0.7449999863100496, 0.0, 0.49933620201845186, 0.0, 0.8168155413547457, 0.11457346224809285, 0.0, 0.0, 0.0, 0.15332753334456128, 0.11864376215788304, 0.0, 0.0, 0.04572095120049149, 0.0, 0.0, 0.31683394463012315, 0.5003078767139268, 0.14059206617597214, 0.19554784897288408, 0.0, 0.43178409369310355, 0.0, 0.07560627618395319, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16352131380333873, 0.0, 0.34449708571742155, 0.0, 0.4618370375665847, 0.0, 0.0, 0.41783342102021986, 0.0, 0.0, 0.0, 0.02165252564332241, 0.0, 0.0, 0.0, 0.47140143149032104, 0.39181349979635294, 0.11892452733738064, 0.0, 0.5433598750764469, 0.0, 0.0, 0.0, 0.07266999506080335, 0.0, 0.6735755940113932, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0041630866962457744, 0.2109647750127205, 0.0, 0.0, 0.0, 0.3715024914369038, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1845463840294256, 0.0, 0.6075440706050301, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.29101469974401967, 0.0, 0.0, 0.032739778024003775, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4875593392929034, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20371408580482323, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0439075644510851, 0.0, 0.5496827874572944, 0.40327797474842975, 0.37238046622077875, 0.0, 0.0, 0.17401376516444506, 0.0, 0.11278871866256229, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15394336215607962, 0.0, 0.0, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = ((ex.bodies[1]).bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of Ct[i, j] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    result = nothing
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    for i_4 = 1:A_lvl.shape[1]
        val = Ct_lvl_2_val
        Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
        B_lvl_ptr_2 = B_lvl_ptr
        B_lvl_ptr = (Finch).moveto(B_lvl_ptr, CPU(Threads.nthreads()))
        B_lvl_tbl1_2 = B_lvl_tbl1
        B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
        B_lvl_tbl2_2 = B_lvl_tbl2
        B_lvl_tbl2 = (Finch).moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
        val_2 = B_lvl_val
        B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
        A_lvl_ptr_2 = A_lvl_ptr
        A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
        A_lvl_tbl1_2 = A_lvl_tbl1
        A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
        A_lvl_tbl2_2 = A_lvl_tbl2
        A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
        val_3 = A_lvl_val
        A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
        Threads.@threads for i_5 = 1:Threads.nthreads()
                B_lvl_q = B_lvl_ptr[1]
                B_lvl_q_stop = B_lvl_ptr[1 + 1]
                if B_lvl_q < B_lvl_q_stop
                    B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
                else
                    B_lvl_i_stop = 0
                end
                phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_5 + -1), Threads.nthreads()))
                phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_5, Threads.nthreads()))
                if phase_stop_2 >= phase_start_2
                    if B_lvl_tbl2[B_lvl_q] < phase_start_2
                        B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    while true
                        B_lvl_i = B_lvl_tbl2[B_lvl_q]
                        B_lvl_q_step = B_lvl_q
                        if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                            B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                        end
                        if B_lvl_i < phase_stop_2
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_4, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_4
                                            if A_lvl_tbl1[A_lvl_q] < i_4
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        else
                            phase_stop_13 = min(B_lvl_i, phase_stop_2)
                            if B_lvl_i == phase_stop_13
                                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_4
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_4, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_4
                                                if A_lvl_tbl1[A_lvl_q] < i_4
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_4, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                                B_lvl_q = B_lvl_q_step
                            end
                            break
                        end
                    end
                end
            end
        Ct_lvl_2_val = val
        B_lvl_ptr = B_lvl_ptr_2
        B_lvl_tbl1 = B_lvl_tbl1_2
        B_lvl_tbl2 = B_lvl_tbl2_2
        B_lvl_val = val_2
        A_lvl_ptr = A_lvl_ptr_2
        A_lvl_tbl1 = A_lvl_tbl1_2
        A_lvl_tbl2 = A_lvl_tbl2_2
        A_lvl_val = val_3
    end
    resize!(Ct_lvl_2_val, A_lvl.shape[1] * B_lvl.shape[2])
    result = (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
    result
end
julia> @finch begin
        Ct .= 0
        for i = _
            for j = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.24622058278177836, 0.0, 0.0, 0.1261597862603059, 0.07342085928314178, 0.12871077894157168, 0.0, 0.0, 0.8034179790657067, 0.0, 0.0, 0.056025142012607894, 1.0596276779513034, 0.0, 0.12141212330168466, 0.0, 0.5574009533856183, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3327944495595785, 0.0, 0.8694342868598712, 0.0, 0.7416396915765561, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7569986463219165, 0.05683836078786318, 0.0, 0.0, 0.8725529080345216, 0.0, 0.2047777547094641, 0.5614660149749067, 0.0, 0.0, 0.36223551246779223, 0.0, 0.0, 0.0, 0.22107392894943306, 0.0, 0.0, 0.0, 0.3259321271119929, 0.5055955427859341, 0.27515457195781157, 0.4169876847813729, 0.018631329176858533, 0.3876953885710935, 0.0, 0.021531556327917045, 0.0, 0.0, 0.28223766305057085, 0.48287552686851865, 0.0, 0.0, 0.10175887870070799, 0.0, 0.0, 0.0, 0.06032870137556734, 0.018726053338837593, 0.045366945649236726, 0.0, 0.0, 0.0, 0.5790914418316278, 0.6815353846022115, 0.0, 0.1332216207413034, 0.0, 0.04017082185996856, 0.0, 0.0, 0.0, 0.0, 0.03484741576880692, 0.893850322287328, 0.4479341395605485, 0.5025126461241762, 0.1987665376735214, 0.31067608319719825, 0.0, 0.0, 0.0, 0.46442597522784795, 0.0, 0.0, 0.4431570212945692, 0.0, 0.0, 0.5121406140989466, 0.0, 0.49477960883115907, 0.0, 0.0, 0.0, 0.0, 0.47025922462023484, 0.3616621631765011, 0.5907253850051586, 0.0, 0.0, 0.44541009068477694, 0.03445733103321199, 0.0, 0.02309502433018302, 0.0, 0.03777890472311954, 0.16345317623186756, 0.16813336048226513, 0.0, 0.0, 0.09235605661534649, 0.0, 0.0, 0.01896246145159963, 0.02753978144349941, 0.0, 0.0, 0.0, 0.30135789429977294, 0.6764584765695312, 0.0, 0.0, 0.3151319553463997, 0.0, 0.04554105431298512, 0.0, 0.08378021558986462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09849622821166114, 0.0, 0.25488776315346307, 0.0, 0.2781851808227525, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2839462446541717, 0.00854309647598918, 0.0, 0.0, 0.3272900456325508, 0.10375527349591217, 0.8492672297194694, 0.0, 0.5876718077641173, 0.03093892983747342, 0.0, 0.0, 0.0, 0.3385535490482335, 0.0, 0.0, 0.0, 0.29218416128587404, 0.0, 0.6966394562456512, 0.0, 0.0, 0.0, 0.522006486105503, 0.0, 0.0, 0.0, 0.11065330711314218, 0.0, 0.23311787887979893, 0.0, 0.31252070060750486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06134998798063221, 0.0, 0.0, 0.3657519298489218, 0.31899283438369924, 0.0, 0.0, 0.0, 0.3676864240590821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0010478172394749591, 0.5287716819761857, 0.0, 0.3903081303690744, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0017717866111426558, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5892681933608603, 0.002977075804572329, 0.0, 0.0, 0.0, 0.0, 0.43072916849878135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.045384364903179854, 0.0, 0.0, 0.0018618199412216315, 0.0, 0.17111256306326456, 0.2672837889083413, 0.0, 0.0, 0.19576072012760673, 0.5948736598005219, 0.08413627091493711, 0.7573648130796338, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33101786250771764, 0.16289357736810944, 0.16515314938780928, 0.0, 0.0, 0.0, 0.0, 0.0, 0.662932866371622, 0.6905568008213746, 0.0, 0.0, 0.0, 0.0, 0.4845748090935882, 0.061923683856559844, 0.0, 0.028719251839850298, 0.0, 0.02009827057946138, 0.0, 0.0, 0.0, 0.11124419756806017, 0.4147463454400596, 0.0, 0.0, 0.34783853396429565, 0.0, 0.09260170356894369, 0.0, 0.0, 0.0, 1.1401462738363544, 0.915922599074042, 0.0, 0.0, 0.0, 0.0, 0.6274522680730893, 0.0, 0.3139030754242789, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3751134192842195, 0.0, 1.257728265907189, 0.0, 0.35074493350028346, 0.268137845429166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016899841379555174, 0.7155448975358032, 0.1680074523773585, 0.0, 0.0, 0.30581125296894957, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9366277987154731, 0.0, 0.0, 0.0, 0.0, 0.07522986257761975, 0.06328419096078892, 0.0, 0.02479071082883737, 0.0, 0.7915629916935427, 0.031935353301522654, 0.0, 0.0, 0.070458258165472, 0.0, 0.0, 0.0, 0.0, 0.052737602183104026, 0.0, 0.013647273418699125, 0.3169788666986562, 0.17235734856254364, 0.0, 0.0, 0.0, 0.0661528775144355, 0.0, 0.027308490724219276, 0.0, 0.0, 0.0, 0.7971915175033586, 0.0, 0.5343175730821937, 0.0, 0.8740381649643706, 0.04598025590229294, 0.029012248678093225, 0.0, 0.0, 0.11141095490761138, 0.0, 0.0, 0.13001430753376275, 0.0, 0.0, 0.0, 0.22549966134722088, 0.0, 0.0, 0.0, 0.3726692665495664, 0.3774382964295704, 0.20411971872540274, 0.06057654493015839, 0.0, 0.5744310538348768, 0.0, 0.0, 0.6232315714035893, 0.0, 0.5072896743166587, 0.0, 0.005553340050755054, 0.5689868928162961, 0.06836125145830099, 0.0, 0.0, 0.49734376521104756, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27904969776436467, 0.0, 0.7814242677772039, 0.5117851675292832, 0.0, 0.0, 0.04886194301383263, 0.0, 0.3067365939854962, 0.0, 0.0, 0.4115088709036164, 1.405104579225751, 0.42840174089528493, 0.48760195358213543, 0.0, 0.0, 0.663844265848651, 0.0, 0.38778436442248837, 0.7776635401136125, 0.15032072106029123, 0.18953081567758903, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7044650329571657, 0.7235946500790119, 0.12987194286479317, 0.19647866132545186, 0.45582545423087456, 0.0, 0.0, 0.0, 0.032958397990191235, 0.0, 0.0, 0.7570462349991403, 0.0941136312544236, 0.0, 0.1276645560961348, 0.0, 0.0, 0.0, 0.755417985546979, 0.8458204729185679, 0.12543050025210123, 0.006393660363144921, 0.0, 0.0, 0.28926738179087036, 0.0, 0.0, 0.4731345211962404, 0.0, 0.7203970460167203, 0.0, 0.0, 0.0, 0.4891311717243447, 0.0031333060836164625, 0.003950606766695812, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8250852706661534, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1940012409565178, 0.0, 0.7255195358366966, 0.0, 0.4862794311033925, 0.0, 0.7981182971725221, 0.0, 0.26385403496014914, 0.0, 0.5139863704027979, 0.35381636124647037, 0.06016159688024159, 0.0, 0.0, 0.0, 0.07675121814682248, 0.0, 0.0, 0.12553662505708466, 0.0, 0.301350309711505, 0.0, 0.0, 0.0, 0.12978101101826656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2180668401971053, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09272141980768678, 0.0, 0.3034931056908601, 0.0, 0.203416238280828, 0.0, 0.33274884560755447, 0.0, 0.12655518438173288, 0.0, 0.13637583261219158, 0.0, 0.0, 0.0, 0.0, 0.020115042222633606, 0.0, 0.11478942012015081, 0.0, 0.044967175485766994, 0.0, 0.27969782401606164, 0.1383242818414881, 0.0, 0.0, 0.18685369035983804, 0.0, 0.06810955828519266, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17579267922314348, 0.0, 0.0, 0.11999285025017485, 0.0, 0.0, 0.14209169595972287, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11399414163060874, 0.019825113051252268, 0.0, 0.049971986914216976, 0.07235420154739257, 0.0, 0.0, 0.08331368376199759, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07280640355864025, 0.0, 0.0, 0.35152520960031586, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13676901662991092, 0.0, 0.0, 0.0, 0.015204774707799058, 0.0, 0.0, 0.0, 0.0, 0.5304031136005922, 0.0, 0.0, 0.0, 0.32819195551919517, 0.1622165373359131, 0.0, 0.0, 0.0, 0.1321652778478459, 0.0, 0.0, 0.2161735452720934, 0.0, 0.27952564740678887, 0.0, 0.0, 0.0, 0.22348236021207268, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375510190195534, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2500091160813631, 0.0, 0.2815132558946093, 0.0, 0.18868424509978618, 0.0, 0.30865021038590384, 0.0, 0.341236683480638, 0.0, 0.23483861551802387, 0.0, 0.0, 0.0, 0.12117745385937667, 0.0, 0.0, 0.3973467888918919, 0.0, 0.2920101397985984, 0.0, 0.12988448302264086, 0.0, 0.3517844506342411, 0.0, 0.0, 0.0, 0.6519761543079575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4428070420734244, 0.0, 0.3964317892338393, 0.0, 0.0, 0.0, 0.3236724995283946, 0.0, 0.0567229736921611, 0.0, 0.0, 0.0, 0.25146743826064083, 0.0, 0.0, 0.011207888208677623, 0.08869787998517195, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0436070974131627, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05218480148875932, 0.0, 0.0, 0.0, 0.28668060177386556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16806127510824798, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1010038649276795, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24411150965510042, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3252831829639798, 0.0, 0.13696611429551586, 0.0, 0.08809099936621054, 0.0, 0.0, 0.0, 0.5834039633369968, 0.2573783852497724, 0.0, 0.34353142253711105, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8226681209013311, 0.43635481618924155, 0.0, 0.21222851603946533, 0.0, 0.6640973088333184, 0.0, 0.1716321278580885, 0.7312656094018032, 0.46618173092618587, 0.7638215749927391, 0.5281765059497923, 0.012259008816529623, 0.9144713112403178, 0.0, 0.0, 0.0, 0.0, 0.7953741970864907, 0.0, 0.0, 0.5850872742568098, 0.0, 0.0, 0.0, 0.7307808207285703, 0.0, 0.6199309753815017, 0.37291372178799176, 0.0, 0.0, 0.22356724549482568, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3221254739496775, 0.11781870844111127, 0.04460774210115785, 0.7133319766560491, 0.17115949268039302, 0.47397294440775345, 0.0, 0.10101149129956093, 0.11755718548933392, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.057090584067732354, 0.6447844891876003, 0.0, 0.0, 0.033829519264607456, 0.0, 0.1190337187097033, 0.30523421948716617, 0.0, 0.0, 0.0, 0.0, 0.6213057774307251, 0.842768502451811, 0.0, 0.3906048759812801, 0.7719833104847825, 0.41168363040706235, 0.1712307445975339, 0.0, 0.05992936357237096, 0.04890539039186202, 0.0, 0.0, 0.012380496199171342, 0.0, 0.2674194407541521, 0.0, 0.0014443996714777229, 0.0, 0.16387003646802972, 0.0, 0.0, 0.03594122433913304, 0.24159569520366886, 0.3747703785250739, 0.0, 0.054699683547073226, 0.0, 0.2873774296478163, 0.0, 0.0, 0.0, 0.14956132991470428, 0.0, 0.3888645476970781, 0.0, 0.0, 0.07542830239130101, 0.07080076609705026, 0.0, 0.0, 0.0, 0.0, 0.09852911382155678, 0.19548481725379033, 0.0, 0.0, 0.4350977845419587, 0.00557774494203208, 0.0, 0.09874991570897035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6034262468294622, 0.2713368059843151, 0.6497013107648627, 0.0, 0.8007029601130917, 0.0, 0.0, 0.0, 0.0, 0.01616961592030159, 0.0, 0.0, 0.0, 0.0, 0.2502876057454169, 0.49212467358206446, 0.2134864810859512, 0.1765833209669342, 0.0, 0.0, 0.0, 0.5401972297237567, 0.03318857704329382, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12219566143679067, 0.0, 0.04829725298673487, 0.0, 0.0, 0.0, 0.0, 0.0, 1.2356793204214815, 0.02263120223723859, 0.7256596881722004, 0.3263003275955702, 0.7654157892976137, 0.08145618747845872, 0.6387218661983024, 0.0, 0.5276231408957645, 0.8184646008092085, 0.0, 0.0, 0.0, 0.6276063069977658, 0.0, 0.6132063830791735, 0.006204111701464058, 0.18851722870999638, 0.375149258411478, 0.7816851453263511, 0.0, 0.587776400734769, 0.16472858834087942, 1.1016458158754883, 0.1459604994495737, 0.0, 0.0, 0.0, 0.037075917094288376, 0.0, 0.2815459504132854, 0.2536212145780303, 0.559150690052304, 0.48027689889482117, 0.0, 0.21566088189458643, 0.058080618345779114, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008439821738835876, 0.0, 0.36408664087256903, 0.0, 0.16156000319694572, 0.22471190561828702, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10732980278519261, 0.0, 0.0, 0.12403718878006863, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27901056312251676, 0.4801492052284416, 0.0, 0.0, 0.10787548181472921, 0.0, 0.0, 0.0, 0.15654727928350437, 0.0, 0.13104284110293218, 0.13666096202593647, 0.0, 0.0, 0.0, 0.07276575816635268, 0.0, 0.0, 0.0, 0.0, 0.6100450928472326, 0.0, 0.0, 0.0, 0.4238237484103886, 0.9941317988293357, 0.0, 0.0, 0.0, 0.0, 0.3355479309040112, 0.0, 0.0, 0.0, 0.0, 0.5350029094771915, 0.0, 0.008586567333100323, 1.1385513331902584, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1850917274468032, 0.0, 0.0, 0.06716099014107918, 1.1525784735402484, 0.0, 0.0, 0.036572260841491885, 0.0, 0.26119964749940466, 0.0, 0.0, 0.8459335464772313, 0.0, 0.0, 0.0, 0.0, 0.38438887602945404, 0.0, 0.0, 0.0, 0.0, 0.7204933523373307, 0.0, 0.0, 0.16060697060474402, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7112792652633437, 0.0, 0.935521710131386, 0.26536974069831215, 0.2935825392525973, 0.0, 0.0, 0.13719141489981915, 0.0, 0.10324823792839577, 0.0, 0.0, 0.0, 0.7256165269576538, 0.0, 0.48634443939716193, 0.0, 0.7955635800994527, 0.1927019559074891, 0.0, 0.0, 0.7058981437597585, 0.0, 0.5236895266156559, 0.0, 0.08128104063747256, 0.0, 0.0, 0.0, 0.5731212091054763, 0.0, 0.0, 0.0419831484236368, 0.11625926299639212, 0.0, 0.0, 0.08927614896350113, 0.217346651106267, 0.13128361330254273, 0.0, 0.07219885291348893, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4218130442222412, 0.0, 0.0, 0.0, 0.0, 0.1699892215644571, 0.0, 0.14639742856012483, 0.0, 0.0, 0.04200062557179813, 0.0, 0.0, 0.0, 0.31146013043119736, 0.0, 0.0, 0.0, 0.24384030368566587, 0.9260558729858251, 0.4296955936776912, 0.0, 0.0, 0.0, 0.73790516731229, 0.0, 0.0, 0.0, 0.16295488074282302, 0.26550524353964333, 0.0, 0.0, 0.0, 0.31144107126852844, 0.0, 0.485858439126786, 0.0, 0.6914937323909329, 0.0, 0.0, 0.0, 0.0, 0.3162528337434342, 0.0, 0.0, 0.0, 0.0, 1.5785215576611167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5161924043112004, 0.0, 0.0, 0.2568302026164213, 0.0, 0.3023415151909343, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5030068615133427, 0.0, 0.0, 0.0, 0.2741080207900236, 0.17120972783029909, 0.0, 0.0, 0.0, 0.0, 0.43685119598979993, 0.02712666041421048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2451085199735546, 0.0, 0.1576437689621315, 0.0, 0.0, 0.0, 0.6857487349281931, 0.05917052173435364, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6537568461306704, 0.0, 0.0, 0.5395442735782994, 0.05374109131453472, 0.0, 0.5917732326059808, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0008206716541601089, 0.19818937147807536, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.535053682582317, 0.0, 0.3513819151899916, 0.04333888481337121, 0.5747915094439204, 0.19540689526232177, 0.01899860952318465, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06595709778063945, 0.0, 0.08799217533170228, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03735300199555871, 0.19147688509049948, 0.0, 0.0, 0.0, 0.2914125830050192, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04799459307428656, 0.0, 0.16480769798416883, 0.0, 0.0, 0.0, 0.04840048223798238, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027156548216558336, 0.0, 0.047181418535814645, 0.0, 0.0, 0.0, 0.0, 0.35341760898781915, 0.5999981948389704, 0.024572219243726254, 0.0, 0.0, 0.08667332426569417, 0.4875808312926843, 0.06369622265280388, 0.0, 0.0, 0.0, 0.07133443039288194, 0.0, 0.0, 0.47894471690358564, 0.4793008357816607, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09658957720985865, 0.3016070141903076, 0.061398864173637636, 0.0, 0.0, 0.0, 0.07060273869520314, 0.35885594789448355, 0.0, 0.06446892769447045, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2497207855356876, 0.018509596151814908, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2926774045716465, 0.0, 0.013651758929637155, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03597721563910695, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07068090962707653, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03642909404914556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2749443358980585, 0.0, 0.0, 0.0, 0.0, 0.5096842842109697, 0.0, 0.0, 0.0, 0.028086239659725638, 0.7421154878137212, 0.0, 0.48123810905471315, 0.0, 0.3067030382176026, 0.6866619523091189, 0.0008295150087239233, 0.5016528107597976, 0.0, 0.08848804808651539, 1.3805931747462472, 0.0, 0.0, 1.04718815925786, 0.0, 0.0, 0.0, 0.42746548493362363, 0.0, 0.5817357352379013, 0.8450882691967682, 0.41665385220387907, 1.0915687546509307, 1.108964330219497, 0.0, 0.0, 0.0, 0.04066069749695377, 0.032733248007639555, 0.0, 0.0, 0.0, 0.07163077100441487, 0.6487063582321942, 0.0, 0.2995103130321618, 0.10283702762113958, 0.33384701220391544, 0.41258982728857174, 0.0, 0.5449670143554067, 0.0, 0.3217489265559145, 0.0, 0.0, 0.0, 0.2849908553676538, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01610871320352385, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2570528176713379, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2969662795916102, 0.0, 0.12312876081315716, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09075655657480966, 0.0, 0.3633373647258237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06617087939273322, 0.0, 0.0, 0.0, 0.00923594008230594, 0.17805875048711786, 0.22450411347361834, 0.0, 0.0, 0.09502241267674551, 0.0, 0.024589605765570302, 0.0, 0.1826423790293924, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039040067948029694, 0.0, 0.06664139722445166, 0.0, 0.04466639301844483, 0.0, 0.22428735118617732, 0.08284705163804024, 0.05227416022819031, 0.0, 0.19522102914015915, 0.8685310451611696, 0.0, 0.0, 0.0, 0.0, 0.8702352191794189, 0.0, 0.0, 0.15684527219370442, 0.0, 0.9243788213991903, 0.22207735215974056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.40507055158794986, 0.30718446392066534, 0.0, 0.46812545232304087, 0.0, 0.5780009938921578, 0.18279458657604497, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02896217836710352, 0.0, 0.0, 0.40712998428742797, 0.7449999863100496, 0.0, 0.49933620201845186, 0.0, 0.8168155413547457, 0.11457346224809285, 0.0, 0.0, 0.0, 0.15332753334456128, 0.11864376215788304, 0.0, 0.0, 0.04572095120049149, 0.0, 0.0, 0.31683394463012315, 0.5003078767139268, 0.14059206617597214, 0.19554784897288408, 0.0, 0.43178409369310355, 0.0, 0.07560627618395319, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16352131380333873, 0.0, 0.34449708571742155, 0.0, 0.4618370375665847, 0.0, 0.0, 0.41783342102021986, 0.0, 0.0, 0.0, 0.02165252564332241, 0.0, 0.0, 0.0, 0.47140143149032104, 0.39181349979635294, 0.11892452733738064, 0.0, 0.5433598750764469, 0.0, 0.0, 0.0, 0.07266999506080335, 0.0, 0.6735755940113932, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0041630866962457744, 0.2109647750127205, 0.0, 0.0, 0.0, 0.3715024914369038, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1845463840294256, 0.0, 0.6075440706050301, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.29101469974401967, 0.0, 0.0, 0.032739778024003775, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4875593392929034, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20371408580482323, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0439075644510851, 0.0, 0.5496827874572944, 0.40327797474842975, 0.37238046622077875, 0.0, 0.0, 0.17401376516444506, 0.0, 0.11278871866256229, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15394336215607962, 0.0, 0.0, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = ((ex.bodies[1]).bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    result = nothing
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = (Finch).moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = (Finch).moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            B_lvl_q = B_lvl_ptr[1]
            B_lvl_q_stop = B_lvl_ptr[1 + 1]
            if B_lvl_q < B_lvl_q_stop
                B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
            else
                B_lvl_i_stop = 0
            end
            phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                if B_lvl_tbl2[B_lvl_q] < phase_start_2
                    B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                end
                while true
                    B_lvl_i = B_lvl_tbl2[B_lvl_q]
                    B_lvl_q_step = B_lvl_q
                    if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                        B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    if B_lvl_i < phase_stop_2
                        Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                        for i_6 = 1:A_lvl.shape[1]
                            Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_6
                            A_lvl_q = A_lvl_ptr[1]
                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                            if A_lvl_q < A_lvl_q_stop
                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                            else
                                A_lvl_i_stop = 0
                            end
                            B_lvl_q_3 = B_lvl_q
                            if B_lvl_q < B_lvl_q_step
                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                            else
                                B_lvl_i_stop_3 = 0
                            end
                            phase_stop_4 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                            if phase_stop_4 >= 1
                                k = 1
                                if A_lvl_tbl2[A_lvl_q] < 1
                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                end
                                if B_lvl_tbl1[B_lvl_q] < 1
                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                end
                                while k <= phase_stop_4
                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                    A_lvl_q_step = A_lvl_q
                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                    phase_stop_5 = min(B_lvl_i_3, phase_stop_4, A_lvl_i)
                                    if A_lvl_i == phase_stop_5 && B_lvl_i_3 == phase_stop_5
                                        B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                        A_lvl_q_2 = A_lvl_q
                                        if A_lvl_q < A_lvl_q_step
                                            A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                        else
                                            A_lvl_i_stop_2 = 0
                                        end
                                        phase_stop_6 = min(i_6, A_lvl_i_stop_2)
                                        if phase_stop_6 >= i_6
                                            if A_lvl_tbl1[A_lvl_q] < i_6
                                                A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_6, A_lvl_q, A_lvl_q_step - 1)
                                            end
                                            while true
                                                A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                if A_lvl_i_2 < phase_stop_6
                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                    Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                    A_lvl_q_2 += 1
                                                else
                                                    phase_stop_8 = min(A_lvl_i_2, phase_stop_6)
                                                    if A_lvl_i_2 == phase_stop_8
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    end
                                                    break
                                                end
                                            end
                                        end
                                        A_lvl_q = A_lvl_q_step
                                        B_lvl_q_3 += 1
                                    elseif B_lvl_i_3 == phase_stop_5
                                        B_lvl_q_3 += 1
                                    elseif A_lvl_i == phase_stop_5
                                        A_lvl_q = A_lvl_q_step
                                    end
                                    k = phase_stop_5 + 1
                                end
                            end
                        end
                        B_lvl_q = B_lvl_q_step
                    else
                        phase_stop_13 = min(B_lvl_i, phase_stop_2)
                        if B_lvl_i == phase_stop_13
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_13
                            for i_8 = 1:A_lvl.shape[1]
                                Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_8
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_3 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_3 = 0
                                end
                                phase_stop_14 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                if phase_stop_14 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_14
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                        phase_stop_15 = min(B_lvl_i_3, A_lvl_i, phase_stop_14)
                                        if A_lvl_i == phase_stop_15 && B_lvl_i_3 == phase_stop_15
                                            B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                            A_lvl_q_4 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_4 = 0
                                            end
                                            phase_stop_16 = min(i_8, A_lvl_i_stop_4)
                                            if phase_stop_16 >= i_8
                                                if A_lvl_tbl1[A_lvl_q] < i_8
                                                    A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_8, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                    if A_lvl_i_4 < phase_stop_16
                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                        A_lvl_q_4 += 1
                                                    else
                                                        phase_stop_18 = min(A_lvl_i_4, phase_stop_16)
                                                        if A_lvl_i_4 == phase_stop_18
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_3 += 1
                                        elseif B_lvl_i_3 == phase_stop_15
                                            B_lvl_q_3 += 1
                                        elseif A_lvl_i == phase_stop_15
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_15 + 1
                                    end
                                end
                            end
                            B_lvl_q = B_lvl_q_step
                        end
                        break
                    end
                end
            end
        end
    resize!(val, A_lvl.shape[1] * B_lvl.shape[2])
    result = (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
    result
end
julia> @finch begin
        Ct .= 0
        for j = parallel(_)
            for i = _
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.24622058278177836, 0.0, 0.0, 0.1261597862603059, 0.07342085928314178, 0.12871077894157168, 0.0, 0.0, 0.8034179790657067, 0.0, 0.0, 0.056025142012607894, 1.0596276779513034, 0.0, 0.12141212330168466, 0.0, 0.5574009533856183, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3327944495595785, 0.0, 0.8694342868598712, 0.0, 0.7416396915765561, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7569986463219165, 0.05683836078786318, 0.0, 0.0, 0.8725529080345216, 0.0, 0.2047777547094641, 0.5614660149749067, 0.0, 0.0, 0.36223551246779223, 0.0, 0.0, 0.0, 0.22107392894943306, 0.0, 0.0, 0.0, 0.3259321271119929, 0.5055955427859341, 0.27515457195781157, 0.4169876847813729, 0.018631329176858533, 0.3876953885710935, 0.0, 0.021531556327917045, 0.0, 0.0, 0.28223766305057085, 0.48287552686851865, 0.0, 0.0, 0.10175887870070799, 0.0, 0.0, 0.0, 0.06032870137556734, 0.018726053338837593, 0.045366945649236726, 0.0, 0.0, 0.0, 0.5790914418316278, 0.6815353846022115, 0.0, 0.1332216207413034, 0.0, 0.04017082185996856, 0.0, 0.0, 0.0, 0.0, 0.03484741576880692, 0.893850322287328, 0.4479341395605485, 0.5025126461241762, 0.1987665376735214, 0.31067608319719825, 0.0, 0.0, 0.0, 0.46442597522784795, 0.0, 0.0, 0.4431570212945692, 0.0, 0.0, 0.5121406140989466, 0.0, 0.49477960883115907, 0.0, 0.0, 0.0, 0.0, 0.47025922462023484, 0.3616621631765011, 0.5907253850051586, 0.0, 0.0, 0.44541009068477694, 0.03445733103321199, 0.0, 0.02309502433018302, 0.0, 0.03777890472311954, 0.16345317623186756, 0.16813336048226513, 0.0, 0.0, 0.09235605661534649, 0.0, 0.0, 0.01896246145159963, 0.02753978144349941, 0.0, 0.0, 0.0, 0.30135789429977294, 0.6764584765695312, 0.0, 0.0, 0.3151319553463997, 0.0, 0.04554105431298512, 0.0, 0.08378021558986462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09849622821166114, 0.0, 0.25488776315346307, 0.0, 0.2781851808227525, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2839462446541717, 0.00854309647598918, 0.0, 0.0, 0.3272900456325508, 0.10375527349591217, 0.8492672297194694, 0.0, 0.5876718077641173, 0.03093892983747342, 0.0, 0.0, 0.0, 0.3385535490482335, 0.0, 0.0, 0.0, 0.29218416128587404, 0.0, 0.6966394562456512, 0.0, 0.0, 0.0, 0.522006486105503, 0.0, 0.0, 0.0, 0.11065330711314218, 0.0, 0.23311787887979893, 0.0, 0.31252070060750486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06134998798063221, 0.0, 0.0, 0.3657519298489218, 0.31899283438369924, 0.0, 0.0, 0.0, 0.3676864240590821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0010478172394749591, 0.5287716819761857, 0.0, 0.3903081303690744, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0017717866111426558, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5892681933608603, 0.002977075804572329, 0.0, 0.0, 0.0, 0.0, 0.43072916849878135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.045384364903179854, 0.0, 0.0, 0.0018618199412216315, 0.0, 0.17111256306326456, 0.2672837889083413, 0.0, 0.0, 0.19576072012760673, 0.5948736598005219, 0.08413627091493711, 0.7573648130796338, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33101786250771764, 0.16289357736810944, 0.16515314938780928, 0.0, 0.0, 0.0, 0.0, 0.0, 0.662932866371622, 0.6905568008213746, 0.0, 0.0, 0.0, 0.0, 0.4845748090935882, 0.061923683856559844, 0.0, 0.028719251839850298, 0.0, 0.02009827057946138, 0.0, 0.0, 0.0, 0.11124419756806017, 0.4147463454400596, 0.0, 0.0, 0.34783853396429565, 0.0, 0.09260170356894369, 0.0, 0.0, 0.0, 1.1401462738363544, 0.915922599074042, 0.0, 0.0, 0.0, 0.0, 0.6274522680730893, 0.0, 0.3139030754242789, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3751134192842195, 0.0, 1.257728265907189, 0.0, 0.35074493350028346, 0.268137845429166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016899841379555174, 0.7155448975358032, 0.1680074523773585, 0.0, 0.0, 0.30581125296894957, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9366277987154731, 0.0, 0.0, 0.0, 0.0, 0.07522986257761975, 0.06328419096078892, 0.0, 0.02479071082883737, 0.0, 0.7915629916935427, 0.031935353301522654, 0.0, 0.0, 0.070458258165472, 0.0, 0.0, 0.0, 0.0, 0.052737602183104026, 0.0, 0.013647273418699125, 0.3169788666986562, 0.17235734856254364, 0.0, 0.0, 0.0, 0.0661528775144355, 0.0, 0.027308490724219276, 0.0, 0.0, 0.0, 0.7971915175033586, 0.0, 0.5343175730821937, 0.0, 0.8740381649643706, 0.04598025590229294, 0.029012248678093225, 0.0, 0.0, 0.11141095490761138, 0.0, 0.0, 0.13001430753376275, 0.0, 0.0, 0.0, 0.22549966134722088, 0.0, 0.0, 0.0, 0.3726692665495664, 0.3774382964295704, 0.20411971872540274, 0.06057654493015839, 0.0, 0.5744310538348768, 0.0, 0.0, 0.6232315714035893, 0.0, 0.5072896743166587, 0.0, 0.005553340050755054, 0.5689868928162961, 0.06836125145830099, 0.0, 0.0, 0.49734376521104756, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27904969776436467, 0.0, 0.7814242677772039, 0.5117851675292832, 0.0, 0.0, 0.04886194301383263, 0.0, 0.3067365939854962, 0.0, 0.0, 0.4115088709036164, 1.405104579225751, 0.42840174089528493, 0.48760195358213543, 0.0, 0.0, 0.663844265848651, 0.0, 0.38778436442248837, 0.7776635401136125, 0.15032072106029123, 0.18953081567758903, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7044650329571657, 0.7235946500790119, 0.12987194286479317, 0.19647866132545186, 0.45582545423087456, 0.0, 0.0, 0.0, 0.032958397990191235, 0.0, 0.0, 0.7570462349991403, 0.0941136312544236, 0.0, 0.1276645560961348, 0.0, 0.0, 0.0, 0.755417985546979, 0.8458204729185679, 0.12543050025210123, 0.006393660363144921, 0.0, 0.0, 0.28926738179087036, 0.0, 0.0, 0.4731345211962404, 0.0, 0.7203970460167203, 0.0, 0.0, 0.0, 0.4891311717243447, 0.0031333060836164625, 0.003950606766695812, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8250852706661534, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1940012409565178, 0.0, 0.7255195358366966, 0.0, 0.4862794311033925, 0.0, 0.7981182971725221, 0.0, 0.26385403496014914, 0.0, 0.5139863704027979, 0.35381636124647037, 0.06016159688024159, 0.0, 0.0, 0.0, 0.07675121814682248, 0.0, 0.0, 0.12553662505708466, 0.0, 0.301350309711505, 0.0, 0.0, 0.0, 0.12978101101826656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2180668401971053, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09272141980768678, 0.0, 0.3034931056908601, 0.0, 0.203416238280828, 0.0, 0.33274884560755447, 0.0, 0.12655518438173288, 0.0, 0.13637583261219158, 0.0, 0.0, 0.0, 0.0, 0.020115042222633606, 0.0, 0.11478942012015081, 0.0, 0.044967175485766994, 0.0, 0.27969782401606164, 0.1383242818414881, 0.0, 0.0, 0.18685369035983804, 0.0, 0.06810955828519266, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17579267922314348, 0.0, 0.0, 0.11999285025017485, 0.0, 0.0, 0.14209169595972287, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11399414163060874, 0.019825113051252268, 0.0, 0.049971986914216976, 0.07235420154739257, 0.0, 0.0, 0.08331368376199759, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07280640355864025, 0.0, 0.0, 0.35152520960031586, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13676901662991092, 0.0, 0.0, 0.0, 0.015204774707799058, 0.0, 0.0, 0.0, 0.0, 0.5304031136005922, 0.0, 0.0, 0.0, 0.32819195551919517, 0.1622165373359131, 0.0, 0.0, 0.0, 0.1321652778478459, 0.0, 0.0, 0.2161735452720934, 0.0, 0.27952564740678887, 0.0, 0.0, 0.0, 0.22348236021207268, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375510190195534, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2500091160813631, 0.0, 0.2815132558946093, 0.0, 0.18868424509978618, 0.0, 0.30865021038590384, 0.0, 0.341236683480638, 0.0, 0.23483861551802387, 0.0, 0.0, 0.0, 0.12117745385937667, 0.0, 0.0, 0.3973467888918919, 0.0, 0.2920101397985984, 0.0, 0.12988448302264086, 0.0, 0.3517844506342411, 0.0, 0.0, 0.0, 0.6519761543079575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4428070420734244, 0.0, 0.3964317892338393, 0.0, 0.0, 0.0, 0.3236724995283946, 0.0, 0.0567229736921611, 0.0, 0.0, 0.0, 0.25146743826064083, 0.0, 0.0, 0.011207888208677623, 0.08869787998517195, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0436070974131627, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05218480148875932, 0.0, 0.0, 0.0, 0.28668060177386556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16806127510824798, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1010038649276795, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24411150965510042, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3252831829639798, 0.0, 0.13696611429551586, 0.0, 0.08809099936621054, 0.0, 0.0, 0.0, 0.5834039633369968, 0.2573783852497724, 0.0, 0.34353142253711105, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8226681209013311, 0.43635481618924155, 0.0, 0.21222851603946533, 0.0, 0.6640973088333184, 0.0, 0.1716321278580885, 0.7312656094018032, 0.46618173092618587, 0.7638215749927391, 0.5281765059497923, 0.012259008816529623, 0.9144713112403178, 0.0, 0.0, 0.0, 0.0, 0.7953741970864907, 0.0, 0.0, 0.5850872742568098, 0.0, 0.0, 0.0, 0.7307808207285703, 0.0, 0.6199309753815017, 0.37291372178799176, 0.0, 0.0, 0.22356724549482568, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3221254739496775, 0.11781870844111127, 0.04460774210115785, 0.7133319766560491, 0.17115949268039302, 0.47397294440775345, 0.0, 0.10101149129956093, 0.11755718548933392, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.057090584067732354, 0.6447844891876003, 0.0, 0.0, 0.033829519264607456, 0.0, 0.1190337187097033, 0.30523421948716617, 0.0, 0.0, 0.0, 0.0, 0.6213057774307251, 0.842768502451811, 0.0, 0.3906048759812801, 0.7719833104847825, 0.41168363040706235, 0.1712307445975339, 0.0, 0.05992936357237096, 0.04890539039186202, 0.0, 0.0, 0.012380496199171342, 0.0, 0.2674194407541521, 0.0, 0.0014443996714777229, 0.0, 0.16387003646802972, 0.0, 0.0, 0.03594122433913304, 0.24159569520366886, 0.3747703785250739, 0.0, 0.054699683547073226, 0.0, 0.2873774296478163, 0.0, 0.0, 0.0, 0.14956132991470428, 0.0, 0.3888645476970781, 0.0, 0.0, 0.07542830239130101, 0.07080076609705026, 0.0, 0.0, 0.0, 0.0, 0.09852911382155678, 0.19548481725379033, 0.0, 0.0, 0.4350977845419587, 0.00557774494203208, 0.0, 0.09874991570897035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6034262468294622, 0.2713368059843151, 0.6497013107648627, 0.0, 0.8007029601130917, 0.0, 0.0, 0.0, 0.0, 0.01616961592030159, 0.0, 0.0, 0.0, 0.0, 0.2502876057454169, 0.49212467358206446, 0.2134864810859512, 0.1765833209669342, 0.0, 0.0, 0.0, 0.5401972297237567, 0.03318857704329382, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12219566143679067, 0.0, 0.04829725298673487, 0.0, 0.0, 0.0, 0.0, 0.0, 1.2356793204214815, 0.02263120223723859, 0.7256596881722004, 0.3263003275955702, 0.7654157892976137, 0.08145618747845872, 0.6387218661983024, 0.0, 0.5276231408957645, 0.8184646008092085, 0.0, 0.0, 0.0, 0.6276063069977658, 0.0, 0.6132063830791735, 0.006204111701464058, 0.18851722870999638, 0.375149258411478, 0.7816851453263511, 0.0, 0.587776400734769, 0.16472858834087942, 1.1016458158754883, 0.1459604994495737, 0.0, 0.0, 0.0, 0.037075917094288376, 0.0, 0.2815459504132854, 0.2536212145780303, 0.559150690052304, 0.48027689889482117, 0.0, 0.21566088189458643, 0.058080618345779114, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008439821738835876, 0.0, 0.36408664087256903, 0.0, 0.16156000319694572, 0.22471190561828702, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10732980278519261, 0.0, 0.0, 0.12403718878006863, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27901056312251676, 0.4801492052284416, 0.0, 0.0, 0.10787548181472921, 0.0, 0.0, 0.0, 0.15654727928350437, 0.0, 0.13104284110293218, 0.13666096202593647, 0.0, 0.0, 0.0, 0.07276575816635268, 0.0, 0.0, 0.0, 0.0, 0.6100450928472326, 0.0, 0.0, 0.0, 0.4238237484103886, 0.9941317988293357, 0.0, 0.0, 0.0, 0.0, 0.3355479309040112, 0.0, 0.0, 0.0, 0.0, 0.5350029094771915, 0.0, 0.008586567333100323, 1.1385513331902584, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1850917274468032, 0.0, 0.0, 0.06716099014107918, 1.1525784735402484, 0.0, 0.0, 0.036572260841491885, 0.0, 0.26119964749940466, 0.0, 0.0, 0.8459335464772313, 0.0, 0.0, 0.0, 0.0, 0.38438887602945404, 0.0, 0.0, 0.0, 0.0, 0.7204933523373307, 0.0, 0.0, 0.16060697060474402, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7112792652633437, 0.0, 0.935521710131386, 0.26536974069831215, 0.2935825392525973, 0.0, 0.0, 0.13719141489981915, 0.0, 0.10324823792839577, 0.0, 0.0, 0.0, 0.7256165269576538, 0.0, 0.48634443939716193, 0.0, 0.7955635800994527, 0.1927019559074891, 0.0, 0.0, 0.7058981437597585, 0.0, 0.5236895266156559, 0.0, 0.08128104063747256, 0.0, 0.0, 0.0, 0.5731212091054763, 0.0, 0.0, 0.0419831484236368, 0.11625926299639212, 0.0, 0.0, 0.08927614896350113, 0.217346651106267, 0.13128361330254273, 0.0, 0.07219885291348893, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4218130442222412, 0.0, 0.0, 0.0, 0.0, 0.1699892215644571, 0.0, 0.14639742856012483, 0.0, 0.0, 0.04200062557179813, 0.0, 0.0, 0.0, 0.31146013043119736, 0.0, 0.0, 0.0, 0.24384030368566587, 0.9260558729858251, 0.4296955936776912, 0.0, 0.0, 0.0, 0.73790516731229, 0.0, 0.0, 0.0, 0.16295488074282302, 0.26550524353964333, 0.0, 0.0, 0.0, 0.31144107126852844, 0.0, 0.485858439126786, 0.0, 0.6914937323909329, 0.0, 0.0, 0.0, 0.0, 0.3162528337434342, 0.0, 0.0, 0.0, 0.0, 1.5785215576611167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5161924043112004, 0.0, 0.0, 0.2568302026164213, 0.0, 0.3023415151909343, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5030068615133427, 0.0, 0.0, 0.0, 0.2741080207900236, 0.17120972783029909, 0.0, 0.0, 0.0, 0.0, 0.43685119598979993, 0.02712666041421048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2451085199735546, 0.0, 0.1576437689621315, 0.0, 0.0, 0.0, 0.6857487349281931, 0.05917052173435364, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6537568461306704, 0.0, 0.0, 0.5395442735782994, 0.05374109131453472, 0.0, 0.5917732326059808, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0008206716541601089, 0.19818937147807536, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.535053682582317, 0.0, 0.3513819151899916, 0.04333888481337121, 0.5747915094439204, 0.19540689526232177, 0.01899860952318465, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06595709778063945, 0.0, 0.08799217533170228, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03735300199555871, 0.19147688509049948, 0.0, 0.0, 0.0, 0.2914125830050192, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04799459307428656, 0.0, 0.16480769798416883, 0.0, 0.0, 0.0, 0.04840048223798238, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027156548216558336, 0.0, 0.047181418535814645, 0.0, 0.0, 0.0, 0.0, 0.35341760898781915, 0.5999981948389704, 0.024572219243726254, 0.0, 0.0, 0.08667332426569417, 0.4875808312926843, 0.06369622265280388, 0.0, 0.0, 0.0, 0.07133443039288194, 0.0, 0.0, 0.47894471690358564, 0.4793008357816607, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09658957720985865, 0.3016070141903076, 0.061398864173637636, 0.0, 0.0, 0.0, 0.07060273869520314, 0.35885594789448355, 0.0, 0.06446892769447045, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2497207855356876, 0.018509596151814908, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2926774045716465, 0.0, 0.013651758929637155, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03597721563910695, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07068090962707653, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03642909404914556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2749443358980585, 0.0, 0.0, 0.0, 0.0, 0.5096842842109697, 0.0, 0.0, 0.0, 0.028086239659725638, 0.7421154878137212, 0.0, 0.48123810905471315, 0.0, 0.3067030382176026, 0.6866619523091189, 0.0008295150087239233, 0.5016528107597976, 0.0, 0.08848804808651539, 1.3805931747462472, 0.0, 0.0, 1.04718815925786, 0.0, 0.0, 0.0, 0.42746548493362363, 0.0, 0.5817357352379013, 0.8450882691967682, 0.41665385220387907, 1.0915687546509307, 1.108964330219497, 0.0, 0.0, 0.0, 0.04066069749695377, 0.032733248007639555, 0.0, 0.0, 0.0, 0.07163077100441487, 0.6487063582321942, 0.0, 0.2995103130321618, 0.10283702762113958, 0.33384701220391544, 0.41258982728857174, 0.0, 0.5449670143554067, 0.0, 0.3217489265559145, 0.0, 0.0, 0.0, 0.2849908553676538, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01610871320352385, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2570528176713379, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2969662795916102, 0.0, 0.12312876081315716, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09075655657480966, 0.0, 0.3633373647258237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06617087939273322, 0.0, 0.0, 0.0, 0.00923594008230594, 0.17805875048711786, 0.22450411347361834, 0.0, 0.0, 0.09502241267674551, 0.0, 0.024589605765570302, 0.0, 0.1826423790293924, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039040067948029694, 0.0, 0.06664139722445166, 0.0, 0.04466639301844483, 0.0, 0.22428735118617732, 0.08284705163804024, 0.05227416022819031, 0.0, 0.19522102914015915, 0.8685310451611696, 0.0, 0.0, 0.0, 0.0, 0.8702352191794189, 0.0, 0.0, 0.15684527219370442, 0.0, 0.9243788213991903, 0.22207735215974056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.40507055158794986, 0.30718446392066534, 0.0, 0.46812545232304087, 0.0, 0.5780009938921578, 0.18279458657604497, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02896217836710352, 0.0, 0.0, 0.40712998428742797, 0.7449999863100496, 0.0, 0.49933620201845186, 0.0, 0.8168155413547457, 0.11457346224809285, 0.0, 0.0, 0.0, 0.15332753334456128, 0.11864376215788304, 0.0, 0.0, 0.04572095120049149, 0.0, 0.0, 0.31683394463012315, 0.5003078767139268, 0.14059206617597214, 0.19554784897288408, 0.0, 0.43178409369310355, 0.0, 0.07560627618395319, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16352131380333873, 0.0, 0.34449708571742155, 0.0, 0.4618370375665847, 0.0, 0.0, 0.41783342102021986, 0.0, 0.0, 0.0, 0.02165252564332241, 0.0, 0.0, 0.0, 0.47140143149032104, 0.39181349979635294, 0.11892452733738064, 0.0, 0.5433598750764469, 0.0, 0.0, 0.0, 0.07266999506080335, 0.0, 0.6735755940113932, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0041630866962457744, 0.2109647750127205, 0.0, 0.0, 0.0, 0.3715024914369038, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1845463840294256, 0.0, 0.6075440706050301, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.29101469974401967, 0.0, 0.0, 0.032739778024003775, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4875593392929034, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20371408580482323, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0439075644510851, 0.0, 0.5496827874572944, 0.40327797474842975, 0.37238046622077875, 0.0, 0.0, 0.17401376516444506, 0.0, 0.11278871866256229, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15394336215607962, 0.0, 0.0, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = ((ex.bodies[1]).bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    result = nothing
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    B_lvl_q = B_lvl_ptr[1]
    B_lvl_q_stop = B_lvl_ptr[1 + 1]
    if B_lvl_q < B_lvl_q_stop
        B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
    else
        B_lvl_i_stop = 0
    end
    phase_stop = min(B_lvl.shape[2], B_lvl_i_stop)
    if phase_stop >= 1
        if B_lvl_tbl2[B_lvl_q] < 1
            B_lvl_q = Finch.scansearch(B_lvl_tbl2, 1, B_lvl_q, B_lvl_q_stop - 1)
        end
        while true
            B_lvl_i = B_lvl_tbl2[B_lvl_q]
            B_lvl_q_step = B_lvl_q
            if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
            end
            if B_lvl_i < phase_stop
                Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                val = Ct_lvl_2_val
                Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                B_lvl_tbl1_2 = B_lvl_tbl1
                B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                B_lvl_tbl2_2 = B_lvl_tbl2
                val_2 = B_lvl_val
                B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
                A_lvl_ptr_2 = A_lvl_ptr
                A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                A_lvl_tbl1_2 = A_lvl_tbl1
                A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                A_lvl_tbl2_2 = A_lvl_tbl2
                A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                val_3 = A_lvl_val
                A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
                Threads.@threads for i_9 = 1:Threads.nthreads()
                        phase_start_6 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_9), Threads.nthreads()))
                        phase_stop_7 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_9, Threads.nthreads()))
                        if phase_stop_7 >= phase_start_6
                            for i_12 = phase_start_6:phase_stop_7
                                Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_12
                                A_lvl_q = A_lvl_ptr[1]
                                A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                if A_lvl_q < A_lvl_q_stop
                                    A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                else
                                    A_lvl_i_stop = 0
                                end
                                B_lvl_q_2 = B_lvl_q
                                if B_lvl_q < B_lvl_q_step
                                    B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                else
                                    B_lvl_i_stop_2 = 0
                                end
                                phase_stop_8 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                if phase_stop_8 >= 1
                                    k = 1
                                    if A_lvl_tbl2[A_lvl_q] < 1
                                        A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                    end
                                    if B_lvl_tbl1[B_lvl_q] < 1
                                        B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                    end
                                    while k <= phase_stop_8
                                        A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                        A_lvl_q_step = A_lvl_q
                                        if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                            A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                        phase_stop_9 = min(B_lvl_i_2, phase_stop_8, A_lvl_i)
                                        if A_lvl_i == phase_stop_9 && B_lvl_i_2 == phase_stop_9
                                            B_lvl_2_val = B_lvl_val[B_lvl_q_2]
                                            A_lvl_q_2 = A_lvl_q
                                            if A_lvl_q < A_lvl_q_step
                                                A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                            else
                                                A_lvl_i_stop_2 = 0
                                            end
                                            phase_stop_10 = min(i_12, A_lvl_i_stop_2)
                                            if phase_stop_10 >= i_12
                                                if A_lvl_tbl1[A_lvl_q] < i_12
                                                    A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_12, A_lvl_q, A_lvl_q_step - 1)
                                                end
                                                while true
                                                    A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                    if A_lvl_i_2 < phase_stop_10
                                                        A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                        Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                        A_lvl_q_2 += 1
                                                    else
                                                        phase_stop_12 = min(A_lvl_i_2, phase_stop_10)
                                                        if A_lvl_i_2 == phase_stop_12
                                                            A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                            Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                            A_lvl_q_2 += 1
                                                        end
                                                        break
                                                    end
                                                end
                                            end
                                            A_lvl_q = A_lvl_q_step
                                            B_lvl_q_2 += 1
                                        elseif B_lvl_i_2 == phase_stop_9
                                            B_lvl_q_2 += 1
                                        elseif A_lvl_i == phase_stop_9
                                            A_lvl_q = A_lvl_q_step
                                        end
                                        k = phase_stop_9 + 1
                                    end
                                end
                            end
                        end
                    end
                Ct_lvl_2_val = val
                B_lvl_tbl1 = B_lvl_tbl1_2
                B_lvl_tbl2 = B_lvl_tbl2_2
                B_lvl_val = val_2
                A_lvl_ptr = A_lvl_ptr_2
                A_lvl_tbl1 = A_lvl_tbl1_2
                A_lvl_tbl2 = A_lvl_tbl2_2
                A_lvl_val = val_3
                B_lvl_q = B_lvl_q_step
            else
                phase_stop_18 = min(B_lvl_i, phase_stop)
                if B_lvl_i == phase_stop_18
                    Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_18
                    val_4 = Ct_lvl_2_val
                    Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                    B_lvl_tbl1_3 = B_lvl_tbl1
                    B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                    B_lvl_tbl2_3 = B_lvl_tbl2
                    val_5 = B_lvl_val
                    B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
                    A_lvl_ptr_3 = A_lvl_ptr
                    A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                    A_lvl_tbl1_3 = A_lvl_tbl1
                    A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                    A_lvl_tbl2_3 = A_lvl_tbl2
                    A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                    val_6 = A_lvl_val
                    A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
                    Threads.@threads for i_19 = 1:Threads.nthreads()
                            phase_start_21 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_19), Threads.nthreads()))
                            phase_stop_23 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_19, Threads.nthreads()))
                            if phase_stop_23 >= phase_start_21
                                for i_22 = phase_start_21:phase_stop_23
                                    Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_22
                                    A_lvl_q = A_lvl_ptr[1]
                                    A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                    if A_lvl_q < A_lvl_q_stop
                                        A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                    else
                                        A_lvl_i_stop = 0
                                    end
                                    B_lvl_q_2 = B_lvl_q
                                    if B_lvl_q < B_lvl_q_step
                                        B_lvl_i_stop_2 = B_lvl_tbl1[B_lvl_q_step - 1]
                                    else
                                        B_lvl_i_stop_2 = 0
                                    end
                                    phase_stop_24 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_2)
                                    if phase_stop_24 >= 1
                                        k = 1
                                        if A_lvl_tbl2[A_lvl_q] < 1
                                            A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                        end
                                        if B_lvl_tbl1[B_lvl_q] < 1
                                            B_lvl_q_2 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                        end
                                        while k <= phase_stop_24
                                            A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                            A_lvl_q_step = A_lvl_q
                                            if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            B_lvl_i_2 = B_lvl_tbl1[B_lvl_q_2]
                                            phase_stop_25 = min(B_lvl_i_2, A_lvl_i, phase_stop_24)
                                            if A_lvl_i == phase_stop_25 && B_lvl_i_2 == phase_stop_25
                                                B_lvl_2_val_3 = B_lvl_val[B_lvl_q_2]
                                                A_lvl_q_4 = A_lvl_q
                                                if A_lvl_q < A_lvl_q_step
                                                    A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                else
                                                    A_lvl_i_stop_4 = 0
                                                end
                                                phase_stop_26 = min(i_22, A_lvl_i_stop_4)
                                                if phase_stop_26 >= i_22
                                                    if A_lvl_tbl1[A_lvl_q] < i_22
                                                        A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_22, A_lvl_q, A_lvl_q_step - 1)
                                                    end
                                                    while true
                                                        A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                        if A_lvl_i_4 < phase_stop_26
                                                            A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                            Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                            A_lvl_q_4 += 1
                                                        else
                                                            phase_stop_28 = min(A_lvl_i_4, phase_stop_26)
                                                            if A_lvl_i_4 == phase_stop_28
                                                                A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                                A_lvl_q_4 += 1
                                                            end
                                                            break
                                                        end
                                                    end
                                                end
                                                A_lvl_q = A_lvl_q_step
                                                B_lvl_q_2 += 1
                                            elseif B_lvl_i_2 == phase_stop_25
                                                B_lvl_q_2 += 1
                                            elseif A_lvl_i == phase_stop_25
                                                A_lvl_q = A_lvl_q_step
                                            end
                                            k = phase_stop_25 + 1
                                        end
                                    end
                                end
                            end
                        end
                    Ct_lvl_2_val = val_4
                    B_lvl_tbl1 = B_lvl_tbl1_3
                    B_lvl_tbl2 = B_lvl_tbl2_3
                    B_lvl_val = val_5
                    A_lvl_ptr = A_lvl_ptr_3
                    A_lvl_tbl1 = A_lvl_tbl1_3
                    A_lvl_tbl2 = A_lvl_tbl2_3
                    A_lvl_val = val_6
                    B_lvl_q = B_lvl_q_step
                end
                break
            end
        end
    end
    resize!(Ct_lvl_2_val, A_lvl.shape[1] * B_lvl.shape[2])
    result = (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
    result
end
julia> @finch begin
        Ct .= 0
        for j = _
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.24622058278177836, 0.0, 0.0, 0.1261597862603059, 0.07342085928314178, 0.12871077894157168, 0.0, 0.0, 0.8034179790657067, 0.0, 0.0, 0.056025142012607894, 1.0596276779513034, 0.0, 0.12141212330168466, 0.0, 0.5574009533856183, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3327944495595785, 0.0, 0.8694342868598712, 0.0, 0.7416396915765561, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7569986463219165, 0.05683836078786318, 0.0, 0.0, 0.8725529080345216, 0.0, 0.2047777547094641, 0.5614660149749067, 0.0, 0.0, 0.36223551246779223, 0.0, 0.0, 0.0, 0.22107392894943306, 0.0, 0.0, 0.0, 0.3259321271119929, 0.5055955427859341, 0.27515457195781157, 0.4169876847813729, 0.018631329176858533, 0.3876953885710935, 0.0, 0.021531556327917045, 0.0, 0.0, 0.28223766305057085, 0.48287552686851865, 0.0, 0.0, 0.10175887870070799, 0.0, 0.0, 0.0, 0.06032870137556734, 0.018726053338837593, 0.045366945649236726, 0.0, 0.0, 0.0, 0.5790914418316278, 0.6815353846022115, 0.0, 0.1332216207413034, 0.0, 0.04017082185996856, 0.0, 0.0, 0.0, 0.0, 0.03484741576880692, 0.893850322287328, 0.4479341395605485, 0.5025126461241762, 0.1987665376735214, 0.31067608319719825, 0.0, 0.0, 0.0, 0.46442597522784795, 0.0, 0.0, 0.4431570212945692, 0.0, 0.0, 0.5121406140989466, 0.0, 0.49477960883115907, 0.0, 0.0, 0.0, 0.0, 0.47025922462023484, 0.3616621631765011, 0.5907253850051586, 0.0, 0.0, 0.44541009068477694, 0.03445733103321199, 0.0, 0.02309502433018302, 0.0, 0.03777890472311954, 0.16345317623186756, 0.16813336048226513, 0.0, 0.0, 0.09235605661534649, 0.0, 0.0, 0.01896246145159963, 0.02753978144349941, 0.0, 0.0, 0.0, 0.30135789429977294, 0.6764584765695312, 0.0, 0.0, 0.3151319553463997, 0.0, 0.04554105431298512, 0.0, 0.08378021558986462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09849622821166114, 0.0, 0.25488776315346307, 0.0, 0.2781851808227525, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2839462446541717, 0.00854309647598918, 0.0, 0.0, 0.3272900456325508, 0.10375527349591217, 0.8492672297194694, 0.0, 0.5876718077641173, 0.03093892983747342, 0.0, 0.0, 0.0, 0.3385535490482335, 0.0, 0.0, 0.0, 0.29218416128587404, 0.0, 0.6966394562456512, 0.0, 0.0, 0.0, 0.522006486105503, 0.0, 0.0, 0.0, 0.11065330711314218, 0.0, 0.23311787887979893, 0.0, 0.31252070060750486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06134998798063221, 0.0, 0.0, 0.3657519298489218, 0.31899283438369924, 0.0, 0.0, 0.0, 0.3676864240590821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0010478172394749591, 0.5287716819761857, 0.0, 0.3903081303690744, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0017717866111426558, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5892681933608603, 0.002977075804572329, 0.0, 0.0, 0.0, 0.0, 0.43072916849878135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.045384364903179854, 0.0, 0.0, 0.0018618199412216315, 0.0, 0.17111256306326456, 0.2672837889083413, 0.0, 0.0, 0.19576072012760673, 0.5948736598005219, 0.08413627091493711, 0.7573648130796338, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33101786250771764, 0.16289357736810944, 0.16515314938780928, 0.0, 0.0, 0.0, 0.0, 0.0, 0.662932866371622, 0.6905568008213746, 0.0, 0.0, 0.0, 0.0, 0.4845748090935882, 0.061923683856559844, 0.0, 0.028719251839850298, 0.0, 0.02009827057946138, 0.0, 0.0, 0.0, 0.11124419756806017, 0.4147463454400596, 0.0, 0.0, 0.34783853396429565, 0.0, 0.09260170356894369, 0.0, 0.0, 0.0, 1.1401462738363544, 0.915922599074042, 0.0, 0.0, 0.0, 0.0, 0.6274522680730893, 0.0, 0.3139030754242789, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3751134192842195, 0.0, 1.257728265907189, 0.0, 0.35074493350028346, 0.268137845429166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016899841379555174, 0.7155448975358032, 0.1680074523773585, 0.0, 0.0, 0.30581125296894957, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9366277987154731, 0.0, 0.0, 0.0, 0.0, 0.07522986257761975, 0.06328419096078892, 0.0, 0.02479071082883737, 0.0, 0.7915629916935427, 0.031935353301522654, 0.0, 0.0, 0.070458258165472, 0.0, 0.0, 0.0, 0.0, 0.052737602183104026, 0.0, 0.013647273418699125, 0.3169788666986562, 0.17235734856254364, 0.0, 0.0, 0.0, 0.0661528775144355, 0.0, 0.027308490724219276, 0.0, 0.0, 0.0, 0.7971915175033586, 0.0, 0.5343175730821937, 0.0, 0.8740381649643706, 0.04598025590229294, 0.029012248678093225, 0.0, 0.0, 0.11141095490761138, 0.0, 0.0, 0.13001430753376275, 0.0, 0.0, 0.0, 0.22549966134722088, 0.0, 0.0, 0.0, 0.3726692665495664, 0.3774382964295704, 0.20411971872540274, 0.06057654493015839, 0.0, 0.5744310538348768, 0.0, 0.0, 0.6232315714035893, 0.0, 0.5072896743166587, 0.0, 0.005553340050755054, 0.5689868928162961, 0.06836125145830099, 0.0, 0.0, 0.49734376521104756, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27904969776436467, 0.0, 0.7814242677772039, 0.5117851675292832, 0.0, 0.0, 0.04886194301383263, 0.0, 0.3067365939854962, 0.0, 0.0, 0.4115088709036164, 1.405104579225751, 0.42840174089528493, 0.48760195358213543, 0.0, 0.0, 0.663844265848651, 0.0, 0.38778436442248837, 0.7776635401136125, 0.15032072106029123, 0.18953081567758903, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7044650329571657, 0.7235946500790119, 0.12987194286479317, 0.19647866132545186, 0.45582545423087456, 0.0, 0.0, 0.0, 0.032958397990191235, 0.0, 0.0, 0.7570462349991403, 0.0941136312544236, 0.0, 0.1276645560961348, 0.0, 0.0, 0.0, 0.755417985546979, 0.8458204729185679, 0.12543050025210123, 0.006393660363144921, 0.0, 0.0, 0.28926738179087036, 0.0, 0.0, 0.4731345211962404, 0.0, 0.7203970460167203, 0.0, 0.0, 0.0, 0.4891311717243447, 0.0031333060836164625, 0.003950606766695812, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8250852706661534, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1940012409565178, 0.0, 0.7255195358366966, 0.0, 0.4862794311033925, 0.0, 0.7981182971725221, 0.0, 0.26385403496014914, 0.0, 0.5139863704027979, 0.35381636124647037, 0.06016159688024159, 0.0, 0.0, 0.0, 0.07675121814682248, 0.0, 0.0, 0.12553662505708466, 0.0, 0.301350309711505, 0.0, 0.0, 0.0, 0.12978101101826656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2180668401971053, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09272141980768678, 0.0, 0.3034931056908601, 0.0, 0.203416238280828, 0.0, 0.33274884560755447, 0.0, 0.12655518438173288, 0.0, 0.13637583261219158, 0.0, 0.0, 0.0, 0.0, 0.020115042222633606, 0.0, 0.11478942012015081, 0.0, 0.044967175485766994, 0.0, 0.27969782401606164, 0.1383242818414881, 0.0, 0.0, 0.18685369035983804, 0.0, 0.06810955828519266, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17579267922314348, 0.0, 0.0, 0.11999285025017485, 0.0, 0.0, 0.14209169595972287, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11399414163060874, 0.019825113051252268, 0.0, 0.049971986914216976, 0.07235420154739257, 0.0, 0.0, 0.08331368376199759, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07280640355864025, 0.0, 0.0, 0.35152520960031586, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13676901662991092, 0.0, 0.0, 0.0, 0.015204774707799058, 0.0, 0.0, 0.0, 0.0, 0.5304031136005922, 0.0, 0.0, 0.0, 0.32819195551919517, 0.1622165373359131, 0.0, 0.0, 0.0, 0.1321652778478459, 0.0, 0.0, 0.2161735452720934, 0.0, 0.27952564740678887, 0.0, 0.0, 0.0, 0.22348236021207268, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375510190195534, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2500091160813631, 0.0, 0.2815132558946093, 0.0, 0.18868424509978618, 0.0, 0.30865021038590384, 0.0, 0.341236683480638, 0.0, 0.23483861551802387, 0.0, 0.0, 0.0, 0.12117745385937667, 0.0, 0.0, 0.3973467888918919, 0.0, 0.2920101397985984, 0.0, 0.12988448302264086, 0.0, 0.3517844506342411, 0.0, 0.0, 0.0, 0.6519761543079575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4428070420734244, 0.0, 0.3964317892338393, 0.0, 0.0, 0.0, 0.3236724995283946, 0.0, 0.0567229736921611, 0.0, 0.0, 0.0, 0.25146743826064083, 0.0, 0.0, 0.011207888208677623, 0.08869787998517195, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0436070974131627, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05218480148875932, 0.0, 0.0, 0.0, 0.28668060177386556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16806127510824798, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1010038649276795, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24411150965510042, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3252831829639798, 0.0, 0.13696611429551586, 0.0, 0.08809099936621054, 0.0, 0.0, 0.0, 0.5834039633369968, 0.2573783852497724, 0.0, 0.34353142253711105, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8226681209013311, 0.43635481618924155, 0.0, 0.21222851603946533, 0.0, 0.6640973088333184, 0.0, 0.1716321278580885, 0.7312656094018032, 0.46618173092618587, 0.7638215749927391, 0.5281765059497923, 0.012259008816529623, 0.9144713112403178, 0.0, 0.0, 0.0, 0.0, 0.7953741970864907, 0.0, 0.0, 0.5850872742568098, 0.0, 0.0, 0.0, 0.7307808207285703, 0.0, 0.6199309753815017, 0.37291372178799176, 0.0, 0.0, 0.22356724549482568, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3221254739496775, 0.11781870844111127, 0.04460774210115785, 0.7133319766560491, 0.17115949268039302, 0.47397294440775345, 0.0, 0.10101149129956093, 0.11755718548933392, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.057090584067732354, 0.6447844891876003, 0.0, 0.0, 0.033829519264607456, 0.0, 0.1190337187097033, 0.30523421948716617, 0.0, 0.0, 0.0, 0.0, 0.6213057774307251, 0.842768502451811, 0.0, 0.3906048759812801, 0.7719833104847825, 0.41168363040706235, 0.1712307445975339, 0.0, 0.05992936357237096, 0.04890539039186202, 0.0, 0.0, 0.012380496199171342, 0.0, 0.2674194407541521, 0.0, 0.0014443996714777229, 0.0, 0.16387003646802972, 0.0, 0.0, 0.03594122433913304, 0.24159569520366886, 0.3747703785250739, 0.0, 0.054699683547073226, 0.0, 0.2873774296478163, 0.0, 0.0, 0.0, 0.14956132991470428, 0.0, 0.3888645476970781, 0.0, 0.0, 0.07542830239130101, 0.07080076609705026, 0.0, 0.0, 0.0, 0.0, 0.09852911382155678, 0.19548481725379033, 0.0, 0.0, 0.4350977845419587, 0.00557774494203208, 0.0, 0.09874991570897035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6034262468294622, 0.2713368059843151, 0.6497013107648627, 0.0, 0.8007029601130917, 0.0, 0.0, 0.0, 0.0, 0.01616961592030159, 0.0, 0.0, 0.0, 0.0, 0.2502876057454169, 0.49212467358206446, 0.2134864810859512, 0.1765833209669342, 0.0, 0.0, 0.0, 0.5401972297237567, 0.03318857704329382, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12219566143679067, 0.0, 0.04829725298673487, 0.0, 0.0, 0.0, 0.0, 0.0, 1.2356793204214815, 0.02263120223723859, 0.7256596881722004, 0.3263003275955702, 0.7654157892976137, 0.08145618747845872, 0.6387218661983024, 0.0, 0.5276231408957645, 0.8184646008092085, 0.0, 0.0, 0.0, 0.6276063069977658, 0.0, 0.6132063830791735, 0.006204111701464058, 0.18851722870999638, 0.375149258411478, 0.7816851453263511, 0.0, 0.587776400734769, 0.16472858834087942, 1.1016458158754883, 0.1459604994495737, 0.0, 0.0, 0.0, 0.037075917094288376, 0.0, 0.2815459504132854, 0.2536212145780303, 0.559150690052304, 0.48027689889482117, 0.0, 0.21566088189458643, 0.058080618345779114, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008439821738835876, 0.0, 0.36408664087256903, 0.0, 0.16156000319694572, 0.22471190561828702, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10732980278519261, 0.0, 0.0, 0.12403718878006863, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27901056312251676, 0.4801492052284416, 0.0, 0.0, 0.10787548181472921, 0.0, 0.0, 0.0, 0.15654727928350437, 0.0, 0.13104284110293218, 0.13666096202593647, 0.0, 0.0, 0.0, 0.07276575816635268, 0.0, 0.0, 0.0, 0.0, 0.6100450928472326, 0.0, 0.0, 0.0, 0.4238237484103886, 0.9941317988293357, 0.0, 0.0, 0.0, 0.0, 0.3355479309040112, 0.0, 0.0, 0.0, 0.0, 0.5350029094771915, 0.0, 0.008586567333100323, 1.1385513331902584, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1850917274468032, 0.0, 0.0, 0.06716099014107918, 1.1525784735402484, 0.0, 0.0, 0.036572260841491885, 0.0, 0.26119964749940466, 0.0, 0.0, 0.8459335464772313, 0.0, 0.0, 0.0, 0.0, 0.38438887602945404, 0.0, 0.0, 0.0, 0.0, 0.7204933523373307, 0.0, 0.0, 0.16060697060474402, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7112792652633437, 0.0, 0.935521710131386, 0.26536974069831215, 0.2935825392525973, 0.0, 0.0, 0.13719141489981915, 0.0, 0.10324823792839577, 0.0, 0.0, 0.0, 0.7256165269576538, 0.0, 0.48634443939716193, 0.0, 0.7955635800994527, 0.1927019559074891, 0.0, 0.0, 0.7058981437597585, 0.0, 0.5236895266156559, 0.0, 0.08128104063747256, 0.0, 0.0, 0.0, 0.5731212091054763, 0.0, 0.0, 0.0419831484236368, 0.11625926299639212, 0.0, 0.0, 0.08927614896350113, 0.217346651106267, 0.13128361330254273, 0.0, 0.07219885291348893, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4218130442222412, 0.0, 0.0, 0.0, 0.0, 0.1699892215644571, 0.0, 0.14639742856012483, 0.0, 0.0, 0.04200062557179813, 0.0, 0.0, 0.0, 0.31146013043119736, 0.0, 0.0, 0.0, 0.24384030368566587, 0.9260558729858251, 0.4296955936776912, 0.0, 0.0, 0.0, 0.73790516731229, 0.0, 0.0, 0.0, 0.16295488074282302, 0.26550524353964333, 0.0, 0.0, 0.0, 0.31144107126852844, 0.0, 0.485858439126786, 0.0, 0.6914937323909329, 0.0, 0.0, 0.0, 0.0, 0.3162528337434342, 0.0, 0.0, 0.0, 0.0, 1.5785215576611167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5161924043112004, 0.0, 0.0, 0.2568302026164213, 0.0, 0.3023415151909343, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5030068615133427, 0.0, 0.0, 0.0, 0.2741080207900236, 0.17120972783029909, 0.0, 0.0, 0.0, 0.0, 0.43685119598979993, 0.02712666041421048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2451085199735546, 0.0, 0.1576437689621315, 0.0, 0.0, 0.0, 0.6857487349281931, 0.05917052173435364, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6537568461306704, 0.0, 0.0, 0.5395442735782994, 0.05374109131453472, 0.0, 0.5917732326059808, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0008206716541601089, 0.19818937147807536, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.535053682582317, 0.0, 0.3513819151899916, 0.04333888481337121, 0.5747915094439204, 0.19540689526232177, 0.01899860952318465, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06595709778063945, 0.0, 0.08799217533170228, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03735300199555871, 0.19147688509049948, 0.0, 0.0, 0.0, 0.2914125830050192, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04799459307428656, 0.0, 0.16480769798416883, 0.0, 0.0, 0.0, 0.04840048223798238, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027156548216558336, 0.0, 0.047181418535814645, 0.0, 0.0, 0.0, 0.0, 0.35341760898781915, 0.5999981948389704, 0.024572219243726254, 0.0, 0.0, 0.08667332426569417, 0.4875808312926843, 0.06369622265280388, 0.0, 0.0, 0.0, 0.07133443039288194, 0.0, 0.0, 0.47894471690358564, 0.4793008357816607, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09658957720985865, 0.3016070141903076, 0.061398864173637636, 0.0, 0.0, 0.0, 0.07060273869520314, 0.35885594789448355, 0.0, 0.06446892769447045, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2497207855356876, 0.018509596151814908, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2926774045716465, 0.0, 0.013651758929637155, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03597721563910695, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07068090962707653, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03642909404914556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2749443358980585, 0.0, 0.0, 0.0, 0.0, 0.5096842842109697, 0.0, 0.0, 0.0, 0.028086239659725638, 0.7421154878137212, 0.0, 0.48123810905471315, 0.0, 0.3067030382176026, 0.6866619523091189, 0.0008295150087239233, 0.5016528107597976, 0.0, 0.08848804808651539, 1.3805931747462472, 0.0, 0.0, 1.04718815925786, 0.0, 0.0, 0.0, 0.42746548493362363, 0.0, 0.5817357352379013, 0.8450882691967682, 0.41665385220387907, 1.0915687546509307, 1.108964330219497, 0.0, 0.0, 0.0, 0.04066069749695377, 0.032733248007639555, 0.0, 0.0, 0.0, 0.07163077100441487, 0.6487063582321942, 0.0, 0.2995103130321618, 0.10283702762113958, 0.33384701220391544, 0.41258982728857174, 0.0, 0.5449670143554067, 0.0, 0.3217489265559145, 0.0, 0.0, 0.0, 0.2849908553676538, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01610871320352385, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2570528176713379, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2969662795916102, 0.0, 0.12312876081315716, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09075655657480966, 0.0, 0.3633373647258237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06617087939273322, 0.0, 0.0, 0.0, 0.00923594008230594, 0.17805875048711786, 0.22450411347361834, 0.0, 0.0, 0.09502241267674551, 0.0, 0.024589605765570302, 0.0, 0.1826423790293924, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039040067948029694, 0.0, 0.06664139722445166, 0.0, 0.04466639301844483, 0.0, 0.22428735118617732, 0.08284705163804024, 0.05227416022819031, 0.0, 0.19522102914015915, 0.8685310451611696, 0.0, 0.0, 0.0, 0.0, 0.8702352191794189, 0.0, 0.0, 0.15684527219370442, 0.0, 0.9243788213991903, 0.22207735215974056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.40507055158794986, 0.30718446392066534, 0.0, 0.46812545232304087, 0.0, 0.5780009938921578, 0.18279458657604497, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02896217836710352, 0.0, 0.0, 0.40712998428742797, 0.7449999863100496, 0.0, 0.49933620201845186, 0.0, 0.8168155413547457, 0.11457346224809285, 0.0, 0.0, 0.0, 0.15332753334456128, 0.11864376215788304, 0.0, 0.0, 0.04572095120049149, 0.0, 0.0, 0.31683394463012315, 0.5003078767139268, 0.14059206617597214, 0.19554784897288408, 0.0, 0.43178409369310355, 0.0, 0.07560627618395319, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16352131380333873, 0.0, 0.34449708571742155, 0.0, 0.4618370375665847, 0.0, 0.0, 0.41783342102021986, 0.0, 0.0, 0.0, 0.02165252564332241, 0.0, 0.0, 0.0, 0.47140143149032104, 0.39181349979635294, 0.11892452733738064, 0.0, 0.5433598750764469, 0.0, 0.0, 0.0, 0.07266999506080335, 0.0, 0.6735755940113932, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0041630866962457744, 0.2109647750127205, 0.0, 0.0, 0.0, 0.3715024914369038, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1845463840294256, 0.0, 0.6075440706050301, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.29101469974401967, 0.0, 0.0, 0.032739778024003775, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4875593392929034, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20371408580482323, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0439075644510851, 0.0, 0.5496827874572944, 0.40327797474842975, 0.37238046622077875, 0.0, 0.0, 0.17401376516444506, 0.0, 0.11278871866256229, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15394336215607962, 0.0, 0.0, 0.0]), 42), 42)),)
julia> @finch_code begin
        Ct .= 0
        for j = parallel(_)
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
quote
    Ct_lvl = ((ex.bodies[1]).bodies[1]).tns.bind.lvl
    Ct_lvl_2 = Ct_lvl.lvl
    Ct_lvl_3 = Ct_lvl_2.lvl
    Ct_lvl_2_val = Ct_lvl_2.lvl.val
    A_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl
    A_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.ptr
    A_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[1]
    A_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[1]).tns.bind.lvl.tbl[2]
    A_lvl_val = A_lvl.lvl.val
    B_lvl = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl
    B_lvl_ptr = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.ptr
    B_lvl_tbl1 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[1]
    B_lvl_tbl2 = (((ex.bodies[1]).bodies[2]).body.body.body.rhs.args[2]).tns.bind.lvl.tbl[2]
    B_lvl_val = B_lvl.lvl.val
    B_lvl.shape[1] == A_lvl.shape[2] || throw(DimensionMismatch("mismatched dimension limits ($(B_lvl.shape[1]) != $(A_lvl.shape[2]))"))
    @warn "Performance Warning: non-concordant traversal of A[i, k] (hint: most arrays prefer column major or first index fast, run in fast mode to ignore this warning)"
    result = nothing
    pos_stop = A_lvl.shape[1] * B_lvl.shape[2]
    Finch.resize_if_smaller!(Ct_lvl_2_val, pos_stop)
    Finch.fill_range!(Ct_lvl_2_val, 0.0, 1, pos_stop)
    val = Ct_lvl_2_val
    Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
    B_lvl_ptr = (Finch).moveto(B_lvl_ptr, CPU(Threads.nthreads()))
    B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
    B_lvl_tbl2 = (Finch).moveto(B_lvl_tbl2, CPU(Threads.nthreads()))
    B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
    A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
    A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
    A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
    A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
    Threads.@threads for i_4 = 1:Threads.nthreads()
            B_lvl_q = B_lvl_ptr[1]
            B_lvl_q_stop = B_lvl_ptr[1 + 1]
            if B_lvl_q < B_lvl_q_stop
                B_lvl_i_stop = B_lvl_tbl2[B_lvl_q_stop - 1]
            else
                B_lvl_i_stop = 0
            end
            phase_start_2 = max(1, 1 + fld(B_lvl.shape[2] * (i_4 + -1), Threads.nthreads()))
            phase_stop_2 = min(B_lvl.shape[2], B_lvl_i_stop, fld(B_lvl.shape[2] * i_4, Threads.nthreads()))
            if phase_stop_2 >= phase_start_2
                if B_lvl_tbl2[B_lvl_q] < phase_start_2
                    B_lvl_q = Finch.scansearch(B_lvl_tbl2, phase_start_2, B_lvl_q, B_lvl_q_stop - 1)
                end
                while true
                    B_lvl_i = B_lvl_tbl2[B_lvl_q]
                    B_lvl_q_step = B_lvl_q
                    if B_lvl_tbl2[B_lvl_q] == B_lvl_i
                        B_lvl_q_step = Finch.scansearch(B_lvl_tbl2, B_lvl_i + 1, B_lvl_q, B_lvl_q_stop - 1)
                    end
                    if B_lvl_i < phase_stop_2
                        Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + B_lvl_i
                        val_4 = Ct_lvl_2_val
                        Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                        A_lvl_ptr_3 = A_lvl_ptr
                        A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                        A_lvl_tbl1_3 = A_lvl_tbl1
                        A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                        A_lvl_tbl2_3 = A_lvl_tbl2
                        A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                        val_5 = A_lvl_val
                        A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
                        B_lvl_ptr_3 = B_lvl_ptr
                        B_lvl_tbl1_3 = B_lvl_tbl1
                        B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                        B_lvl_tbl2_3 = B_lvl_tbl2
                        val_6 = B_lvl_val
                        B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
                        Threads.@threads for i_10 = 1:Threads.nthreads()
                                phase_start_7 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_10), Threads.nthreads()))
                                phase_stop_8 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_10, Threads.nthreads()))
                                if phase_stop_8 >= phase_start_7
                                    for i_13 = phase_start_7:phase_stop_8
                                        Ct_lvl_2_q = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_13
                                        A_lvl_q = A_lvl_ptr[1]
                                        A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                        if A_lvl_q < A_lvl_q_stop
                                            A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                        else
                                            A_lvl_i_stop = 0
                                        end
                                        B_lvl_q_3 = B_lvl_q
                                        if B_lvl_q < B_lvl_q_step
                                            B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                        else
                                            B_lvl_i_stop_3 = 0
                                        end
                                        phase_stop_9 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                        if phase_stop_9 >= 1
                                            k = 1
                                            if A_lvl_tbl2[A_lvl_q] < 1
                                                A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                            end
                                            if B_lvl_tbl1[B_lvl_q] < 1
                                                B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                            end
                                            while k <= phase_stop_9
                                                A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                                A_lvl_q_step = A_lvl_q
                                                if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                    A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                                end
                                                B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                                phase_stop_10 = min(B_lvl_i_3, phase_stop_9, A_lvl_i)
                                                if A_lvl_i == phase_stop_10 && B_lvl_i_3 == phase_stop_10
                                                    B_lvl_2_val = B_lvl_val[B_lvl_q_3]
                                                    A_lvl_q_2 = A_lvl_q
                                                    if A_lvl_q < A_lvl_q_step
                                                        A_lvl_i_stop_2 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                    else
                                                        A_lvl_i_stop_2 = 0
                                                    end
                                                    phase_stop_11 = min(i_13, A_lvl_i_stop_2)
                                                    if phase_stop_11 >= i_13
                                                        if A_lvl_tbl1[A_lvl_q] < i_13
                                                            A_lvl_q_2 = Finch.scansearch(A_lvl_tbl1, i_13, A_lvl_q, A_lvl_q_step - 1)
                                                        end
                                                        while true
                                                            A_lvl_i_2 = A_lvl_tbl1[A_lvl_q_2]
                                                            if A_lvl_i_2 < phase_stop_11
                                                                A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                                Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                                A_lvl_q_2 += 1
                                                            else
                                                                phase_stop_13 = min(A_lvl_i_2, phase_stop_11)
                                                                if A_lvl_i_2 == phase_stop_13
                                                                    A_lvl_2_val = A_lvl_val[A_lvl_q_2]
                                                                    Ct_lvl_2_val[Ct_lvl_2_q] += B_lvl_2_val * A_lvl_2_val
                                                                    A_lvl_q_2 += 1
                                                                end
                                                                break
                                                            end
                                                        end
                                                    end
                                                    A_lvl_q = A_lvl_q_step
                                                    B_lvl_q_3 += 1
                                                elseif B_lvl_i_3 == phase_stop_10
                                                    B_lvl_q_3 += 1
                                                elseif A_lvl_i == phase_stop_10
                                                    A_lvl_q = A_lvl_q_step
                                                end
                                                k = phase_stop_10 + 1
                                            end
                                        end
                                    end
                                end
                            end
                        Ct_lvl_2_val = val_4
                        A_lvl_ptr = A_lvl_ptr_3
                        A_lvl_tbl1 = A_lvl_tbl1_3
                        A_lvl_tbl2 = A_lvl_tbl2_3
                        A_lvl_val = val_5
                        B_lvl_ptr = B_lvl_ptr_3
                        B_lvl_tbl1 = B_lvl_tbl1_3
                        B_lvl_tbl2 = B_lvl_tbl2_3
                        B_lvl_val = val_6
                        B_lvl_q = B_lvl_q_step
                    else
                        phase_stop_19 = min(B_lvl_i, phase_stop_2)
                        if B_lvl_i == phase_stop_19
                            Ct_lvl_q = (1 - 1) * B_lvl.shape[2] + phase_stop_19
                            val_7 = Ct_lvl_2_val
                            Ct_lvl_2_val = (Finch).moveto(Ct_lvl_2_val, CPU(Threads.nthreads()))
                            A_lvl_ptr_4 = A_lvl_ptr
                            A_lvl_ptr = (Finch).moveto(A_lvl_ptr, CPU(Threads.nthreads()))
                            A_lvl_tbl1_4 = A_lvl_tbl1
                            A_lvl_tbl1 = (Finch).moveto(A_lvl_tbl1, CPU(Threads.nthreads()))
                            A_lvl_tbl2_4 = A_lvl_tbl2
                            A_lvl_tbl2 = (Finch).moveto(A_lvl_tbl2, CPU(Threads.nthreads()))
                            val_8 = A_lvl_val
                            A_lvl_val = (Finch).moveto(A_lvl_val, CPU(Threads.nthreads()))
                            B_lvl_ptr_4 = B_lvl_ptr
                            B_lvl_tbl1_4 = B_lvl_tbl1
                            B_lvl_tbl1 = (Finch).moveto(B_lvl_tbl1, CPU(Threads.nthreads()))
                            B_lvl_tbl2_4 = B_lvl_tbl2
                            val_9 = B_lvl_val
                            B_lvl_val = (Finch).moveto(B_lvl_val, CPU(Threads.nthreads()))
                            Threads.@threads for i_20 = 1:Threads.nthreads()
                                    phase_start_22 = max(1, 1 + fld(A_lvl.shape[1] * (-1 + i_20), Threads.nthreads()))
                                    phase_stop_24 = min(A_lvl.shape[1], fld(A_lvl.shape[1] * i_20, Threads.nthreads()))
                                    if phase_stop_24 >= phase_start_22
                                        for i_23 = phase_start_22:phase_stop_24
                                            Ct_lvl_2_q_2 = (Ct_lvl_q - 1) * A_lvl.shape[1] + i_23
                                            A_lvl_q = A_lvl_ptr[1]
                                            A_lvl_q_stop = A_lvl_ptr[1 + 1]
                                            if A_lvl_q < A_lvl_q_stop
                                                A_lvl_i_stop = A_lvl_tbl2[A_lvl_q_stop - 1]
                                            else
                                                A_lvl_i_stop = 0
                                            end
                                            B_lvl_q_3 = B_lvl_q
                                            if B_lvl_q < B_lvl_q_step
                                                B_lvl_i_stop_3 = B_lvl_tbl1[B_lvl_q_step - 1]
                                            else
                                                B_lvl_i_stop_3 = 0
                                            end
                                            phase_stop_25 = min(B_lvl.shape[1], A_lvl_i_stop, B_lvl_i_stop_3)
                                            if phase_stop_25 >= 1
                                                k = 1
                                                if A_lvl_tbl2[A_lvl_q] < 1
                                                    A_lvl_q = Finch.scansearch(A_lvl_tbl2, 1, A_lvl_q, A_lvl_q_stop - 1)
                                                end
                                                if B_lvl_tbl1[B_lvl_q] < 1
                                                    B_lvl_q_3 = Finch.scansearch(B_lvl_tbl1, 1, B_lvl_q, B_lvl_q_step - 1)
                                                end
                                                while k <= phase_stop_25
                                                    A_lvl_i = A_lvl_tbl2[A_lvl_q]
                                                    A_lvl_q_step = A_lvl_q
                                                    if A_lvl_tbl2[A_lvl_q] == A_lvl_i
                                                        A_lvl_q_step = Finch.scansearch(A_lvl_tbl2, A_lvl_i + 1, A_lvl_q, A_lvl_q_stop - 1)
                                                    end
                                                    B_lvl_i_3 = B_lvl_tbl1[B_lvl_q_3]
                                                    phase_stop_26 = min(B_lvl_i_3, A_lvl_i, phase_stop_25)
                                                    if A_lvl_i == phase_stop_26 && B_lvl_i_3 == phase_stop_26
                                                        B_lvl_2_val_3 = B_lvl_val[B_lvl_q_3]
                                                        A_lvl_q_4 = A_lvl_q
                                                        if A_lvl_q < A_lvl_q_step
                                                            A_lvl_i_stop_4 = A_lvl_tbl1[A_lvl_q_step - 1]
                                                        else
                                                            A_lvl_i_stop_4 = 0
                                                        end
                                                        phase_stop_27 = min(i_23, A_lvl_i_stop_4)
                                                        if phase_stop_27 >= i_23
                                                            if A_lvl_tbl1[A_lvl_q] < i_23
                                                                A_lvl_q_4 = Finch.scansearch(A_lvl_tbl1, i_23, A_lvl_q, A_lvl_q_step - 1)
                                                            end
                                                            while true
                                                                A_lvl_i_4 = A_lvl_tbl1[A_lvl_q_4]
                                                                if A_lvl_i_4 < phase_stop_27
                                                                    A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                    Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                                    A_lvl_q_4 += 1
                                                                else
                                                                    phase_stop_29 = min(A_lvl_i_4, phase_stop_27)
                                                                    if A_lvl_i_4 == phase_stop_29
                                                                        A_lvl_2_val_2 = A_lvl_val[A_lvl_q_4]
                                                                        Ct_lvl_2_val[Ct_lvl_2_q_2] += B_lvl_2_val_3 * A_lvl_2_val_2
                                                                        A_lvl_q_4 += 1
                                                                    end
                                                                    break
                                                                end
                                                            end
                                                        end
                                                        A_lvl_q = A_lvl_q_step
                                                        B_lvl_q_3 += 1
                                                    elseif B_lvl_i_3 == phase_stop_26
                                                        B_lvl_q_3 += 1
                                                    elseif A_lvl_i == phase_stop_26
                                                        A_lvl_q = A_lvl_q_step
                                                    end
                                                    k = phase_stop_26 + 1
                                                end
                                            end
                                        end
                                    end
                                end
                            Ct_lvl_2_val = val_7
                            A_lvl_ptr = A_lvl_ptr_4
                            A_lvl_tbl1 = A_lvl_tbl1_4
                            A_lvl_tbl2 = A_lvl_tbl2_4
                            A_lvl_val = val_8
                            B_lvl_ptr = B_lvl_ptr_4
                            B_lvl_tbl1 = B_lvl_tbl1_4
                            B_lvl_tbl2 = B_lvl_tbl2_4
                            B_lvl_val = val_9
                            B_lvl_q = B_lvl_q_step
                        end
                        break
                    end
                end
            end
        end
    resize!(val, A_lvl.shape[1] * B_lvl.shape[2])
    result = (Ct = Tensor((DenseLevel){Int64}((DenseLevel){Int64}(Ct_lvl_3, A_lvl.shape[1]), B_lvl.shape[2])),)
    result
end
julia> @finch begin
        Ct .= 0
        for j = parallel(_)
            for i = parallel(_)
                for k = _
                    Ct[i, j] += A[i, k] * B[k, j]
                end
            end
        end
    end
(Ct = Tensor(Dense{Int64}(Dense{Int64}(Element{0.0, Float64, Int64}([0.24622058278177836, 0.0, 0.0, 0.1261597862603059, 0.07342085928314178, 0.12871077894157168, 0.0, 0.0, 0.8034179790657067, 0.0, 0.0, 0.056025142012607894, 1.0596276779513034, 0.0, 0.12141212330168466, 0.0, 0.5574009533856183, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3327944495595785, 0.0, 0.8694342868598712, 0.0, 0.7416396915765561, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7569986463219165, 0.05683836078786318, 0.0, 0.0, 0.8725529080345216, 0.0, 0.2047777547094641, 0.5614660149749067, 0.0, 0.0, 0.36223551246779223, 0.0, 0.0, 0.0, 0.22107392894943306, 0.0, 0.0, 0.0, 0.3259321271119929, 0.5055955427859341, 0.27515457195781157, 0.4169876847813729, 0.018631329176858533, 0.3876953885710935, 0.0, 0.021531556327917045, 0.0, 0.0, 0.28223766305057085, 0.48287552686851865, 0.0, 0.0, 0.10175887870070799, 0.0, 0.0, 0.0, 0.06032870137556734, 0.018726053338837593, 0.045366945649236726, 0.0, 0.0, 0.0, 0.5790914418316278, 0.6815353846022115, 0.0, 0.1332216207413034, 0.0, 0.04017082185996856, 0.0, 0.0, 0.0, 0.0, 0.03484741576880692, 0.893850322287328, 0.4479341395605485, 0.5025126461241762, 0.1987665376735214, 0.31067608319719825, 0.0, 0.0, 0.0, 0.46442597522784795, 0.0, 0.0, 0.4431570212945692, 0.0, 0.0, 0.5121406140989466, 0.0, 0.49477960883115907, 0.0, 0.0, 0.0, 0.0, 0.47025922462023484, 0.3616621631765011, 0.5907253850051586, 0.0, 0.0, 0.44541009068477694, 0.03445733103321199, 0.0, 0.02309502433018302, 0.0, 0.03777890472311954, 0.16345317623186756, 0.16813336048226513, 0.0, 0.0, 0.09235605661534649, 0.0, 0.0, 0.01896246145159963, 0.02753978144349941, 0.0, 0.0, 0.0, 0.30135789429977294, 0.6764584765695312, 0.0, 0.0, 0.3151319553463997, 0.0, 0.04554105431298512, 0.0, 0.08378021558986462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09849622821166114, 0.0, 0.25488776315346307, 0.0, 0.2781851808227525, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2839462446541717, 0.00854309647598918, 0.0, 0.0, 0.3272900456325508, 0.10375527349591217, 0.8492672297194694, 0.0, 0.5876718077641173, 0.03093892983747342, 0.0, 0.0, 0.0, 0.3385535490482335, 0.0, 0.0, 0.0, 0.29218416128587404, 0.0, 0.6966394562456512, 0.0, 0.0, 0.0, 0.522006486105503, 0.0, 0.0, 0.0, 0.11065330711314218, 0.0, 0.23311787887979893, 0.0, 0.31252070060750486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06134998798063221, 0.0, 0.0, 0.3657519298489218, 0.31899283438369924, 0.0, 0.0, 0.0, 0.3676864240590821, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0010478172394749591, 0.5287716819761857, 0.0, 0.3903081303690744, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0017717866111426558, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5892681933608603, 0.002977075804572329, 0.0, 0.0, 0.0, 0.0, 0.43072916849878135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.045384364903179854, 0.0, 0.0, 0.0018618199412216315, 0.0, 0.17111256306326456, 0.2672837889083413, 0.0, 0.0, 0.19576072012760673, 0.5948736598005219, 0.08413627091493711, 0.7573648130796338, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33101786250771764, 0.16289357736810944, 0.16515314938780928, 0.0, 0.0, 0.0, 0.0, 0.0, 0.662932866371622, 0.6905568008213746, 0.0, 0.0, 0.0, 0.0, 0.4845748090935882, 0.061923683856559844, 0.0, 0.028719251839850298, 0.0, 0.02009827057946138, 0.0, 0.0, 0.0, 0.11124419756806017, 0.4147463454400596, 0.0, 0.0, 0.34783853396429565, 0.0, 0.09260170356894369, 0.0, 0.0, 0.0, 1.1401462738363544, 0.915922599074042, 0.0, 0.0, 0.0, 0.0, 0.6274522680730893, 0.0, 0.3139030754242789, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3751134192842195, 0.0, 1.257728265907189, 0.0, 0.35074493350028346, 0.268137845429166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016899841379555174, 0.7155448975358032, 0.1680074523773585, 0.0, 0.0, 0.30581125296894957, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9366277987154731, 0.0, 0.0, 0.0, 0.0, 0.07522986257761975, 0.06328419096078892, 0.0, 0.02479071082883737, 0.0, 0.7915629916935427, 0.031935353301522654, 0.0, 0.0, 0.070458258165472, 0.0, 0.0, 0.0, 0.0, 0.052737602183104026, 0.0, 0.013647273418699125, 0.3169788666986562, 0.17235734856254364, 0.0, 0.0, 0.0, 0.0661528775144355, 0.0, 0.027308490724219276, 0.0, 0.0, 0.0, 0.7971915175033586, 0.0, 0.5343175730821937, 0.0, 0.8740381649643706, 0.04598025590229294, 0.029012248678093225, 0.0, 0.0, 0.11141095490761138, 0.0, 0.0, 0.13001430753376275, 0.0, 0.0, 0.0, 0.22549966134722088, 0.0, 0.0, 0.0, 0.3726692665495664, 0.3774382964295704, 0.20411971872540274, 0.06057654493015839, 0.0, 0.5744310538348768, 0.0, 0.0, 0.6232315714035893, 0.0, 0.5072896743166587, 0.0, 0.005553340050755054, 0.5689868928162961, 0.06836125145830099, 0.0, 0.0, 0.49734376521104756, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27904969776436467, 0.0, 0.7814242677772039, 0.5117851675292832, 0.0, 0.0, 0.04886194301383263, 0.0, 0.3067365939854962, 0.0, 0.0, 0.4115088709036164, 1.405104579225751, 0.42840174089528493, 0.48760195358213543, 0.0, 0.0, 0.663844265848651, 0.0, 0.38778436442248837, 0.7776635401136125, 0.15032072106029123, 0.18953081567758903, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7044650329571657, 0.7235946500790119, 0.12987194286479317, 0.19647866132545186, 0.45582545423087456, 0.0, 0.0, 0.0, 0.032958397990191235, 0.0, 0.0, 0.7570462349991403, 0.0941136312544236, 0.0, 0.1276645560961348, 0.0, 0.0, 0.0, 0.755417985546979, 0.8458204729185679, 0.12543050025210123, 0.006393660363144921, 0.0, 0.0, 0.28926738179087036, 0.0, 0.0, 0.4731345211962404, 0.0, 0.7203970460167203, 0.0, 0.0, 0.0, 0.4891311717243447, 0.0031333060836164625, 0.003950606766695812, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8250852706661534, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1940012409565178, 0.0, 0.7255195358366966, 0.0, 0.4862794311033925, 0.0, 0.7981182971725221, 0.0, 0.26385403496014914, 0.0, 0.5139863704027979, 0.35381636124647037, 0.06016159688024159, 0.0, 0.0, 0.0, 0.07675121814682248, 0.0, 0.0, 0.12553662505708466, 0.0, 0.301350309711505, 0.0, 0.0, 0.0, 0.12978101101826656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2180668401971053, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09272141980768678, 0.0, 0.3034931056908601, 0.0, 0.203416238280828, 0.0, 0.33274884560755447, 0.0, 0.12655518438173288, 0.0, 0.13637583261219158, 0.0, 0.0, 0.0, 0.0, 0.020115042222633606, 0.0, 0.11478942012015081, 0.0, 0.044967175485766994, 0.0, 0.27969782401606164, 0.1383242818414881, 0.0, 0.0, 0.18685369035983804, 0.0, 0.06810955828519266, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17579267922314348, 0.0, 0.0, 0.11999285025017485, 0.0, 0.0, 0.14209169595972287, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11399414163060874, 0.019825113051252268, 0.0, 0.049971986914216976, 0.07235420154739257, 0.0, 0.0, 0.08331368376199759, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07280640355864025, 0.0, 0.0, 0.35152520960031586, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13676901662991092, 0.0, 0.0, 0.0, 0.015204774707799058, 0.0, 0.0, 0.0, 0.0, 0.5304031136005922, 0.0, 0.0, 0.0, 0.32819195551919517, 0.1622165373359131, 0.0, 0.0, 0.0, 0.1321652778478459, 0.0, 0.0, 0.2161735452720934, 0.0, 0.27952564740678887, 0.0, 0.0, 0.0, 0.22348236021207268, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375510190195534, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2500091160813631, 0.0, 0.2815132558946093, 0.0, 0.18868424509978618, 0.0, 0.30865021038590384, 0.0, 0.341236683480638, 0.0, 0.23483861551802387, 0.0, 0.0, 0.0, 0.12117745385937667, 0.0, 0.0, 0.3973467888918919, 0.0, 0.2920101397985984, 0.0, 0.12988448302264086, 0.0, 0.3517844506342411, 0.0, 0.0, 0.0, 0.6519761543079575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4428070420734244, 0.0, 0.3964317892338393, 0.0, 0.0, 0.0, 0.3236724995283946, 0.0, 0.0567229736921611, 0.0, 0.0, 0.0, 0.25146743826064083, 0.0, 0.0, 0.011207888208677623, 0.08869787998517195, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0436070974131627, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05218480148875932, 0.0, 0.0, 0.0, 0.28668060177386556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16806127510824798, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1010038649276795, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24411150965510042, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3252831829639798, 0.0, 0.13696611429551586, 0.0, 0.08809099936621054, 0.0, 0.0, 0.0, 0.5834039633369968, 0.2573783852497724, 0.0, 0.34353142253711105, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8226681209013311, 0.43635481618924155, 0.0, 0.21222851603946533, 0.0, 0.6640973088333184, 0.0, 0.1716321278580885, 0.7312656094018032, 0.46618173092618587, 0.7638215749927391, 0.5281765059497923, 0.012259008816529623, 0.9144713112403178, 0.0, 0.0, 0.0, 0.0, 0.7953741970864907, 0.0, 0.0, 0.5850872742568098, 0.0, 0.0, 0.0, 0.7307808207285703, 0.0, 0.6199309753815017, 0.37291372178799176, 0.0, 0.0, 0.22356724549482568, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3221254739496775, 0.11781870844111127, 0.04460774210115785, 0.7133319766560491, 0.17115949268039302, 0.47397294440775345, 0.0, 0.10101149129956093, 0.11755718548933392, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.057090584067732354, 0.6447844891876003, 0.0, 0.0, 0.033829519264607456, 0.0, 0.1190337187097033, 0.30523421948716617, 0.0, 0.0, 0.0, 0.0, 0.6213057774307251, 0.842768502451811, 0.0, 0.3906048759812801, 0.7719833104847825, 0.41168363040706235, 0.1712307445975339, 0.0, 0.05992936357237096, 0.04890539039186202, 0.0, 0.0, 0.012380496199171342, 0.0, 0.2674194407541521, 0.0, 0.0014443996714777229, 0.0, 0.16387003646802972, 0.0, 0.0, 0.03594122433913304, 0.24159569520366886, 0.3747703785250739, 0.0, 0.054699683547073226, 0.0, 0.2873774296478163, 0.0, 0.0, 0.0, 0.14956132991470428, 0.0, 0.3888645476970781, 0.0, 0.0, 0.07542830239130101, 0.07080076609705026, 0.0, 0.0, 0.0, 0.0, 0.09852911382155678, 0.19548481725379033, 0.0, 0.0, 0.4350977845419587, 0.00557774494203208, 0.0, 0.09874991570897035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6034262468294622, 0.2713368059843151, 0.6497013107648627, 0.0, 0.8007029601130917, 0.0, 0.0, 0.0, 0.0, 0.01616961592030159, 0.0, 0.0, 0.0, 0.0, 0.2502876057454169, 0.49212467358206446, 0.2134864810859512, 0.1765833209669342, 0.0, 0.0, 0.0, 0.5401972297237567, 0.03318857704329382, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12219566143679067, 0.0, 0.04829725298673487, 0.0, 0.0, 0.0, 0.0, 0.0, 1.2356793204214815, 0.02263120223723859, 0.7256596881722004, 0.3263003275955702, 0.7654157892976137, 0.08145618747845872, 0.6387218661983024, 0.0, 0.5276231408957645, 0.8184646008092085, 0.0, 0.0, 0.0, 0.6276063069977658, 0.0, 0.6132063830791735, 0.006204111701464058, 0.18851722870999638, 0.375149258411478, 0.7816851453263511, 0.0, 0.587776400734769, 0.16472858834087942, 1.1016458158754883, 0.1459604994495737, 0.0, 0.0, 0.0, 0.037075917094288376, 0.0, 0.2815459504132854, 0.2536212145780303, 0.559150690052304, 0.48027689889482117, 0.0, 0.21566088189458643, 0.058080618345779114, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008439821738835876, 0.0, 0.36408664087256903, 0.0, 0.16156000319694572, 0.22471190561828702, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10732980278519261, 0.0, 0.0, 0.12403718878006863, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27901056312251676, 0.4801492052284416, 0.0, 0.0, 0.10787548181472921, 0.0, 0.0, 0.0, 0.15654727928350437, 0.0, 0.13104284110293218, 0.13666096202593647, 0.0, 0.0, 0.0, 0.07276575816635268, 0.0, 0.0, 0.0, 0.0, 0.6100450928472326, 0.0, 0.0, 0.0, 0.4238237484103886, 0.9941317988293357, 0.0, 0.0, 0.0, 0.0, 0.3355479309040112, 0.0, 0.0, 0.0, 0.0, 0.5350029094771915, 0.0, 0.008586567333100323, 1.1385513331902584, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1850917274468032, 0.0, 0.0, 0.06716099014107918, 1.1525784735402484, 0.0, 0.0, 0.036572260841491885, 0.0, 0.26119964749940466, 0.0, 0.0, 0.8459335464772313, 0.0, 0.0, 0.0, 0.0, 0.38438887602945404, 0.0, 0.0, 0.0, 0.0, 0.7204933523373307, 0.0, 0.0, 0.16060697060474402, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7112792652633437, 0.0, 0.935521710131386, 0.26536974069831215, 0.2935825392525973, 0.0, 0.0, 0.13719141489981915, 0.0, 0.10324823792839577, 0.0, 0.0, 0.0, 0.7256165269576538, 0.0, 0.48634443939716193, 0.0, 0.7955635800994527, 0.1927019559074891, 0.0, 0.0, 0.7058981437597585, 0.0, 0.5236895266156559, 0.0, 0.08128104063747256, 0.0, 0.0, 0.0, 0.5731212091054763, 0.0, 0.0, 0.0419831484236368, 0.11625926299639212, 0.0, 0.0, 0.08927614896350113, 0.217346651106267, 0.13128361330254273, 0.0, 0.07219885291348893, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4218130442222412, 0.0, 0.0, 0.0, 0.0, 0.1699892215644571, 0.0, 0.14639742856012483, 0.0, 0.0, 0.04200062557179813, 0.0, 0.0, 0.0, 0.31146013043119736, 0.0, 0.0, 0.0, 0.24384030368566587, 0.9260558729858251, 0.4296955936776912, 0.0, 0.0, 0.0, 0.73790516731229, 0.0, 0.0, 0.0, 0.16295488074282302, 0.26550524353964333, 0.0, 0.0, 0.0, 0.31144107126852844, 0.0, 0.485858439126786, 0.0, 0.6914937323909329, 0.0, 0.0, 0.0, 0.0, 0.3162528337434342, 0.0, 0.0, 0.0, 0.0, 1.5785215576611167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5161924043112004, 0.0, 0.0, 0.2568302026164213, 0.0, 0.3023415151909343, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5030068615133427, 0.0, 0.0, 0.0, 0.2741080207900236, 0.17120972783029909, 0.0, 0.0, 0.0, 0.0, 0.43685119598979993, 0.02712666041421048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2451085199735546, 0.0, 0.1576437689621315, 0.0, 0.0, 0.0, 0.6857487349281931, 0.05917052173435364, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6537568461306704, 0.0, 0.0, 0.5395442735782994, 0.05374109131453472, 0.0, 0.5917732326059808, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0008206716541601089, 0.19818937147807536, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.535053682582317, 0.0, 0.3513819151899916, 0.04333888481337121, 0.5747915094439204, 0.19540689526232177, 0.01899860952318465, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06595709778063945, 0.0, 0.08799217533170228, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03735300199555871, 0.19147688509049948, 0.0, 0.0, 0.0, 0.2914125830050192, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04799459307428656, 0.0, 0.16480769798416883, 0.0, 0.0, 0.0, 0.04840048223798238, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027156548216558336, 0.0, 0.047181418535814645, 0.0, 0.0, 0.0, 0.0, 0.35341760898781915, 0.5999981948389704, 0.024572219243726254, 0.0, 0.0, 0.08667332426569417, 0.4875808312926843, 0.06369622265280388, 0.0, 0.0, 0.0, 0.07133443039288194, 0.0, 0.0, 0.47894471690358564, 0.4793008357816607, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09658957720985865, 0.3016070141903076, 0.061398864173637636, 0.0, 0.0, 0.0, 0.07060273869520314, 0.35885594789448355, 0.0, 0.06446892769447045, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2497207855356876, 0.018509596151814908, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2926774045716465, 0.0, 0.013651758929637155, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03597721563910695, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07068090962707653, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03642909404914556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2749443358980585, 0.0, 0.0, 0.0, 0.0, 0.5096842842109697, 0.0, 0.0, 0.0, 0.028086239659725638, 0.7421154878137212, 0.0, 0.48123810905471315, 0.0, 0.3067030382176026, 0.6866619523091189, 0.0008295150087239233, 0.5016528107597976, 0.0, 0.08848804808651539, 1.3805931747462472, 0.0, 0.0, 1.04718815925786, 0.0, 0.0, 0.0, 0.42746548493362363, 0.0, 0.5817357352379013, 0.8450882691967682, 0.41665385220387907, 1.0915687546509307, 1.108964330219497, 0.0, 0.0, 0.0, 0.04066069749695377, 0.032733248007639555, 0.0, 0.0, 0.0, 0.07163077100441487, 0.6487063582321942, 0.0, 0.2995103130321618, 0.10283702762113958, 0.33384701220391544, 0.41258982728857174, 0.0, 0.5449670143554067, 0.0, 0.3217489265559145, 0.0, 0.0, 0.0, 0.2849908553676538, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01610871320352385, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2570528176713379, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2969662795916102, 0.0, 0.12312876081315716, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09075655657480966, 0.0, 0.3633373647258237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06617087939273322, 0.0, 0.0, 0.0, 0.00923594008230594, 0.17805875048711786, 0.22450411347361834, 0.0, 0.0, 0.09502241267674551, 0.0, 0.024589605765570302, 0.0, 0.1826423790293924, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039040067948029694, 0.0, 0.06664139722445166, 0.0, 0.04466639301844483, 0.0, 0.22428735118617732, 0.08284705163804024, 0.05227416022819031, 0.0, 0.19522102914015915, 0.8685310451611696, 0.0, 0.0, 0.0, 0.0, 0.8702352191794189, 0.0, 0.0, 0.15684527219370442, 0.0, 0.9243788213991903, 0.22207735215974056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.40507055158794986, 0.30718446392066534, 0.0, 0.46812545232304087, 0.0, 0.5780009938921578, 0.18279458657604497, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02896217836710352, 0.0, 0.0, 0.40712998428742797, 0.7449999863100496, 0.0, 0.49933620201845186, 0.0, 0.8168155413547457, 0.11457346224809285, 0.0, 0.0, 0.0, 0.15332753334456128, 0.11864376215788304, 0.0, 0.0, 0.04572095120049149, 0.0, 0.0, 0.31683394463012315, 0.5003078767139268, 0.14059206617597214, 0.19554784897288408, 0.0, 0.43178409369310355, 0.0, 0.07560627618395319, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16352131380333873, 0.0, 0.34449708571742155, 0.0, 0.4618370375665847, 0.0, 0.0, 0.41783342102021986, 0.0, 0.0, 0.0, 0.02165252564332241, 0.0, 0.0, 0.0, 0.47140143149032104, 0.39181349979635294, 0.11892452733738064, 0.0, 0.5433598750764469, 0.0, 0.0, 0.0, 0.07266999506080335, 0.0, 0.6735755940113932, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0041630866962457744, 0.2109647750127205, 0.0, 0.0, 0.0, 0.3715024914369038, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1845463840294256, 0.0, 0.6075440706050301, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.29101469974401967, 0.0, 0.0, 0.032739778024003775, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4875593392929034, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20371408580482323, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0439075644510851, 0.0, 0.5496827874572944, 0.40327797474842975, 0.37238046622077875, 0.0, 0.0, 0.17401376516444506, 0.0, 0.11278871866256229, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15394336215607962, 0.0, 0.0, 0.0]), 42), 42)),)

